---
draft: false
title: "arXiv @ 2024.03.23"
date: 2024-03-23
author: "akitenkrad"
description: ""
tags: ["arXiv", "Published:2024"]
menu:
  sidebar:
    name: "arXiv @ 2024.03.23"
    identifier: arxiv_20240323
    parent: 202403_arxiv
    weight: 10
math: true
---

<figure style="border:none; width:100%; display:flex; justify-content: center">
    <iframe src="pie.html" width=900 height=620 style="border:none"></iframe>
</figure>


## Primary Categories

- [cond-mat.dis-nn (1)](#cond-matdis-nn-1)
- [cs.AI (3)](#csai-3)
- [cs.AR (1)](#csar-1)
- [cs.CL (37)](#cscl-37)
- [cs.CR (4)](#cscr-4)
- [cs.CV (84)](#cscv-84)
- [cs.CY (2)](#cscy-2)
- [cs.DB (1)](#csdb-1)
- [cs.DC (1)](#csdc-1)
- [cs.DS (2)](#csds-2)
- [cs.HC (4)](#cshc-4)
- [cs.IR (3)](#csir-3)
- [cs.LG (27)](#cslg-27)
- [cs.NE (6)](#csne-6)
- [cs.NI (1)](#csni-1)
- [cs.RO (16)](#csro-16)
- [cs.SD (5)](#cssd-5)
- [cs.SE (2)](#csse-2)
- [cs.SI (4)](#cssi-4)
- [eess.AS (2)](#eessas-2)
- [eess.IV (7)](#eessiv-7)
- [eess.SY (12)](#eesssy-12)
- [hep-ex (1)](#hep-ex-1)
- [math.NA (6)](#mathna-6)
- [math.OC (1)](#mathoc-1)
- [q-bio.BM (1)](#q-biobm-1)
- [quant-ph (3)](#quant-ph-3)
- [stat.ML (4)](#statml-4)

## Keywords

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>keyword</th>
      <th>cs.CL</th>
      <th>cs.CV</th>
      <th>cs.LG</th>
      <th>cs.RO</th>
      <th>eess.SY</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Active Learning</td>
      <td>1</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Adversarial Attack</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Adversarial Learning</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Anomaly Detection</td>
      <td></td>
      <td>3</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Autoencoder</td>
      <td></td>
      <td>1</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Automatic Speech Recognition</td>
      <td>4</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>BART</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>BERT</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>BERTScore</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Benchmarking</td>
      <td>9</td>
      <td>31</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Black Box</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Chain-of-thought Prompt</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>ChatGPT</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Chatbot</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Clustering</td>
      <td></td>
      <td>3</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Common-sense Reasoning</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Continual Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Continuous Time</td>
      <td></td>
      <td></td>
      <td></td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Contrastive Learning</td>
      <td></td>
      <td>4</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>ControlNet</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Convolution</td>
      <td></td>
      <td>4</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Convolutional Neural Network</td>
      <td></td>
      <td>8</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Counter-factual</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Data Augmentation</td>
      <td>1</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Dense Retrieval</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Diffusion Model</td>
      <td></td>
      <td>7</td>
      <td>3</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Discrete Time</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td>2</td>
    </tr>
    <tr>
      <td>Distribution Shift</td>
      <td></td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Domain Adaptation</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Fairness</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Federated Learning</td>
      <td></td>
      <td>1</td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Few-shot</td>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Fine-tuning</td>
      <td>13</td>
      <td>6</td>
      <td>3</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Foundation Model</td>
      <td></td>
      <td>4</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>GPT</td>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>GPT-4</td>
      <td>3</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>GPT-4 turbo</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Generative AI</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Generative Adversarial Network</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Geometry</td>
      <td></td>
      <td>5</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Graph</td>
      <td>1</td>
      <td>4</td>
      <td>5</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Graph Attention Networks</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Graph Neural Network</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Grounding</td>
      <td>1</td>
      <td>1</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Image2text</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>In-context Learning</td>
      <td>3</td>
      <td></td>
      <td></td>
      <td></td>
      <td>2</td>
    </tr>
    <tr>
      <td>Information Retrieval</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Instruction Following</td>
      <td>3</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Instruction Tuning</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Knowledge Distillation</td>
      <td>5</td>
      <td>4</td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Knowledge Graph</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Knowledge Transfer</td>
      <td></td>
      <td>3</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>LLaMA</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Large Language Model</td>
      <td>46</td>
      <td>8</td>
      <td>8</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Low-Resource</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Machine Unlearning</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Markov Decision Process</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Mathematical Reasoning</td>
      <td>1</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Message-Passing</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Meta Learning</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Model Compression</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Multi-modal</td>
      <td>11</td>
      <td>11</td>
      <td></td>
      <td>4</td>
      <td></td>
    </tr>
    <tr>
      <td>Multiple Instance Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Mutual Information</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Named Entity Recognition</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Natural Language Generation</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Natural Language Inference</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Neural Machine Translation</td>
      <td>4</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Node Classification</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Object Detection</td>
      <td></td>
      <td>3</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Open-Domain Question Answering</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Out-of-distribution</td>
      <td></td>
      <td>1</td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Perplexity</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Pre-trained Language Model</td>
      <td>1</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Probabilistic Model</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Prompt</td>
      <td>8</td>
      <td>10</td>
      <td>4</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Prompt Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Pruning</td>
      <td></td>
      <td>1</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Quantization</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Question Answering</td>
      <td>8</td>
      <td>3</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Reasoning</td>
      <td>6</td>
      <td>5</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Recommendation</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Reinforcement Learning</td>
      <td>2</td>
      <td></td>
      <td>6</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Reinforcement Learning from Human Feedback</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Representation Learning</td>
      <td></td>
      <td>2</td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Retrieval-Augmented Generation</td>
      <td>5</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Scaling Law</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Self-Attention</td>
      <td></td>
      <td>3</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Self-supervised Learning</td>
      <td></td>
      <td>8</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Simulation</td>
      <td></td>
      <td>2</td>
      <td>2</td>
      <td>5</td>
      <td>6</td>
    </tr>
    <tr>
      <td>Simulator</td>
      <td></td>
      <td>2</td>
      <td>2</td>
      <td>5</td>
      <td>6</td>
    </tr>
    <tr>
      <td>Style Transfer</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Summarization</td>
      <td>2</td>
      <td></td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Supervised Learning</td>
      <td>3</td>
      <td>9</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Text Classification</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Text Embedding</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Text2image</td>
      <td></td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Transfer Learning</td>
      <td></td>
      <td>1</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Transformer</td>
      <td>4</td>
      <td>14</td>
      <td>2</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Unsupervised Learning</td>
      <td>2</td>
      <td>9</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Vision Transformer</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Vision-and-Language</td>
      <td>1</td>
      <td>5</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Visual Question Answering</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Weakly Supervised Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Weakly-supervised Learning</td>
      <td>1</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Word Embedding</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Zero-shot</td>
      <td>2</td>
      <td>12</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Zero-shot Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>human-in-the-loop</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
  </tbody>
</table>

<script>
$(function() {
  $("table").addClass("keyword-table table-bordered border-success");
  $("table thead").addClass("sticky-top");
  $("table tbody td").css("text-align", "");
});
</script>


## cs.CL (37)



### (1/37 | 1/241) Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning (Changtong Zan et al., 2024)

{{<citation>}}

Changtong Zan, Liang Ding, Li Shen, Yibing Zhen, Weifeng Liu, Dacheng Tao. (2024)  
**Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning**
<br/>
<button class="copy-to-clipboard" title="Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning" index=1>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 133  
Keywords: Benchmarking, Few-shot, Fine-tuning, Low-Resource, Supervised Learning, Zero-shot, LLaMA, Instruction Following, In-context Learning, In-context Learning, Instruction Tuning, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14399v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14399v1.pdf" filename="2403.14399v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Translation-tailored <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> exhibit remarkable translation capabilities, even competing with <b>supervised-trained</b> commercial translation systems. However, off-target translation remains an unsolved problem, especially for <b>low-resource</b> languages, hindering us from developing accurate <b>LLMs-based</b> translation models. To mitigate the off-target translation problem and enhance the performance of <b>LLMs</b> on translation, recent works have either designed advanced <b>prompting</b> strategies to highlight the functionality of translation <b>instructions</b> <b>or</b> exploited the <b>in-context</b> <b>learning</b> ability of <b>LLMs</b> by feeding <b>few-shot</b> demonstrations. However, these methods essentially do not improve <b>LLM's</b> ability to follow translation <b>instructions,</b> <b>especially</b> the language direction information. In this work, we design a two-stage <b>fine-tuning</b> algorithm to improve the <b>instruction-following</b> <b>ability</b> (especially the translation direction) of <b>LLMs.</b> Specifically, we first tune <b>LLMs</b> with the maximum likelihood estimation loss on the translation dataset to elicit the basic translation capabilities. In the second stage, we construct <b>instruction-conflicting</b> <b>samples</b> by randomly replacing the translation directions with a wrong one within the <b>instruction,</b> <b>and</b> then introduce an extra unlikelihood loss to learn those samples. Experiments on IWSLT and WMT <b>benchmarks</b> upon the <b>LLaMA</b> model spanning 16 <b>zero-shot</b> directions show that, compared to the competitive baseline -- translation-finetuned <b>LLama,</b> our method could effectively reduce the off-target translation ratio (averagely -53.3\%), thus improving translation quality with average +5.7 SacreBLEU and +16.4 BLEURT. Analysis shows that our method could preserve the model's general task performance on AlpacaEval. Code and models will be released at \url{https://github.com/alphadl/LanguageAware_Tuning}.

{{</citation>}}


### (2/37 | 2/241) LLM-based Extraction of Contradictions from Patents (Stefan Trapp et al., 2024)

{{<citation>}}

Stefan Trapp, Joachim Warschat. (2024)  
**LLM-based Extraction of Contradictions from Patents**
<br/>
<button class="copy-to-clipboard" title="LLM-based Extraction of Contradictions from Patents" index=2>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: I-2-7; H-3-1, cs-CL, cs.CL  
Keyword Score: 110  
Keywords: Dense Retrieval, Fine-tuning, BERT, GPT, GPT-4, Transformer, Question Answering, Large Language Model, Large Language Model, Prompt, Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14258v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14258v1.pdf" filename="2403.14258v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Already since the 1950s TRIZ shows that patents and the technical contradictions they solve are an important source of inspiration for the development of innovative products. However, TRIZ is a heuristic based on a historic patent analysis and does not make use of the ever-increasing number of latest technological solutions in current patents. Because of the huge number of patents, their length, and, last but not least, their complexity there is a need for modern patent retrieval and patent analysis to go beyond keyword-oriented methods. Recent advances in patent retrieval and analysis mainly focus on <b>dense</b> <b>vectors</b> based on neural AI <b>Transformer</b> language models like Google <b>BERT.</b> They are, for example, used for <b>dense</b> <b>retrieval,</b> <b>question</b> <b>answering</b> or <b>summarization</b> and key concept extraction. A research focus within the methods for patent <b>summarization</b> and key concept extraction are generic inventive concepts respectively TRIZ concepts like problems, solutions, advantage of invention, parameters, and contradictions. Succeeding rule-based approaches, <b>finetuned</b> <b>BERT-like</b> language models for sentence-wise classification represent the state-of-the-art of inventive concept extraction. While they work comparatively well for basic concepts like problems or solutions, contradictions - as a more complex abstraction - remain a challenge for these models. This paper goes one step further, as it presents a method to extract TRIZ contradictions from patent texts based on <b>Prompt</b> Engineering using a generative <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM),</b> namely OpenAI's <b>GPT-4.</b> Contradiction detection, sentence extraction, contradiction <b>summarization,</b> parameter extraction and assignment to the 39 abstract TRIZ engineering parameters are all performed in a single <b>prompt</b> using the LangChain framework. Our results show that "off-the-shelf" <b>GPT-4</b> is a serious alternative to existing approaches.

{{</citation>}}


### (3/37 | 3/241) A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science (Clayton Cohn et al., 2024)

{{<citation>}}

Clayton Cohn, Nicole Hutchins, Tuan Le, Gautam Biswas. (2024)  
**A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science**
<br/>
<button class="copy-to-clipboard" title="A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science" index=3>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 100  
Keywords: Active Learning, Few-shot, human-in-the-loop, GPT, GPT-4, Reasoning, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14565v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14565v1.pdf" filename="2403.14565v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper explores the use of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to score and explain short-answer assessments in K-12 science. While existing methods can score more structured math and computer science assessments, they often do not provide explanations for the scores. Our study focuses on employing <b>GPT-4</b> for automated assessment in middle school Earth Science, combining <b>few-shot</b> and <b>active</b> <b>learning</b> with <b>chain-of-thought</b> <b>reasoning.</b> Using a <b>human-in-the-loop</b> approach, we successfully score and provide meaningful explanations for formative assessment responses. A systematic analysis of our method's pros and cons sheds light on the potential for <b>human-in-the-loop</b> techniques to enhance automated grading for open-ended science assessments.

{{</citation>}}


### (4/37 | 4/241) FIT-RAG: Black-Box RAG with Factual Information and Token Reduction (Yuren Mao et al., 2024)

{{<citation>}}

Yuren Mao, Xuemei Dong, Wenyi Xu, Yunjun Gao, Bin Wei, Ying Zhang. (2024)  
**FIT-RAG: Black-Box RAG with Factual Information and Token Reduction**
<br/>
<button class="copy-to-clipboard" title="FIT-RAG: Black-Box RAG with Factual Information and Token Reduction" index=4>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-IR, cs.CL  
Keyword Score: 95  
Keywords: Black Box, Fine-tuning, Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Open-Domain Question Answering, Question Answering, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14374v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14374v1.pdf" filename="2403.14374v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Due to the extraordinarily <b>large</b> <b>number</b> <b>of</b> parameters, <b>fine-tuning</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to update long-tail or out-of-date knowledge is impractical in lots of applications. To avoid <b>fine-tuning,</b> we can alternatively treat a <b>LLM</b> as a <b>black-box</b> <b>(i.e.,</b> freeze the parameters of the <b>LLM)</b> and augment it with a <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> system, namely <b>black-box</b> <b>RAG.</b> Recently, <b>black-box</b> <b>RAG</b> has achieved success in knowledge-intensive tasks and has gained much attention. Existing <b>black-box</b> <b>RAG</b> methods typically <b>fine-tune</b> the retriever to cater to <b>LLMs'</b> preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information. The <b>LLM</b> preferred documents may not contain the factual information for the given <b>question,</b> <b>which</b> can mislead the retriever and hurt the effectiveness of <b>black-box</b> <b>RAG;</b> (2) Waste of Tokens. Simply concatenating all the retrieved documents brings <b>large</b> <b>amounts</b> <b>of</b> unnecessary tokens for <b>LLMs,</b> which degenerates the efficiency of <b>black-box</b> <b>RAG.</b> To address these issues, this paper proposes a novel <b>black-box</b> <b>RAG</b> framework which utilizes the factual information in the <b>retrieval</b> <b>and</b> <b>reduces</b> the number of tokens for augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by constructing a bi-label document scorer. Besides, it reduces the tokens by introducing a self-knowledge recognizer and a sub-document-level token reducer. FIT-RAG achieves both superior effectiveness and efficiency, which is validated by extensive experiments across three <b>open-domain</b> <b>question-answering</b> <b>datasets:</b> TriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of Llama2-13B-Chat by 14.3\% on TriviaQA, 19.9\% on NQ and 27.5\% on PopQA, respectively. Furthermore, it can save approximately half of the tokens on average across the three datasets.

{{</citation>}}


### (5/37 | 5/241) MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation (Longzheng Wang et al., 2024)

{{<citation>}}

Longzheng Wang, Xiaohan Xu, Lei Zhang, Jiarui Lu, Yongxiu Xu, Hongbo Xu, Chuang Zhang. (2024)  
**MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation**
<br/>
<button class="copy-to-clipboard" title="MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation" index=5>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 86  
Keywords: Data Augmentation, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Multi-modal, Multi-modal, Instruction Following, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14171v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14171v1.pdf" filename="2403.14171v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Automatic detection of <b>multimodal</b> misinformation has gained a widespread attention recently. However, the potential of powerful <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for <b>multimodal</b> misinformation detection remains underexplored. Besides, how to teach <b>LLMs</b> to interpret <b>multimodal</b> misinformation in cost-effective and accessible way is still an open question. To address that, we propose MMIDR, a framework designed to teach <b>LLMs</b> in providing fluent and high-quality textual explanations for their decision-making process of <b>multimodal</b> misinformation. To convert <b>multimodal</b> misinformation into an appropriate <b>instruction-following</b> <b>format,</b> we present a <b>data</b> <b>augmentation</b> perspective and pipeline. This pipeline consists of a visual information processing module and an evidence retrieval module. Subsequently, we <b>prompt</b> the proprietary <b>LLMs</b> with processed contents to extract rationales for interpreting the authenticity of <b>multimodal</b> misinformation. Furthermore, we design an efficient <b>knowledge</b> <b>distillation</b> approach to <b>distill</b> the capability of proprietary <b>LLMs</b> in explaining <b>multimodal</b> misinformation into open-source <b>LLMs.</b> To explore several research questions regarding the performance of <b>LLMs</b> in <b>multimodal</b> misinformation detection tasks, we construct an <b>instruction-following</b> <b>multimodal</b> misinformation dataset and conduct comprehensive experiments. The experimental findings reveal that our MMIDR exhibits sufficient detection performance and possesses the capacity to provide compelling rationales to support its assessments.

{{</citation>}}


### (6/37 | 6/241) gTBLS: Generating Tables from Text by Conditional Question Answering (Anirudh Sundar et al., 2024)

{{<citation>}}

Anirudh Sundar, Christopher Richardson, Larry Heck. (2024)  
**gTBLS: Generating Tables from Text by Conditional Question Answering**
<br/>
<button class="copy-to-clipboard" title="gTBLS: Generating Tables from Text by Conditional Question Answering" index=6>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-IR, cs-LG, cs.CL  
Keyword Score: 80  
Keywords: Fine-tuning, Fine-tuning, Knowledge Distillation, Zero-shot, Transformer, Question Answering, BERTScore, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14457v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14457v1.pdf" filename="2403.14457v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Distilling</b> <b>large,</b> <b>unstructured</b> <b>text</b> into a structured, condensed form such as tables is an open research problem. One of the primary challenges in automatically generating tables is ensuring their syntactic validity. Prior approaches address this challenge by including additional parameters in the <b>Transformer's</b> attention mechanism to attend to specific rows and column headers. In contrast to this single-stage method, this paper presents a two-stage approach called Generative Tables (gTBLS). The first stage infers table structure (row and column headers) from the text. The second stage formulates <b>questions</b> <b>using</b> these headers and <b>fine-tunes</b> a causal language model to answer them. Furthermore, the gTBLS approach is amenable to the utilization of pre-trained <b>Large</b> <b>Language</b> <b>Models</b> in a <b>zero-shot</b> configuration, presenting a solution for table generation in situations where <b>fine-tuning</b> is not feasible. gTBLS improves prior approaches by up to 10% in <b>BERTScore</b> on the table construction task and up to 20% on the table content generation task of the E2E, WikiTableText, WikiBio, and RotoWire datasets.

{{</citation>}}


### (7/37 | 7/241) K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional Expression (Kyuhee Kim et al., 2024)

{{<citation>}}

Kyuhee Kim, Surin Lee, Sangah Lee. (2024)  
**K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional Expression**
<br/>
<button class="copy-to-clipboard" title="K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional Expression" index=7>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 78  
Keywords: Graph, Fine-tuning, Knowledge Graph, BART, GPT, GPT-4, GPT-4 turbo, Reasoning, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14253v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14253v1.pdf" filename="2403.14253v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In many literary texts, emotions are indirectly conveyed through descriptions of actions, facial expressions, and appearances, necessitating emotion inference for narrative understanding. In this paper, we introduce K-Act2Emo, a Korean commonsense <b>knowledge</b> <b>graph</b> (CSKG) comprising 1,900 indirect emotional expressions and the emotions inferable from them. We categorize <b>reasoning</b> types into inferences in positive situations, inferences in negative situations, and inferences when expressions do not serve as emotional cues. Unlike existing CSKGs, K-Act2Emo specializes in emotional contexts, and experimental results validate its effectiveness for training emotion inference models. Significantly, the <b>BART-based</b> <b>knowledge</b> <b>model</b> <b>fine-tuned</b> with K-Act2Emo outperforms various existing Korean <b>large</b> <b>language</b> <b>models,</b> achieving performance levels comparable to <b>GPT-4</b> <b>Turbo.</b>

{{</citation>}}


### (8/37 | 8/241) ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting (Xiaoxue Cheng et al., 2024)

{{<citation>}}

Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen. (2024)  
**ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting**
<br/>
<button class="copy-to-clipboard" title="ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting" index=8>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 70  
Keywords: Fine-tuning, LLaMA, Reasoning, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14312v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14312v1.pdf" filename="2403.14312v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Chain-of-Thought</b> <b>(CoT)</b> <b>prompting</b> can enhance the <b>reasoning</b> capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> establishing itself as a primary approach to solving complex <b>reasoning</b> tasks. Existing CoT synthesis approaches usually focus on simpler <b>reasoning</b> tasks and thus result in low-quality and inconsistent CoT <b>prompts.</b> In response to this challenge, we present an empirical investigation of CoT <b>prompting</b> and introduce CoTGenius, a novel framework designed for the automatic generation of superior CoT <b>prompts.</b> CoTGenius is developed based on three major evolution strategies, i.e., complicate, diversify, and specify-alongside two filtering mechanisms: evolutionary success judgement and correctness verification. We further employ CoTGenius to create an extensive CoT dataset, and subsequently <b>fine-tune</b> the <b>Llama</b> 2-Chat 7B and 13B models on this dataset. We call the resulting model ChainLM. To deal with the cumulative error issue in <b>reasoning</b> steps, we propose a step-level debating method, wherein multiple debaters discuss each <b>reasoning</b> step to arrive at the correct answer. Extensive experiments demonstrate that our ChainLM models exhibit enhanced proficiency in addressing a spectrum of complex <b>reasoning</b> problems compared to existing models. In addition, we conduct an in-depth analysis of the impact of data categories within CoTGenius on the model performance. We release our dataset and code at https://github.com/RUCAIBox/ChainLM.

{{</citation>}}


### (9/37 | 9/241) LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding (Masato Fujitake, 2024)

{{<citation>}}

Masato Fujitake. (2024)  
**LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding**
<br/>
<button class="copy-to-clipboard" title="LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding" index=9>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CL  
Keyword Score: 66  
Keywords: Fine-tuning, Fine-tuning, Multi-modal, Multi-modal, Information Retrieval, Instruction Tuning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14252v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14252v1.pdf" filename="2403.14252v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper proposes LayoutLLM, a more flexible document analysis method for understanding imaged documents. Visually Rich Document Understanding tasks, such as document image classification and <b>information</b> <b>extraction,</b> have gained significant attention due to their importance. Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure. However, these methods require <b>fine-tuning</b> for each task and dataset, and the models are expensive to train and operate. To overcome this limitation, we propose a new LayoutLLM that integrates these with <b>large-scale</b> <b>language</b> <b>models</b> <b>(LLMs).</b> By leveraging the strengths of existing research in document image understanding and <b>LLMs'</b> superior language understanding capabilities, the proposed model, <b>fine-tuned</b> with <b>multimodal</b> <b>instruction</b> <b>datasets,</b> performs an understanding of document images in a single model. Our experiments demonstrate improvement over the baseline model in various document analysis tasks.

{{</citation>}}


### (10/37 | 10/241) From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision (Qingwen Lin et al., 2024)

{{<citation>}}

Qingwen Lin, Boyan Xu, Zhengting Huang, Ruichu Cai. (2024)  
**From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision**
<br/>
<button class="copy-to-clipboard" title="From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision" index=10>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 60  
Keywords: Knowledge Distillation, Supervised Learning, Weakly-supervised Learning, ChatGPT, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14390v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14390v1.pdf" filename="2403.14390v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Addressing the challenge of high annotation costs in solving Math Word Problems (MWPs) through full supervision with intermediate equations, recent works have proposed weakly <b>supervised</b> task settings that rely solely on the final answer as a <b>supervised</b> signal. Existing leading approaches typically employ various search techniques to infer intermediate equations, but cannot ensure their semantic consistency with natural language descriptions. The rise of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>ChatGPT</b> has opened up new possibilities for addressing MWPs directly. However, the computational demands of <b>LLMs</b> make them less than ideal for use in settings where resources are tight. In light of these challenges, we introduce an innovative two-stage framework that adeptly transfers mathematical Expertise from <b>large</b> <b>to</b> <b>tiny</b> language models. In \emph{Distillation Stage}, we propose a series of extraction processes that satisfy the properties of MWPs to <b>distill</b> mathematical knowledge from <b>LLMs</b> to construct problem-equation pairs required for <b>supervised</b> training. In \emph{Refinement Stage}, Due to Knowledge <b>distilling</b> method cannot guarantee the full utilization of all data, we further utilize the unsuccessfully searched data effectively by Knowledge Refine method. Finally, We train a small model using <b>distilled</b> data generated through two-stage methods. As our method fully leverages the semantic understanding capabilities during the searching 'problem-equation' pair, it demonstrates significantly improved performance on the Math23K and Weak12K datasets compared to existing small model methods, while maintaining a much lower computational cost than <b>ChatGPT.</b>

{{</citation>}}


### (11/37 | 11/241) Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection (Kyungjae Lee et al., 2024)

{{<citation>}}

Kyungjae Lee, Dasol Hwang, Sunghyun Park, Youngsoo Jang, Moontae Lee. (2024)  
**Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection**
<br/>
<button class="copy-to-clipboard" title="Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection" index=11>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 60  
Keywords: Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Mathematical Reasoning, Reasoning, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14238v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14238v1.pdf" filename="2403.14238v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Despite the promise of <b>RLHF</b> in aligning <b>LLMs</b> with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of <b>LLMs.</b> Underspecified preferences could obscure directions to align the models. Lacking exploration restricts identification of desirable outputs to improve the models. To overcome these challenges, we propose a novel framework: <b>Reinforcement</b> <b>Learning</b> from Reflective Feedback (RLRF), which leverages fine-grained feedback based on detailed criteria to improve the core capabilities of <b>LLMs.</b> RLRF employs a self-reflection mechanism to systematically explore and refine <b>LLM</b> responses, then <b>fine-tuning</b> the models via a RL algorithm along with promising responses. Our experiments across Just-Eval, Factuality, and <b>Mathematical</b> <b>Reasoning</b> demonstrate the efficacy and transformative potential of RLRF beyond superficial surface-level adjustment.

{{</citation>}}


### (12/37 | 12/241) ChatGPT Alternative Solutions: Large Language Models Survey (Hanieh Alipour et al., 2024)

{{<citation>}}

Hanieh Alipour, Nick Pendar, Kohinoor Roy. (2024)  
**ChatGPT Alternative Solutions: Large Language Models Survey**
<br/>
<button class="copy-to-clipboard" title="ChatGPT Alternative Solutions: Large Language Models Survey" index=12>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 56  
Keywords: Benchmarking, Benchmarking, Generative AI, ChatGPT, Chatbot, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14469v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14469v1.pdf" filename="2403.14469v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In recent times, the grandeur of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has not only shone in the realm of natural language processing but has also cast its brilliance across a vast array of applications. This remarkable display of <b>LLM</b> capabilities has ignited a surge in research contributions within this domain, spanning a diverse spectrum of topics. These contributions encompass advancements in neural network architecture, context length enhancements, model alignment, training datasets, <b>benchmarking,</b> efficiency improvements, and more. Recent years have witnessed a dynamic synergy between academia and industry, propelling the field of <b>LLM</b> research to new heights. A notable milestone in this journey is the introduction of <b>ChatGPT,</b> a powerful AI <b>chatbot</b> grounded in <b>LLMs,</b> which has garnered widespread societal attention. The evolving technology of <b>LLMs</b> has begun to reshape the landscape of the entire AI community, promising a revolutionary shift in the way we create and employ AI algorithms. Given this swift-paced technical evolution, our survey embarks on a journey to encapsulate the recent strides made in the world of <b>LLMs.</b> Through an exploration of the background, key discoveries, and prevailing methodologies, we offer an up-to-the-minute review of the literature. By examining multiple <b>LLM</b> models, our paper not only presents a comprehensive overview but also charts a course that identifies existing challenges and points toward potential future research trajectories. This survey furnishes a well-rounded perspective on the current state of <b>generative</b> <b>AI,</b> shedding light on opportunities for further exploration, enhancement, and innovation.

{{</citation>}}


### (13/37 | 13/241) A Multimodal Approach to Device-Directed Speech Detection with Large Language Models (Dominik Wager et al., 2024)

{{<citation>}}

Dominik Wager, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi. (2024)  
**A Multimodal Approach to Device-Directed Speech Detection with Large Language Models**
<br/>
<button class="copy-to-clipboard" title="A Multimodal Approach to Device-Directed Speech Detection with Large Language Models" index=13>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-LG, cs.CL, eess-AS  
Keyword Score: 56  
Keywords: Multi-modal, Multi-modal, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14438v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14438v1.pdf" filename="2403.14438v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command. To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase. We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform. Second, we take the decoder outputs of an <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> system, such as 1-best hypotheses, as input features to a <b>large</b> <b>language</b> <b>model</b> <b>(LLM).</b> Finally, we explore a <b>multimodal</b> system that combines acoustic and lexical features, as well as <b>ASR</b> decoder signals in an <b>LLM.</b> Using <b>multimodal</b> information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%. Increasing the size of the <b>LLM</b> and training with low-rank adaption leads to further relative EER reductions of up to 18% on our dataset.

{{</citation>}}


### (14/37 | 14/241) Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations (Jiaxing Sun et al., 2024)

{{<citation>}}

Jiaxing Sun, Weiquan Huang, Jiang Wu, Chenya Gu, Wei Li, Songyang Zhang, Hang Yan, Conghui He. (2024)  
**Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations**
<br/>
<button class="copy-to-clipboard" title="Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations" index=14>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 56  
Keywords: Benchmarking, Benchmarking, Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14112v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14112v1.pdf" filename="2403.14112v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We introduce CHARM, the first <b>benchmark</b> for comprehensively and in-depth evaluating the <b>commonsense</b> <b>reasoning</b> ability of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in Chinese, which covers both globally known and Chinese-specific <b>commonsense.</b> <b>We</b> evaluated 7 English and 12 Chinese-oriented <b>LLMs</b> on CHARM, employing 5 representative <b>prompt</b> strategies for improving <b>LLMs'</b> <b>reasoning</b> ability, such as Chain-of-Thought. Our findings indicate that the <b>LLM's</b> language orientation and the task's domain influence the effectiveness of the <b>prompt</b> strategy, which enriches previous research findings. We built closely-interconnected <b>reasoning</b> and memorization tasks, and found that some <b>LLMs</b> struggle with memorizing Chinese <b>commonsense,</b> <b>affecting</b> their <b>reasoning</b> ability, while others show differences in <b>reasoning</b> despite similar memorization performance. We also evaluated the <b>LLMs'</b> memorization-independent <b>reasoning</b> abilities and analyzed the typical errors. Our study precisely identified the <b>LLMs'</b> strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at https://github.com/opendatalab/CHARM .

{{</citation>}}


### (15/37 | 15/241) Improving the Robustness of Large Language Models via Consistency Alignment (Zhao Yukun et al., 2024)

{{<citation>}}

Zhao Yukun, Yan Lingyong, Sun Weiwei, Xing Guoliang, Wang Shuaiqiang, Meng Chong, Cheng Zhicong, Ren Zhaochun, Yin Dawei. (2024)  
**Improving the Robustness of Large Language Models via Consistency Alignment**
<br/>
<button class="copy-to-clipboard" title="Improving the Robustness of Large Language Models via Consistency Alignment" index=15>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 50  
Keywords: Fine-tuning, Supervised Learning, Instruction Following, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14221v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14221v1.pdf" filename="2403.14221v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown tremendous success in following user <b>instructions</b> <b>and</b> generating helpful responses. Nevertheless, their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized <b>instructions.</b> <b>Recent</b> literature has explored this inconsistency issue, highlighting the importance of continued improvement in the robustness of response generation. However, systematic analysis and solutions are still lacking. In this paper, we quantitatively define the inconsistency problem and propose a two-stage training framework consisting of <b>instruction-augmented</b> <b>supervised</b> <b>fine-tuning</b> and consistency alignment training. The first stage helps a model generalize on following <b>instructions</b> <b>via</b> similar <b>instruction</b> <b>augmentations.</b> In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations by differentiating subtle differences in similar responses. The training process is accomplished by self-rewards inferred from the trained model at the first stage without referring to external human preference resources. We conduct extensive experiments on recent publicly available <b>LLMs</b> on <b>instruction-following</b> <b>tasks</b> and demonstrate the effectiveness of our training framework.

{{</citation>}}


### (16/37 | 16/241) Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering (Kosuke Akimoto et al., 2024)

{{<citation>}}

Kosuke Akimoto, Kunihiro Takeoka, Masafumi Oyamada. (2024)  
**Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering**
<br/>
<button class="copy-to-clipboard" title="Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering" index=16>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 50  
Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Open-Domain Question Answering, Question Answering, In-context Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14197v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14197v1.pdf" filename="2403.14197v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Retrieval-augmented</b> <b>generation</b> <b>models</b> augment knowledge encoded in a language model by providing additional relevant external knowledge (context) during generation. Although it has been shown that the quantity and quality of context impact the performance of <b>retrieval-augmented</b> <b>generation</b> <b>models</b> during inference, limited research explores how these characteristics affect model training. This paper explores how context quantity and quality during model training affect the performance of Fusion-in-Decoder (FiD), the state-of-the-art <b>retrieval-augmented</b> <b>generation</b> <b>model,</b> in extractive <b>open-domain</b> <b>question</b> <b>answering</b> tasks. Experimental results suggest that FiD models overfit to context quality during training and show suboptimal performance when evaluated on different context quality. Through the experimental results, we also reveal FiD models trained with different context quality have different cross-attention distribution patterns. Specifically, as context quality during training increases, FiD models tend to attend more uniformly to each passage in context. Finally, based on these observations, we propose a method to mitigate overfitting to specific context quality by introducing bias to the cross-attention distribution, which we demonstrate to be effective in improving the performance of FiD models on different context quality.

{{</citation>}}


### (17/37 | 17/241) From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation (Haofei Zhao et al., 2024)

{{<citation>}}

Haofei Zhao, Yilun Liu, Shimin Tao, Weibin Meng, Yimeng Chen, Xiang Geng, Chang Su, Min Zhang, Hao Yang. (2024)  
**From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation**
<br/>
<button class="copy-to-clipboard" title="From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation" index=17>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 50  
Keywords: Neural Machine Translation, Neural Machine Translation, Large Language Model, Large Language Model, Pre-trained Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14118v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14118v1.pdf" filename="2403.14118v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Machine</b> <b>Translation</b> Quality Estimation (MTQE) is the task of estimating the quality of <b>machine-translated</b> <b>text</b> in real time without the need for reference translations, which is of great importance for the development of <b>MT.</b> After two decades of evolution, QE has yielded a wealth of results. This article provides a comprehensive overview of QE datasets, annotation methods, shared tasks, methodologies, challenges, and future research directions. It begins with an introduction to the background and significance of QE, followed by an explanation of the concepts and evaluation metrics for word-level QE, sentence-level QE, document-level QE, and explainable QE. The paper categorizes the methods developed throughout the history of QE into those based on handcrafted features, deep learning, and <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> with a further division of deep learning-based methods into classic deep learning and those incorporating <b>pre-trained</b> <b>language</b> <b>models</b> (LMs). Additionally, the article details the advantages and limitations of each method and offers a straightforward comparison of different approaches. Finally, the paper discusses the current challenges in QE research and provides an outlook on future research directions.

{{</citation>}}


### (18/37 | 18/241) Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity (Soyeong Jeong et al., 2024)

{{<citation>}}

Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park. (2024)  
**Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity**
<br/>
<button class="copy-to-clipboard" title="Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity" index=18>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 40  
Keywords: Question Answering, Question Answering, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14403v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14403v1.pdf" filename="2403.14403v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Retrieval-Augmented <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> which incorporate the non-parametric knowledge from external knowledge bases into <b>LLMs,</b> have emerged as a promising approach to enhancing response accuracy in several tasks, such as <b>Question-Answering</b> <b>(QA).</b> However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive <b>QA</b> framework, that can dynamically select the most suitable strategy for (retrieval-augmented) <b>LLMs</b> from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented <b>LLMs,</b> as well as the no-retrieval methods, in response to a range of query complexities. We validate our model on a set of open-domain <b>QA</b> datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of <b>QA</b> systems, compared to relevant baselines including the adaptive retrieval approaches. Code is available at: https://github.com/starsuzi/Adaptive-RAG.

{{</citation>}}


### (19/37 | 19/241) Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling (Chengxu Zhuang et al., 2024)

{{<citation>}}

Chengxu Zhuang, Evelina Fedorenko, Jacob Andreas. (2024)  
**Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling**
<br/>
<button class="copy-to-clipboard" title="Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling" index=19>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-LG, cs.CL  
Keyword Score: 39  
Keywords: Benchmarking, Multi-modal, Multi-modal, Grounding, Perplexity, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14551v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14551v1.pdf" filename="2403.14551v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Today's most accurate language models are trained on orders of magnitude more language data than human language learners receive - but with no supervision from other sensory modalities that play a crucial role in human learning. Can we make LMs' representations and predictions more accurate (and more human-like) with more ecologically plausible supervision? This paper describes LexiContrastive <b>Grounding</b> (LCG), a grounded language learning procedure that leverages visual supervision to improve textual representations. LexiContrastive <b>Grounding</b> combines a next token prediction strategy with a contrastive visual <b>grounding</b> objective, focusing on early-layer representations that encode lexical information. Across multiple word-learning and sentence-understanding <b>benchmarks,</b> LexiContrastive <b>Grounding</b> not only outperforms standard language-only models in learning efficiency, but also improves upon <b>vision-and-language</b> learning procedures including CLIP, GIT, Flamingo, and Vokenization. Moreover, LexiContrastive <b>Grounding</b> improves <b>perplexity</b> by around 5% on multiple language modeling tasks. This work underscores the potential of incorporating visual <b>grounding</b> into language models, aligning more closely with the <b>multimodal</b> nature of human language acquisition.

{{</citation>}}


### (20/37 | 20/241) Detoxifying Large Language Models via Knowledge Editing (Mengru Wang et al., 2024)

{{<citation>}}

Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen. (2024)  
**Detoxifying Large Language Models via Knowledge Editing**
<br/>
<button class="copy-to-clipboard" title="Detoxifying Large Language Models via Knowledge Editing" index=20>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-CV, cs-HC, cs-LG, cs.CL  
Keyword Score: 33  
Keywords: Benchmarking, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14472v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14472v1.pdf" filename="2403.14472v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper investigates using knowledge editing techniques to detoxify <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> We construct a <b>benchmark,</b> SafeEdit, which covers nine unsafe categories with various powerful attack <b>prompts</b> and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify <b>LLMs</b> with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of <b>LLMs</b> within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of <b>LLMs.</b> Code and <b>benchmark</b> are available at https://github.com/zjunlp/EasyEdit.

{{</citation>}}


### (21/37 | 21/241) Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large Language Models with Machine Learning in tele-dermatology (Dimitrios P. Panagoulias et al., 2024)

{{<citation>}}

Dimitrios P. Panagoulias, Evridiki Tsoureli-Nikita, Maria Virvou, George A. Tsihrintzis. (2024)  
**Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large Language Models with Machine Learning in tele-dermatology**
<br/>
<button class="copy-to-clipboard" title="Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large Language Models with Machine Learning in tele-dermatology" index=21>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-CV, cs.CL  
Keyword Score: 33  
Keywords: Multi-modal, Transformer, Natural Language Inference, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14243v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14243v1.pdf" filename="2403.14243v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The rise of Artificial Intelligence creates great promise in the field of medical discovery, diagnostics and patient management. However, the vast complexity of all medical domains require a more complex approach that combines machine learning algorithms, classifiers, segmentation algorithms and, lately, <b>large</b> <b>language</b> <b>models.</b> In this paper, we describe, implement and assess an Artificial Intelligence-empowered system and methodology aimed at assisting the diagnosis process of skin lesions and other skin conditions within the field of dermatology that aims to holistically address the diagnostic process in this domain. The workflow integrates <b>large</b> <b>language,</b> <b>transformer-based</b> vision models and sophisticated machine learning tools. This holistic approach achieves a nuanced interpretation of dermatological conditions that simulates and facilitates a dermatologist's workflow. We assess our proposed methodology through a thorough cross-model validation technique embedded in an evaluation pipeline that utilizes publicly available medical case studies of skin conditions and relevant images. To quantitatively score the system performance, advanced machine learning and <b>natural</b> <b>language</b> <b>processing</b> tools are employed which focus on similarity comparison and <b>natural</b> <b>language</b> <b>inference.</b> Additionally, we incorporate a human expert evaluation process based on a structured checklist to further validate our results. We implemented the proposed methodology in a system which achieved approximate (weighted) scores of 0.87 for both contextual understanding and diagnostic accuracy, demonstrating the efficacy of our approach in enhancing dermatological analysis. The proposed methodology is expected to prove useful in the development of next-generation tele-dermatology applications, enhancing remote consultation capabilities and access to care, especially in underserved areas.

{{</citation>}}


### (22/37 | 22/241) Large-Scale Label Interpretation Learning for Few-Shot Named Entity Recognition (Jonas Golde et al., 2024)

{{<citation>}}

Jonas Golde, Felix Hamborg, Alan Akbik. (2024)  
**Large-Scale Label Interpretation Learning for Few-Shot Named Entity Recognition**
<br/>
<button class="copy-to-clipboard" title="Large-Scale Label Interpretation Learning for Few-Shot Named Entity Recognition" index=22>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 33  
Keywords: Benchmarking, Few-shot, Named Entity Recognition, Named Entity Recognition  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14222v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14222v1.pdf" filename="2403.14222v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Few-shot</b> <b>named</b> <b>entity</b> <b>recognition</b> <b>(NER)</b> detects <b>named</b> <b>entities</b> <b>within</b> text using only a few annotated examples. One promising line of research is to leverage natural language descriptions of each entity type: the common label PER might, for example, be verbalized as ''person entity.'' In an initial label interpretation learning phase, the model learns to interpret such verbalized descriptions of entity types. In a subsequent <b>few-shot</b> tagset extension phase, this model is then given a description of a previously unseen entity type (such as ''music album'') and optionally a few training examples to perform <b>few-shot</b> <b>NER</b> for this type. In this paper, we systematically explore the impact of a strong semantic prior to interpret verbalizations of new entity types by massively scaling up the number and granularity of entity types used for label interpretation learning. To this end, we leverage an entity linking <b>benchmark</b> to create a dataset with orders of magnitude of more distinct entity types and descriptions as currently used datasets. We find that this increased signal yields strong results in zero- and <b>few-shot</b> <b>NER</b> in in-domain, cross-domain, and even cross-lingual settings. Our findings indicate significant potential for improving <b>few-shot</b> <b>NER</b> through heuristical data-based optimization.

{{</citation>}}


### (23/37 | 23/241) Large Language Models for Multi-Choice Question Classification of Medical Subjects (Víctor Ponce-López, 2024)

{{<citation>}}

Víctor Ponce-López. (2024)  
**Large Language Models for Multi-Choice Question Classification of Medical Subjects**
<br/>
<button class="copy-to-clipboard" title="Large Language Models for Multi-Choice Question Classification of Medical Subjects" index=23>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 30  
Keywords: Question Answering, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14582v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14582v1.pdf" filename="2403.14582v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The aim of this paper is to evaluate whether <b>large</b> <b>language</b> <b>models</b> trained on multi-choice <b>question</b> <b>data</b> can be used to discriminate between medical subjects. This is an important and challenging task for automatic <b>question</b> <b>answering.</b> To achieve this goal, we train deep neural networks for multi-class classification of <b>questions</b> <b>into</b> the inferred medical subjects. Using our Multi-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art results on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their development and test sets, respectively. In this sense, we show the capability of AI and <b>LLMs</b> in particular for multi-classification tasks in the Healthcare domain.

{{</citation>}}


### (24/37 | 24/241) Multi-Level Explanations for Generative Language Models (Lucas Monteiro Paes et al., 2024)

{{<citation>}}

Lucas Monteiro Paes, Dennis Wei, Hyo Jin Do, Hendrik Strobelt, Ronny Luss, Amit Dhurandhar, Manish Nagireddy, Karthikeyan Natesan Ramamurthy, Prasanna Sattigeri, Werner Geyer, Soumya Ghosh. (2024)  
**Multi-Level Explanations for Generative Language Models**
<br/>
<button class="copy-to-clipboard" title="Multi-Level Explanations for Generative Language Models" index=24>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 30  
Keywords: Question Answering, Text Classification, Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14459v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14459v1.pdf" filename="2403.14459v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Perturbation-based explanation methods such as LIME and SHAP are commonly applied to <b>text</b> <b>classification.</b> This work focuses on their extension to generative language models. To address the challenges of <b>text</b> <b>as</b> output and long <b>text</b> <b>inputs,</b> we propose a general framework called MExGen that can be instantiated with different attribution algorithms. To handle <b>text</b> <b>output,</b> we introduce the notion of scalarizers for mapping <b>text</b> <b>to</b> real numbers and investigate multiple possibilities. To handle long inputs, we take a multi-level approach, proceeding from coarser levels of granularity to finer ones, and focus on algorithms with linear scaling in model queries. We conduct a systematic evaluation, both automated and human, of perturbation-based attribution methods for <b>summarization</b> and context-grounded <b>question</b> <b>answering.</b> The results show that our framework can provide more locally faithful explanations of generated outputs.

{{</citation>}}


### (25/37 | 25/241) ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification (Sehee Lim et al., 2024)

{{<citation>}}

Sehee Lim, Yejin Kim, Chi-Hyun Choi, Jy-yong Sohn, Byung-Hoon Kim. (2024)  
**ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification**
<br/>
<button class="copy-to-clipboard" title="ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification" index=25>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-LG, cs.CL  
Keyword Score: 30  
Keywords: Reasoning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14255v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14255v1.pdf" filename="2403.14255v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Improving the accessibility of psychotherapy with the aid of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is garnering a significant attention in recent years. Recognizing cognitive distortions from the interviewee's utterances can be an essential part of psychotherapy, especially for cognitive behavioral therapy. In this paper, we propose ERD, which improves <b>LLM-based</b> cognitive distortion classification performance with the aid of additional modules of (1) extracting the parts related to cognitive distortion, and (2) debating the <b>reasoning</b> steps by multiple agents. Our experimental results on a public dataset show that ERD improves the multi-class F1 score as well as binary specificity score. Regarding the latter score, it turns out that our method is effective in debiasing the baseline method which has high false positive rate, especially when the summary of multi-agent debate is provided to <b>LLMs.</b>

{{</citation>}}


### (26/37 | 26/241) EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling (Shimao Zhang et al., 2024)

{{<citation>}}

Shimao Zhang, Yu Bao, Shujian Huang. (2024)  
**EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling**
<br/>
<button class="copy-to-clipboard" title="EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling" index=26>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 23  
Keywords: Benchmarking, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14541v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14541v1.pdf" filename="2403.14541v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recently, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated outstanding performance across a wide range of downstream language tasks. Temperature sampling is a commonly used decoding strategy for <b>LLMs'</b> generation process. However, a fixed temperature parameter is used in most cases, which may not always be an optimal choice for balancing generation quality and diversity. In this paper, we propose an effective Entropy-based Dynamic Temperature (EDT) Sampling method, to achieve a more balanced performance in terms of both generation quality and diversity by dynamically selecting the temperature parameter. Additionally, we also show model performance and comprehensive analyses for 4 different generation <b>benchmarks.</b> Our experiments show that EDT significantly outperforms the existing strategies across different tasks.

{{</citation>}}


### (27/37 | 27/241) Prediction of Translation Techniques for the Translation Process (Fan Zhou et al., 2024)

{{<citation>}}

Fan Zhou, Vincent Vandeghinste. (2024)  
**Prediction of Translation Techniques for the Translation Process**
<br/>
<button class="copy-to-clipboard" title="Prediction of Translation Techniques for the Translation Process" index=27>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 20  
Keywords: Neural Machine Translation, Neural Machine Translation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14454v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14454v1.pdf" filename="2403.14454v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Machine</b> <b>translation</b> <b>(MT)</b> encompasses a variety of methodologies aimed at enhancing the accuracy of translations. In contrast, the process of human-generated translation relies on a wide range of translation techniques, which are crucial for ensuring linguistic adequacy and fluency. This study suggests that these translation techniques could further optimize <b>machine</b> <b>translation</b> if they are automatically identified before being applied to guide the translation process effectively. The study differentiates between two scenarios of the translation process: from-scratch translation and post-editing. For each scenario, a specific set of experiments has been designed to forecast the most appropriate translation techniques. The findings indicate that the predictive accuracy for from-scratch translation reaches 82%, while the post-editing process exhibits even greater potential, achieving an accuracy rate of 93%.

{{</citation>}}


### (28/37 | 28/241) More than Just Statistical Recurrence: Human and Machine Unsupervised Learning of Māori Word Segmentation across Morphological Processes (Ashvini Varatharaj et al., 2024)

{{<citation>}}

Ashvini Varatharaj, Simon Todd. (2024)  
**More than Just Statistical Recurrence: Human and Machine Unsupervised Learning of Māori Word Segmentation across Morphological Processes**
<br/>
<button class="copy-to-clipboard" title="More than Just Statistical Recurrence: Human and Machine Unsupervised Learning of Māori Word Segmentation across Morphological Processes" index=28>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 20  
Keywords: Unsupervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14444v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14444v1.pdf" filename="2403.14444v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Non-M\=aori-speaking New Zealanders (NMS)are able to segment M\=aori words in a highlysimilar way to fluent speakers (Panther et al.,2024). This ability is assumed to derive through the identification and extraction of statistically recurrent forms. We examine this assumption by asking how NMS segmentations compare to those produced by Morfessor, an <b>unsupervised</b> <b>machine</b> learning model that operates based on statistical recurrence, across words formed by a variety of morphological processes. Both NMS and Morfessor succeed in segmenting words formed by concatenative processes (compounding and affixation without allomorphy), but NMS also succeed for words that invoke templates (reduplication and allomorphy) and other cues to morphological structure, implying that their learning process is sensitive to more than just statistical recurrence.

{{</citation>}}


### (29/37 | 29/241) Locating and Mitigating Gender Bias in Large Language Models (Yuchen Cai et al., 2024)

{{<citation>}}

Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, Enhong Chen. (2024)  
**Locating and Mitigating Gender Bias in Large Language Models**
<br/>
<button class="copy-to-clipboard" title="Locating and Mitigating Gender Bias in Large Language Models" index=29>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14409v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14409v1.pdf" filename="2403.14409v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>language</b> <b>models(LLM)</b> are pre-trained on extensive corpora to learn facts and human cognition which contain human preferences. However, this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society. Prior research has typically tackled the issue of bias through a one-dimensional perspective, concentrating either on locating or mitigating it. This limited perspective has created obstacles in facilitating research on bias to synergistically complement and progressively build upon one another. In this study, we integrate the processes of locating and mitigating bias within a unified framework. Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a <b>large</b> <b>language</b> <b>model.</b> Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets. The experimental results indicate that the primary contributors to gender bias are the bottom MLP modules acting on the last token of occupational pronouns and the top attention module acting on the final word in the sentence. Furthermore, LSDM mitigates gender bias in the model more effectively than the other baselines, while fully preserving the model's capabilities in all other aspects.

{{</citation>}}


### (30/37 | 30/241) WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models (Hichem Ammar Khodja et al., 2024)

{{<citation>}}

Hichem Ammar Khodja, Frédéric Béchet, Quentin Brabant, Alexis Nasr, Gwénolé Lecorvé. (2024)  
**WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models**
<br/>
<button class="copy-to-clipboard" title="WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models" index=30>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14364v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14364v1.pdf" filename="2403.14364v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The factuality of <b>large</b> <b>language</b> <b>model</b> <b>(LLMs)</b> tends to decay over time since events posterior to their training are "unknown" to them. One way to keep models up-to-date could be factual update: the task of inserting, replacing, or removing certain simple (atomic) facts within the model. To study this task, we present WikiFactDiff, a dataset that describes the evolution of factual knowledge between two dates as a collection of simple facts divided into three categories: new, obsolete, and static. We describe several update scenarios arising from various combinations of these three types of basic update. The facts are represented by subject-relation-object triples; indeed, WikiFactDiff was constructed by comparing the state of the Wikidata knowledge base at 4 January 2021 and 27 February 2023. Those fact are accompanied by verbalization templates and cloze tests that enable running update algorithms and their evaluation metrics. Contrary to other datasets, such as zsRE and CounterFact, WikiFactDiff constitutes a realistic update setting that involves various update scenarios, including replacements, archival, and new entity insertions. We also present an evaluation of existing update algorithms on WikiFactDiff.

{{</citation>}}


### (31/37 | 31/241) Automatic Annotation of Grammaticality in Child-Caregiver Conversations (Mitja Nikolaus et al., 2024)

{{<citation>}}

Mitja Nikolaus, Abhishek Agrawal, Petros Kaklamanis, Alex Warstadt, Abdellah Fourtassi. (2024)  
**Automatic Annotation of Grammaticality in Child-Caregiver Conversations**
<br/>
<button class="copy-to-clipboard" title="Automatic Annotation of Grammaticality in Child-Caregiver Conversations" index=31>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 20  
Keywords: Fine-tuning, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14208v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14208v1.pdf" filename="2403.14208v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The acquisition of grammar has been a central question to adjudicate between theories of language acquisition. In order to conduct faster, more reproducible, and larger-scale corpus studies on grammaticality in child-caregiver conversations, tools for automatic annotation can offer an effective alternative to tedious manual annotation. We propose a coding scheme for context-dependent grammaticality in child-caregiver conversations and annotate more than 4,000 utterances from a large corpus of transcribed conversations. Based on these annotations, we train and evaluate a range of NLP models. Our results show that <b>fine-tuned</b> <b>Transformer-based</b> models perform best, achieving human inter-annotation agreement levels.As a first application and sanity check of this tool, we use the trained models to annotate a corpus almost two orders of magnitude larger than the manually annotated data and verify that children's grammaticality shows a steady increase with age.This work contributes to the growing literature on applying state-of-the-art NLP methods to help study child language acquisition at scale.

{{</citation>}}


### (32/37 | 32/241) M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset (Zhe Chen et al., 2024)

{{<citation>}}

Zhe Chen, Heyang Liu, Wenyi Yu, Guangzhi Sun, Hongcheng Liu, Ji Wu, Chao Zhang, Yu Wang, Yanfeng Wang. (2024)  
**M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset**
<br/>
<button class="copy-to-clipboard" title="M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset" index=32>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 16  
Keywords: Multi-modal, Multi-modal, Automatic Speech Recognition  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14168v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14168v1.pdf" filename="2403.14168v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Publishing open-source academic video recordings is an emergent and prevalent approach to sharing knowledge online. Such videos carry rich <b>multimodal</b> information including <b>speech,</b> <b>the</b> facial and body movements of the speakers, as well as the texts and pictures in the slides and possibly even the papers. Although multiple academic video datasets have been constructed and released, few of them support both <b>multimodal</b> content recognition and understanding tasks, which is partially due to the lack of high-quality human annotations. In this paper, we propose a novel <b>multimodal,</b> multigenre, and multipurpose audio-visual academic lecture dataset (M$^3$AV), which has almost 367 hours of videos from five sources covering computer science, mathematics, and medical and biology topics. With high-quality human annotations of the spoken and written words, in particular high-valued name entities, the dataset can be used for multiple audio-visual recognition and understanding tasks. Evaluations performed on contextual <b>speech</b> <b>recognition,</b> <b>speech</b> <b>synthesis,</b> and slide and script generation tasks demonstrate that the diversity of M$^3$AV makes it a challenging dataset.

{{</citation>}}


### (33/37 | 33/241) The Era of Semantic Decoding (Maxime Peyrard et al., 2024)

{{<citation>}}

Maxime Peyrard, Martin Josifoski, Robert West. (2024)  
**The Era of Semantic Decoding**
<br/>
<button class="copy-to-clipboard" title="The Era of Semantic Decoding" index=33>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-HC, cs-MA, cs.CL  
Keyword Score: 10  
Keywords: Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14562v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14562v1.pdf" filename="2403.14562v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent work demonstrated great promise in the idea of orchestrating collaborations between <b>LLMs,</b> human input, and various tools to address the inherent limitations of <b>LLMs.</b> We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize <b>LLMs</b> as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). <b>LLMs</b> are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.

{{</citation>}}


### (34/37 | 34/241) Emergent communication and learning pressures in language models: a language evolution perspective (Lukas Galke et al., 2024)

{{<citation>}}

Lukas Galke, Limor Raviv. (2024)  
**Emergent communication and learning pressures in language models: a language evolution perspective**
<br/>
<button class="copy-to-clipboard" title="Emergent communication and learning pressures in language models: a language evolution perspective" index=34>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: I-2-7, cs-CL, cs.CL  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14427v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14427v1.pdf" filename="2403.14427v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Language models and humans are two types of learning systems. Finding or facilitating commonalities could enable major breakthroughs in our understanding of the acquisition and evolution of language. Many theories of language evolution rely heavily on learning biases and learning pressures. Yet due to substantial differences in learning pressures, it is questionable whether the similarity between humans and machines is sufficient for insights to carry over and to be worth testing with human participants. Here, we review the emergent communication literature, a subfield of multi-agent <b>reinforcement</b> <b>learning,</b> from a language evolution perspective. We find that the emergent communication literature excels at designing and adapting models to recover initially absent linguistic phenomena of natural languages. Based on a short literature review, we identify key pressures that have recovered initially absent human patterns in emergent communication models: communicative success, efficiency, learnability, and other psycho-/sociolinguistic factors. We argue that this may serve as inspiration for how to design language models for language acquisition and language evolution research.

{{</citation>}}


### (35/37 | 35/241) Editing Knowledge Representation of Language Lodel via Rephrased Prefix Prompts (Yuchen Cai et al., 2024)

{{<citation>}}

Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, Enhong Chen. (2024)  
**Editing Knowledge Representation of Language Lodel via Rephrased Prefix Prompts**
<br/>
<button class="copy-to-clipboard" title="Editing Knowledge Representation of Language Lodel via Rephrased Prefix Prompts" index=35>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 10  
Keywords: Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14381v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14381v1.pdf" filename="2403.14381v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Neural language models (LMs) have been extensively trained on vast corpora to store factual knowledge about various aspects of the world described in texts. Current technologies typically employ knowledge editing methods or specific <b>prompts</b> to modify LM outputs. However, existing knowledge editing methods are costly and inefficient, struggling to produce appropriate text. Additionally, <b>prompt</b> engineering is opaque and requires significant effort to find suitable <b>prompts.</b> To address these issues, we introduce a new method called PSPEM (Prefix Soft <b>Prompt</b> Editing Method), that can be used for a lifetime with just one training. It resolves the inefficiencies and generalizability issues in knowledge editing methods and overcomes the opacity of <b>prompt</b> engineering by automatically seeking optimal soft <b>prompts.</b> Specifically, PSPEM utilizes a <b>prompt</b> encoder and an encoding converter to refine key information in <b>prompts</b> and uses <b>prompt</b> alignment techniques to guide model generation, ensuring text consistency and adherence to the intended structure and content, thereby maintaining an optimal balance between efficiency and accuracy. We have validated the effectiveness of PSPEM through knowledge editing and attribute inserting. On the COUNTERFACT dataset, PSPEM achieved nearly 100\% editing accuracy and demonstrated the highest level of fluency. We further analyzed the similarities between PSPEM and original <b>prompts</b> and their impact on the model's internals. The results indicate that PSPEM can serve as an alternative to original <b>prompts,</b> supporting the model in effective editing.

{{</citation>}}


### (36/37 | 36/241) Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives (Jiaxin Liu et al., 2024)

{{<citation>}}

Jiaxin Liu, Yi Yang, Kar Yan Tam. (2024)  
**Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives**
<br/>
<button class="copy-to-clipboard" title="Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives" index=36>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 10  
Keywords: Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14341v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14341v1.pdf" filename="2403.14341v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we introduce the Financial-STS task, a financial domain-specific NLP task designed to measure the nuanced semantic similarity between pairs of financial narratives. These narratives originate from the financial statements of the same company but correspond to different periods, such as year-over-year comparisons. Measuring the subtle semantic differences between these paired narratives enables market stakeholders to gauge changes over time in the company's financial and operational situations, which is critical for financial decision-making. We find that existing pretrained embedding models and <b>LLM</b> embeddings fall short in discerning these subtle financial narrative shifts. To address this gap, we propose an <b>LLM-augmented</b> pipeline specifically designed for the Financial-STS task. Evaluation on a human-annotated dataset demonstrates that our proposed method outperforms existing methods trained on classic STS tasks and generic <b>LLM</b> embeddings.

{{</citation>}}


### (37/37 | 37/241) Is Reference Necessary in the Evaluation of NLG Systems? When and Where? (Shuqian Sheng et al., 2024)

{{<citation>}}

Shuqian Sheng, Yi Xu, Luoyi Fu, Jiaxin Ding, Lei Zhou, Xinbing Wang, Chenghu Zhou. (2024)  
**Is Reference Necessary in the Evaluation of NLG Systems? When and Where?**
<br/>
<button class="copy-to-clipboard" title="Is Reference Necessary in the Evaluation of NLG Systems? When and Where?" index=37>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 10  
Keywords: Natural Language Generation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14275v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14275v1.pdf" filename="2403.14275v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The majority of automatic metrics for evaluating <b>NLG</b> systems are reference-based. However, the challenge of collecting human annotation results in a lack of reliable references in numerous application scenarios. Despite recent advancements in reference-free metrics, it has not been well understood when and where they can be used as an alternative to reference-based metrics. In this study, by employing diverse analytical approaches, we comprehensively assess the performance of both metrics across a wide range of <b>NLG</b> tasks, encompassing eight datasets and eight evaluation models. Based on solid experiments, the results show that reference-free metrics exhibit a higher correlation with human judgment and greater sensitivity to deficiencies in language quality. However, their effectiveness varies across tasks and is influenced by the quality of candidate texts. Therefore, it's important to assess the performance of reference-free metrics before applying them to a new task, especially when inputs are in uncommon form or when the answer space is highly variable. Our study can provide insight into the appropriate application of automatic metrics and the impact of metric choice on evaluation performance.

{{</citation>}}


## cs.AI (3)



### (1/3 | 38/241) ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training (Zonghan Yang et al., 2024)

{{<citation>}}

Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu. (2024)  
**ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training**
<br/>
<button class="copy-to-clipboard" title="ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training" index=38>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs-CL, cs-LG, cs.AI  
Keyword Score: 90  
Keywords: Fine-tuning, Fine-tuning, Foundation Model, GPT, GPT-4, Mistral, Reasoning, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14589v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14589v1.pdf" filename="2403.14589v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Language agents have demonstrated autonomous decision-making abilities by <b>reasoning</b> with <b>foundation</b> <b>models.</b> Recently, efforts have been made to train language agents for performance improvement, with multi-step <b>reasoning</b> and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse <b>prompting</b> frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe <b>prompting</b> agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior <b>reasoning</b> from ActRe to the sampled action. In this way, the ReAct-style agent executes multiple trajectories for the failed tasks, and selects the successful ones to supplement its failed trajectory for contrastive self-training. Realized by policy gradient methods with binarized rewards, the contrastive self-training with accumulated trajectories facilitates a closed loop for multiple rounds of language agent self-improvement. We conduct experiments using QLoRA <b>fine-tuning</b> with the open-sourced <b>Mistral-7B-Instruct-v0.2.</b> In AlfWorld, the agent trained with A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative rounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human average, and 4 rounds of iterative refinement lead to the performance approaching human experts. A$^3$T agents significantly outperform existing techniques, including <b>prompting</b> with <b>GPT-4,</b> advanced agent frameworks, and fully <b>fine-tuned</b> <b>LLMs.</b>

{{</citation>}}


### (2/3 | 39/241) Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics (Shan Jia et al., 2024)

{{<citation>}}

Shan Jia, Reilin Lyu, Kangran Zhao, Yize Chen, Zhiyuan Yan, Yan Ju, Chuanbo Hu, Xin Li, Baoyuan Wu, Siwei Lyu. (2024)  
**Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics**
<br/>
<button class="copy-to-clipboard" title="Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics" index=39>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs-CR, cs.AI  
Keyword Score: 46  
Keywords: Multi-modal, Multi-modal, ChatGPT, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14077v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14077v1.pdf" filename="2403.14077v1.pdf">Download PDF</button>

---


**ABSTRACT**  
DeepFakes, which refer to AI-generated media content, have become an increasing concern due to their use as a means for disinformation. Detecting DeepFakes is currently solved with programmed machine learning algorithms. In this work, we investigate the capabilities of <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in DeepFake detection. We conducted qualitative and quantitative experiments to demonstrate <b>multimodal</b> <b>LLMs</b> and show that they can expose AI-generated images through careful experimental design and <b>prompt</b> engineering. This is interesting, considering that <b>LLMs</b> are not inherently tailored for media forensic tasks, and the process does not require programming. We discuss the limitations of <b>multimodal</b> <b>LLMs</b> for these tasks and suggest possible improvements.

{{</citation>}}


### (3/3 | 40/241) DouRN: Improving DouZero by Residual Neural Networks (Yiquan Chen et al., 2024)

{{<citation>}}

Yiquan Chen, Yingchao Lyu, Di Zhang. (2024)  
**DouRN: Improving DouZero by Residual Neural Networks**
<br/>
<button class="copy-to-clipboard" title="DouRN: Improving DouZero by Residual Neural Networks" index=40>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs-LG, cs.AI  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14102v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14102v1.pdf" filename="2403.14102v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Deep <b>reinforcement</b> <b>learning</b> has made significant progress in games with imperfect information, but its performance in the card game Doudizhu (Chinese Poker/Fight the Landlord) remains unsatisfactory. Doudizhu is different from conventional games as it involves three players and combines elements of cooperation and confrontation, resulting in a large state and action space. In 2021, a Doudizhu program called DouZero\cite{zha2021douzero} surpassed previous models without prior knowledge by utilizing traditional Monte Carlo methods and multilayer perceptrons. Building on this work, our study incorporates residual networks into the model, explores different architectural designs, and conducts multi-role testing. Our findings demonstrate that this model significantly improves the winning rate within the same training time. Additionally, we introduce a call scoring system to assist the agent in deciding whether to become a landlord. With these enhancements, our model consistently outperforms the existing version of DouZero and even experienced human players. \footnote{The source code is available at \url{https://github.com/Yingchaol/Douzero_Resnet.git.}

{{</citation>}}


## cs.LG (27)



### (1/27 | 41/241) Exploring the Potential of Large Language Models in Graph Generation (Yang Yao et al., 2024)

{{<citation>}}

Yang Yao, Xin Wang, Zeyang Zhang, Yijian Qin, Ziwei Zhang, Xu Chu, Yuekui Yang, Wenwu Zhu, Hong Mei. (2024)  
**Exploring the Potential of Large Language Models in Graph Generation**
<br/>
<button class="copy-to-clipboard" title="Exploring the Potential of Large Language Models in Graph Generation" index=41>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG, q-bio-BM  
Keyword Score: 83  
Keywords: Node Classification, Graph, Few-shot, GPT, GPT-4, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14358v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14358v1.pdf" filename="2403.14358v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved great success in many fields, and recent works have studied exploring <b>LLMs</b> for <b>graph</b> discriminative tasks such as <b>node</b> <b>classification.</b> However, the abilities of <b>LLMs</b> for <b>graph</b> generation remain unexplored in the literature. <b>Graph</b> generation requires the <b>LLM</b> to generate <b>graphs</b> with given properties, which has valuable real-world applications such as drug discovery, while tends to be more challenging. In this paper, we propose LLM4GraphGen to explore the ability of <b>LLMs</b> for <b>graph</b> generation with systematical task designs and extensive experiments. Specifically, we propose several tasks tailored with comprehensive experiments to address key questions regarding <b>LLMs'</b> understanding of different <b>graph</b> structure rules, their ability to capture structural type distributions, and their utilization of domain knowledge for property-based <b>graph</b> generation. Our evaluations demonstrate that <b>LLMs,</b> particularly <b>GPT-4,</b> exhibit preliminary abilities in <b>graph</b> generation tasks, including rule-based and distribution-based generation. We also observe that popular <b>prompting</b> methods, such as <b>few-shot</b> and <b>chain-of-thought</b> <b>prompting,</b> do not consistently enhance performance. Besides, <b>LLMs</b> show potential in generating molecules with specific properties. These findings may serve as foundations for designing good <b>LLMs</b> based models for <b>graph</b> generation and provide valuable insights and further research.

{{</citation>}}


### (2/27 | 42/241) Exploring Task Unification in Graph Representation Learning via Generative Approach (Yulan Hu et al., 2024)

{{<citation>}}

Yulan Hu, Sheng Ouyang, Zhirui Yang, Ge Chen, Junchen Wan, Xiao Wang, Yong Liu. (2024)  
**Exploring Task Unification in Graph Representation Learning via Generative Approach**
<br/>
<button class="copy-to-clipboard" title="Exploring Task Unification in Graph Representation Learning via Generative Approach" index=42>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 68  
Keywords: Graph, Adversarial Learning, Autoencoder, Fine-tuning, Representation Learning, Self-supervised Learning, Transfer Learning, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14340v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14340v1.pdf" filename="2403.14340v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Graphs</b> are ubiquitous in real-world scenarios and encompass a diverse range of tasks, from node-, edge-, and <b>graph-level</b> tasks to <b>transfer</b> <b>learning.</b> However, designing specific tasks for each type of <b>graph</b> data is often costly and lacks generalizability. Recent endeavors under the "Pre-training + <b>Fine-tuning"</b> or "Pre-training + <b>Prompt"</b> paradigms aim to design a unified framework capable of generalizing across multiple <b>graph</b> tasks. Among these, <b>graph</b> <b>autoencoders</b> (GAEs), generative <b>self-supervised</b> models, have demonstrated their potential in effectively addressing various <b>graph</b> tasks. Nevertheless, these methods typically employ multi-stage training and require adaptive designs, which on one hand make it difficult to be seamlessly applied to diverse <b>graph</b> tasks and on the other hand overlook the negative impact caused by discrepancies in task objectives between the different stages. To address these challenges, we propose GA^2E, a unified adversarially masked <b>autoencoder</b> capable of addressing the above challenges seamlessly. Specifically, GA^2E proposes to use the subgraph as the meta-structure, which remains consistent across all <b>graph</b> tasks (ranging from node-, edge-, and <b>graph-level</b> to <b>transfer</b> <b>learning)</b> and all stages (both during training and inference). Further, GA^2E operates in a \textbf{"Generate then Discriminate"} manner. It leverages the masked GAE to reconstruct the input subgraph whilst treating it as a generator to compel the reconstructed <b>graphs</b> resemble the input subgraph. Furthermore, GA^2E introduces an auxiliary discriminator to discern the authenticity between the reconstructed (generated) subgraph and the input subgraph, thus ensuring the robustness of the <b>graph</b> <b>representation</b> <b>through</b> <b>adversarial</b> <b>training</b> mechanisms. We validate GA^2E's capabilities through extensive experiments on 21 datasets across four types of <b>graph</b> tasks.

{{</citation>}}


### (3/27 | 43/241) DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning (Jonathan Lebensold et al., 2024)

{{<citation>}}

Jonathan Lebensold, Maziar Sanjabi, Pietro Astolfi, Adriana Romero-Soriano, Kamalika Chaudhuri, Mike Rabbat, Chuan Guo. (2024)  
**DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning**
<br/>
<button class="copy-to-clipboard" title="DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning" index=43>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CR, cs-CV, cs-LG, cs.LG  
Keyword Score: 60  
Keywords: Diffusion Model, Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Text2image, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14421v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14421v1.pdf" filename="2403.14421v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Text-to-image</b> <b>diffusion</b> <b>models</b> have been shown to suffer from sample-level memorization, possibly reproducing near-perfect replica of images that they are trained on, which may be undesirable. To remedy this issue, we develop the first differentially private (DP) <b>retrieval-augmented</b> <b>generation</b> <b>algorithm</b> that is capable of generating high-quality image samples while providing provable privacy guarantees. Specifically, we assume access to a <b>text-to-image</b> <b>diffusion</b> <b>model</b> trained on a small amount of public data, and design a DP <b>retrieval</b> <b>mechanism</b> <b>to</b> augment the text <b>prompt</b> with samples retrieved from a private <b>retrieval</b> <b>dataset.</b> <b>Our</b> \emph{differentially private <b>retrieval-augmented</b> <b>diffusion</b> <b>model}</b> (DP-RDM) requires no <b>fine-tuning</b> on the <b>retrieval</b> <b>dataset</b> <b>to</b> adapt to another domain, and can use state-of-the-art generative models to generate high-quality image samples while satisfying rigorous DP guarantees. For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a privacy budget of $\epsilon=10$, while providing a $3.5$ point improvement in FID compared to public-only <b>retrieval</b> <b>for</b> <b>up</b> to $10,000$ queries.

{{</citation>}}


### (4/27 | 44/241) Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond (Wei Chen et al., 2024)

{{<citation>}}

Wei Chen, Yuxuan Liang, Yuanshao Zhu, Yanchuan Chang, Kang Luo, Haomin Wen, Lei Li, Yanwei Yu, Qingsong Wen, Chao Chen, Kai Zheng, Yunjun Gao, Xiaofang Zhou, Yu Zheng. (2024)  
**Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond**
<br/>
<button class="copy-to-clipboard" title="Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond" index=44>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CY, cs-DB, cs-LG, cs.LG  
Keyword Score: 50  
Keywords: Anomaly Detection, Recommendation, Large Language Model, Large Language Model, Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14151v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14151v1.pdf" filename="2403.14151v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Trajectory computing is a pivotal domain encompassing trajectory data management and mining, garnering widespread attention due to its crucial role in various practical applications such as location services, urban traffic, and public safety. Traditional methods, focusing on simplistic spatio-temporal features, face challenges of complex calculations, limited scalability, and inadequate adaptability to real-world complexities. In this paper, we present a comprehensive review of the development and recent advances in deep learning for trajectory computing (DL4Traj). We first define trajectory data and provide a brief overview of widely-used deep learning models. Systematically, we explore deep learning applications in trajectory management (pre-processing, storage, analysis, and visualization) and mining (trajectory-related forecasting, trajectory-related <b>recommendation,</b> trajectory classification, travel time estimation, <b>anomaly</b> <b>detection,</b> and mobility generation). Notably, we encapsulate recent advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> that hold the potential to augment trajectory computing. Additionally, we <b>summarize</b> application scenarios, public datasets, and toolkits. Finally, we outline current challenges in DL4Traj research and propose future directions. Relevant papers and open-source resources have been collated and are continuously updated at: \href{https://github.com/yoshall/Awesome-Trajectory-Computing}{DL4Traj Repo}.

{{</citation>}}


### (5/27 | 45/241) AI and Memory Wall (Amir Gholami et al., 2024)

{{<citation>}}

Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman Hooper, Michael W. Mahoney, Kurt Keutzer. (2024)  
**AI and Memory Wall**
<br/>
<button class="copy-to-clipboard" title="AI and Memory Wall" index=45>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AR, cs-DC, cs-LG, cs.LG  
Keyword Score: 40  
Keywords: Unsupervised Learning, Transformer, Large Language Model, Scaling Law  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14123v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14123v1.pdf" filename="2403.14123v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The availability of unprecedented <b>unsupervised</b> training data, along with neural <b>scaling</b> <b>laws,</b> has resulted in an unprecedented surge in model size and compute requirements for serving/training <b>LLMs.</b> However, the main performance bottleneck is increasingly shifting to memory bandwidth. Over the past 20 years, peak server hardware FLOPS has been <b>scaling</b> <b>at</b> 3.0x/2yrs, outpacing the growth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and 1.4 times every 2 years, respectively. This disparity has made memory, rather than compute, the primary bottleneck in AI applications, particularly in serving. Here, we analyze encoder and decoder <b>Transformer</b> models and show how memory bandwidth can become the dominant bottleneck for decoder models. We argue for a redesign in model architecture, training, and deployment strategies to overcome this memory limitation.

{{</citation>}}


### (6/27 | 46/241) Heuristic Algorithm-based Action Masking Reinforcement Learning (HAAM-RL) with Ensemble Inference Method (Kyuwon Choi et al., 2024)

{{<citation>}}

Kyuwon Choi, Cheolkyun Rho, Taeyoun Kim, Daewoo Choi. (2024)  
**Heuristic Algorithm-based Action Masking Reinforcement Learning (HAAM-RL) with Ensemble Inference Method**
<br/>
<button class="copy-to-clipboard" title="Heuristic Algorithm-based Action Masking Reinforcement Learning (HAAM-RL) with Ensemble Inference Method" index=46>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 40  
Keywords: Markov Decision Process, Reinforcement Learning, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14110v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14110v1.pdf" filename="2403.14110v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents a novel <b>reinforcement</b> <b>learning</b> (RL) approach called HAAM-RL (Heuristic Algorithm-based Action Masking <b>Reinforcement</b> <b>Learning)</b> for optimizing the color batching re-sequencing problem in automobile painting processes. The existing heuristic algorithms have limitations in adequately reflecting real-world constraints and accurately predicting logistics performance. Our methodology incorporates several key techniques including a tailored <b>Markov</b> <b>Decision</b> <b>Process</b> (MDP) formulation, reward setting including Potential-Based Reward Shaping, action masking using heuristic algorithms (HAAM-RL), and an ensemble inference method that combines multiple RL models. The RL agent is trained and evaluated using FlexSim, a commercial 3D <b>simulation</b> software, integrated with our RL MLOps platform BakingSoDA. Experimental results across 30 scenarios demonstrate that HAAM-RL with an ensemble inference method achieves a 16.25% performance improvement over the conventional heuristic algorithm, with stable and consistent results. The proposed approach exhibits superior performance and generalization capability, indicating its effectiveness in optimizing complex manufacturing processes. The study also discusses future research directions, including alternative state representations, incorporating model-based RL methods, and integrating additional real-world constraints.

{{</citation>}}


### (7/27 | 47/241) RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain (William James Bolton et al., 2024)

{{<citation>}}

William James Bolton, Rafael Poyiadzi, Edward R. Morrell, Gabriela van Bergen Gonzalez Bueno, Lea Goetz. (2024)  
**RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain**
<br/>
<button class="copy-to-clipboard" title="RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain" index=47>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 30  
Keywords: Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14578v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14578v1.pdf" filename="2403.14578v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> increasingly support applications in a wide range of domains, some with potential high societal impact such as biomedicine, yet their reliability in realistic use cases is under-researched. In this work we introduce the Reliability AssesMent for Biomedical <b>LLM</b> Assistants (RAmBLA) framework and evaluate whether four state-of-the-art foundation <b>LLMs</b> can serve as reliable assistants in the biomedical domain. We identify <b>prompt</b> robustness, high recall, and a lack of hallucinations as necessary criteria for this use case. We design shortform tasks and tasks requiring <b>LLM</b> freeform responses mimicking real-world user interactions. We evaluate <b>LLM</b> performance using semantic similarity with a ground truth response, through an evaluator <b>LLM.</b>

{{</citation>}}


### (8/27 | 48/241) Task-optimal data-driven surrogate models for eNMPC via differentiable simulation and optimization (Daniel Mayfrank et al., 2024)

{{<citation>}}

Daniel Mayfrank, Na Young Ahn, Alexander Mitsos, Manuel Dahmen. (2024)  
**Task-optimal data-driven surrogate models for eNMPC via differentiable simulation and optimization**
<br/>
<button class="copy-to-clipboard" title="Task-optimal data-driven surrogate models for eNMPC via differentiable simulation and optimization" index=48>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, math-OC  
Keyword Score: 30  
Keywords: Reinforcement Learning, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14425v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14425v1.pdf" filename="2403.14425v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present a method for end-to-end learning of Koopman surrogate models for optimal performance in control. In contrast to previous contributions that employ standard <b>reinforcement</b> <b>learning</b> (RL) algorithms, we use a training algorithm that exploits the potential differentiability of environments based on mechanistic <b>simulation</b> models. We evaluate the performance of our method by comparing it to that of other controller type and training algorithm combinations on a literature known eNMPC case study. Our method exhibits superior performance on this problem, thereby constituting a promising avenue towards more capable controllers that employ dynamic surrogate models.

{{</citation>}}


### (9/27 | 49/241) Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning (Fazal Muhammad Ali Khan et al., 2024)

{{<citation>}}

Fazal Muhammad Ali Khan, Hatem Abou-Zeid, Aryan Kaushik, Syed Ali Hassan. (2024)  
**Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning**
<br/>
<button class="copy-to-clipboard" title="Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning" index=49>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG, eess-SP  
Keyword Score: 30  
Keywords: Federated Learning, Model Compression, Pruning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14120v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14120v1.pdf" filename="2403.14120v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The industrial Internet of Things (IIoT) under Industry 4.0 heralds an era of interconnected smart devices where data-driven insights and machine learning (ML) fuse to revolutionize manufacturing. A noteworthy development in IIoT is the integration of <b>federated</b> <b>learning</b> (FL), which addresses data privacy and security among devices. FL enables edge sensors, also known as peripheral intelligence units (PIUs) to learn and adapt using their data locally, without explicit sharing of confidential data, to facilitate a collaborative yet confidential learning process. However, the lower memory footprint and computational power of PIUs inherently require deep neural network (DNN) <b>models</b> <b>that</b> have a very compact size. <b>Model</b> <b>compression</b> techniques such as <b>pruning</b> can be used to reduce the size of DNN <b>models</b> <b>by</b> removing unnecessary connections that have little impact on the <b>model's</b> <b>performance,</b> thus making the <b>models</b> <b>more</b> suitable for the limited resources of PIUs. Targeting the notion of compact yet robust DNN <b>models,</b> <b>we</b> propose the integration of iterative magnitude <b>pruning</b> (IMP) of the DNN <b>model</b> <b>being</b> trained in an over-the-air FL (OTA-FL) environment for IIoT. We provide a tutorial overview and also present a case study of the effectiveness of IMP in OTA-FL for an IIoT environment. Finally, we present future directions for enhancing and optimizing these deep compression techniques further, aiming to push the boundaries of IIoT capabilities in acquiring compact yet robust and high-performing DNN models.

{{</citation>}}


### (10/27 | 50/241) DomainLab: A modular Python package for domain generalization in deep learning (Xudong Sun et al., 2024)

{{<citation>}}

Xudong Sun, Carla Feistner, Alexej Gossmann, George Schwarz, Rao Muhammad Umer, Lisa Beer, Patrick Rockenschaub, Rahul Babu Shrestha, Armin Gruber, Nutan Chen, Sayedali Shetab Boushehri, Florian Buettner, Carsten Marr. (2024)  
**DomainLab: A modular Python package for domain generalization in deep learning**
<br/>
<button class="copy-to-clipboard" title="DomainLab: A modular Python package for domain generalization in deep learning" index=50>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs-SE, cs.LG  
Keyword Score: 26  
Keywords: Benchmarking, Benchmarking, Distribution Shift, Distribution Shift, Out-of-distribution  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14356v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14356v1.pdf" filename="2403.14356v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Poor generalization performance caused by <b>distribution</b> <b>shifts</b> in unseen domains often hinders the trustworthy deployment of deep neural networks. Many domain generalization techniques address this problem by adding a domain invariant regularization loss terms during training. However, there is a lack of modular software that allows users to combine the advantages of different methods with minimal effort for reproducibility. DomainLab is a modular Python package for training user specified neural networks with composable regularization loss terms. Its decoupled design allows the separation of neural networks from regularization loss construction. Hierarchical combinations of neural networks, different domain generalization methods, and associated hyperparameters, can all be specified together with other experimental setup in a single configuration file. Hierarchical combinations of neural networks, different domain generalization methods, and associated hyperparameters, can all be specified together with other experimental setup in a single configuration file. In addition, DomainLab offers powerful <b>benchmarking</b> functionality to evaluate the generalization performance of neural networks in <b>out-of-distribution</b> data. The package supports running the specified <b>benchmark</b> on an HPC cluster or on a standalone machine. The package is well tested with over 95 percent coverage and well documented. From the user perspective, it is closed to modification but open to extension. The package is under the MIT license, and its source code, tutorial and documentation can be found at https://github.com/marrlab/DomainLab.

{{</citation>}}


### (11/27 | 51/241) DiffSTOCK: Probabilistic relational Stock Market Predictions using Diffusion Models (Divyanshu Daiya et al., 2024)

{{<citation>}}

Divyanshu Daiya, Monika Yadav, Harshit Singh Rao. (2024)  
**DiffSTOCK: Probabilistic relational Stock Market Predictions using Diffusion Models**
<br/>
<button class="copy-to-clipboard" title="DiffSTOCK: Probabilistic relational Stock Market Predictions using Diffusion Models" index=51>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CE, cs-LG, cs.LG, q-fin-CP, q-fin-PM  
Keyword Score: 23  
Keywords: Diffusion Model, Graph, Probabilistic Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14063v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14063v1.pdf" filename="2403.14063v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this work, we propose an approach to generalize denoising <b>diffusion</b> <b>probabilistic</b> <b>models</b> for stock market predictions and portfolio management. Present works have demonstrated the efficacy of modeling interstock relations for market time-series forecasting and utilized <b>Graph-based</b> learning models for value prediction and portfolio management. Though convincing, these deterministic approaches still fall short of handling uncertainties i.e., due to the low signal-to-noise ratio of the financial data, it is quite challenging to learn effective deterministic models. Since the <b>probabilistic</b> <b>methods</b> have shown to effectively emulate higher uncertainties for time-series predictions. To this end, we showcase effective utilisation of Denoising <b>Diffusion</b> <b>Probabilistic</b> <b>Models</b> (DDPM), to develop an architecture for providing better market predictions conditioned on the historical financial indicators and inter-stock relations. Additionally, we also provide a novel deterministic architecture MaTCHS which uses Masked Relational Transformer(MRT) to exploit inter-stock relations along with historical stock features. We demonstrate that our model achieves SOTA performance for movement predication and Portfolio management.

{{</citation>}}


### (12/27 | 52/241) Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey (Zeyu Han et al., 2024)

{{<citation>}}

Zeyu Han, Chao Gao, Jinyang Liu, Jeff, Zhang, Sai Qian Zhang. (2024)  
**Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey**
<br/>
<button class="copy-to-clipboard" title="Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey" index=52>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 20  
Keywords: Fine-tuning, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14608v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14608v1.pdf" filename="2403.14608v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>models</b> <b>represent</b> a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient <b>Fine-Tuning</b> (PEFT) provides a practical solution by efficiently adapt the <b>large</b> <b>models</b> <b>over</b> the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained <b>large</b> <b>models</b> <b>to</b> adapt it to a specific task while minimizing the number of additional parameters introduced or computational resources required. This approach is particularly important when dealing with <b>large</b> <b>language</b> <b>models</b> with high parameter counts, as <b>fine-tuning</b> these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design. In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead. Moreover, we provide an overview of applications developed using different PEFT algorithms and discuss common techniques employed to mitigate computation costs for PEFT. In addition to the algorithmic perspective, we overview various real-world system designs to investigate the implementation costs associated with different PEFT algorithms. This survey serves as an indispensable resource for researchers aiming to understand both the PEFT algorithm and its system implementation, offering detailed insights into recent advancements and practical applications.

{{</citation>}}


### (13/27 | 53/241) Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery (Yangchun Zhang et al., 2024)

{{<citation>}}

Yangchun Zhang, Yirui Zhou. (2024)  
**Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery**
<br/>
<button class="copy-to-clipboard" title="Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery" index=53>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, stat-ML  
Keyword Score: 20  
Keywords: Markov Decision Process, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14593v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14593v1.pdf" filename="2403.14593v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Adversarial inverse <b>reinforcement</b> <b>learning</b> (AIRL) stands as a cornerstone approach in imitation learning. This paper rethinks the two different angles of AIRL: policy imitation and transferable reward recovery. We begin with substituting the built-in algorithm in AIRL with soft actor-critic (SAC) during the policy optimization process to enhance sample efficiency, thanks to the off-policy formulation of SAC and identifiable <b>Markov</b> <b>decision</b> <b>process</b> (MDP) models with respect to AIRL. It indeed exhibits a significant improvement in policy imitation but accidentally brings drawbacks to transferable reward recovery. To learn this issue, we illustrate that the SAC algorithm itself is not feasible to disentangle the reward function comprehensively during the AIRL training process, and propose a hybrid framework, PPO-AIRL + SAC, for satisfactory transfer effect. Additionally, we analyze the capability of environments to extract disentangled rewards from an algebraic theory perspective.

{{</citation>}}


### (14/27 | 54/241) Contrastive Balancing Representation Learning for Heterogeneous Dose-Response Curves Estimation (Minqin Zhu et al., 2024)

{{<citation>}}

Minqin Zhu, Anpeng Wu, Haoxuan Li, Ruoxuan Xiong, Bo Li, Xiaoqing Yang, Xuan Qin, Peng Zhen, Jiecheng Guo, Fei Wu, Kun Kuang. (2024)  
**Contrastive Balancing Representation Learning for Heterogeneous Dose-Response Curves Estimation**
<br/>
<button class="copy-to-clipboard" title="Contrastive Balancing Representation Learning for Heterogeneous Dose-Response Curves Estimation" index=54>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 15  
Keywords: Counter-factual, Representation Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14232v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14232v1.pdf" filename="2403.14232v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Estimating the individuals' potential response to varying treatment doses is crucial for decision-making in areas such as precision medicine and management science. Most recent studies predict <b>counterfactual</b> outcomes by learning a covariate <b>representation</b> <b>that</b> is independent of the treatment variable. However, such independence constraints neglect much of the covariate information that is useful for <b>counterfactual</b> prediction, especially when the treatment variables are continuous. To tackle the above issue, in this paper, we first theoretically demonstrate the importance of the balancing and prognostic <b>representations</b> <b>for</b> unbiased estimation of the heterogeneous dose-response curves, that is, the learned <b>representations</b> <b>are</b> constrained to satisfy the conditional independence between the covariates and both of the treatment variables and the potential responses. Based on this, we propose a novel Contrastive balancing <b>Representation</b> <b>learning</b> Network using a partial distance measure, called CRNet, for estimating the heterogeneous dose-response curves without losing the continuity of treatments. Extensive experiments are conducted on synthetic and real-world datasets demonstrating that our proposal significantly outperforms previous methods.

{{</citation>}}


### (15/27 | 55/241) Hypothesis-Driven Deep Learning for Out of Distribution Detection (Yasith Jayawardana et al., 2024)

{{<citation>}}

Yasith Jayawardana, Azeem Ahmad, Balpreet S. Ahluwalia, Rafi Ahmad, Sampath Jayarathna, Dushan N. Wadduwage. (2024)  
**Hypothesis-Driven Deep Learning for Out of Distribution Detection**
<br/>
<button class="copy-to-clipboard" title="Hypothesis-Driven Deep Learning for Out of Distribution Detection" index=55>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, stat-ML  
Keyword Score: 15  
Keywords: Black Box, Out-of-distribution  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14058v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14058v1.pdf" filename="2403.14058v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Predictions of opaque <b>black-box</b> <b>systems</b> are frequently deployed in high-stakes applications such as healthcare. For such applications, it is crucial to assess how models handle samples beyond the domain of training data. While several metrics and tests exist to detect <b>out-of-distribution</b> (OoD) data from in-distribution (InD) data to a deep neural network (DNN), their performance varies significantly across datasets, models, and tasks, which limits their practical use. In this paper, we propose a hypothesis-driven approach to quantify whether a new sample is InD or OoD. Given a trained DNN and some input, we first feed the input through the DNN and compute an ensemble of OoD metrics, which we term latent responses. We then formulate the OoD detection problem as a hypothesis test between latent responses of different groups, and use permutation-based resampling to infer the significance of the observed latent responses under a null hypothesis. We adapt our method to detect an unseen sample of bacteria to a trained deep learning model, and show that it reveals interpretable differences between InD and OoD latent responses. Our work has implications for systematic novelty detection and informed decision-making from classifiers trained on a subset of labels.

{{</citation>}}


### (16/27 | 56/241) Soft Learning Probabilistic Circuits (Soroush Ghandi et al., 2024)

{{<citation>}}

Soroush Ghandi, Benjamin Quost, Cassio de Campos. (2024)  
**Soft Learning Probabilistic Circuits**
<br/>
<button class="copy-to-clipboard" title="Soft Learning Probabilistic Circuits" index=56>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 13  
Keywords: Clustering, Probabilistic Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14504v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14504v1.pdf" filename="2403.14504v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Probabilistic</b> <b>Circuits</b> (PCs) are prominent tractable <b>probabilistic</b> <b>models,</b> allowing for a range of exact inferences. This paper focuses on the main algorithm for training PCs, LearnSPN, a gold standard due to its efficiency, performance, and ease of use, in particular for tabular data. We show that LearnSPN is a greedy likelihood maximizer under mild assumptions. While inferences in PCs may use the entire circuit structure for processing queries, LearnSPN applies a hard method for learning them, propagating at each sum node a data point through one and only one of the children/edges as in a hard <b>clustering</b> process. We propose a new learning procedure named SoftLearn, that induces a PC using a soft <b>clustering</b> process. We investigate the effect of this learning-inference compatibility in PCs. Our experiments show that SoftLearn outperforms LearnSPN in many situations, yielding better likelihoods and arguably better samples. We also analyze comparable tractable models to highlight the differences between soft/hard learning and model querying.

{{</citation>}}


### (17/27 | 57/241) HyperGALE: ASD Classification via Hypergraph Gated Attention with Learnable Hyperedges (Mehul Arora et al., 2024)

{{<citation>}}

Mehul Arora, Chirag Shantilal Jain, Lalith Bharadwaj Baru, Kamalaker Dadi, Bapi Raju Surampudi. (2024)  
**HyperGALE: ASD Classification via Hypergraph Gated Attention with Learnable Hyperedges**
<br/>
<button class="copy-to-clipboard" title="HyperGALE: ASD Classification via Hypergraph Gated Attention with Learnable Hyperedges" index=57>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CV, cs-LG, cs-NE, cs.LG  
Keyword Score: 13  
Keywords: Graph Attention Networks, Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14484v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14484v1.pdf" filename="2403.14484v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Autism Spectrum Disorder (ASD) is a neurodevelopmental condition characterized by varied social cognitive challenges and repetitive behavioral patterns. Identifying reliable brain imaging-based biomarkers for ASD has been a persistent challenge due to the spectrum's diverse symptomatology. Existing baselines in the field have made significant strides in this direction, yet there remains room for improvement in both performance and interpretability. We propose \emph{HyperGALE}, which builds upon the hypergraph by incorporating learned hyperedges and <b>gated</b> attention mechanisms. This approach has led to substantial improvements in the model's ability to interpret complex brain <b>graph</b> data, offering deeper insights into ASD biomarker characterization. Evaluated on the extensive ABIDE II dataset, \emph{HyperGALE} not only improves interpretability but also demonstrates statistically significant enhancements in key performance metrics compared to both previous baselines and the foundational hypergraph model. The advancement \emph{HyperGALE} brings to ASD research highlights the potential of sophisticated <b>graph-based</b> techniques in neurodevelopmental studies. The source code and implementation instructions are available at GitHub:https://github.com/mehular0ra/HyperGALE.

{{</citation>}}


### (18/27 | 58/241) Constrained Reinforcement Learning with Smoothed Log Barrier Function (Baohe Zhang et al., 2024)

{{<citation>}}

Baohe Zhang, Yuan Zhang, Lilli Frison, Thomas Brox, Joschka Bödecker. (2024)  
**Constrained Reinforcement Learning with Smoothed Log Barrier Function**
<br/>
<button class="copy-to-clipboard" title="Constrained Reinforcement Learning with Smoothed Log Barrier Function" index=58>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs-SY, cs.LG, eess-SY  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14508v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14508v1.pdf" filename="2403.14508v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Reinforcement</b> <b>Learning</b> (RL) has been widely applied to many control tasks and substantially improved the performances compared to conventional control methods in many domains where the reward function is well defined. However, for many real-world problems, it is often more convenient to formulate optimization problems in terms of rewards and constraints simultaneously. Optimizing such constrained problems via reward shaping can be difficult as it requires tedious manual tuning of reward functions with several interacting terms. Recent formulations which include constraints mostly require a pre-training phase, which often needs human expertise to collect data or assumes having a sub-optimal policy readily available. We propose a new constrained RL method called CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which achieves competitive performance without any pre-training by applying a linear smoothed log barrier function to an additional safety critic. It implements an adaptive penalty for policy learning and alleviates the numerical issues that are known to complicate the application of the log barrier function method. As a result, we show that with CSAC-LB, we achieve state-of-the-art performance on several constrained control tasks with different levels of difficulty and evaluate our methods in a locomotion task on a real quadruped robot platform.

{{</citation>}}


### (19/27 | 59/241) Universal Feature Selection for Simultaneous Interpretability of Multitask Datasets (Matt Raymond et al., 2024)

{{<citation>}}

Matt Raymond, Jacob Charles Saldinger, Paolo Elvati, Clayton Scott, Angela Violi. (2024)  
**Universal Feature Selection for Simultaneous Interpretability of Multitask Datasets**
<br/>
<button class="copy-to-clipboard" title="Universal Feature Selection for Simultaneous Interpretability of Multitask Datasets" index=59>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Knowledge Transfer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14466v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14466v1.pdf" filename="2403.14466v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Extracting meaningful features from complex, high-dimensional datasets across scientific domains remains challenging. Current methods often struggle with scalability, limiting their applicability to large datasets, or make restrictive assumptions about feature-property relationships, hindering their ability to capture complex interactions. BoUTS's general and scalable feature selection algorithm surpasses these limitations to identify both universal features relevant to all datasets and task-specific features predictive for specific subsets. Evaluated on seven diverse chemical regression datasets, BoUTS achieves state-of-the-art feature sparsity while maintaining prediction accuracy comparable to specialized methods. Notably, BoUTS's universal features enable domain-specific <b>knowledge</b> <b>transfer</b> between datasets, and suggest deep connections in seemingly-disparate chemical datasets. We expect these results to have important repercussions in manually-guided inverse problems. Beyond its current application, BoUTS holds immense potential for elucidating data-poor systems by leveraging information from similar data-rich systems. BoUTS represents a significant leap in cross-domain feature selection, potentially leading to advancements in various scientific fields.

{{</citation>}}


### (20/27 | 60/241) Physics-Informed Diffusion Models (Jan-Hendrik Bastek et al., 2024)

{{<citation>}}

Jan-Hendrik Bastek, WaiChing Sun, Dennis M. Kochmann. (2024)  
**Physics-Informed Diffusion Models**
<br/>
<button class="copy-to-clipboard" title="Physics-Informed Diffusion Models" index=60>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CE, cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Diffusion Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14404v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14404v1.pdf" filename="2403.14404v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Generative models such as denoising <b>diffusion</b> <b>models</b> are quickly advancing their ability to approximate highly complex data distributions. They are also increasingly leveraged in scientific machine learning, where samples from the implied data distribution are expected to adhere to specific governing equations. We present a framework to inform denoising <b>diffusion</b> <b>models</b> on underlying constraints on such generated samples during model training. Our approach improves the alignment of the generated samples with the imposed constraints and significantly outperforms existing methods without affecting inference speed. Additionally, our findings suggest that incorporating such constraints during training provides a natural regularization against overfitting. Our framework is easy to implement and versatile in its applicability for imposing equality and inequality constraints as well as auxiliary optimization objectives.

{{</citation>}}


### (21/27 | 61/241) Loop Improvement: An Efficient Approach for Extracting Shared Features from Heterogeneous Data without Central Server (Fei Li et al., 2024)

{{<citation>}}

Fei Li, Chu Kiong Loo, Wei Shiung Liew, Xiaofeng Liu. (2024)  
**Loop Improvement: An Efficient Approach for Extracting Shared Features from Heterogeneous Data without Central Server**
<br/>
<button class="copy-to-clipboard" title="Loop Improvement: An Efficient Approach for Extracting Shared Features from Heterogeneous Data without Central Server" index=61>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-DC, cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Federated Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14371v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14371v1.pdf" filename="2403.14371v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In <b>federated</b> <b>learning,</b> data heterogeneity significantly impacts performance. A typical solution involves segregating these parameters into shared and personalized components, a concept also relevant in multi-task learning. Addressing this, we propose "Loop Improvement" (LI), a novel method enhancing this separation and feature extraction without necessitating a central server or data interchange among participants. Our experiments reveal LI's superiority in several aspects: In personalized <b>federated</b> <b>learning</b> environments, LI consistently outperforms the advanced FedALA algorithm in accuracy across diverse scenarios. Additionally, LI's feature extractor closely matches the performance achieved when aggregating data from all clients. In global model contexts, employing LI with stacked personalized layers and an additional network also yields comparable results to combined client data scenarios. Furthermore, LI's adaptability extends to multi-task learning, streamlining the extraction of common features across tasks and obviating the need for simultaneous training. This approach not only enhances individual task performance but also achieves accuracy levels on par with classic multi-task learning methods where all tasks are trained simultaneously. LI integrates a loop topology with layer-wise and end-to-end training, compatible with various neural network models. This paper also delves into the theoretical underpinnings of LI's effectiveness, offering insights into its potential applications. The code is on https://github.com/axedge1983/LI

{{</citation>}}


### (22/27 | 62/241) $\nabla τ$: Gradient-based and Task-Agnostic machine Unlearning (Daniel Trippa et al., 2024)

{{<citation>}}

Daniel Trippa, Cesare Campagnano, Maria Sofia Bucarelli, Gabriele Tolomei, Fabrizio Silvestri. (2024)  
**$\nabla τ$: Gradient-based and Task-Agnostic machine Unlearning**
<br/>
<button class="copy-to-clipboard" title="$\nabla τ$: Gradient-based and Task-Agnostic machine Unlearning" index=62>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Machine Unlearning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14339v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14339v1.pdf" filename="2403.14339v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Machine</b> <b>Unlearning,</b> the process of selectively eliminating the influence of certain data examples used during a model's training, has gained significant attention as a means for practitioners to comply with recent data protection regulations. However, existing unlearning methods face critical drawbacks, including their prohibitively high cost, often associated with a large number of hyperparameters, and the limitation of forgetting only relatively small data portions. This often makes retraining the model from scratch a quicker and more effective solution. In this study, we introduce Gradient-based and Task-Agnostic <b>machine</b> <b>Unlearning</b> ($\nabla \tau$), an optimization framework designed to remove the influence of a subset of training data efficiently. It applies adaptive gradient ascent to the data to be forgotten while using standard gradient descent for the remaining data. $\nabla \tau$ offers multiple benefits over existing approaches. It enables the unlearning of large sections of the training dataset (up to 30%). It is versatile, supporting various unlearning tasks (such as subset forgetting or class removal) and applicable across different domains (images, text, etc.). Importantly, $\nabla \tau$ requires no hyperparameter adjustments, making it a more appealing option than retraining the model from scratch. We evaluate our framework's effectiveness using a set of well-established Membership Inference Attack metrics, demonstrating up to 10% enhancements in performance compared to state-of-the-art methods without compromising the original model's accuracy.

{{</citation>}}


### (23/27 | 63/241) How to be fair? A study of label and selection bias (Marco Favier et al., 2024)

{{<citation>}}

Marco Favier, Toon Calders, Sam Pinxteren, Jonathan Meyer. (2024)  
**How to be fair? A study of label and selection bias**
<br/>
<button class="copy-to-clipboard" title="How to be fair? A study of label and selection bias" index=63>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CY, cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Fairness  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14282v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14282v1.pdf" filename="2403.14282v1.pdf">Download PDF</button>

---


**ABSTRACT**  
It is widely accepted that biased data leads to biased and thus potentially unfair models. Therefore, several measures for bias in data and model predictions have been proposed, as well as bias mitigation techniques whose aim is to learn models that are fair by design. Despite the myriad of mitigation techniques developed in the past decade, however, it is still poorly understood under what circumstances which methods work. Recently, Wick et al. showed, with experiments on synthetic data, that there exist situations in which bias mitigation techniques lead to more accurate models when measured on unbiased data. Nevertheless, in the absence of a thorough mathematical analysis, it remains unclear which techniques are effective under what circumstances. We propose to address this problem by establishing relationships between the type of bias and the effectiveness of a mitigation technique, where we categorize the mitigation techniques by the bias measure they optimize. In this paper we illustrate this principle for label and selection bias on the one hand, and demographic parity and ``We're All Equal'' on the other hand. Our theoretical analysis allows to explain the results of Wick et al. and we also show that there are situations where minimizing <b>fairness</b> measures does not result in the fairest possible distribution.

{{</citation>}}


### (24/27 | 64/241) A Unified Framework for Model Editing (Akshat Gupta et al., 2024)

{{<citation>}}

Akshat Gupta, Dev Sajnani, Gopala Anumanchipalli. (2024)  
**A Unified Framework for Model Editing**
<br/>
<button class="copy-to-clipboard" title="A Unified Framework for Model Editing" index=64>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CL, cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14236v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14236v1.pdf" filename="2403.14236v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading "locate-and-edit" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the "preservation-memorization" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objective of MEMIT and show that these edit-distribution algorithms should be considered separate entities worthy of their own line of research. Finally, we present EMMET - an Equality-constrained Mass Model Editing algorithm for <b>Transformers,</b> a new batched memory-editing algorithm. With EMMET, we present a closed form solution for the equality-constrained version of the preservation-memorization objective. We show that EMMET is able to perform batched-edits on par with MEMIT up to a batch-size of 256 and discuss the challenges in stabilizing EMMET. By articulating the "locate-and-edit" model editing algorithms under a simple conceptual framework of "preservation-memorization", we aim to bridge the gap between intuition and mathematics and hope to simplify the journey for future researchers in model editing.

{{</citation>}}


### (25/27 | 65/241) Policy Mirror Descent with Lookahead (Kimon Protopapas et al., 2024)

{{<citation>}}

Kimon Protopapas, Anas Barakat. (2024)  
**Policy Mirror Descent with Lookahead**
<br/>
<button class="copy-to-clipboard" title="Policy Mirror Descent with Lookahead" index=65>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG, stat-ML  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14156v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14156v1.pdf" filename="2403.14156v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Policy Mirror Descent (PMD) stands as a versatile algorithmic framework encompassing several seminal policy gradient algorithms such as natural policy gradient, with connections with state-of-the-art <b>reinforcement</b> <b>learning</b> (RL) algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration algorithm implementing regularized 1-step greedy policy improvement. However, 1-step greedy policies might not be the best choice and recent remarkable empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that greedy approaches with respect to multiple steps outperform their 1-step counterpart. In this work, we propose a new class of PMD algorithms called $h$-PMD which incorporates multi-step greedy policy improvement with lookahead depth $h$ to the PMD update rule. To solve discounted infinite horizon Markov Decision Processes with discount factor $\gamma$, we show that $h$-PMD which generalizes the standard PMD enjoys a faster dimension-free $\gamma^h$-linear convergence rate, contingent on the computation of multi-step greedy policies. We propose an inexact version of $h$-PMD where lookahead action values are estimated. Under a generative model, we establish a sample complexity for $h$-PMD which improves over prior work. Finally, we extend our result to linear function approximation to scale to large state spaces. Under suitable assumptions, our sample complexity only involves dependence on the dimension of the feature map space instead of the state space size.

{{</citation>}}


### (26/27 | 66/241) Carbon Footprint Reduction for Sustainable Data Centers in Real-Time (Soumyendu Sarkar et al., 2024)

{{<citation>}}

Soumyendu Sarkar, Avisek Naug, Ricardo Luna, Antonio Guillen, Vineet Gundecha, Sahand Ghorbanpour, Sajad Mousavi, Dejan Markovikj, Ashwin Ramesh Babu. (2024)  
**Carbon Footprint Reduction for Sustainable Data Centers in Real-Time**
<br/>
<button class="copy-to-clipboard" title="Carbon Footprint Reduction for Sustainable Data Centers in Real-Time" index=66>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs-MA, cs-SY, cs.LG, eess-SY  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14092v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14092v1.pdf" filename="2403.14092v1.pdf">Download PDF</button>

---


**ABSTRACT**  
As machine learning workloads significantly increase energy consumption, sustainable data centers with low carbon emissions are becoming a top priority for governments and corporations worldwide. This requires a paradigm shift in optimizing power consumption in cooling and IT loads, shifting flexible loads based on the availability of renewable energy in the power grid, and leveraging battery storage from the uninterrupted power supply in data centers, using collaborative agents. The complex association between these optimization strategies and their dependencies on variable external factors like weather and the power grid carbon intensity makes this a hard problem. Currently, a real-time controller to optimize all these goals simultaneously in a dynamic real-world setting is lacking. We propose a Data Center Carbon Footprint Reduction (DC-CFR) multi-agent <b>Reinforcement</b> <b>Learning</b> (MARL) framework that optimizes data centers for the multiple objectives of carbon footprint reduction, energy consumption, and energy cost. The results show that the DC-CFR MARL agents effectively resolved the complex interdependencies in optimizing cooling, load shifting, and energy storage in real-time for various locations under real-world dynamic weather and grid carbon intensity conditions. DC-CFR significantly outperformed the industry standard ASHRAE controller with a considerable reduction in carbon emissions (14.5%), energy usage (14.4%), and energy cost (13.7%) when evaluated over one year across multiple geographical regions.

{{</citation>}}


### (27/27 | 67/241) Investigating the validity of structure learning algorithms in identifying risk factors for intervention in patients with diabetes (Sheresh Zahoor et al., 2024)

{{<citation>}}

Sheresh Zahoor, Anthony C. Constantinou, Tim M Curtis, Mohammed Hasanuzzaman. (2024)  
**Investigating the validity of structure learning algorithms in identifying risk factors for intervention in patients with diabetes**
<br/>
<button class="copy-to-clipboard" title="Investigating the validity of structure learning algorithms in identifying risk factors for intervention in patients with diabetes" index=67>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14327v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14327v1.pdf" filename="2403.14327v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Diabetes, a pervasive and enduring health challenge, imposes significant global implications on health, financial healthcare systems, and societal well-being. This study undertakes a comprehensive exploration of various structural learning algorithms to discern causal pathways amongst potential risk factors influencing diabetes progression. The methodology involves the application of these algorithms to relevant diabetes data, followed by the conversion of their output <b>graphs</b> into Causal Bayesian Networks (CBNs), enabling predictive analysis and the evaluation of discrepancies in the effect of hypothetical interventions within our context-specific case study. This study highlights the substantial impact of algorithm selection on intervention outcomes. To consolidate insights from diverse algorithms, we employ a model-averaging technique that helps us obtain a unique causal model for diabetes derived from a varied set of structural learning algorithms. We also investigate how each of those individual <b>graphs,</b> as well as the average <b>graph,</b> compare to the structures elicited by a domain expert who categorised <b>graph</b> edges into high confidence, moderate, and low confidence types, leading into three individual <b>graphs</b> corresponding to the three levels of confidence. The resulting causal model and data are made available online, and serve as a valuable resource and a guide for informed decision-making by healthcare practitioners, offering a comprehensive understanding of the interactions between relevant risk factors and the effect of hypothetical interventions. Therefore, this research not only contributes to the academic discussion on diabetes, but also provides practical guidance for healthcare professionals in developing efficient intervention and risk management strategies.

{{</citation>}}


## cs.CV (84)



### (1/84 | 68/241) Empowering Segmentation Ability to Multi-modal Large Language Models (Yuqi Yang et al., 2024)

{{<citation>}}

Yuqi Yang, Peng-Tao Jiang, Jing Wang, Hao Zhang, Kai Zhao, Jinwei Chen, Bo Li. (2024)  
**Empowering Segmentation Ability to Multi-modal Large Language Models**
<br/>
<button class="copy-to-clipboard" title="Empowering Segmentation Ability to Multi-modal Large Language Models" index=68>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 63  
Keywords: Fine-tuning, Multi-modal, Reasoning, Chain-of-thought Prompt, Large Language Model, Prompt, Word Embedding  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14141v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14141v1.pdf" filename="2403.14141v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Multi-modal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) can understand image-language <b>prompts</b> and demonstrate impressive <b>reasoning</b> ability. In this paper, we extend MLLMs' output by empowering MLLMs with the segmentation ability. The extended MLLMs can both output language responses to the image-language <b>prompts</b> and segment the regions that the complex question or query in the language <b>prompts</b> focuses on. To this end, the existing work, LISA, enlarges the original <b>word</b> <b>embeddings</b> with an additional segment token and <b>fine-tunes</b> dialogue generation and query-focused segmentation together, where the feature of the segment token is used to <b>prompt</b> the segment-anything model. Although they achieve superior segmentation performance, we observe that the dialogue ability decreases by a <b>large</b> <b>margin</b> <b>compared</b> to the original MLLMs. To maintain the original MLLMs' dialogue ability, we propose a novel MLLMs framework, coined as LLaVASeg, which leverages a <b>chain-of-thought</b> <b>prompting</b> strategy to instruct the MLLMs to segment the target region queried by the user. The MLLMs are first <b>prompted</b> to reason about the simple description of the target region from the complicated user query, then extract the visual attributes of the target region according to the understanding of MLLMs to the image. These visual attributes, such as color and relative locations, are utilized to <b>prompt</b> the downstream segmentation model. Experiments show that the proposed method keeps the original dialogue ability and equips the MLLMs' model with strong <b>reasoning</b> segmentation ability. The code is available at https://github.com/YuqiYang213/LLaVASeg.

{{</citation>}}


### (2/84 | 69/241) MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems? (Renrui Zhang et al., 2024)

{{<citation>}}

Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li. (2024)  
**MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?**
<br/>
<button class="copy-to-clipboard" title="MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?" index=69>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV  
Keyword Score: 56  
Keywords: Benchmarking, Multi-modal, GPT, Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14624v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14624v1.pdf" filename="2403.14624v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The remarkable progress of <b>Multi-modal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) has garnered unparalleled attention, due to their superior performance in visual contexts. However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood. We investigate current <b>benchmarks</b> to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams. To this end, we introduce MathVerse, an all-around visual math <b>benchmark</b> designed for an equitable and in-depth evaluation of MLLMs. We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources. Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total. This approach allows MathVerse to comprehensively assess whether and how much MLLMs can truly understand the visual diagrams for <b>mathematical</b> <b>reasoning.</b> In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a fine-grained assessment of the output answers. Rather than naively judging True or False, we employ <b>GPT-4(V)</b> to adaptively extract crucial <b>reasoning</b> steps, and then score each step with detailed error analysis, which can reveal the intermediate CoT <b>reasoning</b> quality by MLLMs. We hope the MathVerse <b>benchmark</b> may provide unique insights to guide the future development of MLLMs. Project page: https://mathverse-cuhk.github.io

{{</citation>}}


### (3/84 | 70/241) Language Repository for Long Video Understanding (Kumara Kahatapitiya et al., 2024)

{{<citation>}}

Kumara Kahatapitiya, Kanchana Ranasinghe, Jongwoo Park, Michael S. Ryoo. (2024)  
**Language Repository for Long Video Understanding**
<br/>
<button class="copy-to-clipboard" title="Language Repository for Long Video Understanding" index=70>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 56  
Keywords: Benchmarking, Multi-modal, Pruning, Zero-shot, Question Answering, Visual Question Answering, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14622v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14622v1.pdf" filename="2403.14622v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Language has become a prominent modality in computer vision with the rise of <b>multi-modal</b> <b>LLMs.</b> Despite supporting long context-lengths, their effectiveness in handling long-term information gradually declines with input length. This becomes critical, especially in applications such as long-form video understanding. In this paper, we introduce a Language Repository (LangRepo) for <b>LLMs,</b> that maintains concise and structured information as an interpretable (i.e., all-textual) representation. Our repository is updated iteratively based on multi-scale video chunks. We introduce write and read operations that focus on <b>pruning</b> redundancies in text, and extracting information at various temporal scales. The proposed framework is evaluated on <b>zero-shot</b> <b>visual</b> <b>question-answering</b> <b>benchmarks</b> including EgoSchema, NExT-QA, IntentQA and NExT-GQA, showing state-of-the-art performance at its scale. Our code is available at https://github.com/kkahatapitiya/LangRepo.

{{</citation>}}


### (4/84 | 71/241) Learning to Project for Cross-Task Knowledge Distillation (Dylan Auty et al., 2024)

{{<citation>}}

Dylan Auty, Roy Miles, Benedikt Kolbeinsson, Krystian Mikolajczyk. (2024)  
**Learning to Project for Cross-Task Knowledge Distillation**
<br/>
<button class="copy-to-clipboard" title="Learning to Project for Cross-Task Knowledge Distillation" index=71>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 50  
Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Knowledge Transfer, Image2text  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14494v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14494v1.pdf" filename="2403.14494v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Traditional <b>knowledge</b> <b>distillation</b> <b>(KD)</b> relies on a proficient teacher trained on the target task, which is not always available. In this setting, cross-task <b>distillation</b> can be used, enabling the use of any teacher model trained on a different task. However, many <b>KD</b> methods prove ineffective when applied to this cross-task setting. To address this limitation, we propose a simple modification: the use of an inverted projection. We show that this drop-in replacement for a standard projector is effective by learning to disregard any task-specific features which might degrade the student's performance. We find that this simple modification is sufficient for extending many <b>KD</b> methods to the cross-task setting, where the teacher and student tasks can be very different. In doing so, we obtain up to a 1.9% improvement in the cross-task setting compared to the traditional projection, at no additional cost. Our method can obtain significant performance improvements (up to 7%) when using even a randomly-initialised teacher on various tasks such as depth estimation, <b>image</b> <b>translation,</b> and semantic segmentation, despite the lack of any learned <b>knowledge</b> <b>to</b> transfer. To provide conceptual and analytical insights into this result, we show that using an inverted projection allows the <b>distillation</b> loss to be decomposed into a <b>knowledge</b> <b>transfer</b> and a spectral regularisation component. Through this analysis we are additionally able to propose a novel regularisation loss that allows teacher-free <b>distillation,</b> enabling performance improvements of up to 8.57% on ImageNet with no additional training costs.

{{</citation>}}


### (5/84 | 72/241) Unsupervised Audio-Visual Segmentation with Modality Alignment (Swapnil Bhosale et al., 2024)

{{<citation>}}

Swapnil Bhosale, Haosen Yang, Diptesh Kanojia, Jiangkang Deng, Xiatian Zhu. (2024)  
**Unsupervised Audio-Visual Segmentation with Modality Alignment**
<br/>
<button class="copy-to-clipboard" title="Unsupervised Audio-Visual Segmentation with Modality Alignment" index=72>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 50  
Keywords: Contrastive Learning, Foundation Model, Supervised Learning, Unsupervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14203v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14203v1.pdf" filename="2403.14203v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Audio-Visual Segmentation (AVS) aims to identify, at the pixel level, the object in a visual scene that produces a given sound. Current AVS methods rely on costly fine-grained annotations of mask-audio pairs, making them impractical for scalability. To address this, we introduce <b>unsupervised</b> <b>AVS,</b> eliminating the need for such expensive annotation. To tackle this more challenging problem, we propose an <b>unsupervised</b> <b>learning</b> method, named Modality Correspondence Alignment (MoCA), which seamlessly integrates off-the-shelf <b>foundation</b> <b>models</b> like DINO, SAM, and ImageBind. This approach leverages their knowledge complementarity and optimizes their joint usage for multi-modality association. Initially, we estimate positive and negative image pairs in the feature space. For pixel-level association, we introduce an audio-visual adapter and a novel pixel matching aggregation strategy within the image-level <b>contrastive</b> <b>learning</b> framework. This allows for a flexible connection between object appearance and audio signal at the pixel level, with tolerance to imaging variations such as translation and rotation. Extensive experiments on the AVSBench (single and multi-object splits) and AVSS datasets demonstrate that our MoCA outperforms strongly designed baseline methods and approaches <b>supervised</b> counterparts, particularly in complex scenarios with multiple auditory objects. Notably when comparing mIoU, MoCA achieves a substantial improvement over baselines in both the AVSBench (S4: +17.24%; MS3: +67.64%) and AVSS (+19.23%) audio-visual segmentation challenges.

{{</citation>}}


### (6/84 | 73/241) OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation (Kwanyoung Kim et al., 2024)

{{<citation>}}

Kwanyoung Kim, Yujin Oh, Jong Chul Ye. (2024)  
**OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation**
<br/>
<button class="copy-to-clipboard" title="OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation" index=73>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs.CV, stat-ML  
Keyword Score: 49  
Keywords: Benchmarking, Multi-modal, Multi-modal, Zero-shot, Transformer, Prompt, Text Embedding  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14183v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14183v1.pdf" filename="2403.14183v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The recent success of CLIP has demonstrated promising results in <b>zero-shot</b> semantic segmentation by transferring muiltimodal knowledge to pixel-level classification. However, leveraging pre-trained CLIP knowledge to closely align <b>text</b> <b>embeddings</b> with pixel embeddings still has limitations in existing approaches. To address this issue, we propose OTSeg, a novel <b>multimodal</b> attention mechanism aimed at enhancing the potential of multiple <b>text</b> <b>prompts</b> for matching associated pixel embeddings. We first propose Multi-Prompts Sinkhorn (MPS) based on the Optimal Transport (OT) algorithm, which leads multiple <b>text</b> <b>prompts</b> to selectively focus on various semantic features within image pixels. Moreover, inspired by the success of Sinkformers in unimodal settings, we introduce the extension of MPS, called Multi-Prompts Sinkhorn Attention (MPSA), which effectively replaces cross-attention mechanisms within <b>Transformer</b> framework in <b>multimodal</b> settings. Through extensive experiments, we demonstrate that OTSeg achieves state-of-the-art (SOTA) performance with significant gains on <b>Zero-Shot</b> Semantic Segmentation (ZS3) tasks across three <b>benchmark</b> datasets.

{{</citation>}}


### (7/84 | 74/241) Transfer Learning for Cross-dataset Isolated Sign Language Recognition in Under-Resourced Datasets (Ahmet Alp Kindiroglu et al., 2024)

{{<citation>}}

Ahmet Alp Kindiroglu, Ozgur Kara, Ogulcan Ozdemir, Lale Akarun. (2024)  
**Transfer Learning for Cross-dataset Isolated Sign Language Recognition in Under-Resourced Datasets**
<br/>
<button class="copy-to-clipboard" title="Transfer Learning for Cross-dataset Isolated Sign Language Recognition in Under-Resourced Datasets" index=74>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 46  
Keywords: Graph, Benchmarking, Convolution, Fine-tuning, Supervised Learning, Transfer Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14534v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14534v1.pdf" filename="2403.14534v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Sign language recognition (SLR) has recently achieved a breakthrough in performance thanks to deep neural networks trained on large annotated sign datasets. Of the many different sign languages, these annotated datasets are only available for a select few. Since acquiring gloss-level labels on sign language videos is difficult, learning by transferring knowledge from existing annotated sources is useful for recognition in under-resourced sign languages. This study provides a publicly available cross-dataset <b>transfer</b> <b>learning</b> <b>benchmark</b> from two existing public Turkish SLR datasets. We use a temporal <b>graph</b> <b>convolution-based</b> sign language recognition approach to evaluate five <b>supervised</b> <b>transfer</b> <b>learning</b> approaches and experiment with closed-set and partial-set cross-dataset <b>transfer</b> <b>learning.</b> Experiments demonstrate that improvement over <b>finetuning</b> based <b>transfer</b> <b>learning</b> is possible with specialized <b>supervised</b> <b>transfer</b> <b>learning</b> methods.

{{</citation>}}


### (8/84 | 75/241) Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics (Jiaqi Yue et al., 2024)

{{<citation>}}

Jiaqi Yue, Jiancheng Zhao, Chunhui Zhao. (2024)  
**Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics**
<br/>
<button class="copy-to-clipboard" title="Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics" index=75>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 43  
Keywords: Benchmarking, Zero-shot, Large Language Model, Large Language Model, Zero-shot Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14362v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14362v1.pdf" filename="2403.14362v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Generalized <b>zero-shot</b> <b>learning</b> (GZSL) focuses on recognizing seen and unseen classes against domain shift problem (DSP) where data of unseen classes may be misclassified as seen classes. However, existing GZSL is still limited to seen domains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which addresses GZSL towards unseen domains. Different from existing GZSL methods which alleviate DSP by generating features of unseen classes with semantics, CDGZSL needs to construct a common feature space across domains and acquire the corresponding intrinsic semantics shared among domains to transfer from seen to unseen domains. Considering the information asymmetry problem caused by redundant class semantics annotated with <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> we present Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR consists of two parts: Inter-class Similarity Alignment (ISA), which eliminates the non-intrinsic semantics not shared across all domains under the guidance of inter-class feature relationships, and Unseen-class Meta Generation (UMG), which preserves intrinsic semantics to maintain connectivity between seen and unseen classes by simulating feature generation. MDASR effectively aligns the redundant semantic space with the common feature space, mitigating the information asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on the Office-Home and Mini-DomainNet, and we have shared the <b>LLM-based</b> semantics for these datasets as the <b>benchmark.</b>

{{</citation>}}


### (9/84 | 76/241) T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy (Qing Jiang et al., 2024)

{{<citation>}}

Qing Jiang, Feng Li, Zhaoyang Zeng, Tianhe Ren, Shilong Liu, Lei Zhang. (2024)  
**T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy**
<br/>
<button class="copy-to-clipboard" title="T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy" index=76>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Object Detection, Contrastive Learning, Zero-shot, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14610v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14610v1.pdf" filename="2403.14610v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present T-Rex2, a highly practical model for open-set <b>object</b> <b>detection.</b> Previous open-set <b>object</b> <b>detection</b> methods relying on text <b>prompts</b> effectively encapsulate the abstract concept of common <b>objects,</b> <b>but</b> struggle with rare or complex <b>object</b> <b>representation</b> due to data scarcity and descriptive limitations. Conversely, visual <b>prompts</b> excel in depicting novel <b>objects</b> <b>through</b> concrete visual examples, but fall short in conveying the abstract concept of <b>objects</b> <b>as</b> effectively as text <b>prompts.</b> Recognizing the complementary strengths and weaknesses of both text and visual <b>prompts,</b> we introduce T-Rex2 that synergizes both <b>prompts</b> within a single model through <b>contrastive</b> <b>learning.</b> T-Rex2 accepts inputs in diverse formats, including text <b>prompts,</b> visual <b>prompts,</b> and the combination of both, so that it can handle different scenarios by switching between the two <b>prompt</b> modalities. Comprehensive experiments demonstrate that T-Rex2 exhibits remarkable <b>zero-shot</b> <b>object</b> <b>detection</b> capabilities across a wide spectrum of scenarios. We show that text <b>prompts</b> and visual <b>prompts</b> can benefit from each other within the synergy, which is essential to cover massive and complicated real-world scenarios and pave the way towards generic <b>object</b> <b>detection.</b> Model API is now available at \url{https://github.com/IDEA-Research/T-Rex}.

{{</citation>}}


### (10/84 | 77/241) A Lightweight Attention-based Deep Network via Multi-Scale Feature Fusion for Multi-View Facial Expression Recognition (Ali Ezati et al., 2024)

{{<citation>}}

Ali Ezati, Mohammadreza Dezyani, Rajib Rana, Roozbeh Rajabi, Ahmad Ayatollahi. (2024)  
**A Lightweight Attention-based Deep Network via Multi-Scale Feature Fusion for Multi-View Facial Expression Recognition**
<br/>
<button class="copy-to-clipboard" title="A Lightweight Attention-based Deep Network via Multi-Scale Feature Fusion for Multi-View Facial Expression Recognition" index=77>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14318v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14318v1.pdf" filename="2403.14318v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> and their variations have shown effectiveness in facial expression recognition (FER). However, they face challenges when dealing with high computational complexity and multi-view head poses in real-world scenarios. We introduce a lightweight attentional network incorporating multi-scale feature fusion (LANMSFF) to tackle these issues. For the first challenge, we have carefully designed a lightweight fully <b>convolutional</b> <b>network</b> <b>(FCN).</b> We address the second challenge by presenting two novel components, namely mass attention (MassAtt) and point wise feature selection (PWFS) blocks. The MassAtt block simultaneously generates channel and spatial attention maps to recalibrate feature maps by emphasizing important features while suppressing irrelevant ones. On the other hand, the PWFS block employs a feature selection mechanism that discards less meaningful features prior to the fusion process. This mechanism distinguishes it from previous methods that directly fuse multi-scale features. Our proposed approach achieved results comparable to state-of-the-art methods in terms of parameter counts and robustness to pose variation, with accuracy rates of 90.77% on KDEF, 70.44% on FER-2013, and 86.96% on FERPlus datasets. The code for LANMSFF is available at https://github.com/AE-1129/LANMSFF.

{{</citation>}}


### (11/84 | 78/241) MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical Image Segmentation (Bin Xie et al., 2024)

{{<citation>}}

Bin Xie, Hao Tang, Bin Duan, Dawen Cai, Yan Yan. (2024)  
**MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical Image Segmentation**
<br/>
<button class="copy-to-clipboard" title="MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical Image Segmentation" index=78>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Foundation Model, Zero-shot, Transformer, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14103v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14103v1.pdf" filename="2403.14103v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Segment Anything Model~(SAM), a <b>prompt-driven</b> <b>foundation</b> <b>model</b> for natural image segmentation, has demonstrated impressive <b>zero-shot</b> performance. However, SAM does not work when directly applied to medical image segmentation tasks, since SAM lacks the functionality to predict semantic labels for predicted masks and needs to provide extra <b>prompts,</b> such as points or boxes, to segment target regions. Meanwhile, there is a huge gap between 2D natural images and 3D medical images, so the performance of SAM is imperfect for medical image segmentation tasks. Following the above issues, we propose MaskSAM, a novel mask classification <b>prompt-free</b> SAM adaptation framework for medical image segmentation. We design a <b>prompt</b> generator combined with the image encoder in SAM to generate a set of auxiliary classifier tokens, auxiliary binary masks, and auxiliary bounding boxes. Each pair of auxiliary mask and box <b>prompts,</b> which can solve the requirements of extra <b>prompts,</b> is associated with class label predictions by the sum of the auxiliary classifier token and the learnable global classifier tokens in the mask decoder of SAM to solve the predictions of semantic labels. Meanwhile, we design a 3D depth-convolution adapter for image embeddings and a 3D depth-MLP adapter for <b>prompt</b> embeddings. We inject one of them into each <b>transformer</b> block in the image encoder and mask decoder to enable pre-trained 2D SAM models to extract 3D information and adapt to 3D medical images. Our method achieves state-of-the-art performance on AMOS2022, 90.52% Dice, which improved by 2.7% compared to nnUNet. Our method surpasses nnUNet by 1.7% on ACDC and 1.0% on Synapse datasets.

{{</citation>}}


### (12/84 | 79/241) Text-Enhanced Data-free Approach for Federated Class-Incremental Learning (Minh-Tuan Tran et al., 2024)

{{<citation>}}

Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Harandi, Dinh Phung. (2024)  
**Text-Enhanced Data-free Approach for Federated Class-Incremental Learning**
<br/>
<button class="copy-to-clipboard" title="Text-Enhanced Data-free Approach for Federated Class-Incremental Learning" index=79>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CL, cs-CV, cs-LG, cs.CV  
Keyword Score: 40  
Keywords: Federated Learning, Knowledge Transfer, Pre-trained Language Model, Text Embedding  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14101v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14101v1.pdf" filename="2403.14101v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Federated</b> <b>Class-Incremental</b> Learning (FCIL) is an underexplored yet pivotal issue, involving the dynamic addition of new classes in the context of <b>federated</b> <b>learning.</b> In this field, Data-Free <b>Knowledge</b> <b>Transfer</b> (DFKT) plays a crucial role in addressing catastrophic forgetting and data privacy problems. However, prior approaches lack the crucial synergy between DFKT and the model training phases, causing DFKT to encounter difficulties in generating high-quality data from a non-anchored latent space of the old task model. In this paper, we introduce LANDER (Label <b>Text</b> <b>Centered</b> Data-Free <b>Knowledge</b> <b>Transfer)</b> to address this issue by utilizing label <b>text</b> <b>embeddings</b> (LTE) produced by <b>pretrained</b> <b>language</b> <b>models.</b> Specifically, during the model training phase, our approach treats LTE as anchor points and constrains the feature embeddings of corresponding training samples around them, enriching the surrounding area with more meaningful information. In the DFKT phase, by using these LTE anchors, LANDER can synthesize more meaningful samples, thereby effectively addressing the forgetting problem. Additionally, instead of tightly constraining embeddings toward the anchor, the Bounding Loss is introduced to encourage sample embeddings to remain flexible within a defined radius. This approach preserves the natural differences in sample embeddings and mitigates the embedding overlap caused by heterogeneous <b>federated</b> <b>settings.</b> Extensive experiments conducted on CIFAR100, Tiny-ImageNet, and ImageNet demonstrate that LANDER significantly outperforms previous methods and achieves state-of-the-art performance in FCIL. The code is available at https://github.com/tmtuan1307/lander.

{{</citation>}}


### (13/84 | 80/241) Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference (Han Zhao et al., 2024)

{{<citation>}}

Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, Donglin Wang. (2024)  
**Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference**
<br/>
<button class="copy-to-clipboard" title="Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference" index=80>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 39  
Keywords: Benchmarking, Foundation Model, Multi-modal, Multi-modal, Transformer, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14520v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14520v1.pdf" filename="2403.14520v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In recent years, the application of <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLM) in various fields has achieved remarkable success. However, as the <b>foundation</b> <b>model</b> for many downstream tasks, current MLLMs are composed of the well-known <b>Transformer</b> network, which has a less efficient quadratic computation complexity. To improve the efficiency of such basic models, we propose Cobra, a linear computational complexity MLLM. Specifically, Cobra integrates the efficient Mamba language model into the visual modality. Moreover, we explore and study various modal fusion schemes to create an effective <b>multi-modal</b> Mamba. Extensive experiments demonstrate that (1) Cobra achieves extremely competitive performance with current computationally efficient state-of-the-art methods, \textit{e.g.}, LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due to Cobra's linear sequential modeling. (2) Interestingly, the results of closed-set challenging prediction <b>benchmarks</b> show that Cobra performs well in overcoming visual illusions and spatial relationship judgments. (3) Notably, Cobra even achieves comparable performance to LLaVA with about 43% of the number of parameters. We will make all codes of Cobra open-source and hope that the proposed method can facilitate future research on complexity problems in MLLM. Our project page is available at: https://sites.google.com/view/cobravlm.

{{</citation>}}


### (14/84 | 81/241) Hierarchical Text-to-Vision Self Supervised Alignment for Improved Histopathology Representation Learning (Hasindri Watawana et al., 2024)

{{<citation>}}

Hasindri Watawana, Kanchana Ranasinghe, Tariq Mahmood, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan. (2024)  
**Hierarchical Text-to-Vision Self Supervised Alignment for Improved Histopathology Representation Learning**
<br/>
<button class="copy-to-clipboard" title="Hierarchical Text-to-Vision Self Supervised Alignment for Improved Histopathology Representation Learning" index=81>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 38  
Keywords: Benchmarking, Representation Learning, Self-supervised Learning, Self-supervised Learning, Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14616v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14616v1.pdf" filename="2403.14616v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Self-supervised</b> <b>representation</b> <b>learning</b> has been highly promising for histopathology image analysis with numerous approaches leveraging their patient-slide-patch hierarchy to learn better <b>representations.</b> <b>In</b> this paper, we explore how the combination of domain specific natural language information with such hierarchical visual <b>representations</b> <b>can</b> benefit rich <b>representation</b> <b>learning</b> for medical image tasks. Building on automated language description generation for features visible in histopathology images, we present a novel language-tied <b>self-supervised</b> <b>learning</b> framework, Hierarchical Language-tied Self-Supervision (HLSS) for histopathology images. We explore contrastive objectives and granular language description based text alignment at multiple hierarchies to inject language modality information into the visual <b>representations.</b> <b>Our</b> resulting model achieves state-of-the-art performance on two medical imaging <b>benchmarks,</b> OpenSRH and TCGA datasets. Our framework also provides better interpretability with our language aligned <b>representation</b> <b>space.</b> Code is available at https://github.com/Hasindri/HLSS.

{{</citation>}}


### (15/84 | 82/241) Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild (Junhyeong Cho et al., 2024)

{{<citation>}}

Junhyeong Cho, Kim Youwang, Hunmin Yang, Tae-Hyun Oh. (2024)  
**Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild**
<br/>
<button class="copy-to-clipboard" title="Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild" index=82>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs.CV  
Keyword Score: 38  
Keywords: ControlNet, Benchmarking, Geometry, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14539v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14539v1.pdf" filename="2403.14539v1.pdf">Download PDF</button>

---


**ABSTRACT**  
One of the biggest challenges in single-view 3D shape reconstruction in the wild is the scarcity of <3D shape, 2D image>-paired data from real-world environments. Inspired by remarkable achievements via domain randomization, we propose ObjectDR which synthesizes such paired data via a random <b>simulation</b> of visual variations in object appearances and backgrounds. Our data synthesis framework exploits a conditional generative model (e.g., <b>ControlNet)</b> to generate images conforming to spatial conditions such as 2.5D sketches, which are obtainable through a rendering process of 3D shapes from object collections (e.g., Objaverse-XL). To simulate diverse variations while preserving object silhouettes embedded in spatial conditions, we also introduce a disentangled framework which leverages an initial object guidance. After synthesizing a wide range of data, we pre-train a model on them so that it learns to capture a domain-invariant <b>geometry</b> prior which is consistent across various domains. We validate its effectiveness by substantially improving 3D shape reconstruction models on a real-world <b>benchmark.</b> In a scale-up evaluation, our pre-training achieves 23.6% superior results compared with the pre-training on high-quality computer graphics renderings.

{{</citation>}}


### (16/84 | 83/241) PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model (Zheng Zhang et al., 2024)

{{<citation>}}

Zheng Zhang, Yeyao Ma, Enming Zhang, Xiang Bai. (2024)  
**PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model**
<br/>
<button class="copy-to-clipboard" title="PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model" index=83>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 36  
Keywords: Benchmarking, Multi-modal, Zero-shot, GPT, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14598v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14598v1.pdf" filename="2403.14598v1.pdf">Download PDF</button>

---


**ABSTRACT**  
PSALM is a powerful extension of the Large <b>Multi-modal</b> Model (LMM) to address the segmentation task challenges. To overcome the limitation of the LMM being limited to textual output, PSALM incorporates a mask decoder and a well-designed input schema to handle a variety of segmentation tasks. This schema includes images, task instructions, conditional <b>prompts,</b> and mask tokens, which enable the model to generate and classify segmentation masks effectively. The flexible design of PSALM supports joint training across multiple datasets and tasks, leading to improved performance and task generalization. PSALM achieves superior results on several <b>benchmarks,</b> such as RefCOCO/RefCOCO+/RefCOCOg, COCO Panoptic Segmentation, and COCO-Interactive, and further exhibits <b>zero-shot</b> capabilities on unseen tasks, such as open-vocabulary segmentation, generalized referring expression segmentation and video object segmentation, making a significant step towards a <b>GPT</b> moment in computer vision. Through extensive experiments, PSALM demonstrates its potential to transform the domain of image segmentation, leveraging the robust visual understanding capabilities of LMMs as seen in natural language processing. Code and models are available at https://github.com/zamling/PSALM.

{{</citation>}}


### (17/84 | 84/241) Unified Static and Dynamic Network: Efficient Temporal Filtering for Video Grounding (Jingjing Hu et al., 2024)

{{<citation>}}

Jingjing Hu, Dan Guo, Kun Li, Zhan Si, Xun Yang, Xiaojun Chang, Meng Wang. (2024)  
**Unified Static and Dynamic Network: Efficient Temporal Filtering for Video Grounding**
<br/>
<button class="copy-to-clipboard" title="Unified Static and Dynamic Network: Efficient Temporal Filtering for Video Grounding" index=84>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 36  
Keywords: Message-Passing, Graph, Benchmarking, Convolution, Grounding  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14174v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14174v1.pdf" filename="2403.14174v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Inspired by the activity-silent and persistent activity mechanisms in human visual perception biology, we design a Unified Static and Dynamic Network (UniSDNet), to learn the semantic association between the video and text/audio queries in a cross-modal environment for efficient video <b>grounding.</b> For static modeling, we devise a novel residual structure (ResMLP) to boost the global comprehensive interaction between the video segments and queries, achieving more effective semantic enhancement/supplement. For dynamic modeling, we effectively exploit three characteristics of the persistent activity mechanism in our network design for a better video context comprehension. Specifically, we construct a diffusely connected video clip <b>graph</b> on the basis of 2D sparse temporal masking to reflect the "short-term effect" relationship. We innovatively consider the temporal distance and relevance as the joint "auxiliary evidence clues" and design a multi-kernel Temporal Gaussian Filter to expand the context clue into high-dimensional space, simulating the "complex visual perception", and then conduct element level filtering <b>convolution</b> operations on neighbour clip nodes in message passing stage for finally generating and ranking the candidate proposals. Our UniSDNet is applicable to both Natural Language Video <b>Grounding</b> (NLVG) and Spoken Language Video <b>Grounding</b> (SLVG) tasks. Our UniSDNet achieves SOTA performance on three widely used datasets for NLVG, as well as three datasets for SLVG, e.g., reporting new records at 38.88% R@1,IoU@0.7 on ActivityNet Captions and 40.26% R@1,IoU@0.5 on TACoS. To facilitate this field, we collect two new datasets (Charades-STA Speech and TACoS Speech) for SLVG task. Meanwhile, the inference speed of our UniSDNet is 1.56$\times$ faster than the strong multi-query <b>benchmark.</b> Code is available at: https://github.com/xian-sh/UniSDNet.

{{</citation>}}


### (18/84 | 85/241) Adversary-Robust Graph-Based Learning of WSIs (Saba Heidari Gheshlaghi et al., 2024)

{{<citation>}}

Saba Heidari Gheshlaghi, Milan Aryal, Nasim Yahyasoltani, Masoud Ganji. (2024)  
**Adversary-Robust Graph-Based Learning of WSIs**
<br/>
<button class="copy-to-clipboard" title="Adversary-Robust Graph-Based Learning of WSIs" index=85>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 33  
Keywords: Graph, Graph Neural Network, Transformer, Adversarial Attack  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14489v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14489v1.pdf" filename="2403.14489v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Enhancing the robustness of deep learning models against <b>adversarial</b> <b>attacks</b> is crucial, especially in critical domains like healthcare where significant financial interests heighten the risk of such attacks. Whole slide images (WSIs) are high-resolution, digitized versions of tissue samples mounted on glass slides, scanned using sophisticated imaging equipment. The digital analysis of WSIs presents unique challenges due to their gigapixel size and multi-resolution storage format. In this work, we aim at improving the robustness of cancer Gleason grading classification systems against <b>adversarial</b> <b>attacks,</b> addressing challenges at both the image and <b>graph</b> levels. As regards the proposed algorithm, we develop a novel and innovative <b>graph-based</b> model which utilizes <b>GNN</b> to extract features from the <b>graph</b> representation of WSIs. A denoising module, along with a pooling layer is incorporated to manage the impact of <b>adversarial</b> <b>attacks</b> on the WSIs. The process concludes with a <b>transformer</b> module that classifies various grades of prostate cancer based on the processed data. To assess the effectiveness of the proposed method, we conducted a comparative analysis using two scenarios. Initially, we trained and tested the model without the denoiser using WSIs that had not been exposed to any attack. We then introduced a range of attacks at either the image or <b>graph</b> level and processed them through the proposed network. The performance of the model was evaluated in terms of accuracy and kappa scores. The results from this comparison showed a significant improvement in cancer diagnosis accuracy, highlighting the robustness and efficiency of the proposed method in handling <b>adversarial</b> <b>challenges</b> in the context of medical imaging.

{{</citation>}}


### (19/84 | 86/241) OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation (Bohao Peng et al., 2024)

{{<citation>}}

Bohao Peng, Xiaoyang Wu, Li Jiang, Yukang Chen, Hengshuang Zhao, Zhuotao Tian, Jiaya Jia. (2024)  
**OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation**
<br/>
<button class="copy-to-clipboard" title="OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation" index=86>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 33  
Keywords: Benchmarking, Convolutional Neural Network, Transformer, Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14418v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14418v1.pdf" filename="2403.14418v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The booming of 3D recognition in the 2020s began with the introduction of point cloud <b>transformers.</b> They quickly overwhelmed sparse <b>CNNs</b> and became state-of-the-art models, especially in 3D semantic segmentation. However, sparse <b>CNNs</b> are still valuable networks, due to their efficiency treasure, and ease of application. In this work, we reexamine the design distinctions and test the limits of what a sparse <b>CNN</b> can achieve. We discover that the key credit to the performance difference is adaptivity. Specifically, we propose two key components, i.e., adaptive receptive fields (spatially) and adaptive relation, to bridge the gap. This exploration led to the creation of Omni-Adaptive 3D <b>CNNs</b> (OA-CNNs), a family of networks that integrates a lightweight module to greatly enhance the adaptivity of sparse <b>CNNs</b> at minimal computational cost. Without any <b>self-attention</b> modules, OA-CNNs favorably surpass point <b>transformers</b> in terms of accuracy in both indoor and outdoor scenes, with much less latency and memory cost. Notably, it achieves 76.1%, 78.9%, and 70.6% mIoU on ScanNet v2, nuScenes, and SemanticKITTI validation <b>benchmarks</b> respectively, while maintaining at most 5x better speed than <b>transformer</b> counterparts. This revelation highlights the potential of pure sparse <b>CNNs</b> to outperform <b>transformer-related</b> networks.

{{</citation>}}


### (20/84 | 87/241) Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection (Tim Salzmann et al., 2024)

{{<citation>}}

Tim Salzmann, Markus Ryll, Alex Bewley, Matthias Minderer. (2024)  
**Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection**
<br/>
<button class="copy-to-clipboard" title="Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection" index=87>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CL, cs-CV, cs-LG, cs-RO, cs.CV  
Keyword Score: 33  
Keywords: Object Detection, Benchmarking, Zero-shot, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14270v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14270v1.pdf" filename="2403.14270v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Visual relationship detection aims to identify <b>objects</b> <b>and</b> their relationships in images. Prior methods approach this task by adding separate relationship modules or decoders to existing <b>object</b> <b>detection</b> architectures. This separation increases complexity and hinders end-to-end training, which limits performance. We propose a simple and highly efficient decoder-free architecture for open-vocabulary visual relationship detection. Our model consists of a <b>Transformer-based</b> image encoder that represents <b>objects</b> <b>as</b> tokens and models their relationships implicitly. To extract relationship information, we introduce an attention mechanism that selects <b>object</b> <b>pairs</b> likely to form a relationship. We provide a single-stage recipe to train this model on a mixture of <b>object</b> <b>and</b> relationship detection data. Our approach achieves state-of-the-art relationship detection performance on Visual Genome and on the large-vocabulary GQA <b>benchmark</b> at real-time inference speeds. We provide analyses of <b>zero-shot</b> performance, ablations, and real-world qualitative examples.

{{</citation>}}


### (21/84 | 88/241) EventDance: Unsupervised Source-free Cross-modal Adaptation for Event-based Object Recognition (Xu Zheng et al., 2024)

{{<citation>}}

Xu Zheng, Lin Wang. (2024)  
**EventDance: Unsupervised Source-free Cross-modal Adaptation for Event-based Object Recognition**
<br/>
<button class="copy-to-clipboard" title="EventDance: Unsupervised Source-free Cross-modal Adaptation for Event-based Object Recognition" index=88>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 33  
Keywords: Benchmarking, Knowledge Transfer, Self-supervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14082v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14082v1.pdf" filename="2403.14082v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we make the first attempt at achieving the cross-modal (i.e., image-to-events) adaptation for event-based object recognition without accessing any labeled source image data owning to privacy and commercial issues. Tackling this novel problem is non-trivial due to the novelty of event cameras and the distinct modality gap between images and events. In particular, as only the source model is available, a hurdle is how to extract the <b>knowledge</b> <b>from</b> the source model by only using the unlabeled target event data while achieving <b>knowledge</b> <b>transfer.</b> To this end, we propose a novel framework, dubbed EventDance for this <b>unsupervised</b> source-free cross-modal adaptation problem. Importantly, inspired by event-to-video reconstruction methods, we propose a reconstruction-based modality bridging (RMB) module, which reconstructs intensity frames from events in a <b>self-supervised</b> manner. This makes it possible to build up the surrogate images to extract the <b>knowledge</b> <b>(i.e.,</b> labels) from the source model. We then propose a multi-representation <b>knowledge</b> <b>adaptation</b> (MKA) module that transfers the <b>knowledge</b> <b>to</b> target models learning events with multiple representation types for fully exploring the spatiotemporal information of events. The two modules connecting the source and target models are mutually updated so as to achieve the best performance. Experiments on three <b>benchmark</b> datasets with two adaption settings show that EventDance is on par with prior methods utilizing the source data.

{{</citation>}}


### (22/84 | 89/241) MyVLM: Personalizing VLMs for User-Specific Queries (Yuval Alaluf et al., 2024)

{{<citation>}}

Yuval Alaluf, Elad Richardson, Sergey Tulyakov, Kfir Aberman, Daniel Cohen-Or. (2024)  
**MyVLM: Personalizing VLMs for User-Specific Queries**
<br/>
<button class="copy-to-clipboard" title="MyVLM: Personalizing VLMs for User-Specific Queries" index=89>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Question Answering, Visual Question Answering, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14599v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14599v1.pdf" filename="2403.14599v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent large-scale <b>vision-language</b> models (VLMs) have demonstrated remarkable capabilities in understanding and generating textual descriptions for <b>visual</b> <b>content.</b> <b>However,</b> these models lack an understanding of user-specific concepts. In this work, we take a first step toward the personalization of VLMs, enabling them to learn and reason over user-provided concepts. For example, we explore whether these models can learn to recognize you in an image and communicate what you are doing, tailoring the model to reflect your personal experiences and relationships. To effectively recognize a variety of user-specific concepts, we augment the VLM with external concept heads that function as toggles for the model, enabling the VLM to identify the presence of specific target concepts in a given image. Having recognized the concept, we learn a new concept embedding in the intermediate feature space of the VLM. This embedding is tasked with guiding the language model to naturally integrate the target concept in its generated response. We apply our technique to BLIP-2 and LLaVA for personalized image captioning and further show its applicability for personalized <b>visual</b> <b>question-answering.</b> <b>Our</b> experiments demonstrate our ability to generalize to unseen images of learned concepts while preserving the model behavior on unrelated inputs.

{{</citation>}}


### (23/84 | 90/241) Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer (Junyi Wu et al., 2024)

{{<citation>}}

Junyi Wu, Bin Duan, Weitai Kang, Hao Tang, Yan Yan. (2024)  
**Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer**
<br/>
<button class="copy-to-clipboard" title="Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer" index=90>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Vision Transformer, Transformer, Vision Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14552v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14552v1.pdf" filename="2403.14552v1.pdf">Download PDF</button>

---


**ABSTRACT**  
While <b>Transformers</b> have rapidly gained popularity in various computer <b>vision</b> <b>applications,</b> post-hoc explanations of their internal mechanisms remain largely unexplored. <b>Vision</b> <b>Transformers</b> extract visual information by representing image regions as transformed tokens and integrating them via attention weights. However, existing post-hoc explanation methods merely consider these attention weights, neglecting crucial information from the transformed tokens, which fails to accurately illustrate the rationales behind the models' predictions. To incorporate the influence of token transformation into interpretation, we propose TokenTM, a novel post-hoc explanation method that utilizes our introduced measurement of token transformation effects. Specifically, we quantify token transformation effects by measuring changes in token lengths and correlations in their directions pre- and post-transformation. Moreover, we develop initialization and aggregation rules to integrate both attention weights and token transformation effects across all layers, capturing holistic token contributions throughout the model. Experimental results on segmentation and perturbation tests demonstrate the superiority of our proposed TokenTM compared to state-of-the-art <b>Vision</b> <b>Transformer</b> explanation methods.

{{</citation>}}


### (24/84 | 91/241) Estimating Physical Information Consistency of Channel Data Augmentation for Remote Sensing Images (Tom Burgert et al., 2024)

{{<citation>}}

Tom Burgert, Begüm Demir. (2024)  
**Estimating Physical Information Consistency of Channel Data Augmentation for Remote Sensing Images**
<br/>
<button class="copy-to-clipboard" title="Estimating Physical Information Consistency of Channel Data Augmentation for Remote Sensing Images" index=91>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 30  
Keywords: Data Augmentation, Self-supervised Learning, Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14547v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14547v1.pdf" filename="2403.14547v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The application of <b>data</b> <b>augmentation</b> for deep learning (DL) methods plays an important role in achieving state-of-the-art results in <b>supervised,</b> semi-supervised, and <b>self-supervised</b> image classification. In particular, channel transformations (e.g., solarize, grayscale, brightness adjustments) are integrated into <b>data</b> <b>augmentation</b> pipelines for remote sensing (RS) image classification tasks. However, contradicting beliefs exist about their proper applications to RS images. A common point of critique is that the application of channel augmentation techniques may lead to physically inconsistent spectral <b>data</b> <b>(i.e.,</b> pixel signatures). To shed light on the open debate, we propose an approach to estimate whether a channel augmentation technique affects the physical information of RS images. To this end, the proposed approach estimates a score that measures the alignment of a pixel signature within a time series that can be naturally subject to deviations caused by factors such as acquisition conditions or phenological states of vegetation. We compare the scores associated with original and augmented pixel signatures to evaluate the physical consistency. Experimental results on a multi-label image classification task show that channel augmentations yielding a score that exceeds the expected deviation of original pixel signatures can not improve the performance of a baseline model trained without augmentation.

{{</citation>}}


### (25/84 | 92/241) HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression (Yihang Chen et al., 2024)

{{<citation>}}

Yihang Chen, Qianyi Wu, Jianfei Cai, Mehrtash Harandi, Weiyao Lin. (2024)  
**HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression**
<br/>
<button class="copy-to-clipboard" title="HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression" index=92>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Mutual Information, Quantization, Quantization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14530v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14530v1.pdf" filename="2403.14530v1.pdf">Download PDF</button>

---


**ABSTRACT**  
3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. To address this, we make use of the relations between the unorganized anchors and the structured hash grid, leveraging their <b>mutual</b> <b>information</b> for context modeling, and propose a Hash-grid Assisted Context (HAC) framework for highly compact 3DGS representation. Our approach introduces a binary hash grid to establish continuous spatial consistencies, allowing us to unveil the inherent spatial relations of anchors through a carefully designed context model. To facilitate entropy coding, we utilize Gaussian distributions to accurately estimate the probability of each <b>quantized</b> attribute, where an adaptive <b>quantization</b> module is proposed to enable high-precision <b>quantization</b> of these attributes for improved fidelity restoration. Additionally, we incorporate an adaptive masking strategy to eliminate invalid Gaussians and anchors. Importantly, our work is the pioneer to explore context-based compression for 3DGS representation, resulting in a remarkable size reduction of over $75\times$ compared to vanilla 3DGS, while simultaneously improving fidelity, and achieving over $11\times$ size reduction over SOTA 3DGS compression approach Scaffold-GS. Our code is available here: https://github.com/YihangChen-ee/HAC

{{</citation>}}


### (26/84 | 93/241) Tensor network compressibility of convolutional models (Sukhbinder Singh et al., 2024)

{{<citation>}}

Sukhbinder Singh, Saeed S. Jahromi, Roman Orus. (2024)  
**Tensor network compressibility of convolutional models**
<br/>
<button class="copy-to-clipboard" title="Tensor network compressibility of convolutional models" index=93>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV, quant-ph  
Keyword Score: 30  
Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14379v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14379v1.pdf" filename="2403.14379v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> represent one of the most widely used neural network architectures, showcasing state-of-the-art performance in computer vision tasks. Although larger <b>CNNs</b> generally exhibit higher accuracy, their size can be effectively reduced by "tensorization" while maintaining accuracy. Tensorization consists of replacing the <b>convolution</b> kernels with compact decompositions such as Tucker, Canonical Polyadic decompositions, or quantum-inspired decompositions such as matrix product states, and directly training the factors in the decompositions to bias the learning towards low-rank decompositions. But why doesn't tensorization seem to impact the accuracy adversely? We explore this by assessing how truncating the <b>convolution</b> kernels of dense (untensorized) <b>CNNs</b> impact their accuracy. Specifically, we truncated the kernels of (i) a vanilla four-layer <b>CNN</b> and (ii) ResNet-50 pre-trained for image classification on CIFAR-10 and CIFAR-100 datasets. We found that kernels (especially those inside deeper layers) could often be truncated along several cuts resulting in significant loss in kernel norm but not in classification accuracy. This suggests that such ``correlation compression'' (underlying tensorization) is an intrinsic feature of how information is encoded in dense <b>CNNs.</b> We also found that aggressively truncated models could often recover the pre-truncation accuracy after only a few epochs of re-training, suggesting that compressing the internal correlations of <b>convolution</b> layers does not often transport the model to a worse minimum. Our results can be applied to tensorize and compress <b>CNN</b> models more effectively.

{{</citation>}}


### (27/84 | 94/241) CFPL-FAS: Class Free Prompt Learning for Generalizable Face Anti-spoofing (Ajian Liu et al., 2024)

{{<citation>}}

Ajian Liu, Shuai Xue, Jianwen Gan, Jun Wan, Yanyan Liang, Jiankang Deng, Sergio Escalera, Zhen Lei. (2024)  
**CFPL-FAS: Class Free Prompt Learning for Generalizable Face Anti-spoofing**
<br/>
<button class="copy-to-clipboard" title="CFPL-FAS: Class Free Prompt Learning for Generalizable Face Anti-spoofing" index=94>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Transformer, Prompt, Prompt Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14333v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14333v1.pdf" filename="2403.14333v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Domain generalization (DG) based Face Anti-Spoofing (FAS) aims to improve the model's performance on unseen domains. Existing methods either rely on domain labels to align domain-invariant feature spaces, or disentangle generalizable features from the whole sample, which inevitably lead to the distortion of semantic feature structures and achieve limited generalization. In this work, we make use of large-scale VLMs like CLIP and leverage the textual feature to dynamically adjust the classifier's weights for exploring generalizable visual features. Specifically, we propose a novel Class Free <b>Prompt</b> <b>Learning</b> (CFPL) paradigm for DG FAS, which utilizes two lightweight <b>transformers,</b> namely Content Q-Former (CQF) and Style Q-Former (SQF), to learn the different semantic <b>prompts</b> <b>conditioned</b> on content and style features by using a set of learnable query vectors, respectively. Thus, the generalizable <b>prompt</b> <b>can</b> be learned by two improvements: (1) A <b>Prompt-Text</b> <b>Matched</b> (PTM) supervision is introduced to ensure CQF learns visual representation that is most informative of the content description. (2) A Diversified Style <b>Prompt</b> <b>(DSP)</b> technology is proposed to diversify the learning of style <b>prompts</b> <b>by</b> mixing feature statistics between instance-specific styles. Finally, the learned text features modulate visual features to generalization through the designed <b>Prompt</b> <b>Modulation</b> (PM). Extensive experiments show that the CFPL is effective and outperforms the state-of-the-art methods on several cross-domain datasets.

{{</citation>}}


### (28/84 | 95/241) Open-Vocabulary Attention Maps with Token Optimization for Semantic Segmentation in Diffusion Models (Pablo Marcos-Manchón et al., 2024)

{{<citation>}}

Pablo Marcos-Manchón, Roberto Alcover-Couso, Juan C. SanMiguel, Jose M. Martínez. (2024)  
**Open-Vocabulary Attention Maps with Token Optimization for Semantic Segmentation in Diffusion Models**
<br/>
<button class="copy-to-clipboard" title="Open-Vocabulary Attention Maps with Token Optimization for Semantic Segmentation in Diffusion Models" index=95>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Diffusion Model, Text2image, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14291v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14291v1.pdf" filename="2403.14291v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Diffusion</b> <b>models</b> represent a new paradigm in <b>text-to-image</b> generation. Beyond generating high-quality images from text <b>prompts,</b> models such as Stable <b>Diffusion</b> <b>have</b> been successfully extended to the joint generation of semantic segmentation pseudo-masks. However, current extensions primarily rely on extracting attentions linked to <b>prompt</b> words used for image synthesis. This approach limits the generation of segmentation masks derived from word tokens not contained in the text <b>prompt.</b> In this work, we introduce Open-Vocabulary Attention Maps (OVAM)-a training-free method for <b>text-to-image</b> <b>diffusion</b> <b>models</b> that enables the generation of attention maps for any word. In addition, we propose a lightweight optimization process based on OVAM for finding tokens that generate accurate attention maps for an object class with a single annotation. We evaluate these tokens within existing state-of-the-art Stable <b>Diffusion</b> <b>extensions.</b> The best-performing model improves its mIoU from 52.1 to 86.6 for the synthetic images' pseudo-masks, demonstrating that our optimized tokens are an efficient way to improve the performance of existing methods without architectural changes or retraining.

{{</citation>}}


### (29/84 | 96/241) Weak Supervision with Arbitrary Single Frame for Micro- and Macro-expression Spotting (Wang-Wang Yu et al., 2024)

{{<citation>}}

Wang-Wang Yu, Xian-Shi Zhang, Fu-Ya Luo, Yijun Cao, Kai-Fu Yang, Hong-Mei Yan, Yong-Jie Li. (2024)  
**Weak Supervision with Arbitrary Single Frame for Micro- and Macro-expression Spotting**
<br/>
<button class="copy-to-clipboard" title="Weak Supervision with Arbitrary Single Frame for Micro- and Macro-expression Spotting" index=96>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Contrastive Learning, Weakly-supervised Learning, Weakly Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14240v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14240v1.pdf" filename="2403.14240v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Frame-level micro- and macro-expression spotting methods require time-consuming frame-by-frame observation during annotation. Meanwhile, video-level spotting lacks sufficient information about the location and number of expressions during training, resulting in significantly inferior performance compared with fully-supervised spotting. To bridge this gap, we propose a point-level <b>weakly-supervised</b> expression spotting (PWES) framework, where each expression requires to be annotated with only one random frame (i.e., a point). To mitigate the issue of sparse label distribution, the prevailing solution is pseudo-label mining, which, however, introduces new problems: localizing contextual background snippets results in inaccurate boundaries and discarding foreground snippets leads to fragmentary predictions. Therefore, we design the strategies of multi-refined pseudo label generation (MPLG) and distribution-guided feature <b>contrastive</b> <b>learning</b> (DFCL) to address these problems. Specifically, MPLG generates more reliable pseudo labels by merging class-specific probabilities, attention scores, fused features, and point-level labels. DFCL is utilized to enhance feature similarity for the same categories and feature variability for different categories while capturing global representations across the entire datasets. Extensive experiments on the CAS(ME)^2, CAS(ME)^3, and SAMM-LV datasets demonstrate PWES achieves promising performance comparable to that of recent fully-supervised methods.

{{</citation>}}


### (30/84 | 97/241) Toward Multi-class Anomaly Detection: Exploring Class-aware Unified Model against Inter-class Interference (Xi Jiang et al., 2024)

{{<citation>}}

Xi Jiang, Ying Chen, Qiang Nie, Jianlin Liu, Yong Liu, Chengjie Wang, Feng Zheng. (2024)  
**Toward Multi-class Anomaly Detection: Exploring Class-aware Unified Model against Inter-class Interference**
<br/>
<button class="copy-to-clipboard" title="Toward Multi-class Anomaly Detection: Exploring Class-aware Unified Model against Inter-class Interference" index=97>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Anomaly Detection, Supervised Learning, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14213v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14213v1.pdf" filename="2403.14213v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In the context of high usability in single-class <b>anomaly</b> <b>detection</b> models, recent academic research has become concerned about the more complex multi-class <b>anomaly</b> <b>detection.</b> Although several papers have designed unified models for this task, they often overlook the utility of class labels, a potent tool for mitigating inter-class interference. To address this issue, we introduce a Multi-class Implicit Neural representation <b>Transformer</b> for unified <b>Anomaly</b> <b>Detection</b> (MINT-AD), which leverages the fine-grained category information in the training stage. By learning the multi-class distributions, the model generates class-aware query embeddings for the <b>transformer</b> decoder, mitigating inter-class interference within the reconstruction model. Utilizing such an implicit neural representation network, MINT-AD can project category and position information into a feature embedding space, further <b>supervised</b> by classification and prior probability loss functions. Experimental results on multiple datasets demonstrate that MINT-AD outperforms existing unified training models.

{{</citation>}}


### (31/84 | 98/241) Harmonizing Visual and Textual Embeddings for Zero-Shot Text-to-Image Customization (Yeji Song et al., 2024)

{{<citation>}}

Yeji Song, Jimyeong Kim, Wonhark Park, Wonsik Shin, Wonjong Rhee, Nojun Kwak. (2024)  
**Harmonizing Visual and Textual Embeddings for Zero-Shot Text-to-Image Customization**
<br/>
<button class="copy-to-clipboard" title="Harmonizing Visual and Textual Embeddings for Zero-Shot Text-to-Image Customization" index=98>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Zero-shot, Text2image, Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14155v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14155v1.pdf" filename="2403.14155v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In a surge of <b>text-to-image</b> (T2I) models and their customization methods that generate new images of a user-provided subject, current works focus on alleviating the costs incurred by a lengthy per-subject optimization. These <b>zero-shot</b> customization methods encode the image of a specified subject into a visual embedding which is then utilized alongside the textual embedding for diffusion guidance. The visual embedding incorporates intrinsic information about the subject, while the textual embedding provides a new, transient context. However, the existing methods often 1) are significantly affected by the input images, eg., generating images with the same pose, and 2) exhibit deterioration in the subject's identity. We first pin down the problem and show that redundant pose information in the visual embedding interferes with the textual embedding containing the desired pose information. To address this issue, we propose orthogonal visual embedding which effectively harmonizes with the given textual embedding. We also adopt the visual-only embedding and inject the subject's clear features utilizing a <b>self-attention</b> swap. Our results demonstrate the effectiveness and robustness of our method, which offers highly flexible <b>zero-shot</b> generation while effectively maintaining the subject's identity.

{{</citation>}}


### (32/84 | 99/241) Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition (Sihyun Yu et al., 2024)

{{<citation>}}

Sihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, Anima Anandkumar. (2024)  
**Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition**
<br/>
<button class="copy-to-clipboard" title="Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition" index=99>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 30  
Keywords: Diffusion Model, Autoencoder, Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14148v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14148v1.pdf" filename="2403.14148v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Video <b>diffusion</b> <b>models</b> have recently made great progress in generation quality, but are still limited by the high memory and computational requirements. This is because current video <b>diffusion</b> <b>models</b> often attempt to process high-dimensional videos directly. To tackle this issue, we propose content-motion latent <b>diffusion</b> <b>model</b> (CMD), a novel efficient extension of pretrained image <b>diffusion</b> <b>models</b> for video generation. Specifically, we propose an <b>autoencoder</b> that succinctly encodes a video as a combination of a content frame (like an image) and a low-dimensional motion latent representation. The former represents the common content, and the latter represents the underlying motion in the video, respectively. We generate the content frame by <b>fine-tuning</b> a pretrained image <b>diffusion</b> <b>model,</b> and we generate the motion latent representation by training a new lightweight <b>diffusion</b> <b>model.</b> A key innovation here is the design of a compact latent space that can directly utilizes a pretrained image <b>diffusion</b> <b>model,</b> which has not been done in previous latent video <b>diffusion</b> <b>models.</b> This leads to considerably better quality generation and reduced computational costs. For instance, CMD can sample a video 7.7$\times$ faster than prior approaches by generating a video of 512$\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMD achieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previous state-of-the-art of 292.4.

{{</citation>}}


### (33/84 | 100/241) C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion (Hee Suk Yoon et al., 2024)

{{<citation>}}

Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark Hasegawa-Johnson, Yingzhen Li, Chang D. Yoo. (2024)  
**C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion**
<br/>
<button class="copy-to-clipboard" title="C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion" index=100>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV  
Keyword Score: 30  
Keywords: Fine-tuning, Prompt, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14119v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14119v1.pdf" filename="2403.14119v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In deep learning, test-time adaptation has gained attention as a method for model <b>fine-tuning</b> without the need for labeled data. A prime exemplification is the recently proposed test-time <b>prompt</b> tuning for large-scale <b>vision-language</b> models such as CLIP. Unfortunately, these <b>prompts</b> have been mainly developed to improve accuracy, overlooking the importance of calibration-a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time <b>prompt</b> tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the <b>prompt</b> choice significantly affects the calibration in CLIP, where the <b>prompts</b> leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion (ATFD), we establish its relationship with calibration error and present a novel method, Calibrated Test-time <b>Prompt</b> Tuning (C-TPT), for optimizing <b>prompts</b> during test-time with enhanced calibration. Through extensive experiments on different CLIP architectures and datasets, we show that C-TPT can effectively improve the calibration of test-time <b>prompt</b> tuning without needing labeled data.

{{</citation>}}


### (34/84 | 101/241) Semantics from Space: Satellite-Guided Thermal Semantic Segmentation Annotation for Aerial Field Robots (Connor Lee et al., 2024)

{{<citation>}}

Connor Lee, Saraswati Soedarmadji, Matthew Anderson, Anthony J. Clark, Soon-Jo Chung. (2024)  
**Semantics from Space: Satellite-Guided Thermal Semantic Segmentation Annotation for Aerial Field Robots**
<br/>
<button class="copy-to-clipboard" title="Semantics from Space: Satellite-Guided Thermal Semantic Segmentation Annotation for Aerial Field Robots" index=101>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-RO, cs.CV  
Keyword Score: 30  
Keywords: Foundation Model, Zero-shot, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14056v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14056v1.pdf" filename="2403.14056v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present a new method to automatically generate semantic segmentation annotations for thermal imagery captured from an aerial vehicle by utilizing satellite-derived data products alongside onboard global positioning and attitude estimates. This new capability overcomes the challenge of developing thermal semantic perception algorithms for field robots due to the lack of annotated thermal field datasets and the time and costs of manual annotation, enabling precise and rapid annotation of thermal data from field collection efforts at a massively-parallelizable scale. By incorporating a thermal-conditioned refinement step with visual <b>foundation</b> <b>models,</b> our approach can produce highly-precise semantic segmentation labels using low-resolution satellite land cover data for little-to-no cost. It achieves 98.5% of the performance from using costly high-resolution options and demonstrates between 70-160% improvement over popular <b>zero-shot</b> semantic segmentation methods based on large <b>vision-language</b> models currently used for generating annotations for RGB imagery. Code will be available at: https://github.com/connorlee77/aerial-auto-segment.

{{</citation>}}


### (35/84 | 102/241) GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning (Sanqing Qu et al., 2024)

{{<citation>}}

Sanqing Qu, Tianpei Zou, Florian Röhrbein, Cewu Lu, Guang Chen, Dacheng Tao, Changjun Jiang. (2024)  
**GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning**
<br/>
<button class="copy-to-clipboard" title="GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning" index=102>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs.CV  
Keyword Score: 26  
Keywords: Benchmarking, Clustering, Contrastive Learning, Domain Adaptation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14410v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14410v1.pdf" filename="2403.14410v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Deep neural networks often exhibit sub-optimal performance under covariate and category shifts. Source-Free <b>Domain</b> <b>Adaptation</b> (SFDA) presents a promising solution to this dilemma, yet most SFDA approaches are restricted to closed-set scenarios. In this paper, we explore Source-Free Universal <b>Domain</b> <b>Adaptation</b> (SF-UniDA) aiming to accurately classify "known" data belonging to common categories and segregate them from target-private "unknown" data. We propose a novel Global and Local <b>Clustering</b> (GLC) technique, which comprises an adaptive one-vs-all global <b>clustering</b> algorithm to discern between target classes, complemented by a local k-NN <b>clustering</b> strategy to mitigate negative transfer. Despite the effectiveness, the inherent closed-set source architecture leads to uniform treatment of "unknown" data, impeding the identification of distinct "unknown" categories. To address this, we evolve GLC to GLC++, integrating a <b>contrastive</b> <b>affinity</b> learning strategy. We examine the superiority of GLC and GLC++ across multiple <b>benchmarks</b> and category shift scenarios. Remarkably, in the most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by 16.7% and 18.6% in H-score on VisDA, respectively. GLC++ enhances the novel category <b>clustering</b> accuracy of GLC by 4.3% in open-set scenarios on Office-Home. Furthermore, the introduced <b>contrastive</b> <b>learning</b> strategy not only enhances GLC but also significantly facilitates existing methodologies.

{{</citation>}}


### (36/84 | 103/241) Zero-Shot Multi-Object Shape Completion (Shun Iwase et al., 2024)

{{<citation>}}

Shun Iwase, Katherine Liu, Vitor Guizilini, Adrien Gaidon, Kris Kitani, Rares Ambrus, Sergey Zakharov. (2024)  
**Zero-Shot Multi-Object Shape Completion**
<br/>
<button class="copy-to-clipboard" title="Zero-Shot Multi-Object Shape Completion" index=103>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 25  
Keywords: Geometry, Zero-shot, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14628v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14628v1.pdf" filename="2403.14628v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present a 3D shape completion method that recovers the complete <b>geometry</b> of multiple objects in complex scenes from a single RGB-D image. Despite notable advancements in single object 3D shape completion, high-quality reconstructions in highly cluttered real-world multi-object scenes remains a challenge. To address this issue, we propose OctMAE, an architecture that leverages an Octree U-Net and a latent 3D MAE to achieve high-quality and near real-time multi-object shape completion through both local and global geometric <b>reasoning.</b> Because a na\"ive 3D MAE can be computationally intractable and memory intensive even in the latent space, we introduce a novel occlusion masking strategy and adopt 3D rotary embeddings, which significantly improves the runtime and shape completion quality. To generalize to a wide range of objects in diverse scenes, we create a large-scale photorealistic dataset, featuring a diverse set of 12K 3D object models from the Objaverse dataset which are rendered in multi-object scenes with physics-based positioning. Our method outperforms the current state-of-the-art on both synthetic and real-world datasets and demonstrates a strong <b>zero-shot</b> capability.

{{</citation>}}


### (37/84 | 104/241) Science based AI model certification for untrained operational environments with application in traffic state estimation (Daryl Mupupuni et al., 2024)

{{<citation>}}

Daryl Mupupuni, Anupama Guntu, Liang Hong, Kamrul Hasan, Leehyun Keel. (2024)  
**Science based AI model certification for untrained operational environments with application in traffic state estimation**
<br/>
<button class="copy-to-clipboard" title="Science based AI model certification for untrained operational environments with application in traffic state estimation" index=104>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 25  
Keywords: Black Box, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14093v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14093v1.pdf" filename="2403.14093v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The expanding role of Artificial Intelligence (AI) in diverse engineering domains highlights the challenges associated with deploying AI models in new operational environments, involving substantial investments in data collection and model training. Rapid application of AI necessitates evaluating the feasibility of utilizing pre-trained models in unobserved operational settings with minimal or no additional data. However, interpreting the opaque nature of AI's <b>black-box</b> <b>models</b> remains a persistent challenge. Addressing this issue, this paper proposes a science-based certification methodology to assess the viability of employing pre-trained data-driven models in untrained operational environments. The methodology advocates a profound integration of domain knowledge, leveraging theoretical and analytical models from physics and related disciplines, with data-driven AI models. This novel approach introduces tools to facilitate the development of secure engineering systems, providing decision-makers with confidence in the trustworthiness and safety of AI-based models across diverse environments characterized by limited training data and dynamic, uncertain conditions. The paper demonstrates the efficacy of this methodology in real-world safety-critical scenarios, particularly in the context of traffic state estimation. Through <b>simulation</b> results, the study illustrates how the proposed methodology efficiently quantifies physical inconsistencies exhibited by pre-trained AI models. By utilizing analytical models, the methodology offers a means to gauge the applicability of pre-trained AI models in new operational environments. This research contributes to advancing the understanding and deployment of AI models, offering a robust certification framework that enhances confidence in their reliability and safety across a spectrum of operational conditions.

{{</citation>}}


### (38/84 | 105/241) DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single Video (Narek Tumanyan et al., 2024)

{{<citation>}}

Narek Tumanyan, Assaf Singer, Shai Bagon, Tali Dekel. (2024)  
**DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single Video**
<br/>
<button class="copy-to-clipboard" title="DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single Video" index=105>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Benchmarking, Self-supervised Learning, Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14548v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14548v1.pdf" filename="2403.14548v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present DINO-Tracker -- a new framework for long-term dense tracking in video. The pillar of our approach is combining test-time training on a single video, with the powerful localized semantic features learned by a pre-trained DINO-ViT model. Specifically, our framework simultaneously adopts DINO's features to fit to the motion observations of the test video, while training a tracker that directly leverages the refined features. The entire framework is trained end-to-end using a combination of <b>self-supervised</b> losses, and regularization that allows us to retain and benefit from DINO's semantic prior. Extensive evaluation demonstrates that our method achieves state-of-the-art results on known <b>benchmarks.</b> DINO-tracker significantly outperforms <b>self-supervised</b> methods and is competitive with state-of-the-art <b>supervised</b> trackers, while outperforming them in challenging cases of tracking under long-term occlusions.

{{</citation>}}


### (39/84 | 106/241) Ranking Distillation for Open-Ended Video Question Answering with Insufficient Labels (Tianming Liang et al., 2024)

{{<citation>}}

Tianming Liang, Chaolei Tan, Beihao Xia, Wei-Shi Zheng, Jian-Fang Hu. (2024)  
**Ranking Distillation for Open-Ended Video Question Answering with Insufficient Labels**
<br/>
<button class="copy-to-clipboard" title="Ranking Distillation for Open-Ended Video Question Answering with Insufficient Labels" index=106>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Benchmarking, Knowledge Distillation, Question Answering  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14430v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14430v1.pdf" filename="2403.14430v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper focuses on open-ended video <b>question</b> <b>answering,</b> which aims to find the correct answers from a large answer set in response to a video-related <b>question.</b> <b>This</b> is essentially a multi-label classification task, since a <b>question</b> <b>may</b> have multiple answers. However, due to annotation costs, the labels in existing <b>benchmarks</b> are always extremely insufficient, typically one answer per <b>question.</b> <b>As</b> a result, existing works tend to directly treat all the unlabeled answers as negative labels, leading to limited ability for generalization. In this work, we introduce a simple yet effective ranking <b>distillation</b> framework (RADI) to mitigate this problem without additional manual annotation. RADI employs a teacher model trained with incomplete labels to generate rankings for potential answers, which contain rich knowledge about label priority as well as label-associated visual cues, thereby enriching the insufficient labeling information. To avoid overconfidence in the imperfect teacher model, we further present two robust and parameter-free ranking <b>distillation</b> approaches: a pairwise approach which introduces adaptive soft margins to dynamically refine the optimization constraints on various pairwise rankings, and a listwise approach which adopts sampling-based partial listwise learning to resist the bias in teacher ranking. Extensive experiments on five popular <b>benchmarks</b> consistently show that both our pairwise and listwise RADIs outperform state-of-the-art methods. Further analysis demonstrates the effectiveness of our methods on the insufficient labeling problem.

{{</citation>}}


### (40/84 | 107/241) Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination (Dingchen Yang et al., 2024)

{{<citation>}}

Dingchen Yang, Bowen Cao, Guang Chen, Changjun Jiang. (2024)  
**Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination**
<br/>
<button class="copy-to-clipboard" title="Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination" index=107>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Multi-modal, Large Language Model, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14401v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14401v1.pdf" filename="2403.14401v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Multi-modal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) demonstrate remarkable success across various <b>vision-language</b> tasks. However, they suffer from visual hallucination, where the generated responses diverge from the provided image. Are MLLMs completely oblivious to accurate visual cues when they hallucinate? Our investigation reveals that the visual branch may simultaneously advocate both accurate and non-existent content. To address this issue, we propose Pensieve, a training-free method inspired by our observation that analogous visual hallucinations can arise among images sharing common semantic and appearance characteristics. During inference, Pensieve enables MLLMs to retrospect relevant images as references and compare them with the test image. This paradigm assists MLLMs in downgrading hallucinatory content mistakenly supported by the visual input. Experiments on Whoops, MME, POPE, and LLaVA Bench demonstrate the efficacy of Pensieve in mitigating visual hallucination, surpassing other advanced decoding strategies. Additionally, Pensieve aids MLLMs in identifying details in the image and enhancing the specificity of image descriptions.

{{</citation>}}


### (41/84 | 108/241) A Bag of Tricks for Few-Shot Class-Incremental Learning (Shuvendu Roy et al., 2024)

{{<citation>}}

Shuvendu Roy, Chunjong Park, Aldi Fahrezi, Ali Etemad. (2024)  
**A Bag of Tricks for Few-Shot Class-Incremental Learning**
<br/>
<button class="copy-to-clipboard" title="A Bag of Tricks for Few-Shot Class-Incremental Learning" index=108>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 23  
Keywords: Benchmarking, Continual Learning, Few-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14392v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14392v1.pdf" filename="2403.14392v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present a bag of tricks framework for <b>few-shot</b> class-incremental learning (FSCIL), which is a challenging form of <b>continual</b> <b>learning</b> that involves continuous adaptation to new tasks with limited samples. FSCIL requires both stability and adaptability, i.e., preserving proficiency in previously learned tasks while learning new ones. Our proposed bag of tricks brings together eight key and highly influential techniques that improve stability, adaptability, and overall performance under a unified framework for FSCIL. We organize these tricks into three categories: stability tricks, adaptability tricks, and training tricks. Stability tricks aim to mitigate the forgetting of previously learned classes by enhancing the separation between the embeddings of learned classes and minimizing interference when learning new ones. On the other hand, adaptability tricks focus on the effective learning of new classes. Finally, training tricks improve the overall performance without compromising stability or adaptability. We perform extensive experiments on three <b>benchmark</b> datasets, CIFAR-100, CUB-200, and miniIMageNet, to evaluate the impact of our proposed framework. Our detailed analysis shows that our approach substantially improves both stability and adaptability, establishing a new state-of-the-art by outperforming prior works in the area. We believe our method provides a go-to solution and establishes a robust baseline for future research in this area.

{{</citation>}}


### (42/84 | 109/241) Annotation-Efficient Polyp Segmentation via Active Learning (Duojun Huang et al., 2024)

{{<citation>}}

Duojun Huang, Xinyu Xiong, De-Jun Fan, Feng Gao, Xiao-Jian Wu, Guanbin Li. (2024)  
**Annotation-Efficient Polyp Segmentation via Active Learning**
<br/>
<button class="copy-to-clipboard" title="Annotation-Efficient Polyp Segmentation via Active Learning" index=109>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Active Learning, Clustering, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14350v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14350v1.pdf" filename="2403.14350v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Deep learning-based techniques have proven effective in polyp segmentation tasks when provided with sufficient pixel-wise labeled data. However, the high cost of manual annotation has created a bottleneck for model generalization. To minimize annotation costs, we propose a deep <b>active</b> <b>learning</b> framework for annotation-efficient polyp segmentation. In practice, we measure the uncertainty of each sample by examining the similarity between features masked by the prediction map of the polyp and the background area. Since the segmentation model tends to perform weak in samples with indistinguishable features of foreground and background areas, uncertainty sampling facilitates the fitting of under-learning data. Furthermore, <b>clustering</b> image-level features weighted by uncertainty identify samples that are both uncertain and representative. To enhance the selectivity of the <b>active</b> <b>selection</b> strategy, we propose a novel <b>unsupervised</b> feature discrepancy learning mechanism. The selection strategy and feature optimization work in tandem to achieve optimal performance with a limited annotation budget. Extensive experimental results have demonstrated that our proposed method achieved state-of-the-art performance compared to other competitors on both a public dataset and a large-scale in-house dataset.

{{</citation>}}


### (43/84 | 110/241) SoftPatch: Unsupervised Anomaly Detection with Noisy Data (Xi Jiang et al., 2024)

{{<citation>}}

Xi Jiang, Ying Chen, Qiang Nie, Yong Liu, Jianlin Liu, Bin-Bin Gao, Jun Liu, Chengjie Wang, Feng Zheng. (2024)  
**SoftPatch: Unsupervised Anomaly Detection with Noisy Data**
<br/>
<button class="copy-to-clipboard" title="SoftPatch: Unsupervised Anomaly Detection with Noisy Data" index=110>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs.CV  
Keyword Score: 23  
Keywords: Anomaly Detection, Benchmarking, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14233v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14233v1.pdf" filename="2403.14233v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Although mainstream <b>unsupervised</b> <b>anomaly</b> <b>detection</b> (AD) algorithms perform well in academic datasets, their performance is limited in practical application due to the ideal experimental setting of clean training data. Training with noisy data is an inevitable problem in real-world <b>anomaly</b> <b>detection</b> but is seldom discussed. This paper considers label-level noise in image sensory <b>anomaly</b> <b>detection</b> for the first time. To solve this problem, we proposed a memory-based <b>unsupervised</b> AD method, SoftPatch, which efficiently denoises the data at the patch level. Noise discriminators are utilized to generate outlier scores for patch-level noise elimination before coreset construction. The scores are then stored in the memory bank to soften the <b>anomaly</b> <b>detection</b> boundary. Compared with existing methods, SoftPatch maintains a strong modeling ability of normal data and alleviates the overconfidence problem in coreset. Comprehensive experiments in various noise scenes demonstrate that SoftPatch outperforms the state-of-the-art AD methods on the MVTecAD and BTAD <b>benchmarks</b> and is comparable to those methods under the setting without noise.

{{</citation>}}


### (44/84 | 111/241) Unleashing Unlabeled Data: A Paradigm for Cross-View Geo-Localization (Guopeng Li et al., 2024)

{{<citation>}}

Guopeng Li, Ming Qian, Gui-Song Xia. (2024)  
**Unleashing Unlabeled Data: A Paradigm for Cross-View Geo-Localization**
<br/>
<button class="copy-to-clipboard" title="Unleashing Unlabeled Data: A Paradigm for Cross-View Geo-Localization" index=111>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Benchmarking, Supervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14198v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14198v1.pdf" filename="2403.14198v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper investigates the effective utilization of unlabeled data for large-area cross-view geo-localization (CVGL), encompassing both <b>unsupervised</b> and semi-supervised settings. Common approaches to CVGL rely on ground-satellite image pairs and employ label-driven <b>supervised</b> training. However, the cost of collecting precise cross-view image pairs hinders the deployment of CVGL in real-life scenarios. Without the pairs, CVGL will be more challenging to handle the significant imaging and spatial gaps between ground and satellite images. To this end, we propose an <b>unsupervised</b> framework including a cross-view projection to guide the model for retrieving initial pseudo-labels and a fast re-ranking mechanism to refine the pseudo-labels by leveraging the fact that ``the perfectly paired ground-satellite image is located in a unique and identical scene". The framework exhibits competitive performance compared with <b>supervised</b> works on three open-source <b>benchmarks.</b> Our code and models will be released on https://github.com/liguopeng0923/UCVGL.

{{</citation>}}


### (45/84 | 112/241) External Knowledge Enhanced 3D Scene Generation from Sketch (Zijie Wu et al., 2024)

{{<citation>}}

Zijie Wu, Mingtao Feng, Yaonan Wang, He Xie, Weisheng Dong, Bo Miao, Ajmal Mian. (2024)  
**External Knowledge Enhanced 3D Scene Generation from Sketch**
<br/>
<button class="copy-to-clipboard" title="External Knowledge Enhanced 3D Scene Generation from Sketch" index=112>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Graph, Transformer, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14121v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14121v1.pdf" filename="2403.14121v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Generating realistic 3D scenes is challenging due to the complexity of room layouts and object geometries.We propose a sketch based knowledge enhanced diffusion architecture (SEK) for generating customized, diverse, and plausible 3D scenes. SEK conditions the denoising process with a hand-drawn sketch of the target scene and cues from an object relationship knowledge base. We first construct an external knowledge base containing object relationships and then leverage knowledge enhanced <b>graph</b> <b>reasoning</b> to assist our model in understanding hand-drawn sketches. A scene is represented as a combination of 3D objects and their relationships, and then incrementally diffused to reach a Gaussian distribution.We propose a 3D denoising scene <b>transformer</b> that learns to reverse the diffusion process, conditioned by a hand-drawn sketch along with knowledge cues, to regressively generate the scene including the 3D object instances as well as their layout. Experiments on the 3D-FRONT dataset show that our model improves FID, CKL by 17.41%, 37.18% in 3D scene generation and FID, KID by 19.12%, 20.06% in 3D scene completion compared to the nearest competitor DiffuScene.

{{</citation>}}


### (46/84 | 113/241) GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation (Yinghao Xu et al., 2024)

{{<citation>}}

Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, Gordon Wetzstein. (2024)  
**GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation**
<br/>
<button class="copy-to-clipboard" title="GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation" index=113>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Diffusion Model, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14621v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14621v1.pdf" filename="2403.14621v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward <b>transformer-based</b> model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our <b>transformer</b> architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view <b>diffusion</b> <b>models.</b> Our project website is at: https://justimyhxu.github.io/projects/grm/.

{{</citation>}}


### (47/84 | 114/241) DreamReward: Text-to-3D Generation with Human Preference (Junliang Ye et al., 2024)

{{<citation>}}

Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou Wang, Yueqi Duan, Jun Zhu. (2024)  
**DreamReward: Text-to-3D Generation with Human Preference**
<br/>
<button class="copy-to-clipboard" title="DreamReward: Text-to-3D Generation with Human Preference" index=114>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CL, cs-CV, cs-LG, cs.CV  
Keyword Score: 20  
Keywords: Diffusion Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14613v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14613v1.pdf" filename="2403.14613v1.pdf">Download PDF</button>

---


**ABSTRACT**  
3D content creation from text <b>prompts</b> has shown remarkable success recently. However, current text-to-3D methods often generate 3D results that do not align well with human preferences. In this paper, we present a comprehensive framework, coined DreamReward, to learn and improve text-to-3D models from human preference feedback. To begin with, we collect 25k expert comparisons based on a systematic annotation pipeline including rating and ranking. Then, we build Reward3D -- the first general-purpose text-to-3D human preference reward model to effectively encode human preferences. Building upon the 3D reward model, we finally perform theoretical analysis and present the Reward3D Feedback Learning (DreamFL), a direct tuning algorithm to optimize the multi-view <b>diffusion</b> <b>models</b> with a redefined scorer. Grounded by theoretical proof and extensive experiment comparisons, our DreamReward successfully generates high-fidelity and 3D consistent results with significant boosts in <b>prompt</b> alignment with human intention. Our results demonstrate the great potential for learning from human feedback to improve text-to-3D models.

{{</citation>}}


### (48/84 | 115/241) Implicit Style-Content Separation using B-LoRA (Yarden Frenkel et al., 2024)

{{<citation>}}

Yarden Frenkel, Yael Vinker, Ariel Shamir, Daniel Cohen-Or. (2024)  
**Implicit Style-Content Separation using B-LoRA**
<br/>
<button class="copy-to-clipboard" title="Implicit Style-Content Separation using B-LoRA" index=115>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Fine-tuning, Style Transfer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14572v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14572v1.pdf" filename="2403.14572v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Image stylization involves manipulating the visual appearance and texture <b>(style)</b> <b>of</b> an image while preserving its underlying objects, structures, and concepts (content). The separation of <b>style</b> <b>and</b> content is essential for manipulating the image's <b>style</b> <b>independently</b> from its content, ensuring a harmonious and visually pleasing result. Achieving this separation requires a deep understanding of both the visual and semantic characteristics of images, often necessitating the training of specialized models or employing heavy optimization. In this paper, we introduce B-LoRA, a method that leverages LoRA (Low-Rank Adaptation) to implicitly separate the <b>style</b> <b>and</b> content components of a single image, facilitating various image stylization tasks. By analyzing the architecture of SDXL combined with LoRA, we find that jointly learning the LoRA weights of two specific blocks (referred to as B-LoRAs) achieves <b>style-content</b> <b>separation</b> that cannot be achieved by training each B-LoRA independently. Consolidating the training into only two blocks and separating <b>style</b> <b>and</b> content allows for significantly improving <b>style</b> <b>manipulation</b> and overcoming overfitting issues often associated with model <b>fine-tuning.</b> Once trained, the two B-LoRAs can be used as independent components to allow various image stylization tasks, including image <b>style</b> <b>transfer,</b> text-based image stylization, consistent <b>style</b> <b>generation,</b> and <b>style-content</b> <b>mixing.</b>

{{</citation>}}


### (49/84 | 116/241) DesignEdit: Multi-Layered Latent Decomposition and Fusion for Unified & Accurate Image Editing (Yueru Jia et al., 2024)

{{<citation>}}

Yueru Jia, Yuhui Yuan, Aosong Cheng, Chuke Wang, Ji Li, Huizhu Jia, Shanghang Zhang. (2024)  
**DesignEdit: Multi-Layered Latent Decomposition and Fusion for Unified & Accurate Image Editing**
<br/>
<button class="copy-to-clipboard" title="DesignEdit: Multi-Layered Latent Decomposition and Fusion for Unified & Accurate Image Editing" index=116>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Text2image, Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14487v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14487v1.pdf" filename="2403.14487v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recently, how to achieve precise image editing has attracted increasing attention, especially given the remarkable success of <b>text-to-image</b> generation models. To unify various spatial-aware image editing abilities into one framework, we adopt the concept of layers from the design domain to manipulate objects flexibly with various operations. The key insight is to transform the spatial-aware image editing task into a combination of two sub-tasks: multi-layered latent decomposition and multi-layered latent fusion. First, we segment the latent representations of the source images into multiple layers, which include several object layers and one incomplete background layer that necessitates reliable inpainting. To avoid extra tuning, we further explore the inner inpainting ability within the <b>self-attention</b> mechanism. We introduce a key-masking <b>self-attention</b> scheme that can propagate the surrounding context information into the masked region while mitigating its impact on the regions outside the mask. Second, we propose an instruction-guided latent fusion that pastes the multi-layered latent representations onto a canvas latent. We also introduce an artifact suppression scheme in the latent space to enhance the inpainting quality. Due to the inherent modular advantages of such multi-layered representations, we can achieve accurate image editing, and we demonstrate that our approach consistently surpasses the latest spatial editing methods, including Self-Guidance and DiffEditor. Last, we show that our approach is a unified framework that supports various accurate image editing tasks on more than six different editing tasks.

{{</citation>}}


### (50/84 | 117/241) AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks (Max Ku et al., 2024)

{{<citation>}}

Max Ku, Cong Wei, Weiming Ren, Huan Yang, Wenhu Chen. (2024)  
**AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks**
<br/>
<button class="copy-to-clipboard" title="AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks" index=117>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-MM, cs.CV  
Keyword Score: 20  
Keywords: Style Transfer, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14468v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14468v1.pdf" filename="2403.14468v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Video-to-video editing involves editing a source video along with additional control (such as text <b>prompts,</b> subjects, or <b>styles)</b> <b>to</b> generate a new video that aligns with the source video and the provided control. Traditional methods have been constrained to certain editing types, limiting their ability to meet the wide range of user demands. In this paper, we introduce AnyV2V, a novel training-free framework designed to simplify video editing into two primary steps: (1) employing an off-the-shelf image editing model (e.g. InstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an existing image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion and feature injection. In the first stage, AnyV2V can plug in any existing image editing tools to support an extensive array of video editing tasks. Beyond the traditional <b>prompt-based</b> editing methods, AnyV2V also can support novel video editing tasks, including reference-based <b>style</b> <b>transfer,</b> subject-driven editing, and identity manipulation, which were unattainable by previous methods. In the second stage, AnyV2V can plug in any existing image-to-video models to perform DDIM inversion and intermediate feature injection to maintain the appearance and motion consistency with the source video. On the <b>prompt-based</b> editing, we show that AnyV2V can outperform the previous best approach by 35\% on <b>prompt</b> alignment, and 25\% on human preference. On the three novel tasks, we show that AnyV2V also achieves a high success rate. We believe AnyV2V will continue to thrive due to its ability to seamlessly integrate the fast-evolving image editing methods. Such compatibility can help AnyV2V to increase its versatility to cater to diverse user demands.

{{</citation>}}


### (51/84 | 118/241) Style-Extracting Diffusion Models for Semi-Supervised Histopathology Segmentation (Mathias Öttl et al., 2024)

{{<citation>}}

Mathias Öttl, Frauke Wilm, Jana Steenpass, Jingna Qiu, Matthias Rübner, Arndt Hartmann, Matthias Beckmann, Peter Fasching, Andreas Maier, Ramona Erber, Bernhard Kainz, Katharina Breininger. (2024)  
**Style-Extracting Diffusion Models for Semi-Supervised Histopathology Segmentation**
<br/>
<button class="copy-to-clipboard" title="Style-Extracting Diffusion Models for Semi-Supervised Histopathology Segmentation" index=118>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs.CV  
Keyword Score: 20  
Keywords: Diffusion Model, Zero-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14429v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14429v1.pdf" filename="2403.14429v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Deep learning-based image generation has seen significant advancements with <b>diffusion</b> <b>models,</b> notably improving the quality of generated images. Despite these developments, generating images with unseen characteristics beneficial for downstream tasks has received limited attention. To bridge this gap, we propose Style-Extracting <b>Diffusion</b> <b>Models,</b> featuring two conditioning mechanisms. Specifically, we utilize 1) a style conditioning mechanism which allows to inject style information of previously unseen images during image generation and 2) a content conditioning which can be targeted to a downstream task, e.g., layout for segmentation. We introduce a trainable style encoder to extract style information from images, and an aggregation block that merges style information from multiple style inputs. This architecture enables the generation of images with unseen styles in a <b>zero-shot</b> manner, by leveraging styles from unseen images, resulting in more diverse generations. In this work, we use the image layout as target condition and first show the capability of our method on a natural image dataset as a proof-of-concept. We further demonstrate its versatility in histopathology, where we combine prior knowledge about tissue composition and unannotated data to create diverse synthetic images with known layouts. This allows us to generate additional synthetic data to train a segmentation network in a semi-supervised fashion. We verify the added value of the generated images by showing improved segmentation results and lower performance variability between patients when synthetic images are included during segmentation training. Our code will be made publicly available at [LINK].

{{</citation>}}


### (52/84 | 119/241) Enabling Visual Composition and Animation in Unsupervised Video Generation (Aram Davtyan et al., 2024)

{{<citation>}}

Aram Davtyan, Sepehr Sameni, Björn Ommer, Paolo Favaro. (2024)  
**Enabling Visual Composition and Animation in Unsupervised Video Generation**
<br/>
<button class="copy-to-clipboard" title="Enabling Visual Composition and Animation in Unsupervised Video Generation" index=119>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Self-supervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14368v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14368v1.pdf" filename="2403.14368v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this work we propose a novel method for <b>unsupervised</b> controllable video generation. Once trained on a dataset of unannotated videos, at inference our model is capable of both composing scenes of predefined object parts and animating them in a plausible and controlled way. This is achieved by conditioning video generation on a randomly selected subset of local pre-trained <b>self-supervised</b> features during training. We call our model CAGE for visual Composition and Animation for video GEneration. We conduct a series of experiments to demonstrate capabilities of CAGE in various settings. Project website: https://araachie.github.io/cage.

{{</citation>}}


### (53/84 | 120/241) SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance Field (Lizhe Liu et al., 2024)

{{<citation>}}

Lizhe Liu, Bohua Wang, Hongwei Xie, Daqi Liu, Li Liu, Zhiqiang Tian, Kuiyuan Yang, Bing Wang. (2024)  
**SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance Field**
<br/>
<button class="copy-to-clipboard" title="SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance Field" index=120>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Supervised Learning, Weakly-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14366v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14366v1.pdf" filename="2403.14366v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Vision-centric 3D environment understanding is both vital and challenging for autonomous driving systems. Recently, object-free methods have attracted considerable attention. Such methods perceive the world by predicting the semantics of discrete voxel grids but fail to construct continuous and accurate obstacle surfaces. To this end, in this paper, we propose SurroundSDF to implicitly predict the signed distance field (SDF) and semantic field for the continuous perception from surround images. Specifically, we introduce a query-based approach and utilize SDF constrained by the Eikonal formulation to accurately describe the surfaces of obstacles. Furthermore, considering the absence of precise SDF ground truth, we propose a novel weakly <b>supervised</b> paradigm for SDF, referred to as the Sandwich Eikonal formulation, which emphasizes applying correct and dense constraints on both sides of the surface, thereby enhancing the perceptual accuracy of the surface. Experiments suggest that our method achieves SOTA for both occupancy prediction and 3D scene reconstruction tasks on the nuScenes dataset.

{{</citation>}}


### (54/84 | 121/241) Varroa destructor detection on honey bees using hyperspectral imagery (Zina-Sabrina Duma et al., 2024)

{{<citation>}}

Zina-Sabrina Duma, Tomas Zemcik, Simon Bilik, Tuomas Sihvonen, Peter Honec, Satu-Pia Reinikainen, Karel Horak. (2024)  
**Varroa destructor detection on honey bees using hyperspectral imagery**
<br/>
<button class="copy-to-clipboard" title="Varroa destructor detection on honey bees using hyperspectral imagery" index=121>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 20  
Keywords: Supervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14359v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14359v1.pdf" filename="2403.14359v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Hyperspectral (HS) imagery in agriculture is becoming increasingly common. These images have the advantage of higher spectral resolution. Advanced spectral processing techniques are required to unlock the information potential in these HS images. The present paper introduces a method rooted in multivariate statistics designed to detect parasitic Varroa destructor mites on the body of western honey bee Apis mellifera, enabling easier and continuous monitoring of the bee hives. The methodology explores <b>unsupervised</b> (K-means++) and recently developed <b>supervised</b> (Kernel Flows - Partial Least-Squares, KF-PLS) methods for parasitic identification. Additionally, in light of the emergence of custom-band multispectral cameras, the present research outlines a strategy for identifying the specific wavelengths necessary for effective bee-mite separation, suitable for implementation in a custom-band camera. Illustrated with a real-case dataset, our findings demonstrate that as few as four spectral bands are sufficient for accurate parasite identification.

{{</citation>}}


### (55/84 | 122/241) Zero123-6D: Zero-shot Novel View Synthesis for RGB Category-level 6D Pose Estimation (Francesco Di Felice et al., 2024)

{{<citation>}}

Francesco Di Felice, Alberto Remus, Stefano Gasperini, Benjamin Busam, Lionel Ott, Federico Tombari, Roland Siegwart, Carlo Alberto Avizzano. (2024)  
**Zero123-6D: Zero-shot Novel View Synthesis for RGB Category-level 6D Pose Estimation**
<br/>
<button class="copy-to-clipboard" title="Zero123-6D: Zero-shot Novel View Synthesis for RGB Category-level 6D Pose Estimation" index=122>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Diffusion Model, Zero-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14279v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14279v1.pdf" filename="2403.14279v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Estimating the pose of objects through vision is essential to make robotic platforms interact with the environment. Yet, it presents many challenges, often related to the lack of flexibility and generalizability of state-of-the-art solutions. <b>Diffusion</b> <b>models</b> are a cutting-edge neural architecture transforming 2D and 3D computer vision, outlining remarkable performances in <b>zero-shot</b> novel-view synthesis. Such a use case is particularly intriguing for reconstructing 3D objects. However, localizing objects in unstructured environments is rather unexplored. To this end, this work presents Zero123-6D to demonstrate the utility of <b>Diffusion</b> <b>Model-based</b> novel-view-synthesizers in enhancing RGB 6D pose estimation at category-level by integrating them with feature extraction techniques. The outlined method exploits such a novel view synthesizer to expand a sparse set of RGB-only reference views for the <b>zero-shot</b> 6D pose estimation task. Experiments are quantitatively analyzed on the CO3D dataset, showcasing increased performance over baselines, a substantial reduction in data requirements, and the removal of the necessity of depth information.

{{</citation>}}


### (56/84 | 123/241) On the Concept Trustworthiness in Concept Bottleneck Models (Qihan Huang et al., 2024)

{{<citation>}}

Qihan Huang, Jie Song, Jingwen Hu, Haofei Zhang, Yong Wang, Mingli Song. (2024)  
**On the Concept Trustworthiness in Concept Bottleneck Models**
<br/>
<button class="copy-to-clipboard" title="On the Concept Trustworthiness in Concept Bottleneck Models" index=123>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 18  
Keywords: Benchmarking, Black Box, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14349v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14349v1.pdf" filename="2403.14349v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Concept Bottleneck Models (CBMs), which break down the <b>reasoning</b> process into the input-to-concept mapping and the concept-to-label prediction, have garnered significant attention due to their remarkable interpretability achieved by the interpretable concept bottleneck. However, despite the transparency of the concept-to-label prediction, the mapping from the input to the intermediate concept remains a <b>black</b> <b>box,</b> giving rise to concerns about the trustworthiness of the learned concepts (i.e., these concepts may be predicted based on spurious cues). The issue of concept untrustworthiness greatly hampers the interpretability of CBMs, thereby hindering their further advancement. To conduct a comprehensive analysis on this issue, in this study we establish a <b>benchmark</b> to assess the trustworthiness of concepts in CBMs. A pioneering metric, referred to as concept trustworthiness score, is proposed to gauge whether the concepts are derived from relevant regions. Additionally, an enhanced CBM is introduced, enabling concept predictions to be made specifically from distinct parts of the feature map, thereby facilitating the exploration of their related regions. Besides, we introduce three modules, namely the cross-layer alignment (CLA) module, the cross-image alignment (CIA) module, and the prediction alignment (PA) module, to further enhance the concept trustworthiness within the elaborated CBM. The experiments on five datasets across ten architectures demonstrate that without using any concept localization annotations during training, our model improves the concept trustworthiness by a large margin, meanwhile achieving superior accuracy to the state-of-the-arts. Our code is available at https://github.com/hqhQAQ/ProtoCBM.

{{</citation>}}


### (57/84 | 124/241) Volumetric Environment Representation for Vision-Language Navigation (Rui Liu et al., 2024)

{{<citation>}}

Rui Liu, Wenguan Wang, Yi Yang. (2024)  
**Volumetric Environment Representation for Vision-Language Navigation**
<br/>
<button class="copy-to-clipboard" title="Volumetric Environment Representation for Vision-Language Navigation" index=124>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 18  
Keywords: Benchmarking, Geometry, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14158v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14158v1.pdf" filename="2403.14158v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Vision-language</b> navigation (VLN) requires an agent to navigate through an 3D environment based on visual observations and natural language instructions. It is clear that the pivotal factor for successful navigation lies in the comprehensive scene understanding. Previous VLN agents employ monocular frameworks to extract 2D features of perspective views directly. Though straightforward, they struggle for capturing 3D <b>geometry</b> and semantics, leading to a partial and incomplete environment representation. To achieve a comprehensive 3D representation with fine-grained details, we introduce a Volumetric Environment Representation (VER), which voxelizes the physical world into structured 3D cells. For each cell, VER aggregates multi-view 2D features into such a unified 3D space via 2D-3D sampling. Through coarse-to-fine feature extraction and multi-task learning for VER, our agent predicts 3D occupancy, 3D room layout, and 3D bounding boxes jointly. Based on online collected VERs, our agent performs volume state estimation and builds episodic memory for predicting the next step. Experimental results show our environment representations from multi-task learning lead to evident performance gains on VLN. Our model achieves state-of-the-art performance across VLN <b>benchmarks</b> (R2R, REVERIE, and R4R).

{{</citation>}}


### (58/84 | 125/241) Learning Decomposable and Debiased Representations via Attribute-Centric Information Bottlenecks (Jinyung Hong et al., 2024)

{{<citation>}}

Jinyung Hong, Eun Som Jeon, Changhoon Kim, Keun Hee Park, Utkarsh Nath, Yezhou Yang, Pavan Turaga, Theodore P. Pavlic. (2024)  
**Learning Decomposable and Debiased Representations via Attribute-Centric Information Bottlenecks**
<br/>
<button class="copy-to-clipboard" title="Learning Decomposable and Debiased Representations via Attribute-Centric Information Bottlenecks" index=125>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 15  
Keywords: Out-of-distribution, Representation Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14140v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14140v1.pdf" filename="2403.14140v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Biased attributes, spuriously correlated with target labels in a dataset, can problematically lead to neural networks that learn improper shortcuts for classifications and limit their capabilities for <b>out-of-distribution</b> (OOD) generalization. Although many debiasing approaches have been proposed to ensure correct predictions from biased datasets, few studies have considered learning latent embedding consisting of intrinsic and biased attributes that contribute to improved performance and explain how the model pays attention to attributes. In this paper, we propose a novel debiasing framework, Debiasing Global Workspace, introducing attention-based information bottlenecks for learning compositional <b>representations</b> <b>of</b> attributes without defining specific bias types. Based on our observation that learning shape-centric <b>representation</b> <b>helps</b> robust performance on OOD datasets, we adopt those abilities to learn robust and generalizable <b>representations</b> <b>of</b> decomposable latent embeddings corresponding to intrinsic and biasing attributes. We conduct comprehensive evaluations on biased datasets, along with both quantitative and qualitative analyses, to showcase our approach's efficacy in attribute-centric <b>representation</b> <b>learning</b> and its ability to differentiate between intrinsic and bias-related features.

{{</citation>}}


### (59/84 | 126/241) VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition (Yun-Jin Li et al., 2024)

{{<citation>}}

Yun-Jin Li, Mariia Gladkova, Yan Xia, Rui Wang, Daniel Cremers. (2024)  
**VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition**
<br/>
<button class="copy-to-clipboard" title="VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition" index=126>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-RO, cs.CV  
Keyword Score: 13  
Keywords: Benchmarking, Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14594v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14594v1.pdf" filename="2403.14594v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent works on the global place recognition treat the task as a retrieval problem, where an off-the-shelf global descriptor is commonly designed in image-based and LiDAR-based modalities. However, it is non-trivial to perform accurate image-LiDAR global place recognition since extracting consistent and robust global descriptors from different domains (2D images and 3D point clouds) is challenging. To address this issue, we propose a novel Voxel-Cross-Pixel (VXP) approach, which establishes voxel and pixel correspondences in a <b>self-supervised</b> manner and brings them into a shared feature space. Specifically, VXP is trained in a two-stage manner that first explicitly exploits local feature correspondences and enforces similarity of global descriptors. Extensive experiments on the three <b>benchmarks</b> (Oxford RobotCar, ViViD++ and KITTI) demonstrate our method surpasses the state-of-the-art cross-modal retrieval by a large margin.

{{</citation>}}


### (60/84 | 127/241) MULDE: Multiscale Log-Density Estimation via Denoising Score Matching for Video Anomaly Detection (Jakub Micorek et al., 2024)

{{<citation>}}

Jakub Micorek, Horst Possegger, Dominik Narnhofer, Horst Bischof, Mateusz Kozinski. (2024)  
**MULDE: Multiscale Log-Density Estimation via Denoising Score Matching for Video Anomaly Detection**
<br/>
<button class="copy-to-clipboard" title="MULDE: Multiscale Log-Density Estimation via Denoising Score Matching for Video Anomaly Detection" index=127>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 13  
Keywords: Anomaly Detection, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14497v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14497v1.pdf" filename="2403.14497v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We propose a novel approach to video <b>anomaly</b> <b>detection:</b> we treat feature vectors extracted from videos as realizations of a random variable with a fixed distribution and model this distribution with a neural network. This lets us estimate the likelihood of test videos and detect video anomalies by thresholding the likelihood estimates. We train our video <b>anomaly</b> <b>detector</b> using a modification of denoising score matching, a method that injects training data with noise to facilitate modeling its distribution. To eliminate hyperparameter selection, we model the distribution of noisy video features across a range of noise levels and introduce a regularizer that tends to align the models for different levels of noise. At test time, we combine <b>anomaly</b> <b>indications</b> at multiple noise scales with a Gaussian mixture model. Running our video <b>anomaly</b> <b>detector</b> induces minimal delays as inference requires merely extracting the features and forward-propagating them through a shallow neural network and a Gaussian mixture model. Our experiments on five popular video <b>anomaly</b> <b>detection</b> <b>benchmarks</b> demonstrate state-of-the-art performance, both in the object-centric and in the frame-centric setup.

{{</citation>}}


### (61/84 | 128/241) LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors (Saksham Suri et al., 2024)

{{<citation>}}

Saksham Suri, Matthew Walmer, Kamal Gupta, Abhinav Shrivastava. (2024)  
**LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors**
<br/>
<button class="copy-to-clipboard" title="LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors" index=128>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14625v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14625v1.pdf" filename="2403.14625v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present a simple <b>self-supervised</b> method to enhance the performance of ViT features for dense downstream tasks. Our Lightweight Feature Transform (LiFT) is a straightforward and compact postprocessing network that can be applied to enhance the features of any pre-trained ViT backbone. LiFT is fast and easy to train with a <b>self-supervised</b> objective, and it boosts the density of ViT features for minimal extra inference cost. Furthermore, we demonstrate that LiFT can be applied with approaches that use additional task-specific downstream modules, as we integrate LiFT with ViTDet for COCO detection and segmentation. Despite the simplicity of LiFT, we find that it is not simply learning a more complex version of bilinear interpolation. Instead, our LiFT training protocol leads to several desirable emergent properties that benefit ViT features in dense downstream tasks. This includes greater scale invariance for features, and better object boundary maps. By simply training LiFT for a few epochs, we show improved performance on keypoint correspondence, detection, segmentation, and object discovery tasks. Overall, LiFT provides an easy way to unlock the benefits of denser feature arrays for a fraction of the computational cost. For more details, refer to our project page at https://www.cs.umd.edu/~sakshams/LiFT/.

{{</citation>}}


### (62/84 | 129/241) Explorative Inbetweening of Time and Space (Haiwen Feng et al., 2024)

{{<citation>}}

Haiwen Feng, Zheng Ding, Zhihao Xia, Simon Niklaus, Victoria Abrevaya, Michael J. Black, Xuaner Zhang. (2024)  
**Explorative Inbetweening of Time and Space**
<br/>
<button class="copy-to-clipboard" title="Explorative Inbetweening of Time and Space" index=129>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14611v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14611v1.pdf" filename="2403.14611v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We introduce bounded generation as a generalized task to control video generation to synthesize arbitrary camera and subject motion based only on a given start and end frame. Our objective is to fully leverage the inherent generalization capability of an image-to-video model without additional training or <b>fine-tuning</b> of the original model. This is achieved through the proposed new sampling strategy, which we call Time Reversal Fusion, that fuses the temporally forward and backward denoising paths conditioned on the start and end frame, respectively. The fused path results in a video that smoothly connects the two frames, generating inbetweening of faithful subject motion, novel views of static scenes, and seamless video looping when the two bounding frames are identical. We curate a diverse evaluation dataset of image pairs and compare against the closest existing methods. We find that Time Reversal Fusion outperforms related work on all subtasks, exhibiting the ability to generate complex motions and 3D-consistent views guided by bounded frames. See project page at https://time-reversal.github.io.

{{</citation>}}


### (63/84 | 130/241) ReNoise: Real Image Inversion Through Iterative Noising (Daniel Garibi et al., 2024)

{{<citation>}}

Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, Daniel Cohen-Or. (2024)  
**ReNoise: Real Image Inversion Through Iterative Noising**
<br/>
<button class="copy-to-clipboard" title="ReNoise: Real Image Inversion Through Iterative Noising" index=130>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-GR, cs-LG, cs.CV, eess-IV  
Keyword Score: 10  
Keywords: Diffusion Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14602v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14602v1.pdf" filename="2403.14602v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advancements in text-guided <b>diffusion</b> <b>models</b> have unlocked powerful image manipulation capabilities. However, applying these methods to real images necessitates the inversion of the images into the domain of the pretrained <b>diffusion</b> <b>model.</b> Achieving faithful inversion remains a challenge, particularly for more recent models trained to generate images with a small number of denoising steps. In this work, we introduce an inversion method with a high quality-to-operation ratio, enhancing reconstruction accuracy without increasing the number of operations. Building on reversing the <b>diffusion</b> <b>sampling</b> process, our method employs an iterative renoising mechanism at each inversion sampling step. This mechanism refines the approximation of a predicted point along the forward <b>diffusion</b> <b>trajectory,</b> by iteratively applying the pretrained <b>diffusion</b> <b>model,</b> and averaging these predictions. We evaluate the performance of our ReNoise technique using various sampling algorithms and models, including recent accelerated <b>diffusion</b> <b>models.</b> Through comprehensive evaluations and comparisons, we show its effectiveness in terms of both accuracy and speed. Furthermore, we confirm that our method preserves editability by demonstrating text-driven image editing on real images.

{{</citation>}}


### (64/84 | 131/241) View-decoupled Transformer for Person Re-identification under Aerial-ground Camera Network (Quan Zhang et al., 2024)

{{<citation>}}

Quan Zhang, Lei Wang, Vishal M. Patel, Xiaohua Xie, Jianhuang Lai. (2024)  
**View-decoupled Transformer for Person Re-identification under Aerial-ground Camera Network**
<br/>
<button class="copy-to-clipboard" title="View-decoupled Transformer for Person Re-identification under Aerial-ground Camera Network" index=131>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14513v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14513v1.pdf" filename="2403.14513v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Existing person re-identification methods have achieved remarkable advances in appearance-based identity association across homogeneous cameras, such as ground-ground matching. However, as a more practical scenario, aerial-ground person re-identification (AGPReID) among heterogeneous cameras has received minimal attention. To alleviate the disruption of discriminative identity representation by dramatic view discrepancy as the most significant challenge in AGPReID, the view-decoupled <b>transformer</b> (VDT) is proposed as a simple yet effective framework. Two major components are designed in VDT to decouple view-related and view-unrelated features, namely hierarchical subtractive separation and orthogonal loss, where the former separates these two features inside the VDT, and the latter constrains these two to be independent. In addition, we contribute a large-scale AGPReID dataset called CARGO, consisting of five/eight aerial/ground cameras, 5,000 identities, and 108,563 images. Experiments on two datasets show that VDT is a feasible and effective solution for AGPReID, surpassing the previous method on mAP/Rank1 by up to 5.0%/2.7% on CARGO and 3.7%/5.2% on AG-ReID, keeping the same magnitude of computational complexity. Our project is available at https://github.com/LinlyAC/VDT-AGPReID

{{</citation>}}


### (65/84 | 132/241) Raw Instinct: Trust Your Classifiers and Skip the Conversion (Christos Kantas et al., 2024)

{{<citation>}}

Christos Kantas, Bjørk Antoniussen, Mathias V. Andersen, Rasmus Munksø, Shobhit Kotnala, Simon B. Jensen, Andreas Møgelmose, Lau Nørgaard, Thomas B. Moeslund. (2024)  
**Raw Instinct: Trust Your Classifiers and Skip the Conversion**
<br/>
<button class="copy-to-clipboard" title="Raw Instinct: Trust Your Classifiers and Skip the Conversion" index=132>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14439v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14439v1.pdf" filename="2403.14439v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Using RAW-images in computer vision problems is surprisingly underexplored considering that converting from RAW to RGB does not introduce any new capture information. In this paper, we show that a sufficiently advanced classifier can yield equivalent results on RAW input compared to RGB and present a new public dataset consisting of RAW images and the corresponding converted RGB images. Classifying images directly from RAW is attractive, as it allows for skipping the conversion to RGB, lowering computation time significantly. Two <b>CNN</b> classifiers are used to classify the images in both formats, confirming that classification performance can indeed be preserved. We furthermore show that the total computation time from RAW image data to classification results for RAW images can be up to 8.46 times faster than RGB. These results contribute to the evidence found in related works, that using RAW images as direct input to computer vision algorithms looks very promising.

{{</citation>}}


### (66/84 | 133/241) CombiNeRF: A Combination of Regularization Techniques for Few-Shot Neural Radiance Field View Synthesis (Matteo Bonotto et al., 2024)

{{<citation>}}

Matteo Bonotto, Luigi Sarrocco, Daniele Evangelista, Marco Imperoli, Alberto Pretto. (2024)  
**CombiNeRF: A Combination of Regularization Techniques for Few-Shot Neural Radiance Field View Synthesis**
<br/>
<button class="copy-to-clipboard" title="CombiNeRF: A Combination of Regularization Techniques for Few-Shot Neural Radiance Field View Synthesis" index=133>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Few-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14412v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14412v1.pdf" filename="2403.14412v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Neural Radiance Fields (NeRFs) have shown impressive results for novel view synthesis when a sufficiently large amount of views are available. When dealing with <b>few-shot</b> settings, i.e. with a small set of input views, the training could overfit those views, leading to artifacts and geometric and chromatic inconsistencies in the resulting rendering. Regularization is a valid solution that helps NeRF generalization. On the other hand, each of the most recent NeRF regularization techniques aim to mitigate a specific rendering problem. Starting from this observation, in this paper we propose CombiNeRF, a framework that synergically combines several regularization techniques, some of them novel, in order to unify the benefits of each. In particular, we regularize single and neighboring rays distributions and we add a smoothness term to regularize near geometries. After these geometric approaches, we propose to exploit Lipschitz regularization to both NeRF density and color networks and to use encoding masks for input features regularization. We show that CombiNeRF outperforms the state-of-the-art methods with <b>few-shot</b> settings in several publicly available datasets. We also present an ablation study on the LLFF and NeRF-Synthetic datasets that support the choices made. We release with this paper the open-source implementation of our framework.

{{</citation>}}


### (67/84 | 134/241) LDTR: Transformer-based Lane Detection with Anchor-chain Representation (Zhongyu Yang et al., 2024)

{{<citation>}}

Zhongyu Yang, Chen Shen, Wei Shao, Tengfei Xing, Runbo Hu, Pengfei Xu, Hua Chai, Ruini Xue. (2024)  
**LDTR: Transformer-based Lane Detection with Anchor-chain Representation**
<br/>
<button class="copy-to-clipboard" title="LDTR: Transformer-based Lane Detection with Anchor-chain Representation" index=134>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14354v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14354v1.pdf" filename="2403.14354v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Despite recent advances in lane detection methods, scenarios with limited- or no-visual-clue of lanes due to factors such as lighting conditions and occlusion remain challenging and crucial for automated driving. Moreover, current lane representations require complex post-processing and struggle with specific instances. Inspired by the DETR architecture, we propose LDTR, a <b>transformer-based</b> model to address these issues. Lanes are modeled with a novel anchor-chain, regarding a lane as a whole from the beginning, which enables LDTR to handle special lanes inherently. To enhance lane instance perception, LDTR incorporates a novel multi-referenced deformable attention module to distribute attention around the object. Additionally, LDTR incorporates two line IoU algorithms to improve convergence efficiency and employs a Gaussian heatmap auxiliary branch to enhance model representation capability during training. To evaluate lane detection models, we rely on Frechet distance, parameterized F1-score, and additional synthetic metrics. Experimental results demonstrate that LDTR achieves state-of-the-art performance on well-known datasets.

{{</citation>}}


### (68/84 | 135/241) Towards Efficient Information Fusion: Concentric Dual Fusion Attention Based Multiple Instance Learning for Whole Slide Images (Yujian Liu et al., 2024)

{{<citation>}}

Yujian Liu, Ruoxuan Wu, Xinjie Shen, Zihuang Lu, Lingyu Liang, Haiyu Zhou, Shipu Xu, Shaoai Cai, Shidang Xu. (2024)  
**Towards Efficient Information Fusion: Concentric Dual Fusion Attention Based Multiple Instance Learning for Whole Slide Images**
<br/>
<button class="copy-to-clipboard" title="Towards Efficient Information Fusion: Concentric Dual Fusion Attention Based Multiple Instance Learning for Whole Slide Images" index=135>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Multiple Instance Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14346v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14346v1.pdf" filename="2403.14346v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In the realm of digital pathology, multi-magnification <b>Multiple</b> <b>Instance</b> <b>Learning</b> (multi-mag MIL) has proven effective in leveraging the hierarchical structure of Whole Slide Images (WSIs) to reduce information loss and redundant data. However, current methods fall short in bridging the domain gap between pretrained models and medical imaging, and often fail to account for spatial relationships across different magnifications. Addressing these challenges, we introduce the Concentric Dual Fusion Attention-MIL (CDFA-MIL) framework,which innovatively combines point-to-area feature-colum attention and point-to-point concentric-row attention using concentric patch. This approach is designed to effectively fuse correlated information, enhancing feature representation and providing stronger correlation guidance for WSI analysis. CDFA-MIL distinguishes itself by offering a robust fusion strategy that leads to superior WSI recognition. Its application has demonstrated exceptional performance, significantly surpassing existing MIL methods in accuracy and F1 scores on prominent datasets like Camelyon16 and TCGA-NSCLC. Specifically, CDFA-MIL achieved an average accuracy and F1-score of 93.7\% and 94.1\% respectively on these datasets, marking a notable advancement over traditional MIL approaches.

{{</citation>}}


### (69/84 | 136/241) Enhancing Historical Image Retrieval with Compositional Cues (Tingyu Lin et al., 2024)

{{<citation>}}

Tingyu Lin, Robert Sablatnig. (2024)  
**Enhancing Historical Image Retrieval with Compositional Cues**
<br/>
<button class="copy-to-clipboard" title="Enhancing Historical Image Retrieval with Compositional Cues" index=136>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV, eess-IV  
Keyword Score: 10  
Keywords: Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14287v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14287v1.pdf" filename="2403.14287v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In analyzing vast amounts of digitally stored historical image data, existing content-based retrieval methods often overlook significant non-semantic information, limiting their effectiveness for flexible exploration across varied themes. To broaden the applicability of image retrieval methods for diverse purposes and uncover more general patterns, we innovatively introduce a crucial factor from computational aesthetics, namely image composition, into this topic. By explicitly integrating composition-related information extracted by <b>CNN</b> into the designed retrieval model, our method considers both the image's composition rules and semantic information. Qualitative and quantitative experiments demonstrate that the image retrieval network guided by composition information outperforms those relying solely on content information, facilitating the identification of images in databases closer to the target image in human perception. Please visit https://github.com/linty5/CCBIR to try our codes.

{{</citation>}}


### (70/84 | 137/241) StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN (Jongwoo Choi et al., 2024)

{{<citation>}}

Jongwoo Choi, Kwanggyoon Seo, Amirsaman Ashtari, Junyong Noh. (2024)  
**StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN**
<br/>
<button class="copy-to-clipboard" title="StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN" index=137>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-GR, cs.CV  
Keyword Score: 10  
Keywords: Generative Adversarial Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14186v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14186v1.pdf" filename="2403.14186v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We propose a method that can generate cinemagraphs automatically from a still landscape image using a pre-trained StyleGAN. Inspired by the success of recent unconditional video generation, we leverage a powerful pre-trained image generator to synthesize high-quality cinemagraphs. Unlike previous approaches that mainly utilize the latent space of a pre-trained StyleGAN, our approach utilizes its deep feature space for both <b>GAN</b> inversion and cinemagraph generation. Specifically, we propose multi-scale deep feature warping (MSDFW), which warps the intermediate features of a pre-trained StyleGAN at different resolutions. By using MSDFW, the generated cinemagraphs are of high resolution and exhibit plausible looping animation. We demonstrate the superiority of our method through user studies and quantitative comparisons with state-of-the-art cinemagraph generation methods and a video generation method that uses a pre-trained StyleGAN.

{{</citation>}}


### (71/84 | 138/241) 3D Object Detection from Point Cloud via Voting Step Diffusion (Haoran Hou et al., 2024)

{{<citation>}}

Haoran Hou, Mingtao Feng, Zijie Wu, Weisheng Dong, Qing Zhu, Yaonan Wang, Ajmal Mian. (2024)  
**3D Object Detection from Point Cloud via Voting Step Diffusion**
<br/>
<button class="copy-to-clipboard" title="3D Object Detection from Point Cloud via Voting Step Diffusion" index=138>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Object Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14133v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14133v1.pdf" filename="2403.14133v1.pdf">Download PDF</button>

---


**ABSTRACT**  
3D <b>object</b> <b>detection</b> is a fundamental task in scene understanding. Numerous research efforts have been dedicated to better incorporate Hough voting into the 3D <b>object</b> <b>detection</b> pipeline. However, due to the noisy, cluttered, and partial nature of real 3D scans, existing voting-based methods often receive votes from the partial surfaces of individual <b>objects</b> <b>together</b> with severe noises, leading to sub-optimal detection performance. In this work, we focus on the distributional properties of point clouds and formulate the voting process as generating new points in the high-density region of the distribution of <b>object</b> <b>centers.</b> To achieve this, we propose a new method to move random 3D points toward the high-density region of the distribution by estimating the score function of the distribution with a noise conditioned score network. Specifically, we first generate a set of <b>object</b> <b>center</b> proposals to coarsely identify the high-density region of the <b>object</b> <b>center</b> distribution. To estimate the score function, we perturb the generated <b>object</b> <b>center</b> proposals by adding normalized Gaussian noise, and then jointly estimate the score function of all perturbed distributions. Finally, we generate new votes by moving random 3D points to the high-density region of the <b>object</b> <b>center</b> distribution according to the estimated score function. Extensive experiments on two large scale indoor 3D scene datasets, SUN RGB-D and ScanNet V2, demonstrate the superiority of our proposed method. The code will be released at https://github.com/HHrEtvP/DiffVote.

{{</citation>}}


### (72/84 | 139/241) Soft Masked Transformer for Point Cloud Processing with Skip Attention-Based Upsampling (Yong He et al., 2024)

{{<citation>}}

Yong He, Hongshan Yu, Muhammad Ibrahim, Xiaoyan Liu, Tongjia Chen, Anwaar Ulhaq, Ajmal Mian. (2024)  
**Soft Masked Transformer for Point Cloud Processing with Skip Attention-Based Upsampling**
<br/>
<button class="copy-to-clipboard" title="Soft Masked Transformer for Point Cloud Processing with Skip Attention-Based Upsampling" index=139>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14124v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14124v1.pdf" filename="2403.14124v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Point cloud processing methods leverage local and global point features %at the feature level to cater to downstream tasks, yet they often overlook the task-level context inherent in point clouds during the encoding stage. We argue that integrating task-level information into the encoding stage significantly enhances performance. To that end, we propose SMTransformer which incorporates task-level information into a vector-based <b>transformer</b> by utilizing a soft mask generated from task-level queries and keys to learn the attention weights. Additionally, to facilitate effective communication between features from the encoding and decoding layers in high-level tasks such as segmentation, we introduce a skip-attention-based up-sampling block. This block dynamically fuses features from various resolution points across the encoding and decoding layers. To mitigate the increase in network parameters and training time resulting from the complexity of the aforementioned blocks, we propose a novel shared position encoding strategy. This strategy allows various <b>transformer</b> blocks to share the same position information over the same resolution points, thereby reducing network parameters and training time without compromising accuracy.Experimental comparisons with existing methods on multiple datasets demonstrate the efficacy of SMTransformer and skip-attention-based up-sampling for point cloud processing tasks, including semantic segmentation and classification. In particular, we achieve state-of-the-art semantic segmentation results of 73.4% mIoU on S3DIS Area 5 and 62.4% mIoU on SWAN dataset

{{</citation>}}


### (73/84 | 140/241) Test-time Similarity Modification for Person Re-identification toward Temporal Distribution Shift (Kazuki Adachi et al., 2024)

{{<citation>}}

Kazuki Adachi, Shohei Enomoto, Taku Sasaki, Shin'ya Yamaguchi. (2024)  
**Test-time Similarity Modification for Person Re-identification toward Temporal Distribution Shift**
<br/>
<button class="copy-to-clipboard" title="Test-time Similarity Modification for Person Re-identification toward Temporal Distribution Shift" index=140>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Distribution Shift, Distribution Shift  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14114v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14114v1.pdf" filename="2403.14114v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Person re-identification (re-id), which aims to retrieve images of the same person in a given image from a database, is one of the most practical image recognition applications. In the real world, however, the environments that the images are taken from change over time. This causes a <b>distribution</b> <b>shift</b> between training and testing and degrades the performance of re-id. To maintain re-id performance, models should continue adapting to the test environment's temporal changes. Test-time adaptation (TTA), which aims to adapt models to the test environment with only unlabeled test data, is a promising way to handle this problem because TTA can adapt models instantly in the test environment. However, the previous TTA methods are designed for classification and cannot be directly applied to re-id. This is because the set of people's identities in the dataset differs between training and testing in re-id, whereas the set of classes is fixed in the current TTA methods designed for classification. To improve re-id performance in changing test environments, we propose TEst-time similarity Modification for Person re-identification (TEMP), a novel TTA method for re-id. TEMP is the first fully TTA method for re-id, which does not require any modification to pre-training. Inspired by TTA methods that refine the prediction uncertainty in classification, we aim to refine the uncertainty in re-id. However, the uncertainty cannot be computed in the same way as classification in re-id since it is an open-set task, which does not share person labels between training and testing. Hence, we propose re-id entropy, an alternative uncertainty measure for re-id computed based on the similarity between the feature vectors. Experiments show that the re-id entropy can measure the uncertainty on re-id and TEMP improves the performance of re-id in online settings where the <b>distribution</b> <b>changes</b> over time.

{{</citation>}}


### (74/84 | 141/241) Unsupervised Intrinsic Image Decomposition with LiDAR Intensity Enhanced Training (Shogo Sato et al., 2024)

{{<citation>}}

Shogo Sato, Takuhiro Kaneko, Kazuhiko Murasaki, Taiga Yoshida, Ryuichi Tanida, Akisato Kimura. (2024)  
**Unsupervised Intrinsic Image Decomposition with LiDAR Intensity Enhanced Training**
<br/>
<button class="copy-to-clipboard" title="Unsupervised Intrinsic Image Decomposition with LiDAR Intensity Enhanced Training" index=141>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14089v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14089v1.pdf" filename="2403.14089v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Unsupervised</b> intrinsic image decomposition (IID) is the process of separating a natural image into albedo and shade without these ground truths. A recent model employing light detection and ranging (LiDAR) intensity demonstrated impressive performance, though the necessity of LiDAR intensity during inference restricts its practicality. Thus, IID models employing only a single image during inference while keeping as high IID quality as the one with an image plus LiDAR intensity are highly desired. To address this challenge, we propose a novel approach that utilizes only an image during inference while utilizing an image and LiDAR intensity during training. Specifically, we introduce a partially-shared model that accepts an image and LiDAR intensity individually using a different specific encoder but processes them together in specific components to learn shared representations. In addition, to enhance IID quality, we propose albedo-alignment loss and image-LiDAR conversion (ILC) paths. Albedo-alignment loss aligns the gray-scale albedo from an image to that inferred from LiDAR intensity, thereby reducing cast shadows in albedo from an image due to the absence of cast shadows in LiDAR intensity. Furthermore, to translate the input image into albedo and shade style while keeping the image contents, the input image is separated into style code and content code by encoders. The ILC path mutually translates the image and LiDAR intensity, which share content but differ in style, contributing to the distinct differentiation of style from content. Consequently, LIET achieves comparable IID quality to the existing model with LiDAR intensity, while utilizing only an image without LiDAR intensity during inference.

{{</citation>}}


### (75/84 | 142/241) MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images (Yuedong Chen et al., 2024)

{{<citation>}}

Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai. (2024)  
**MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images**
<br/>
<button class="copy-to-clipboard" title="MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images" index=142>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 8  
Keywords: Benchmarking, Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14627v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14627v1.pdf" filename="2403.14627v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model learned from sparse multi-view images. To accurately localize the Gaussian centers, we propose to build a cost volume representation via plane sweeping in the 3D space, where the cross-view feature similarities stored in the cost volume can provide valuable <b>geometry</b> cues to the estimation of depth. We learn the Gaussian primitives' opacities, covariances, and spherical harmonics coefficients jointly with the Gaussian centers while only relying on photometric supervision. We demonstrate the importance of the cost volume representation in learning feed-forward Gaussian Splatting models via extensive experimental evaluations. On the large-scale RealEstate10K and ACID <b>benchmarks,</b> our model achieves state-of-the-art performance with the fastest feed-forward inference speed (22 fps). Compared to the latest state-of-the-art method pixelSplat, our model uses $10\times $ fewer parameters and infers more than $2\times$ faster while providing higher appearance and <b>geometry</b> quality as well as better cross-dataset generalization.

{{</citation>}}


### (76/84 | 143/241) RoDLA: Benchmarking the Robustness of Document Layout Analysis Models (Yufan Chen et al., 2024)

{{<citation>}}

Yufan Chen, Jiaming Zhang, Kunyu Peng, Junwei Zheng, Ruiping Liu, Philip Torr, Rainer Stiefelhagen. (2024)  
**RoDLA: Benchmarking the Robustness of Document Layout Analysis Models**
<br/>
<button class="copy-to-clipboard" title="RoDLA: Benchmarking the Robustness of Document Layout Analysis Models" index=143>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 6  
Keywords: Benchmarking, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14442v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14442v1.pdf" filename="2403.14442v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Before developing a Document Layout Analysis (DLA) model in real-world applications, conducting comprehensive robustness testing is essential. However, the robustness of DLA models remains underexplored in the literature. To address this, we are the first to introduce a robustness <b>benchmark</b> for DLA models, which includes 450K document images of three datasets. To cover realistic corruptions, we propose a perturbation taxonomy with 36 common document perturbations inspired by real-world document processing. Additionally, to better understand document perturbation impacts, we propose two metrics, Mean Perturbation Effect (mPE) for perturbation assessment and Mean Robustness Degradation (mRD) for robustness evaluation. Furthermore, we introduce a self-titled model, i.e., Robust Document Layout Analyzer (RoDLA), which improves attention mechanisms to boost extraction of robust features. Experiments on the proposed <b>benchmarks</b> (PubLayNet-P, DocLayNet-P, and M$^6$Doc-P) demonstrate that RoDLA obtains state-of-the-art mRD scores of 115.7, 135.4, and 150.4, respectively. Compared to previous methods, RoDLA achieves notable improvements in mAP of +3.8%, +7.1% and +12.1%, respectively.

{{</citation>}}


### (77/84 | 144/241) Leveraging Thermal Modality to Enhance Reconstruction in Low-Light Conditions (Jiacong Xu et al., 2024)

{{<citation>}}

Jiacong Xu, Mingqian Liao, K Ram Prabhakar, Vishal M. Patel. (2024)  
**Leveraging Thermal Modality to Enhance Reconstruction in Low-Light Conditions**
<br/>
<button class="copy-to-clipboard" title="Leveraging Thermal Modality to Enhance Reconstruction in Low-Light Conditions" index=144>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-GR, cs.CV  
Keyword Score: 6  
Keywords: Multi-modal, Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14053v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14053v1.pdf" filename="2403.14053v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Neural Radiance Fields (NeRF) accomplishes photo-realistic novel view synthesis by learning the implicit volumetric representation of a scene from multi-view images, which faithfully convey the colorimetric information. However, sensor noises will contaminate low-value pixel signals, and the lossy camera image signal processor will further remove near-zero intensities in extremely dark situations, deteriorating the synthesis performance. Existing approaches reconstruct low-light scenes from raw images but struggle to recover texture and boundary details in dark regions. Additionally, they are unsuitable for high-speed models relying on explicit representations. To address these issues, we present Thermal-NeRF, which takes thermal and visible raw images as inputs, considering the thermal camera is robust to the illumination variation and raw images preserve any possible clues in the dark, to accomplish visible and thermal view synthesis simultaneously. Also, the first multi-view thermal and visible dataset (MVTV) is established to support the research on <b>multimodal</b> NeRF. Thermal-NeRF achieves the best trade-off between detail preservation and noise smoothing and provides better synthesis performance than previous work. Finally, we demonstrate that both modalities are beneficial to each other in 3D reconstruction.

{{</citation>}}


### (78/84 | 145/241) Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering (Yuanhao Gong et al., 2024)

{{<citation>}}

Yuanhao Gong, Lantao Yu, Guanghui Yue. (2024)  
**Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering**
<br/>
<button class="copy-to-clipboard" title="Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering" index=145>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs.CV, eess-IV  
Keyword Score: 5  
Keywords: Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14244v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14244v1.pdf" filename="2403.14244v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The 3D Gaussian splatting method has drawn a lot of attention, thanks to its high performance in training and high quality of the rendered image. However, it uses anisotropic Gaussian kernels to represent the scene. Although such anisotropic kernels have advantages in representing the <b>geometry,</b> they lead to difficulties in terms of computation, such as splitting or merging two kernels. In this paper, we propose to use isotropic Gaussian kernels to avoid such difficulties in the computation, leading to a higher performance method. The experiments confirm that the proposed method is about {\bf 100X} faster without losing the <b>geometry</b> representation accuracy. The proposed method can be applied in a large range applications where the radiance field is needed, such as 3D reconstruction, view synthesis, and dynamic object modeling.

{{</citation>}}


### (79/84 | 146/241) ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D Decomposition (Tianhao Wu et al., 2024)

{{<citation>}}

Tianhao Wu, Chuanxia Zheng, Tat-Jen Cham, Qianyi Wu. (2024)  
**ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D Decomposition**
<br/>
<button class="copy-to-clipboard" title="ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D Decomposition" index=146>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 3  
Keywords: Clustering  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14619v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14619v1.pdf" filename="2403.14619v1.pdf">Download PDF</button>

---


**ABSTRACT**  
3D decomposition/segmentation still remains a challenge as large-scale 3D annotated data is not readily available. Contemporary approaches typically leverage 2D machine-generated segments, integrating them for 3D consistency. While the majority of these methods are based on NeRFs, they face a potential weakness that the instance/semantic embedding features derive from independent MLPs, thus preventing the segmentation network from learning the geometric details of the objects directly through radiance and density. In this paper, we propose ClusteringSDF, a novel approach to achieve both segmentation and reconstruction in 3D via the neural implicit surface representation, specifically Signal Distance Function (SDF), where the segmentation rendering is directly integrated with the volume rendering of neural implicit surfaces. Although based on ObjectSDF++, ClusteringSDF no longer requires the ground-truth segments for supervision while maintaining the capability of reconstructing individual object surfaces, but purely with the noisy and inconsistent labels from pre-trained models.As the core of ClusteringSDF, we introduce a high-efficient <b>clustering</b> mechanism for lifting the 2D labels to 3D and the experimental results on the challenging scenes from ScanNet and Replica datasets show that ClusteringSDF can achieve competitive performance compared against the state-of-the-art with significantly reduced training time.

{{</citation>}}


### (80/84 | 147/241) Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion (Xiang Fan et al., 2024)

{{<citation>}}

Xiang Fan, Anand Bhattad, Ranjay Krishna. (2024)  
**Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion**
<br/>
<button class="copy-to-clipboard" title="Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion" index=147>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs.CV  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14617v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14617v1.pdf" filename="2403.14617v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We introduce Videoshop, a training-free video editing algorithm for localized semantic edits. Videoshop allows users to use any editing software, including Photoshop and generative inpainting, to modify the first frame; it automatically propagates those changes, with semantic, spatial, and temporally consistent motion, to the remaining frames. Unlike existing methods that enable edits only through imprecise textual instructions, Videoshop allows users to add or remove objects, semantically change objects, insert stock photos into videos, etc. with fine-grained control over locations and appearance. We achieve this through image-based video editing by inverting latents with noise extrapolation, from which we generate videos conditioned on the edited image. Videoshop produces higher quality edits against 6 baselines on 2 editing <b>benchmarks</b> using 10 evaluation metrics.

{{</citation>}}


### (81/84 | 148/241) Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation (Ruyi Lian et al., 2024)

{{<citation>}}

Ruyi Lian, Haibin Ling. (2024)  
**Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation**
<br/>
<button class="copy-to-clipboard" title="Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation" index=148>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14559v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14559v1.pdf" filename="2403.14559v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Localizing predefined 3D keypoints in a 2D image is an effective way to establish 3D-2D correspondences for 6DoF object pose estimation. However, unreliable localization results of invisible keypoints degrade the quality of correspondences. In this paper, we address this issue by localizing the important keypoints in terms of visibility. Since keypoint visibility information is currently missing in dataset collection process, we propose an efficient way to generate binary visibility labels from available object-level annotations, for keypoints of both asymmetric objects and symmetric objects. We further derive real-valued visibility-aware importance from binary labels based on PageRank algorithm. Taking advantage of the flexibility of our visibility-aware importance, we construct VAPO (Visibility-Aware POse estimator) by integrating the visibility-aware importance with a state-of-the-art pose estimation algorithm, along with additional positional encoding. Extensive experiments are conducted on popular pose estimation <b>benchmarks</b> including Linemod, Linemod-Occlusion, and YCB-V. The results show that, VAPO improves both the keypoint correspondences and final estimated poses, and clearly achieves state-of-the-art performances.

{{</citation>}}


### (82/84 | 149/241) Exploring 3D Human Pose Estimation and Forecasting from the Robot's Perspective: The HARPER Dataset (Andrea Avogaro. Andrea Toaiari et al., 2024)

{{<citation>}}

Andrea Avogaro. Andrea Toaiari, Federico Cunico, Xiangmin Xu, Haralambos Dafas, Alessandro Vinciarelli, Emma Li, Marco Cristani. (2024)  
**Exploring 3D Human Pose Estimation and Forecasting from the Robot's Perspective: The HARPER Dataset**
<br/>
<button class="copy-to-clipboard" title="Exploring 3D Human Pose Estimation and Forecasting from the Robot's Perspective: The HARPER Dataset" index=149>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-RO, cs.CV  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14447v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14447v1.pdf" filename="2403.14447v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We introduce HARPER, a novel dataset for 3D body pose estimation and forecast in dyadic interactions between users and \spot, the quadruped robot manufactured by Boston Dynamics. The key-novelty is the focus on the robot's perspective, i.e., on the data captured by the robot's sensors. These make 3D body pose analysis challenging because being close to the ground captures humans only partially. The scenario underlying HARPER includes 15 actions, of which 10 involve physical contact between the robot and users. The Corpus contains not only the recordings of the built-in stereo cameras of Spot, but also those of a 6-camera OptiTrack system (all recordings are synchronized). This leads to ground-truth skeletal representations with a precision lower than a millimeter. In addition, the Corpus includes reproducible <b>benchmarks</b> on 3D Human Pose Estimation, Human Pose Forecasting, and Collision Prediction, all based on publicly available baseline approaches. This enables future HARPER users to rigorously compare their results with those we provide in this work.

{{</citation>}}


### (83/84 | 150/241) Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians (Guangchi Fang et al., 2024)

{{<citation>}}

Guangchi Fang, Bing Wang. (2024)  
**Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians**
<br/>
<button class="copy-to-clipboard" title="Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians" index=150>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14166v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14166v1.pdf" filename="2403.14166v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this study, we explore the challenge of efficiently representing scenes with a constrained number of Gaussians. Our analysis shifts from traditional graphics and 2D computer vision to the perspective of point clouds, highlighting the inefficient spatial distribution of Gaussian representation as a key limitation in model performance. To address this, we introduce strategies for densification including blur split and depth reinitialization, and simplification through Gaussian binarization and sampling. These techniques reorganize the spatial positions of the Gaussians, resulting in significant improvements across various datasets and <b>benchmarks</b> in terms of rendering quality, resource consumption, and storage compression. Our proposed Mini-Splatting method integrates seamlessly with the original rasterization pipeline, providing a strong baseline for future research in Gaussian-Splatting-based works.

{{</citation>}}


### (84/84 | 151/241) Existence Is Chaos: Enhancing 3D Human Motion Prediction with Uncertainty Consideration (Zhihao Wang et al., 2024)

{{<citation>}}

Zhihao Wang, Yulin Zhou, Ningyu Zhang, Xiaosong Yang, Jun Xiao, Zhao Wang. (2024)  
**Existence Is Chaos: Enhancing 3D Human Motion Prediction with Uncertainty Consideration**
<br/>
<button class="copy-to-clipboard" title="Existence Is Chaos: Enhancing 3D Human Motion Prediction with Uncertainty Consideration" index=151>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14104v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14104v1.pdf" filename="2403.14104v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Human motion prediction is consisting in forecasting future body poses from historically observed sequences. It is a longstanding challenge due to motion's complex dynamics and uncertainty. Existing methods focus on building up complicated neural networks to model the motion dynamics. The predicted results are required to be strictly similar to the training samples with L2 loss in current training pipeline. However, little attention has been paid to the uncertainty property which is crucial to the prediction task. We argue that the recorded motion in training data could be an observation of possible future, rather than a predetermined result. In addition, existing works calculate the predicted error on each future frame equally during training, while recent work indicated that different frames could play different roles. In this work, a novel computationally efficient encoder-decoder model with uncertainty consideration is proposed, which could learn proper characteristics for future frames by a dynamic function. Experimental results on <b>benchmark</b> datasets demonstrate that our uncertainty consideration approach has obvious advantages both in quantity and quality. Moreover, the proposed method could produce motion sequences with much better quality that avoids the intractable shaking artefacts. We believe our work could provide a novel perspective to consider the uncertainty quality for the general motion prediction task and encourage the studies in this field. The code will be available in https://github.com/Motionpre/Adaptive-Salient-Loss-SAGGB.

{{</citation>}}


## cs.IR (3)



### (1/3 | 152/241) Knowledge-Enhanced Recommendation with User-Centric Subgraph Network (Guangyi Liu et al., 2024)

{{<citation>}}

Guangyi Liu, Quanming Yao, Yongqi Zhang, Lei Chen. (2024)  
**Knowledge-Enhanced Recommendation with User-Centric Subgraph Network**
<br/>
<button class="copy-to-clipboard" title="Knowledge-Enhanced Recommendation with User-Centric Subgraph Network" index=152>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-AI, cs-IR, cs-LG, cs.IR  
Keyword Score: 53  
Keywords: Graph, Graph Neural Network, Graph Neural Network, Node Embedding, Knowledge Graph, Knowledge Graph, Recommendation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14377v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14377v1.pdf" filename="2403.14377v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Recommendation</b> systems, as widely implemented nowadays on various platforms, recommend relevant items to users based on their preferences. The classical methods which rely on user-item interaction matrices has limitations, especially in scenarios where there is a lack of interaction data for new items. <b>Knowledge</b> <b>graph</b> <b>(KG)-based</b> <b>recommendation</b> systems have emerged as a promising solution. However, most <b>KG-based</b> methods adopt <b>node</b> <b>embeddings,</b> which do not provide personalized <b>recommendations</b> for different users and cannot generalize well to the new items. To address these limitations, we propose <b>Knowledge-enhanced</b> <b>User-Centric</b> subgraph Network (KUCNet), a subgraph learning approach with <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> for effective <b>recommendation.</b> KUCNet constructs a U-I subgraph for each user-item pair that captures both the historical information of user-item interactions and the side information provided in <b>KG.</b> An attention-based <b>GNN</b> is designed to encode the U-I subgraphs for <b>recommendation.</b> Considering efficiency, the pruned user-centric computation <b>graph</b> <b>is</b> <b>further</b> introduced such that multiple U-I subgraphs can be simultaneously computed and that the size can be pruned by Personalized PageRank. Our proposed method achieves accurate, efficient, and interpretable <b>recommendations</b> especially for new items. Experimental results demonstrate the superiority of KUCNet over state-of-the-art <b>KG-based</b> and collaborative filtering (CF)-based methods.

{{</citation>}}


### (2/3 | 153/241) M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain Multi-Hop Dense Sentence Retrieval (Yang Bai et al., 2024)

{{<citation>}}

Yang Bai, Anthony Colas, Christan Grant, Daisy Zhe Wang. (2024)  
**M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain Multi-Hop Dense Sentence Retrieval**
<br/>
<button class="copy-to-clipboard" title="M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain Multi-Hop Dense Sentence Retrieval" index=153>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-CL, cs-IR, cs-LG, cs.IR  
Keyword Score: 38  
Keywords: Benchmarking, Contrastive Learning, Dense Retrieval, Representation Learning, Fact Verification  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14074v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14074v1.pdf" filename="2403.14074v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In recent research, <b>contrastive</b> <b>learning</b> has proven to be a highly effective method for <b>representation</b> <b>learning</b> and is widely used for <b>dense</b> <b>retrieval.</b> However, we identify that relying solely on <b>contrastive</b> <b>learning</b> can lead to suboptimal retrieval performance. On the other hand, despite many retrieval datasets supporting various learning objectives beyond <b>contrastive</b> <b>learning,</b> combining them efficiently in multi-task learning scenarios can be challenging. In this paper, we introduce M3, an advanced recursive Multi-hop <b>dense</b> <b>sentence</b> retrieval system built upon a novel Multi-task Mixed-objective approach for <b>dense</b> <b>text</b> <b>representation</b> <b>learning,</b> addressing the aforementioned challenges. Our approach yields state-of-the-art performance on a large-scale open-domain <b>fact</b> <b>verification</b> <b>benchmark</b> dataset, FEVER. Code and data are available at: https://github.com/TonyBY/M3

{{</citation>}}


### (3/3 | 154/241) Understanding the Ranking Loss for Recommendation with Sparse User Feedback (Zhutian Lin et al., 2024)

{{<citation>}}

Zhutian Lin, Junwei Pan, Shangyu Zhang, Ximei Wang, Xi Xiao, Shudong Huang, Lei Xiao, Jie Jiang. (2024)  
**Understanding the Ranking Loss for Recommendation with Sparse User Feedback**
<br/>
<button class="copy-to-clipboard" title="Understanding the Ranking Loss for Recommendation with Sparse User Feedback" index=154>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-IR, cs.IR  
Keyword Score: 10  
Keywords: Recommendation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14144v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14144v1.pdf" filename="2403.14144v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Click-through rate (CTR) prediction holds significant importance in the realm of online advertising. While many existing approaches treat it as a binary classification problem and utilize binary cross entropy (BCE) as the optimization objective, recent advancements have indicated that combining BCE loss with ranking loss yields substantial performance improvements. However, the full efficacy of this combination loss remains incompletely understood. In this paper, we uncover a new challenge associated with BCE loss in scenarios with sparse positive feedback, such as CTR prediction: the gradient vanishing for negative samples. Subsequently, we introduce a novel perspective on the effectiveness of ranking loss in CTR prediction, highlighting its ability to generate larger gradients on negative samples, thereby mitigating their optimization issues and resulting in improved classification ability. Our perspective is supported by extensive theoretical analysis and empirical evaluation conducted on publicly available datasets. Furthermore, we successfully deployed the ranking loss in Tencent's online advertising system, achieving notable lifts of 0.70% and 1.26% in Gross Merchandise Value (GMV) for two main scenarios. The code for our approach is openly accessible at the following GitHub repository: https://github.com/SkylerLinn/Understanding-the-Ranking-Loss.

{{</citation>}}


## math.NA (6)



### (1/6 | 155/241) A LSTM-enhanced surrogate model to simulate the dynamics of particle-laden fluid systems (Arash Hajisharifi et al., 2024)

{{<citation>}}

Arash Hajisharifi, Rahul Halder, Michele Girfoglio, Andrea Beccari, Domenico Bonanni, Gianluigi Rozza. (2024)  
**A LSTM-enhanced surrogate model to simulate the dynamics of particle-laden fluid systems**
<br/>
<button class="copy-to-clipboard" title="A LSTM-enhanced surrogate model to simulate the dynamics of particle-laden fluid systems" index=155>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.NA  
Categories: cs-NA, math-NA, math.NA  
Keyword Score: 53  
Keywords: Benchmarking, Simulation, Simulator, LSTM, LSTM, LSTM  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14283v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14283v1.pdf" filename="2403.14283v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The numerical treatment of fluid-particle systems is a very challenging problem because of the complex coupling phenomena occurring between the two phases. Although accurate mathematical modelling is available to address this kind of application, the computational cost of the numerical <b>simulations</b> is very expensive. The use of the most modern high-performance computing infrastructures could help to mitigate such an issue but not completely fix it. In this work, we develop a non-intrusive data-driven reduced order model (ROM) for Computational Fluid Dynamics (CFD) - Discrete Element Method (DEM) <b>simulations.</b> The ROM is built using the proper orthogonal decomposition (POD) for the computation of the reduced basis space and the <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> network for the computation of the reduced coefficients. We are interested in dealing both with system identification and prediction. The most relevant novelties rely on (i) a filtering procedure of the full-order snapshots to reduce the dimensionality of the reduced problem and (ii) a preliminary treatment of the particle phase. The accuracy of our ROM approach is assessed against the classic Goldschmidt fluidized bed <b>benchmark</b> problem. Finally, we also provide some insights about the efficiency of our ROM approach.

{{</citation>}}


### (2/6 | 156/241) Time filtering methods for electrohydrodynamics models (Li Conghui, 2024)

{{<citation>}}

Li Conghui. (2024)  
**Time filtering methods for electrohydrodynamics models**
<br/>
<button class="copy-to-clipboard" title="Time filtering methods for electrohydrodynamics models" index=156>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.NA  
Categories: cs-NA, math-NA, math.NA  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14308v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14308v1.pdf" filename="2403.14308v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Electrohydrodynamics is a discipline that studies the interaction between fluid motion and electric field. Finite element method, finite difference method and other numerical <b>simulations</b> are effective numerical calculation methods for electrofluid dynamics models. In this paper, the finite element format of the electrofluid dynamics model is established, and the second-order convergence accuracy of the format is achieved through time filtering method. Finally, a numerical example is given to verify the convergence.

{{</citation>}}


### (3/6 | 157/241) Structure-preserving, weighted implicit-explicit schemes for multi-phase incompressible Navier-Stokes/Darcy coupled nonlocal Allen-Cahn model (Meng Li et al., 2024)

{{<citation>}}

Meng Li, Ke Wang, Nan Wang. (2024)  
**Structure-preserving, weighted implicit-explicit schemes for multi-phase incompressible Navier-Stokes/Darcy coupled nonlocal Allen-Cahn model**
<br/>
<button class="copy-to-clipboard" title="Structure-preserving, weighted implicit-explicit schemes for multi-phase incompressible Navier-Stokes/Darcy coupled nonlocal Allen-Cahn model" index=157>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.NA  
Categories: cs-NA, math-NA, math.NA  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14086v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14086v1.pdf" filename="2403.14086v1.pdf">Download PDF</button>

---


**ABSTRACT**  
A multitude of substances exist as mixtures comprising multiple chemical components in the natural world. These substances undergo morphological changes under external influences. the phase field model coupled with fluid flow, the dynamic movement and evolution of the phase interface intricately interact with the fluid motion. This article focuses on the N-component models that couple the conservative Allen-Cahn equation with two types of incompressible fluid flow systems: the Navier-Stokes equation and the Darcy equation. By utilizing the scalar auxiliary variable method and the projection method, we innovatively construct two types of structure-preserving weighted implicit-explicit schemes for the coupled models, resulting in fully decoupled linear systems and second-order accuracy in time. The schemes are proved to be mass-conservative. In addition, with the application of $G$-norm inspired by the idea of $G$-stability, we rigorously establish its unconditional energy stability. Finally, the performance of the proposed scheme is verified by some numerical <b>simulations.</b>

{{</citation>}}


### (4/6 | 158/241) Learning-based Multi-continuum Model for Multiscale Flow Problems (Fan Wang et al., 2024)

{{<citation>}}

Fan Wang, Yating Wang, Wing Tat Leung, Zongben Xu. (2024)  
**Learning-based Multi-continuum Model for Multiscale Flow Problems**
<br/>
<button class="copy-to-clipboard" title="Learning-based Multi-continuum Model for Multiscale Flow Problems" index=158>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.NA  
Categories: cs-LG, cs-NA, math-NA, math.NA  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14084v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14084v1.pdf" filename="2403.14084v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Multiscale problems can usually be approximated through numerical homogenization by an equation with some effective parameters that can capture the macroscopic behavior of the original system on the coarse grid to speed up the <b>simulation.</b> However, this approach usually assumes scale separation and that the heterogeneity of the solution can be approximated by the solution average in each coarse block. For complex multiscale problems, the computed single effective properties/continuum might be inadequate. In this paper, we propose a novel learning-based multi-continuum model to enrich the homogenized equation and improve the accuracy of the single continuum model for multiscale problems with some given data. Without loss of generalization, we consider a two-continuum case. The first flow equation keeps the information of the original homogenized equation with an additional interaction term. The second continuum is newly introduced, and the effective permeability in the second flow equation is determined by a neural network. The interaction term between the two continua aligns with that used in the Dual-porosity model but with a learnable coefficient determined by another neural network. The new model with neural network terms is then optimized using trusted data. We discuss both direct back-propagation and the adjoint method for the PDE-constraint optimization problem. Our proposed learning-based multi-continuum model can resolve multiple interacted media within each coarse grid block and describe the mass transfer among them, and it has been demonstrated to significantly improve the <b>simulation</b> results through numerical experiments involving both linear and nonlinear flow equations.

{{</citation>}}


### (5/6 | 159/241) Low-rank tensor product Richardson iteration for radiative transfer in plane-parallel geometry (Markus Bachmayr et al., 2024)

{{<citation>}}

Markus Bachmayr, Riccardo Bardin, Matthias Schlottbom. (2024)  
**Low-rank tensor product Richardson iteration for radiative transfer in plane-parallel geometry**
<br/>
<button class="copy-to-clipboard" title="Low-rank tensor product Richardson iteration for radiative transfer in plane-parallel geometry" index=159>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.NA  
Categories: 65F10, 65F55, 65N22, 65N30, cs-NA, math-NA, math.NA  
Keyword Score: 5  
Keywords: Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14229v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14229v1.pdf" filename="2403.14229v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The radiative transfer equation (RTE) has been established as a fundamental tool for the description of energy transport, absorption and scattering in many relevant societal applications, and requires numerical approximations. However, classical numerical algorithms scale unfavorably with respect to the dimensionality of such radiative transfer problems, where solutions depend on physical as well as angular variables. In this paper we address this dimensionality issue by developing a low-rank tensor product framework for the RTE in plane-parallel <b>geometry.</b> We exploit the tensor product nature of the phase space to recover an operator equation where the operator is given by a short sum of Kronecker products. This equation is solved by a preconditioned and rank-controlled Richardson iteration in Hilbert spaces. Using exponential sums approximations we construct a preconditioner that is compatible with the low-rank tensor product framework. The use of suitable preconditioning techniques yields a transformation of the operator equation in Hilbert space into a sequence space with Euclidean inner product, enabling rigorous error and rank control in the Euclidean metric.

{{</citation>}}


### (6/6 | 160/241) Fast and accurate log-determinant approximations (Owen Deen et al., 2024)

{{<citation>}}

Owen Deen, Colton River Waller, John Paul Ward. (2024)  
**Fast and accurate log-determinant approximations**
<br/>
<button class="copy-to-clipboard" title="Fast and accurate log-determinant approximations" index=160>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.NA  
Categories: 65F40, 65F50, cs-NA, math-NA, math.NA  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14609v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14609v1.pdf" filename="2403.14609v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We consider the problem of estimating log-determinants of large, sparse, positive definite matrices. A key focus of our algorithm is to reduce computational cost, and it is based on sparse approximate inverses. The algorithm can be implemented to be adaptive, and it uses <b>graph</b> spline approximation to improve accuracy. We illustrate our approach on classes of large sparse matrices.

{{</citation>}}


## cs.HC (4)



### (1/4 | 161/241) Empowering Personalized Learning through a Conversation-based Tutoring System with Student Modeling (Minju Park et al., 2024)

{{<citation>}}

Minju Park, Sojung Kim, Seunghyun Lee, Soonwoo Kwon, Kyuseok Kim. (2024)  
**Empowering Personalized Learning through a Conversation-based Tutoring System with Student Modeling**
<br/>
<button class="copy-to-clipboard" title="Empowering Personalized Learning through a Conversation-based Tutoring System with Student Modeling" index=161>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-HC, cs.HC  
Keyword Score: 50  
Keywords: Few-shot, Zero-shot, Reasoning, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14071v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14071v1.pdf" filename="2403.14071v1.pdf">Download PDF</button>

---


**ABSTRACT**  
As the recent Large Language Models(LLM's) become increasingly competent in <b>zero-shot</b> and <b>few-shot</b> <b>reasoning</b> across various domains, educators are showing a growing interest in leveraging these <b>LLM's</b> in conversation-based tutoring systems. However, building a conversation-based personalized tutoring system poses considerable challenges in accurately assessing the student and strategically incorporating the assessment into teaching within the conversation. In this paper, we discuss design considerations for a personalized tutoring system that involves the following two key components: (1) a student modeling with diagnostic components, and (2) a conversation-based tutor utilizing <b>LLM</b> with <b>prompt</b> engineering that incorporates student assessment outcomes and various instructional strategies. Based on these design considerations, we created a proof-of-concept tutoring system focused on personalization and tested it with 20 participants. The results substantiate that our system's framework facilitates personalization, with particular emphasis on the elements constituting student modeling. A web demo of our system is available at http://rlearning-its.com.

{{</citation>}}


### (2/4 | 162/241) PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning (Jiawen Liu et al., 2024)

{{<citation>}}

Jiawen Liu, Yuanyuan Yao, Pengcheng An, Qi Wang. (2024)  
**PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning**
<br/>
<button class="copy-to-clipboard" title="PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning" index=162>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-AI, cs-HC, cs.HC  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14227v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14227v1.pdf" filename="2403.14227v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In children's collaborative learning, effective peer conversations can significantly enhance the quality of children's collaborative interactions. The integration of <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants. We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem. The peer conversation transcripts were analyzed using thematic analysis. We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded. As participants, they foster children's creative thinking but may not consistently provide timely feedback. These findings highlight potential design improvements and considerations for peer agents in both roles.

{{</citation>}}


### (3/4 | 163/241) How Human-Centered Explainable AI Interface Are Designed and Evaluated: A Systematic Survey (Thu Nguyen et al., 2024)

{{<citation>}}

Thu Nguyen, Alessandro Canossa, Jichen Zhu. (2024)  
**How Human-Centered Explainable AI Interface Are Designed and Evaluated: A Systematic Survey**
<br/>
<button class="copy-to-clipboard" title="How Human-Centered Explainable AI Interface Are Designed and Evaluated: A Systematic Survey" index=163>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-AI, cs-HC, cs.HC  
Keyword Score: 10  
Keywords: Explainable AI  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14496v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14496v1.pdf" filename="2403.14496v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Despite its technological breakthroughs, <b>eXplainable</b> <b>Artificial</b> Intelligence (XAI) research has limited success in producing the {\em effective explanations} needed by users. In order to improve XAI systems' usability, practical interpretability, and efficacy for real users, the emerging area of {\em <b>Explainable</b> <b>Interfaces}</b> (EIs) focuses on the user interface and user experience design aspects of XAI. This paper presents a systematic survey of 53 publications to identify current trends in human-XAI interaction and promising directions for EI design and development. This is among the first systematic survey of EI research.

{{</citation>}}


### (4/4 | 164/241) Recourse for reclamation: Chatting with generative language models (Jennifer Chien et al., 2024)

{{<citation>}}

Jennifer Chien, Kevin R. McKee, Jackie Kay, William Isaac. (2024)  
**Recourse for reclamation: Chatting with generative language models**
<br/>
<button class="copy-to-clipboard" title="Recourse for reclamation: Chatting with generative language models" index=164>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-CL, cs-CY, cs-HC, cs.HC  
Keyword Score: 10  
Keywords: Information Retrieval  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14467v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14467v1.pdf" filename="2403.14467v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Researchers and developers increasingly rely on toxicity scoring to moderate generative language model outputs, in settings such as customer service, <b>information</b> <b>retrieval,</b> and content generation. However, toxicity scoring may render pertinent <b>information</b> <b>inaccessible,</b> rigidify or "value-lock" cultural norms, and prevent language reclamation processes, particularly for marginalized people. In this work, we extend the concept of algorithmic recourse to generative language models: we provide users a novel mechanism to achieve their desired prediction by dynamically setting thresholds for toxicity filtering. Users thereby exercise increased agency relative to interactions with the baseline system. A pilot study ($n = 30$) supports the potential of our proposed recourse mechanism, indicating improvements in usability compared to fixed-threshold toxicity-filtering of model outputs. Future work should explore the intersection of toxicity scoring, model controllability, user agency, and language reclamation processes -- particularly with regard to the bias that many communities encounter when interacting with generative language models.

{{</citation>}}


## cs.SD (5)



### (1/5 | 165/241) XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception (HyoJung Han et al., 2024)

{{<citation>}}

HyoJung Han, Mohamed Anwar, Juan Pino, Wei-Ning Hsu, Marine Carpuat, Bowen Shi, Changhan Wang. (2024)  
**XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception**
<br/>
<button class="copy-to-clipboard" title="XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception" index=165>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SD  
Categories: cs-CL, cs-SD, cs.SD, eess-AS  
Keyword Score: 48  
Keywords: Benchmarking, Fine-tuning, Representation Learning, Zero-shot, Automatic Speech Recognition, BLEU  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14402v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14402v1.pdf" filename="2403.14402v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Speech</b> <b>recognition</b> and translation systems perform poorly on noisy inputs, which are frequent in realistic environments. Augmenting these systems with visual signals has the potential to improve robustness to noise. However, audio-visual (AV) data is only available in limited amounts and for fewer languages than audio-only resources. To address this gap, we present XLAVS-R, a cross-lingual audio-visual <b>speech</b> <b>representation</b> <b>model</b> for noise-robust <b>speech</b> <b>recognition</b> and translation in over 100 languages. It is designed to maximize the benefits of limited multilingual AV pre-training data, by building on top of audio-only multilingual pre-training and simplifying existing pre-training schemes. Extensive evaluation on the MuAViC <b>benchmark</b> shows the strength of XLAVS-R on downstream audio-visual <b>speech</b> <b>recognition</b> and translation tasks, where it outperforms the previous state of the art by up to 18.5% WER and 4.7 <b>BLEU</b> given noisy AV inputs, and enables strong <b>zero-shot</b> audio-visual ability with audio-only <b>fine-tuning.</b>

{{</citation>}}


### (2/5 | 166/241) Exploring Green AI for Audio Deepfake Detection (Subhajit Saha et al., 2024)

{{<citation>}}

Subhajit Saha, Md Sahidullah, Swagatam Das. (2024)  
**Exploring Green AI for Audio Deepfake Detection**
<br/>
<button class="copy-to-clipboard" title="Exploring Green AI for Audio Deepfake Detection" index=166>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SD  
Categories: cs-CV, cs-LG, cs-SD, cs.SD, eess-AS  
Keyword Score: 40  
Keywords: Fine-tuning, Logistic Regression, Self-supervised Learning, Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14290v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14290v1.pdf" filename="2403.14290v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The state-of-the-art audio deepfake detectors leveraging deep neural networks exhibit impressive recognition performance. Nonetheless, this advantage is accompanied by a significant carbon footprint. This is mainly due to the use of high-performance computing with accelerators and high training time. Studies show that average deep NLP model produces around 626k lbs of CO\textsubscript{2} which is equivalent to five times of average US car emission at its lifetime. This is certainly a massive threat to the environment. To tackle this challenge, this study presents a novel framework for audio deepfake detection that can be seamlessly trained using standard CPU resources. Our proposed framework utilizes off-the-shelve <b>self-supervised</b> <b>learning</b> (SSL) based models which are pre-trained and available in public repositories. In contrast to existing methods that <b>fine-tune</b> SSL models and employ additional deep neural networks for downstream tasks, we exploit classical machine learning algorithms such as <b>logistic</b> <b>regression</b> and shallow neural networks using the SSL embeddings extracted using the pre-trained model. Our approach shows competitive results compared to the commonly used high-carbon footprint approaches. In experiments with the ASVspoof 2019 LA dataset, we achieve a 0.90\% equal error rate (EER) with less than 1k trainable model parameters. To encourage further research in this direction and support reproducible results, the Python code will be made publicly accessible following acceptance. Github: https://github.com/sahasubhajit/Speech-Spoofing-

{{</citation>}}


### (3/5 | 167/241) emoDARTS: Joint Optimisation of CNN & Sequential Neural Network Architectures for Superior Speech Emotion Recognition (Thejan Rajapakshe et al., 2024)

{{<citation>}}

Thejan Rajapakshe, Rajib Rana, Sara Khalifa, Berrak Sisman, Bjorn W. Schuller, Carlos Busso. (2024)  
**emoDARTS: Joint Optimisation of CNN & Sequential Neural Network Architectures for Superior Speech Emotion Recognition**
<br/>
<button class="copy-to-clipboard" title="emoDARTS: Joint Optimisation of CNN & Sequential Neural Network Architectures for Superior Speech Emotion Recognition" index=167>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SD  
Categories: cs-LG, cs-SD, cs.SD, eess-AS  
Keyword Score: 40  
Keywords: Convolutional Neural Network, LSTM, Recurrent Neural Network, Emotion Recognition  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14083v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14083v1.pdf" filename="2403.14083v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Speech <b>Emotion</b> <b>Recognition</b> (SER) is crucial for enabling computers to understand the <b>emotions</b> <b>conveyed</b> in human communication. With recent advancements in Deep Learning (DL), the performance of SER models has significantly improved. However, designing an optimal DL architecture requires specialised knowledge and experimental assessments. Fortunately, Neural Architecture Search (NAS) provides a potential solution for automatically determining the best DL model. The Differentiable Architecture Search (DARTS) is a particularly efficient method for discovering optimal models. This study presents emoDARTS, a DARTS-optimised joint <b>CNN</b> and Sequential Neural Network (SeqNN: <b>LSTM,</b> <b>RNN)</b> architecture that enhances SER performance. The literature supports the selection of <b>CNN</b> and <b>LSTM</b> coupling to improve performance. While DARTS has previously been used to choose <b>CNN</b> and <b>LSTM</b> operations independently, our technique adds a novel mechanism for selecting <b>CNN</b> and SeqNN operations in conjunction using DARTS. Unlike earlier work, we do not impose limits on the layer order of the <b>CNN.</b> Instead, we let DARTS choose the best layer order inside the DARTS cell. We demonstrate that emoDARTS outperforms conventionally designed <b>CNN-LSTM</b> models and surpasses the best-reported SER results achieved through DARTS on <b>CNN-LSTM</b> by evaluating our approach on the IEMOCAP, MSP-IMPROV, and MSP-Podcast datasets.

{{</citation>}}


### (4/5 | 168/241) The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio Benchmarks and Novel Data (Alice Baird et al., 2024)

{{<citation>}}

Alice Baird, Rachel Manzelli, Panagiotis Tzirakis, Chris Gagne, Haoqi Li, Sadie Allen, Sander Dieleman, Brian Kulis, Shrikanth S. Narayanan, Alan Cowen. (2024)  
**The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio Benchmarks and Novel Data**
<br/>
<button class="copy-to-clipboard" title="The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio Benchmarks and Novel Data" index=168>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SD  
Categories: cs-CL, cs-SD, cs.SD, eess-AS  
Keyword Score: 23  
Keywords: Benchmarking, Emotion Recognition, Event Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14048v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14048v1.pdf" filename="2403.14048v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The NeurIPS 2023 Machine Learning for Audio Workshop brings together machine learning (ML) experts from various audio domains. There are several valuable audio-driven ML tasks, from speech <b>emotion</b> <b>recognition</b> to audio <b>event</b> <b>detection,</b> but the community is sparse compared to other ML areas, e.g., computer vision or natural language processing. A major limitation with audio is the available data; with audio being a time-dependent modality, high-quality data collection is time-consuming and costly, making it challenging for academic groups to apply their often state-of-the-art strategies to a larger, more generalizable dataset. In this short white paper, to encourage researchers with limited access to large-datasets, the organizers first outline several open-source datasets that are available to the community, and for the duration of the workshop are making several propriety datasets available. Namely, three vocal datasets, Hume-Prosody, Hume-VocalBurst, an acted <b>emotional</b> <b>speech</b> dataset Modulate-Sonata, and an in-game streamer dataset Modulate-Stream. We outline the current baselines on these datasets but encourage researchers from across audio to utilize them outside of the initial baseline tasks.

{{</citation>}}


### (5/5 | 169/241) Assessing the Robustness of Spectral Clustering for Deep Speaker Diarization (Nikhil Raghav et al., 2024)

{{<citation>}}

Nikhil Raghav, Md Sahidullah. (2024)  
**Assessing the Robustness of Spectral Clustering for Deep Speaker Diarization**
<br/>
<button class="copy-to-clipboard" title="Assessing the Robustness of Spectral Clustering for Deep Speaker Diarization" index=169>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SD  
Categories: cs-CV, cs-LG, cs-SD, cs.SD, eess-AS  
Keyword Score: 3  
Keywords: Clustering  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14286v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14286v1.pdf" filename="2403.14286v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Clustering</b> speaker embeddings is crucial in speaker diarization but hasn't received as much focus as other components. Moreover, the robustness of speaker diarization across various datasets hasn't been explored when the development and evaluation data are from different domains. To bridge this gap, this study thoroughly examines spectral <b>clustering</b> for both same-domain and cross-domain speaker diarization. Our extensive experiments on two widely used corpora, AMI and DIHARD, reveal the performance trend of speaker diarization in the presence of domain mismatch. We observe that the performance difference between two different domain conditions can be attributed to the role of spectral <b>clustering.</b> In particular, keeping other modules unchanged, we show that differences in optimal tuning parameters as well as speaker count estimation originates due to the mismatch. This study opens several future directions for speaker diarization research.

{{</citation>}}


## cs.RO (16)



### (1/16 | 170/241) Physics-Based Causal Reasoning for Safe & Robust Next-Best Action Selection in Robot Manipulation Tasks (Ricardo Cannizzaro et al., 2024)

{{<citation>}}

Ricardo Cannizzaro, Michael Groom, Jonathan Routley, Robert Osazuwa Ness, Lars Kunze. (2024)  
**Physics-Based Causal Reasoning for Safe & Robust Next-Best Action Selection in Robot Manipulation Tasks**
<br/>
<button class="copy-to-clipboard" title="Physics-Based Causal Reasoning for Safe & Robust Next-Best Action Selection in Robot Manipulation Tasks" index=170>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: I-2-9; I-2-8; I-2-3; G-3; I-2-6; I-6-8; I-2-4; I-2-10, cs-AI, cs-LG, cs-RO, cs.RO, stat-AP  
Keyword Score: 40  
Keywords: Probabilistic Model, Simulation, Simulator, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14488v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14488v1.pdf" filename="2403.14488v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Safe and efficient object manipulation is a key enabler of many real-world robot applications. However, this is challenging because robot operation must be robust to a range of sensor and actuator uncertainties. In this paper, we present a physics-informed causal-inference-based framework for a robot to probabilistically reason about candidate actions in a block stacking task in a partially observable setting. We integrate a physics-based <b>simulation</b> of the rigid-body system dynamics with a causal Bayesian network (CBN) formulation to define a causal generative <b>probabilistic</b> <b>model</b> of the robot decision-making process. Using <b>simulation-based</b> Monte Carlo experiments, we demonstrate our framework's ability to successfully: (1) predict block tower stability with high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best action for the block stacking task, for execution by an integrated robot system, achieving 94.2% task success rate. We also demonstrate our framework's suitability for real-world robot systems by demonstrating successful task executions with a domestic support robot, with perception and manipulation sub-system integration. Hence, we show that by embedding physics-based causal <b>reasoning</b> into robots' decision-making processes, we can make robot task execution safer, more reliable, and more robust to various types of uncertainty.

{{</citation>}}


### (2/16 | 171/241) Distilling Reinforcement Learning Policies for Interpretable Robot Locomotion: Gradient Boosting Machines and Symbolic Regression (Fernando Acero et al., 2024)

{{<citation>}}

Fernando Acero, Zhibin Li. (2024)  
**Distilling Reinforcement Learning Policies for Interpretable Robot Locomotion: Gradient Boosting Machines and Symbolic Regression**
<br/>
<button class="copy-to-clipboard" title="Distilling Reinforcement Learning Policies for Interpretable Robot Locomotion: Gradient Boosting Machines and Symbolic Regression" index=171>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-AI, cs-LG, cs-RO, cs.RO  
Keyword Score: 40  
Keywords: Distribution Shift, Distribution Shift, Knowledge Distillation, Knowledge Distillation, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14328v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14328v1.pdf" filename="2403.14328v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advancements in <b>reinforcement</b> <b>learning</b> (RL) have led to remarkable achievements in robot locomotion capabilities. However, the complexity and ``black-box'' nature of neural network-based RL policies hinder their interpretability and broader acceptance, particularly in applications demanding high levels of safety and reliability. This paper introduces a novel approach to <b>distill</b> neural RL policies into more interpretable forms using Gradient Boosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic Regression. By leveraging the inherent interpretability of generalized additive models, decision trees, and analytical expressions, we transform opaque neural network policies into more transparent ``glass-box'' models. We train expert neural network policies using RL and subsequently <b>distill</b> them into (i) GBMs, (ii) EBMs, and (iii) symbolic policies. To address the inherent <b>distribution</b> <b>shift</b> challenge of behavioral cloning, we propose to use the Dataset Aggregation (DAgger) algorithm with a curriculum of episode-dependent alternation of actions between expert and <b>distilled</b> policies, to enable efficient <b>distillation</b> of feedback control policies. We evaluate our approach on various robot locomotion gaits -- walking, trotting, bounding, and pacing -- and study the importance of different observations in joint actions for <b>distilled</b> policies using various methods. We train neural expert policies for 205 hours of simulated experience and <b>distill</b> interpretable policies with only 10 minutes of simulated interaction for each gait using the proposed method.

{{</citation>}}


### (3/16 | 172/241) Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors (Nikolaos Tsagkas et al., 2024)

{{<citation>}}

Nikolaos Tsagkas, Jack Rome, Subramanian Ramamoorthy, Oisin Mac Aodha, Chris Xiaoxuan Lu. (2024)  
**Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors**
<br/>
<button class="copy-to-clipboard" title="Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors" index=172>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-AI, cs-CV, cs-RO, cs.RO  
Keyword Score: 35  
Keywords: Geometry, Zero-shot, Grounding, Text2image  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14526v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14526v1.pdf" filename="2403.14526v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Precise manipulation that is generalizable across scenes and objects remains a persistent challenge in robotics. Current approaches for this task heavily depend on having a significant number of training instances to handle objects with pronounced visual and/or geometric part ambiguities. Our work explores the <b>grounding</b> of fine-grained part descriptors for precise manipulation in a <b>zero-shot</b> setting by utilizing web-trained <b>text-to-image</b> diffusion-based generative models. We tackle the problem by framing it as a dense semantic part correspondence task. Our model returns a gripper pose for manipulating a specific part, using as reference a user-defined click from a source image of a visually different instance of the same object. We require no manual grasping demonstrations as we leverage the intrinsic object <b>geometry</b> and features. Practical experiments in a real-world tabletop scenario validate the efficacy of our approach, demonstrating its potential for advancing semantic-aware robotics manipulation. Web page: https://tsagkas.github.io/click2grasp

{{</citation>}}


### (4/16 | 173/241) SDP Synthesis of Maximum Coverage Trees for Probabilistic Planning under Control Constraints (Naman Aggarwal et al., 2024)

{{<citation>}}

Naman Aggarwal, Jonathan P. How. (2024)  
**SDP Synthesis of Maximum Coverage Trees for Probabilistic Planning under Control Constraints**
<br/>
<button class="copy-to-clipboard" title="SDP Synthesis of Maximum Coverage Trees for Probabilistic Planning under Control Constraints" index=173>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs-SY, cs.RO, eess-SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14605v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14605v1.pdf" filename="2403.14605v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The paper presents Maximal Covariance Backward Reachable Trees (MAXCOVAR BRT), which is a multi-query algorithm for planning of dynamic systems under stochastic motion uncertainty and constraints on the control input with explicit coverage guarantees. In contrast to existing roadmap-based probabilistic planning methods that sample belief nodes randomly and draw edges between them \cite{csbrm_tro2024}, under control constraints, the reachability of belief nodes needs to be explicitly established and is determined by checking the feasibility of a non-convex program. Moreover, there is no explicit consideration of coverage of the roadmap while adding nodes and edges during the construction procedure for the existing methods. Our contribution is a novel optimization formulation to add nodes and construct the corresponding edge controllers such that the generated roadmap results in provably maximal coverage under control constraints as compared to any other method of adding nodes and edges. We characterize formally the notion of coverage of a roadmap in this stochastic domain via introduction of the h-$\operatorname{BRS}$ (Backward Reachable Set of Distributions) of a tree of distributions under control constraints, and also support our method with extensive <b>simulations</b> on a 6 DoF model.

{{</citation>}}


### (5/16 | 174/241) Bayesian Optimization for Sample-Efficient Policy Improvement in Robotic Manipulation (Adrian Röfer et al., 2024)

{{<citation>}}

Adrian Röfer, Iman Nematollahi, Tim Welschehold, Wolfram Burgard, Abhinav Valada. (2024)  
**Bayesian Optimization for Sample-Efficient Policy Improvement in Robotic Manipulation**
<br/>
<button class="copy-to-clipboard" title="Bayesian Optimization for Sample-Efficient Policy Improvement in Robotic Manipulation" index=174>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14305v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14305v1.pdf" filename="2403.14305v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Sample efficient learning of manipulation skills poses a major challenge in robotics. While recent approaches demonstrate impressive advances in the type of task that can be addressed and the sensing modalities that can be incorporated, they still require large amounts of training data. Especially with regard to learning actions on robots in the real world, this poses a major problem due to the high costs associated with both demonstrations and real-world robot interactions. To address this challenge, we introduce BOpt-GMM, a hybrid approach that combines imitation learning with own experience collection. We first learn a skill model as a dynamical system encoded in a Gaussian Mixture Model from a few demonstrations. We then improve this model with Bayesian optimization building on a small number of autonomous skill executions in a sparse reward setting. We demonstrate the sample efficiency of our approach on multiple complex manipulation skills in both <b>simulations</b> and real-world experiments. Furthermore, we make the code and pre-trained models publicly available at http://bopt-gmm. cs.uni-freiburg.de.

{{</citation>}}


### (6/16 | 175/241) DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic Supervision (Yutong Hu et al., 2024)

{{<citation>}}

Yutong Hu, Kehan Wen, Fisher Yu. (2024)  
**DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic Supervision**
<br/>
<button class="copy-to-clipboard" title="DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic Supervision" index=175>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-AI, cs-RO, cs.RO  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14300v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14300v1.pdf" filename="2403.14300v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Learning dexterous locomotion policy for legged robots is becoming increasingly popular due to its ability to handle diverse terrains and resemble intelligent behaviors. However, joint manipulation of moving objects and locomotion with legs, such as playing soccer, receive scant attention in the learning community, although it is natural for humans and smart animals. A key challenge to solve this multitask problem is to infer the objectives of locomotion from the states and targets of the manipulated objects. The implicit relation between the object states and robot locomotion can be hard to capture directly from the training experience. We propose adding a feedback control block to compute the necessary body-level movement accurately and using the outputs as dynamic joint-level locomotion supervision explicitly. We further utilize an improved ball dynamic model, an extended context-aided estimator, and a comprehensive ball observer to facilitate transferring policy learned in <b>simulation</b> to the real world. We observe that our learning scheme can not only make the policy network converge faster but also enable soccer robots to perform sophisticated maneuvers like sharp cuts and turns on flat surfaces, a capability that was lacking in previous methods. Video and code are available at https://github.com/SysCV/soccer-player

{{</citation>}}


### (7/16 | 176/241) Extrinsic Calibration of Multiple LiDARs for a Mobile Robot based on Floor Plane And Object Segmentation (Shun Niijima et al., 2024)

{{<citation>}}

Shun Niijima, Atsushi Suzuki, Ryoichi Tsuzaki, Masaya Kinoshita. (2024)  
**Extrinsic Calibration of Multiple LiDARs for a Mobile Robot based on Floor Plane And Object Segmentation**
<br/>
<button class="copy-to-clipboard" title="Extrinsic Calibration of Multiple LiDARs for a Mobile Robot based on Floor Plane And Object Segmentation" index=176>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14161v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14161v1.pdf" filename="2403.14161v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Mobile robots equipped with multiple light detection and ranging (LiDARs) and capable of recognizing their surroundings are increasing due to the minitualization and cost reduction of LiDAR. This paper proposes a target-less extrinsic calibration method of multiple LiDARs with non-overlapping field of view (FoV). The proposed method uses accumulated point clouds of floor plane and objects while in motion. It enables accurate calibration with challenging configuration of LiDARs that directed towards the floor plane, caused by biased feature values. Additionally, the method includes a noise removal module that considers the scanning pattern to address bleeding points, which are noises of significant source of error in point cloud alignment using high-density LiDARs. Evaluations through <b>simulation</b> demonstrate that the proposed method achieved higher accuracy extrinsic calibration with two and four LiDARs than conventional methods, regardless type of objects. Furthermore, the experiments using a real mobile robot has shown that our proposed noise removal module can eliminate noise more precisely than conventional methods, and the estimated extrinsic parameters have successfully created consistent 3D maps.

{{</citation>}}


### (8/16 | 177/241) A Roadmap Towards Automated and Regulated Robotic Systems (Yihao Liu et al., 2024)

{{<citation>}}

Yihao Liu, Mehran Armand. (2024)  
**A Roadmap Towards Automated and Regulated Robotic Systems**
<br/>
<button class="copy-to-clipboard" title="A Roadmap Towards Automated and Regulated Robotic Systems" index=177>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-AI, cs-HC, cs-RO, cs.RO  
Keyword Score: 18  
Keywords: Graph, Black Box, human-in-the-loop  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14049v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14049v1.pdf" filename="2403.14049v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The rapid development of generative technology opens up possibility for higher level of automation, and artificial intelligence (AI) embodiment in robotic systems is imminent. However, due to the blackbox nature of the generative technology, the generation of the knowledge and workflow scheme is uncontrolled, especially in a dynamic environment and a complex scene. This poses challenges to regulations in safety-demanding applications such as medical scenes. We argue that the unregulated generative processes from AI is fitted for low level end tasks, but intervention in the form of manual or automated regulation should happen post-workflow-generation and pre-robotic-execution. To address this, we propose a roadmap that can lead to fully automated and regulated robotic systems. In this paradigm, the high level policies are generated as structured <b>graph</b> data, enabling regulatory oversight and reusability, while the code base for lower level tasks is generated by generative models. Our approach aims the transitioning from expert knowledge to regulated action, akin to the iterative processes of study, practice, scrutiny, and execution in human tasks. We identify the generative and deterministic processes in a design cycle, where generative processes serve as a text-based world simulator and the deterministic processes generate the executable system. We propose State Machine Seralization Language (SMSL) to be the conversion point between text simulator and executable workflow control. From there, we analyze the modules involved based on the current literature, and discuss human in the loop. As a roadmap, this work identifies the current possible implementation and future work. This work does not provide an implemented system but envisions to inspire the researchers working on the direction in the roadmap. We implement the SMSL and D-SFO paradigm that serve as the starting point of the roadmap.

{{</citation>}}


### (9/16 | 178/241) Bringing Robots Home: The Rise of AI Robots in Consumer Electronics (Haiwei Dong et al., 2024)

{{<citation>}}

Haiwei Dong, Yang Liu, Ted Chu, Abdulmotaleb El Saddik. (2024)  
**Bringing Robots Home: The Rise of AI Robots in Consumer Electronics**
<br/>
<button class="copy-to-clipboard" title="Bringing Robots Home: The Rise of AI Robots in Consumer Electronics" index=178>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-MM, cs-RO, cs.RO  
Keyword Score: 16  
Keywords: Generative AI, Multi-modal, Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14449v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14449v1.pdf" filename="2403.14449v1.pdf">Download PDF</button>

---


**ABSTRACT**  
On March 18, 2024, NVIDIA unveiled Project GR00T, a general-purpose <b>multimodal</b> <b>generative</b> <b>AI</b> model designed specifically for training humanoid robots. Preceding this event, Tesla's unveiling of the Optimus Gen 2 humanoid robot on December 12, 2023, underscored the profound impact robotics is poised to have on reshaping various facets of our daily lives. While robots have long dominated industrial settings, their presence within our homes is a burgeoning phenomenon. This can be attributed, in part, to the complexities of domestic environments and the challenges of creating robots that can seamlessly integrate into our daily routines.

{{</citation>}}


### (10/16 | 179/241) Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation (Leyuan Sun et al., 2024)

{{<citation>}}

Leyuan Sun, Asako Kanezaki, Guillaume Caron, Yusuke Yoshiyasu. (2024)  
**Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation**
<br/>
<button class="copy-to-clipboard" title="Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation" index=179>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-AI, cs-CV, cs-RO, cs.RO  
Keyword Score: 16  
Keywords: Multi-modal, Multi-modal, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14163v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14163v1.pdf" filename="2403.14163v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Object-goal navigation is a crucial engineering task for the community of embodied navigation; it involves navigating to an instance of a specified object category within unseen environments. Although extensive investigations have been conducted on both end-to-end and modular-based, data-driven approaches, fully enabling an agent to comprehend the environment through perceptual knowledge and perform object-goal navigation as efficiently as humans remains a significant challenge. Recently, <b>large</b> <b>language</b> <b>models</b> have shown potential in this task, thanks to their powerful capabilities for knowledge extraction and integration. In this study, we propose a data-driven, modular-based approach, trained on a dataset that incorporates common-sense knowledge of object-to-room relationships extracted from a <b>large</b> <b>language</b> <b>model.</b> We utilize the multi-channel Swin-Unet architecture to conduct multi-task learning incorporating with <b>multimodal</b> inputs. The results in the Habitat simulator demonstrate that our framework outperforms the baseline by an average of 10.6% in the efficiency metric, Success weighted by Path Length (SPL). The real-world demonstration shows that the proposed approach can efficiently conduct this task by traversing several rooms. For more details and real-world demonstrations, please check our project webpage (https://sunleyuan.github.io/ObjectNav).

{{</citation>}}


### (11/16 | 180/241) ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras Based on Transformer (Tianye Ding et al., 2024)

{{<citation>}}

Tianye Ding, Hongyu Li, Huaizu Jiang. (2024)  
**ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras Based on Transformer**
<br/>
<button class="copy-to-clipboard" title="ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras Based on Transformer" index=180>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-CV, cs-RO, cs.RO  
Keyword Score: 13  
Keywords: Benchmarking, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14626v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14626v1.pdf" filename="2403.14626v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Obstacle detection and tracking represent a critical component in robot autonomous navigation. In this paper, we propose ODTFormer, a <b>Transformer-based</b> model to address both obstacle detection and tracking problems. For the detection task, our approach leverages deformable attention to construct a 3D cost volume, which is decoded progressively in the form of voxel occupancy grids. We further track the obstacles by matching the voxels between consecutive frames. The entire model can be optimized in an end-to-end manner. Through extensive experiments on DrivingStereo and KITTI <b>benchmarks,</b> our model achieves state-of-the-art performance in the obstacle detection task. We also report comparable accuracy to state-of-the-art obstacle tracking models while requiring only a fraction of their computation cost, typically ten-fold to twenty-fold less. The code and model weights will be publicly released.

{{</citation>}}


### (12/16 | 181/241) Exosense: A Vision-Centric Scene Understanding System For Safe Exoskeleton Navigation (Jianeng Wang et al., 2024)

{{<citation>}}

Jianeng Wang, Matias Mattamala, Christina Kassab, Lintong Zhang, Maurice Fallon. (2024)  
**Exosense: A Vision-Centric Scene Understanding System For Safe Exoskeleton Navigation**
<br/>
<button class="copy-to-clipboard" title="Exosense: A Vision-Centric Scene Understanding System For Safe Exoskeleton Navigation" index=181>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-CV, cs-RO, cs.RO  
Keyword Score: 13  
Keywords: Graph, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14320v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14320v1.pdf" filename="2403.14320v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Exoskeletons for daily use by those with mobility impairments are being developed. They will require accurate and robust scene understanding systems. Current research has used vision to identify immediate terrain and geometric obstacles, however these approaches are constrained to detections directly in front of the user and are limited to classifying a finite range of terrain types (e.g., stairs, ramps and level-ground). This paper presents Exosense, a vision-centric scene understanding system which is capable of generating rich, globally-consistent elevation maps, incorporating both semantic and terrain traversability information. It features an elastic Atlas mapping framework associated with a visual SLAM pose <b>graph,</b> embedded with open-vocabulary room labels from a <b>Vision-Language</b> Model (VLM). The device's design includes a wide field-of-view (FoV) fisheye multi-camera system to mitigate the challenges introduced by the exoskeleton walking pattern. We demonstrate the system's robustness to the challenges of typical periodic walking gaits, and its ability to construct accurate semantically-rich maps in indoor settings. Additionally, we showcase its potential for motion planning -- providing a step towards safe navigation for exoskeletons.

{{</citation>}}


### (13/16 | 182/241) Extended Reality for Enhanced Human-Robot Collaboration: a Human-in-the-Loop Approach (Yehor Karpichev et al., 2024)

{{<citation>}}

Yehor Karpichev, Todd Charter, Homayoun Najjaran. (2024)  
**Extended Reality for Enhanced Human-Robot Collaboration: a Human-in-the-Loop Approach**
<br/>
<button class="copy-to-clipboard" title="Extended Reality for Enhanced Human-Robot Collaboration: a Human-in-the-Loop Approach" index=182>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-HC, cs-LG, cs-RO, cs.RO  
Keyword Score: 10  
Keywords: human-in-the-loop  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14597v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14597v1.pdf" filename="2403.14597v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The rise of automation has provided an opportunity to achieve higher efficiency in manufacturing processes, yet it often compromises the flexibility required to promptly respond to evolving market needs and meet the demand for customization. Human-robot collaboration attempts to tackle these challenges by combining the strength and precision of machines with human ingenuity and perceptual understanding. In this paper, we conceptualize and propose an implementation framework for an autonomous, machine learning-based manipulator that incorporates <b>human-in-the-loop</b> principles and leverages Extended Reality (XR) to facilitate intuitive communication and programming between humans and robots. Furthermore, the conceptual framework foresees human involvement directly in the robot learning process, resulting in higher adaptability and task generalization. The paper highlights key technologies enabling the proposed framework, emphasizing the importance of developing the digital ecosystem as a whole. Additionally, we review the existent implementation approaches of XR in human-robot collaboration, showcasing diverse perspectives and methodologies. The challenges and future outlooks are discussed, delving into the major obstacles and potential research avenues of XR for more natural human-robot interaction and integration in the industrial landscape.

{{</citation>}}


### (14/16 | 183/241) UAV-Assisted Maritime Search and Rescue: A Holistic Approach (Martin Messmer et al., 2024)

{{<citation>}}

Martin Messmer, Benjamin Kiefer, Leon Amadeus Varga, Andreas Zell. (2024)  
**UAV-Assisted Maritime Search and Rescue: A Holistic Approach**
<br/>
<button class="copy-to-clipboard" title="UAV-Assisted Maritime Search and Rescue: A Holistic Approach" index=183>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 10  
Keywords: Object Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14281v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14281v1.pdf" filename="2403.14281v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we explore the application of Unmanned Aerial Vehicles (UAVs) in maritime search and rescue (mSAR) missions, focusing on medium-sized fixed-wing drones and quadcopters. We address the challenges and limitations inherent in operating some of the different classes of UAVs, particularly in search operations. Our research includes the development of a comprehensive software framework designed to enhance the efficiency and efficacy of SAR operations. This framework combines preliminary detection onboard UAVs with advanced <b>object</b> <b>detection</b> at ground stations, aiming to reduce visual strain and improve decision-making for operators. It will be made publicly available upon publication. We conduct experiments to evaluate various Region of Interest (RoI) proposal methods, especially by imposing simulated limited bandwidth on them, an important consideration when flying remote or offshore operations. This forces the algorithm to prioritize some predictions over others.

{{</citation>}}


### (15/16 | 184/241) ReFeree: Radar-based efficient global descriptor using a Feature and Free space for Place Recognition (Byunghee Choi et al., 2024)

{{<citation>}}

Byunghee Choi, Hogyun Kim, Younggun Cho. (2024)  
**ReFeree: Radar-based efficient global descriptor using a Feature and Free space for Place Recognition**
<br/>
<button class="copy-to-clipboard" title="ReFeree: Radar-based efficient global descriptor using a Feature and Free space for Place Recognition" index=184>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 10  
Keywords: Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14176v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14176v1.pdf" filename="2403.14176v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Radar is highlighted for robust sensing capabilities in adverse weather conditions (e.g. dense fog, heavy rain, or snowfall). In addition, Radar can cover wide areas and penetrate small particles. Despite these advantages, Radar-based place recognition remains in the early stages compared to other sensors due to its unique characteristics such as low resolution, and significant noise. In this paper, we propose a Radarbased place recognition utilizing a descriptor called ReFeree using a feature and free space. Unlike traditional methods, we overwhelmingly <b>summarize</b> the Radar image. Despite being lightweight, it contains semi-metric information and is also outstanding from the perspective of place recognition performance. For concrete validation, we test a single session from the MulRan dataset and a multi-session from the Oxford Radar RobotCar and the Boreas dataset.

{{</citation>}}


### (16/16 | 185/241) HCTO: Optimality-Aware LiDAR Inertial Odometry with Hybrid Continuous Time Optimization for Compact Wearable Mapping System (Jianping Li et al., 2024)

{{<citation>}}

Jianping Li, Shenghai Yuan, Muqing Cao, Thien-Minh Nguyen, Kun Cao, Lihua Xie. (2024)  
**HCTO: Optimality-Aware LiDAR Inertial Odometry with Hybrid Continuous Time Optimization for Compact Wearable Mapping System**
<br/>
<button class="copy-to-clipboard" title="HCTO: Optimality-Aware LiDAR Inertial Odometry with Hybrid Continuous Time Optimization for Compact Wearable Mapping System" index=185>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 10  
Keywords: Continuous Time, Continuous Time  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14173v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14173v1.pdf" filename="2403.14173v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Compact wearable mapping system (WMS) has gained significant attention due to their convenience in various applications. Specifically, it provides an efficient way to collect prior maps for 3D structure inspection and robot-based "last-mile delivery" in complex environments. However, vibrations in human motion and the uneven distribution of point cloud features in complex environments often lead to rapid drift, which is a prevalent issue when applying existing LiDAR Inertial Odometry (LIO) methods on low-cost WMS. To address these limitations, we propose a novel LIO for WMSs based on Hybrid <b>Continuous</b> <b>Time</b> Optimization (HCTO) considering the optimality of Lidar correspondences. First, HCTO recognizes patterns in human motion (high-frequency part, low-frequency part, and constant velocity part) by analyzing raw IMU measurements. Second, HCTO constructs hybrid IMU factors according to different motion states, which enables robust and accurate estimation against vibration-induced noise in the IMU measurements. Third, the best point correspondences are selected using optimal design to achieve real-time performance and better odometry accuracy. We conduct experiments on head-mounted WMS datasets to evaluate the performance of our system, demonstrating significant advantages over state-of-the-art methods. Video recordings of experiments can be found on the project page of HCTO: \href{https://github.com/kafeiyin00/HCTO}{https://github.com/kafeiyin00/HCTO}.

{{</citation>}}


## cs.CY (2)



### (1/2 | 186/241) The Ethics of ChatGPT in Medicine and Healthcare: A Systematic Review on Large Language Models (LLMs) (Joschka Haltaufderheide et al., 2024)

{{<citation>}}

Joschka Haltaufderheide, Robert Ranisch. (2024)  
**The Ethics of ChatGPT in Medicine and Healthcare: A Systematic Review on Large Language Models (LLMs)**
<br/>
<button class="copy-to-clipboard" title="The Ethics of ChatGPT in Medicine and Healthcare: A Systematic Review on Large Language Models (LLMs)" index=186>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CY  
Categories: cs-CY, cs.CY  
Keyword Score: 40  
Keywords: Fairness, ChatGPT, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14473v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14473v1.pdf" filename="2403.14473v1.pdf">Download PDF</button>

---


**ABSTRACT**  
With the introduction of <b>ChatGPT,</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have received enormous attention in healthcare. Despite their potential benefits, researchers have underscored various ethical implications. While individual instances have drawn much attention, the debate lacks a systematic overview of practical applications currently researched and ethical issues connected to them. Against this background, this work aims to map the ethical landscape surrounding the current stage of deployment of <b>LLMs</b> in medicine and healthcare. Electronic databases and preprint servers were queried using a comprehensive search strategy. Studies were screened and extracted following a modified rapid review approach. Methodological quality was assessed using a hybrid approach. For 53 records, a meta-aggregative synthesis was performed. Four fields of applications emerged and testify to a vivid exploration phase. Advantages of using <b>LLMs</b> are attributed to their capacity in data analysis, personalized information provisioning, support in decision-making, mitigating information loss and enhancing information accessibility. However, we also identifies recurrent ethical concerns connected to <b>fairness,</b> bias, non-maleficence, transparency, and privacy. A distinctive concern is the tendency to produce harmful misinformation or convincingly but inaccurate content. A recurrent plea for ethical guidance and human oversight is evident. Given the variety of use cases, it is suggested that the ethical guidance debate be reframed to focus on defining what constitutes acceptable human oversight across the spectrum of applications. This involves considering diverse settings, varying potentials for harm, and different acceptable thresholds for performance and certainty in healthcare. In addition, a critical inquiry is necessary to determine the extent to which the current experimental use of <b>LLMs</b> is necessary and justified.

{{</citation>}}


### (2/2 | 187/241) On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial (Francesco Salvi et al., 2024)

{{<citation>}}

Francesco Salvi, Manoel Horta Ribeiro, Riccardo Gallotti, Robert West. (2024)  
**On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial**
<br/>
<button class="copy-to-clipboard" title="On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial" index=187>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CY  
Categories: cs-CY, cs.CY  
Keyword Score: 40  
Keywords: GPT, GPT-4, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14380v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14380v1.pdf" filename="2403.14380v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The development and popularization of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have raised concerns that they will be used to create tailor-made, convincing arguments to push false or misleading narratives online. Early work has found that language models can generate content perceived as at least on par and often more persuasive than human-written messages. However, there is still limited knowledge about <b>LLMs'</b> persuasive capabilities in direct conversations with human counterparts and how personalization can improve their performance. In this pre-registered study, we analyze the effect of AI-driven persuasion in a controlled, harmless setting. We create a web-based platform where participants engage in short, multiple-round debates with a live opponent. Each participant is randomly assigned to one of four treatment conditions, corresponding to a two-by-two factorial design: (1) Games are either played between two humans or between a human and an <b>LLM;</b> (2) Personalization might or might not be enabled, granting one of the two players access to basic sociodemographic information about their opponent. We found that participants who debated <b>GPT-4</b> with access to their personal information had 81.7% (p < 0.01; N=820 unique participants) higher odds of increased agreement with their opponents compared to participants who debated humans. Without personalization, <b>GPT-4</b> still outperforms humans, but the effect is lower and statistically non-significant (p=0.31). Overall, our results suggest that concerns around personalization are meaningful and have important implications for the governance of social media and the design of new online environments.

{{</citation>}}


## eess.IV (7)



### (1/7 | 188/241) CathFlow: Self-Supervised Segmentation of Catheters in Interventional Ultrasound Using Optical Flow and Transformers (Alex Ranne et al., 2024)

{{<citation>}}

Alex Ranne, Liming Kuang, Yordanka Velikova, Nassir Navab, Ferdinando Rodriguez y Baena. (2024)  
**CathFlow: Self-Supervised Segmentation of Catheters in Interventional Ultrasound Using Optical Flow and Transformers**
<br/>
<button class="copy-to-clipboard" title="CathFlow: Self-Supervised Segmentation of Catheters in Interventional Ultrasound Using Optical Flow and Transformers" index=188>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 40  
Keywords: Self-supervised Learning, Simulation, Simulator, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14465v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14465v1.pdf" filename="2403.14465v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In minimally invasive endovascular procedures, contrast-enhanced angiography remains the most robust imaging technique. However, it is at the expense of the patient and clinician's health due to prolonged radiation exposure. As an alternative, interventional ultrasound has notable benefits such as being radiation-free, fast to deploy, and having a small footprint in the operating room. Yet, ultrasound is hard to interpret, and highly prone to artifacts and noise. Additionally, interventional radiologists must undergo extensive training before they become qualified to diagnose and treat patients effectively, leading to a shortage of staff, and a lack of open-source datasets. In this work, we seek to address both problems by introducing a <b>self-supervised</b> deep learning architecture to segment catheters in longitudinal ultrasound images, without demanding any labeled data. The network architecture builds upon AiAReSeg, a segmentation <b>transformer</b> built with the Attention in Attention mechanism, and is capable of learning feature changes across time and space. To facilitate training, we used synthetic ultrasound data based on physics-driven catheter insertion <b>simulations,</b> and translated the data into a unique CT-Ultrasound common domain, CACTUSS, to improve the segmentation performance. We generated ground truth segmentation masks by computing the optical flow between adjacent frames using FlowNet2, and performed thresholding to obtain a binary map estimate. Finally, we validated our model on a test dataset, consisting of unseen synthetic data and images collected from silicon aorta phantoms, thus demonstrating its potential for applications to clinical data in the future.

{{</citation>}}


### (2/7 | 189/241) Diffusion Models with Ensembled Structure-Based Anomaly Scoring for Unsupervised Anomaly Detection (Finn Behrendt et al., 2024)

{{<citation>}}

Finn Behrendt, Debayan Bhattacharya, Lennart Maack, Julia Krüger, Roland Opfer, Robin Mieling, Alexander Schlaefer. (2024)  
**Diffusion Models with Ensembled Structure-Based Anomaly Scoring for Unsupervised Anomaly Detection**
<br/>
<button class="copy-to-clipboard" title="Diffusion Models with Ensembled Structure-Based Anomaly Scoring for Unsupervised Anomaly Detection" index=189>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, cs-LG, eess-IV, eess.IV  
Keyword Score: 40  
Keywords: Diffusion Model, Anomaly Detection, Supervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14262v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14262v1.pdf" filename="2403.14262v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Supervised</b> deep learning techniques show promise in medical image analysis. However, they require comprehensive annotated data sets, which poses challenges, particularly for rare diseases. Consequently, <b>unsupervised</b> <b>anomaly</b> <b>detection</b> (UAD) emerges as a viable alternative for pathology segmentation, as only healthy data is required for training. However, recent UAD <b>anomaly</b> <b>scoring</b> functions often focus on intensity only and neglect structural differences, which impedes the segmentation performance. This work investigates the potential of Structural Similarity (SSIM) to bridge this gap. SSIM captures both intensity and structural disparities and can be advantageous over the classical $l1$ error. However, we show that there is more than one optimal kernel size for the SSIM calculation for different pathologies. Therefore, we investigate an adaptive ensembling strategy for various kernel sizes to offer a more pathology-agnostic scoring mechanism. We demonstrate that this ensembling strategy can enhance the performance of DMs and mitigate the sensitivity to different kernel sizes across varying pathologies, highlighting its promise for brain MRI <b>anomaly</b> <b>detection.</b>

{{</citation>}}


### (3/7 | 190/241) ResNet101 and DAE for Enhance Quality and Classification Accuracy in Skin Cancer Imaging (Sibasish Dhibar, 2024)

{{<citation>}}

Sibasish Dhibar. (2024)  
**ResNet101 and DAE for Enhance Quality and Classification Accuracy in Skin Cancer Imaging**
<br/>
<button class="copy-to-clipboard" title="ResNet101 and DAE for Enhance Quality and Classification Accuracy in Skin Cancer Imaging" index=190>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: 68U10, 94A08, J-3; I-4-3; I-4-9, cs-CV, eess-IV, eess.IV  
Keyword Score: 40  
Keywords: Autoencoder, Convolution, Convolutional Neural Network, Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14248v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14248v1.pdf" filename="2403.14248v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Skin cancer is a crucial health issue that requires timely detection for higher survival rates. Traditional computer vision techniques face challenges in addressing the advanced variability of skin lesion features, a gap partially bridged by <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs).</b> To overcome the existing issues, we introduce an innovative <b>convolutional</b> <b>ensemble</b> <b>network</b> approach named deep <b>autoencoder</b> (DAE) with ResNet101. This method utilizes <b>convolution-based</b> deep neural networks for the detection of skin cancer. The ISIC-2018 public data taken from the source is used for experimental results, which demonstrate remarkable performance with the different in terms of performance metrics. The methods result in 96.03% of accuracy, 95.40 % of precision, 96.05% of recall, 0.9576 of F-measure, 0.98 of AUC.

{{</citation>}}


### (4/7 | 191/241) QSMDiff: Unsupervised 3D Diffusion Models for Quantitative Susceptibility Mapping (Zhuang Xiong et al., 2024)

{{<citation>}}

Zhuang Xiong, Wei Jiang, Yang Gao, Feng Liu, Hongfu Sun. (2024)  
**QSMDiff: Unsupervised 3D Diffusion Models for Quantitative Susceptibility Mapping**
<br/>
<button class="copy-to-clipboard" title="QSMDiff: Unsupervised 3D Diffusion Models for Quantitative Susceptibility Mapping" index=191>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV, physics-med-ph  
Keyword Score: 30  
Keywords: Diffusion Model, Supervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14070v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14070v1.pdf" filename="2403.14070v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Quantitative Susceptibility Mapping (QSM) dipole inversion is an ill-posed inverse problem for quantifying magnetic susceptibility distributions from MRI tissue phases. While <b>supervised</b> deep learning methods have shown success in specific QSM tasks, their generalizability across different acquisition scenarios remains constrained. Recent developments in <b>diffusion</b> <b>models</b> have demonstrated potential for solving 2D medical imaging inverse problems. However, their application to 3D modalities, such as QSM, remains challenging due to high computational demands. In this work, we developed a 3D image patch-based <b>diffusion</b> <b>model,</b> namely QSMDiff, for robust QSM reconstruction across different scan parameters, alongside simultaneous super-resolution and image-denoising tasks. QSMDiff adopts <b>unsupervised</b> 3D image patch training and full-size measurement guidance during inference for controlled image generation. Evaluation on simulated and in-vivo human brains, using gradient-echo and echo-planar imaging sequences across different acquisition parameters, demonstrates superior performance. The method proposed in QSMDiff also holds promise for impacting other 3D medical imaging applications beyond QSM.

{{</citation>}}


### (5/7 | 192/241) Denoising Diffusion Models for 3D Healthy Brain Tissue Inpainting (Alicia Durrer et al., 2024)

{{<citation>}}

Alicia Durrer, Julia Wolleb, Florentin Bieder, Paul Friedrich, Lester Melie-Garcia, Mario Ocampo-Pineda, Cosmin I. Bercea, Ibrahim E. Hamamci, Benedikt Wiestler, Marie Piraud, Özgür Yaldizli, Cristina Granziera, Bjoern H. Menze, Philippe C. Cattin, Florian Kofler. (2024)  
**Denoising Diffusion Models for 3D Healthy Brain Tissue Inpainting**
<br/>
<button class="copy-to-clipboard" title="Denoising Diffusion Models for 3D Healthy Brain Tissue Inpainting" index=192>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 20  
Keywords: Diffusion Model, Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14499v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14499v1.pdf" filename="2403.14499v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Monitoring diseases that affect the brain's structural integrity requires automated analysis of magnetic resonance (MR) images, e.g., for the evaluation of volumetric changes. However, many of the evaluation tools are optimized for analyzing healthy tissue. To enable the evaluation of scans containing pathological tissue, it is therefore required to restore healthy tissue in the pathological areas. In this work, we explore and extend denoising <b>diffusion</b> <b>models</b> for consistent inpainting of healthy 3D brain tissue. We modify state-of-the-art 2D, pseudo-3D, and 3D methods working in the image space, as well as 3D latent and 3D wavelet <b>diffusion</b> <b>models,</b> and train them to synthesize healthy brain tissue. Our evaluation shows that the pseudo-3D model performs best regarding the structural-similarity index, peak signal-to-noise ratio, and mean squared error. To emphasize the clinical relevance, we <b>fine-tune</b> this model on data containing synthetic MS lesions and evaluate it on a downstream brain tissue segmentation task, whereby it outperforms the established FMRIB Software Library (FSL) lesion-filling method.

{{</citation>}}


### (6/7 | 193/241) Analysing Diffusion Segmentation for Medical Images (Mathias Öttl et al., 2024)

{{<citation>}}

Mathias Öttl, Siyuan Mei, Frauke Wilm, Jana Steenpass, Matthias Rübner, Arndt Hartmann, Matthias Beckmann, Peter Fasching, Andreas Maier, Ramona Erber, Katharina Breininger. (2024)  
**Analysing Diffusion Segmentation for Medical Images**
<br/>
<button class="copy-to-clipboard" title="Analysing Diffusion Segmentation for Medical Images" index=193>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, cs-LG, eess-IV, eess.IV  
Keyword Score: 10  
Keywords: Probabilistic Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14440v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14440v1.pdf" filename="2403.14440v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Denoising Diffusion <b>Probabilistic</b> <b>models</b> have become increasingly popular due to their ability to offer <b>probabilistic</b> <b>modeling</b> and generate diverse outputs. This versatility inspired their adaptation for image segmentation, where multiple predictions of the model can produce segmentation results that not only achieve high quality but also capture the uncertainty inherent in the model. Here, powerful architectures were proposed for improving diffusion segmentation performance. However, there is a notable lack of analysis and discussions on the differences between diffusion segmentation and image generation, and thorough evaluations are missing that distinguish the improvements these architectures provide for segmentation in general from their benefit for diffusion segmentation specifically. In this work, we critically analyse and discuss how diffusion segmentation for medical images differs from diffusion image generation, with a particular focus on the training behavior. Furthermore, we conduct an assessment how proposed diffusion segmentation architectures perform when trained directly for segmentation. Lastly, we explore how different medical segmentation tasks influence the diffusion segmentation behavior and the diffusion process could be adapted accordingly. With these analyses, we aim to provide in-depth insights into the behavior of diffusion segmentation that allow for a better design and evaluation of diffusion segmentation methods in the future.

{{</citation>}}


### (7/7 | 194/241) LeFusion: Synthesizing Myocardial Pathology on Cardiac MRI via Lesion-Focus Diffusion Models (Hantao Zhang et al., 2024)

{{<citation>}}

Hantao Zhang, Jiancheng Yang, Shouhong Wan, Pascal Fua. (2024)  
**LeFusion: Synthesizing Myocardial Pathology on Cardiac MRI via Lesion-Focus Diffusion Models**
<br/>
<button class="copy-to-clipboard" title="LeFusion: Synthesizing Myocardial Pathology on Cardiac MRI via Lesion-Focus Diffusion Models" index=194>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 10  
Keywords: Diffusion Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14066v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14066v1.pdf" filename="2403.14066v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Data generated in clinical practice often exhibits biases, such as long-tail imbalance and algorithmic unfairness. This study aims to mitigate these challenges through data synthesis. Previous efforts in medical imaging synthesis have struggled with separating lesion information from background context, leading to difficulties in generating high-quality backgrounds and limited control over the synthetic output. Inspired by <b>diffusion-based</b> <b>image</b> inpainting, we propose LeFusion, lesion-focused <b>diffusion</b> <b>models.</b> By redesigning the <b>diffusion</b> <b>learning</b> objectives to concentrate on lesion areas, it simplifies the model learning process and enhance the controllability of the synthetic output, while preserving background by integrating forward-diffused background contexts into the reverse <b>diffusion</b> <b>process.</b> Furthermore, we generalize it to jointly handle multi-class lesions, and further introduce a generative model for lesion masks to increase synthesis diversity. Validated on the DE-MRI cardiac lesion segmentation dataset (Emidec), our methodology employs the popular nnUNet to demonstrate that the synthetic data make it possible to effectively enhance a state-of-the-art model. Code and model are available at https://github.com/M3DV/LeFusion.

{{</citation>}}


## cs.NE (6)



### (1/6 | 195/241) SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks (Xinyu Shi et al., 2024)

{{<citation>}}

Xinyu Shi, Zecheng Hao, Zhaofei Yu. (2024)  
**SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks**
<br/>
<button class="copy-to-clipboard" title="SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks" index=195>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NE  
Categories: cs-CV, cs-LG, cs-NE, cs.NE  
Keyword Score: 40  
Keywords: Vision Transformer, Transformer, Self-Attention, Vision Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14302v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14302v1.pdf" filename="2403.14302v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The remarkable success of <b>Vision</b> <b>Transformers</b> in Artificial Neural Networks (ANNs) has led to a growing interest in incorporating the <b>self-attention</b> mechanism and <b>transformer-based</b> architecture into Spiking Neural Networks (SNNs). While existing methods propose spiking <b>self-attention</b> mechanisms that are compatible with SNNs, they lack reasonable scaling methods, and the overall architectures proposed by these methods suffer from a bottleneck in effectively extracting local features. To address these challenges, we propose a novel spiking <b>self-attention</b> mechanism named Dual Spike <b>Self-Attention</b> (DSSA) with a reasonable scaling method. Based on DSSA, we propose a novel spiking <b>Vision</b> <b>Transformer</b> architecture called SpikingResformer, which combines the ResNet-based multi-stage architecture with our proposed DSSA to improve both performance and energy efficiency while reducing parameters. Experimental results show that SpikingResformer achieves higher accuracy with fewer parameters and lower energy consumption than other spiking <b>Vision</b> <b>Transformer</b> counterparts. Notably, our SpikingResformer-L achieves 79.40% top-1 accuracy on ImageNet with 4 time-steps, which is the state-of-the-art result in the SNN field.

{{</citation>}}


### (2/6 | 196/241) Reactor Optimization Benchmark by Reinforcement Learning (Deborah Schwarcz et al., 2024)

{{<citation>}}

Deborah Schwarcz, Nadav Schneider, Gal Oren, Uri Steinitz. (2024)  
**Reactor Optimization Benchmark by Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="Reactor Optimization Benchmark by Reinforcement Learning" index=196>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NE  
Categories: cs-AI, cs-NE, cs.NE  
Keyword Score: 33  
Keywords: Benchmarking, Reinforcement Learning, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14273v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14273v1.pdf" filename="2403.14273v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Neutronic calculations for reactors are a daunting task when using Monte Carlo (MC) methods. As high-performance computing has advanced, the <b>simulation</b> of a reactor is nowadays more readily done, but design and optimization with multiple parameters is still a computational challenge. MC transport <b>simulations,</b> coupled with machine learning techniques, offer promising avenues for enhancing the efficiency and effectiveness of nuclear reactor optimization. This paper introduces a novel <b>benchmark</b> problem within the OpenNeoMC framework designed specifically for <b>reinforcement</b> <b>learning.</b> The <b>benchmark</b> involves optimizing a unit cell of a research reactor with two varying parameters (fuel density and water spacing) to maximize neutron flux while maintaining reactor criticality. The test case features distinct local optima, representing different physical regimes, thus posing a challenge for learning algorithms. Through extensive <b>simulations</b> utilizing evolutionary and neuroevolutionary algorithms, we demonstrate the effectiveness of <b>reinforcement</b> <b>learning</b> in navigating complex optimization landscapes with strict constraints. Furthermore, we propose acceleration techniques within the OpenNeoMC framework, including model updating and cross-section usage by RAM utilization, to expedite <b>simulation</b> times. Our findings emphasize the importance of machine learning integration in reactor optimization and contribute to advancing methodologies for addressing intricate optimization challenges in nuclear engineering. The sources of this work are available at our GitHub repository: https://github.com/Scientific-Computing-Lab-NRCN/RLOpenNeoMC

{{</citation>}}


### (3/6 | 197/241) Model Uncertainty in Evolutionary Optimization and Bayesian Optimization: A Comparative Analysis (Hao Hao et al., 2024)

{{<citation>}}

Hao Hao, Xiaoqun Zhang, Aimin Zhou. (2024)  
**Model Uncertainty in Evolutionary Optimization and Bayesian Optimization: A Comparative Analysis**
<br/>
<button class="copy-to-clipboard" title="Model Uncertainty in Evolutionary Optimization and Bayesian Optimization: A Comparative Analysis" index=197>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NE  
Categories: cs-LG, cs-NE, cs.NE  
Keyword Score: 25  
Keywords: Black Box, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14413v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14413v1.pdf" filename="2403.14413v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Black-box</b> <b>optimization</b> problems, which are common in many real-world applications, require optimization through input-output interactions without access to internal workings. This often leads to significant computational resources being consumed for <b>simulations.</b> Bayesian Optimization (BO) and Surrogate-Assisted Evolutionary Algorithm (SAEA) are two widely used gradient-free optimization techniques employed to address such challenges. Both approaches follow a similar iterative procedure that relies on surrogate models to guide the search process. This paper aims to elucidate the similarities and differences in the utilization of model uncertainty between these two methods, as well as the impact of model inaccuracies on algorithmic performance. A novel model-assisted strategy is introduced, which utilizes unevaluated solutions to generate offspring, leveraging the population-based search capabilities of evolutionary algorithm to enhance the effectiveness of model-assisted optimization. Experimental results demonstrate that the proposed approach outperforms mainstream Bayesian optimization algorithms in terms of accuracy and efficiency.

{{</citation>}}


### (4/6 | 198/241) A reinforcement learning guided hybrid evolutionary algorithm for the latency location routing problem (Yuji Zou et al., 2024)

{{<citation>}}

Yuji Zou, Jin-Kao Hao, Qinghua Wu. (2024)  
**A reinforcement learning guided hybrid evolutionary algorithm for the latency location routing problem**
<br/>
<button class="copy-to-clipboard" title="A reinforcement learning guided hybrid evolutionary algorithm for the latency location routing problem" index=198>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NE  
Categories: cs-DM, cs-NE, cs.NE  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14405v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14405v1.pdf" filename="2403.14405v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The latency location routing problem integrates the facility location problem and the multi-depot cumulative capacitated vehicle routing problem. This problem involves making simultaneous decisions about depot locations and vehicle routes to serve customers while aiming to minimize the sum of waiting (arriving) times for all customers. To address this computationally challenging problem, we propose a <b>reinforcement</b> <b>learning</b> guided hybrid evolutionary algorithm following the framework of the memetic algorithm. The proposed algorithm relies on a diversity-enhanced multi-parent edge assembly crossover to build promising offspring and a <b>reinforcement</b> <b>learning</b> guided variable neighborhood descent to determine the exploration order of multiple neighborhoods. Additionally, strategic oscillation is used to achieve a balanced exploration of both feasible and infeasible solutions. The competitiveness of the algorithm against state-of-the-art methods is demonstrated by experimental results on the three sets of 76 popular instances, including 51 improved best solutions (new upper bounds) for the 59 instances with unknown optima and equal best results for the remaining instances. We also conduct additional experiments to shed light on the key components of the algorithm.

{{</citation>}}


### (5/6 | 199/241) Stitching for Neuroevolution: Recombining Deep Neural Networks without Breaking Them (Arthur Guijt et al., 2024)

{{<citation>}}

Arthur Guijt, Dirk Thierens, Tanja Alderliesten, Peter A. N. Bosman. (2024)  
**Stitching for Neuroevolution: Recombining Deep Neural Networks without Breaking Them**
<br/>
<button class="copy-to-clipboard" title="Stitching for Neuroevolution: Recombining Deep Neural Networks without Breaking Them" index=199>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NE  
Categories: cs-NE, cs.NE  
Keyword Score: 10  
Keywords: Transfer Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14224v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14224v1.pdf" filename="2403.14224v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Traditional approaches to neuroevolution often start from scratch. This becomes prohibitively expensive in terms of computational and data requirements when targeting modern, deep neural networks. Using a warm start could be highly advantageous, e.g., using previously trained networks, potentially from different sources. This moreover enables leveraging the benefits of <b>transfer</b> <b>learning</b> (in particular vastly reduced training effort). However, recombining trained networks is non-trivial because architectures and feature representations typically differ. Consequently, a straightforward exchange of layers tends to lead to a performance breakdown. We overcome this by matching the layers of parent networks based on their connectivity, identifying potential crossover points. To correct for differing feature representations between these layers we employ stitching, which merges the networks by introducing new layers at crossover points. To train the merged network, only stitching layers need to be considered. New networks can then be created by selecting a subnetwork by choosing which stitching layers to (not) use. Assessing their performance is efficient as only their evaluation on data is required. We experimentally show that our approach enables finding networks that represent novel trade-offs between performance and computational cost, with some even dominating the original networks.

{{</citation>}}


### (6/6 | 200/241) Evolving Benchmark Functions to Compare Evolutionary Algorithms via Genetic Programming (Yifan He et al., 2024)

{{<citation>}}

Yifan He, Claus Aranha. (2024)  
**Evolving Benchmark Functions to Compare Evolutionary Algorithms via Genetic Programming**
<br/>
<button class="copy-to-clipboard" title="Evolving Benchmark Functions to Compare Evolutionary Algorithms via Genetic Programming" index=200>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NE  
Categories: cs-AI, cs-NE, cs.NE  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14146v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14146v1.pdf" filename="2403.14146v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this study, we use Genetic Programming (GP) to compose new optimization <b>benchmark</b> functions. Optimization <b>benchmarks</b> have the important role of showing the differences between evolutionary algorithms, making it possible for further analysis and comparisons. We show that the <b>benchmarks</b> generated by GP are able to differentiate algorithms better than human-made <b>benchmark</b> functions. The fitness measure of the GP is the Wasserstein distance of the solutions found by a pair of optimizers. Additionally, we use MAP-Elites to both enhance the search power of the GP and also illustrate how the difference between optimizers changes by various landscape features. Our approach provides a novel way to automate the design of <b>benchmark</b> functions and to compare evolutionary algorithms.

{{</citation>}}


## cs.CR (4)



### (1/4 | 201/241) Large Language Models for Blockchain Security: A Systematic Literature Review (Zheyuan He et al., 2024)

{{<citation>}}

Zheyuan He, Zihao Li, Sen Yang. (2024)  
**Large Language Models for Blockchain Security: A Systematic Literature Review**
<br/>
<button class="copy-to-clipboard" title="Large Language Models for Blockchain Security: A Systematic Literature Review" index=201>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs.CR  
Keyword Score: 40  
Keywords: Anomaly Detection, Large Language Model, Large Language Model, Adversarial Attack  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14280v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14280v1.pdf" filename="2403.14280v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have emerged as powerful tools in various domains involving blockchain security (BS). Several recent studies are exploring <b>LLMs</b> applied to BS. However, there remains a gap in our understanding regarding the full scope of applications, impacts, and potential constraints of <b>LLMs</b> on blockchain security. To fill this gap, we conduct a literature review on LLM4BS. As the first review of <b>LLM's</b> application on blockchain security, our study aims to comprehensively analyze existing research and elucidate how <b>LLMs</b> contribute to enhancing the security of blockchain systems. Through a thorough examination of scholarly works, we delve into the integration of <b>LLMs</b> into various aspects of blockchain security. We explore the mechanisms through which <b>LLMs</b> can bolster blockchain security, including their applications in smart contract auditing, identity verification, <b>anomaly</b> <b>detection,</b> vulnerable repair, and so on. Furthermore, we critically assess the challenges and limitations associated with leveraging <b>LLMs</b> for blockchain security, considering factors such as scalability, privacy concerns, and <b>adversarial</b> <b>attacks.</b> Our review sheds light on the opportunities and potential risks inherent in this convergence, providing valuable insights for researchers, practitioners, and policymakers alike.

{{</citation>}}


### (2/4 | 202/241) Adversary-Augmented Simulation to evaluate client-fairness on HyperLedger Fabric (Erwan Mahe et al., 2024)

{{<citation>}}

Erwan Mahe, Rouwaida Abdallah, Sara Tucci-Piergiovanni, Pierre-Yves Piriou. (2024)  
**Adversary-Augmented Simulation to evaluate client-fairness on HyperLedger Fabric**
<br/>
<button class="copy-to-clipboard" title="Adversary-Augmented Simulation to evaluate client-fairness on HyperLedger Fabric" index=202>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs-DC, cs-MA, cs.CR  
Keyword Score: 30  
Keywords: Simulation, Simulator, Adversarial Attack  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14342v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14342v1.pdf" filename="2403.14342v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents a novel adversary model specifically tailored to distributed systems, with the aim to asses the security of blockchain technologies. Building upon literature on <b>adversarial</b> <b>assumptions</b> and capabilities, we include classical notions of failure and communication models to classify and bind the use of <b>adversarial</b> <b>actions.</b> We focus on the effect of these actions on properties of distributed protocols. A significant effort of our research is the integration of this model into the Multi-Agent eXperimenter (MAX) framework. This integration enables realistic <b>simulations</b> of <b>adversarial</b> <b>attacks</b> on blockchain systems. In particular, we have simulated attacks violating a form of client-fairness on HyperLedger Fabric.

{{</citation>}}


### (3/4 | 203/241) HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic Encryption (Seewoo Lee et al., 2024)

{{<citation>}}

Seewoo Lee, Garam Lee, Jung Woo Kim, Junbum Shin, Mun-Kyu Lee. (2024)  
**HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic Encryption**
<br/>
<button class="copy-to-clipboard" title="HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic Encryption" index=203>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs-LG, cs.CR  
Keyword Score: 23  
Keywords: Benchmarking, Fine-tuning, Transfer Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14111v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14111v1.pdf" filename="2403.14111v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Transfer</b> <b>learning</b> is a de facto standard method for efficiently training machine learning models for data-scarce problems by adding and <b>fine-tuning</b> new classification layers to a model pre-trained on large datasets. Although numerous previous studies proposed to use homomorphic encryption to resolve the data privacy issue in <b>transfer</b> <b>learning</b> in the machine learning as a service setting, most of them only focused on encrypted inference. In this study, we present HETAL, an efficient Homomorphic Encryption based <b>Transfer</b> <b>Learning</b> algorithm, that protects the client's privacy in training tasks by encrypting the client data using the CKKS homomorphic encryption scheme. HETAL is the first practical scheme that strictly provides encrypted training, adopting validation-based early stopping and achieving the accuracy of nonencrypted training. We propose an efficient encrypted matrix multiplication algorithm, which is 1.8 to 323 times faster than prior methods, and a highly precise softmax approximation algorithm with increased coverage. The experimental results for five well-known <b>benchmark</b> datasets show total training times of 567-3442 seconds, which is less than an hour.

{{</citation>}}


### (4/4 | 204/241) FHAUC: Privacy Preserving AUC Calculation for Federated Learning using Fully Homomorphic Encryption (Cem Ata Baykara et al., 2024)

{{<citation>}}

Cem Ata Baykara, Ali Burak Ünal, Mete Akgün. (2024)  
**FHAUC: Privacy Preserving AUC Calculation for Federated Learning using Fully Homomorphic Encryption**
<br/>
<button class="copy-to-clipboard" title="FHAUC: Privacy Preserving AUC Calculation for Federated Learning using Fully Homomorphic Encryption" index=204>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs.CR  
Keyword Score: 20  
Keywords: Federated Learning, Differential Privacy  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14428v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14428v1.pdf" filename="2403.14428v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Ensuring data privacy is a significant challenge for machine learning applications, not only during model training but also during evaluation. <b>Federated</b> <b>learning</b> has gained significant research interest in recent years as a result. Current research on <b>federated</b> <b>learning</b> primarily focuses on preserving privacy during the training phase. However, model evaluation has not been adequately addressed, despite the potential for significant privacy leaks during this phase as well. In this paper, we demonstrate that the state-of-the-art AUC computation method for <b>federated</b> <b>learning</b> systems, which utilizes <b>differential</b> <b>privacy,</b> still leaks sensitive information about the test data while also requiring a trusted central entity to perform the computations. More importantly, we show that the performance of this method becomes completely unusable as the data size decreases. In this context, we propose an efficient, accurate, robust, and more secure evaluation algorithm capable of computing the AUC in horizontal <b>federated</b> <b>learning</b> systems. Our approach not only enhances security compared to the current state-of-the-art but also surpasses the state-of-the-art AUC computation method in both approximation performance and computational robustness, as demonstrated by experimental results. To illustrate, our approach can efficiently calculate the AUC of a <b>federated</b> <b>learning</b> system involving 100 parties, achieving 99.93% accuracy in just 0.68 seconds, regardless of data size, while providing complete data privacy.

{{</citation>}}


## cs.DC (1)



### (1/1 | 205/241) Accelerating ViT Inference on FPGA through Static and Dynamic Pruning (Dhruv Parikh et al., 2024)

{{<citation>}}

Dhruv Parikh, Shouyi Li, Bingyi Zhang, Rajgopal Kannan, Carl Busart, Viktor Prasanna. (2024)  
**Accelerating ViT Inference on FPGA through Static and Dynamic Pruning**
<br/>
<button class="copy-to-clipboard" title="Accelerating ViT Inference on FPGA through Static and Dynamic Pruning" index=205>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DC  
Categories: cs-AR, cs-CV, cs-DC, cs.DC  
Keyword Score: 40  
Keywords: Vision Transformer, Pruning, Transformer, Vision Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14047v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14047v1.pdf" filename="2403.14047v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Vision</b> <b>Transformers</b> (ViTs) have achieved state-of-the-art accuracy on various computer <b>vision</b> <b>tasks.</b> However, their high computational complexity prevents them from being applied to many real-world applications. Weight and token <b>pruning</b> are two well-known methods for reducing complexity: weight <b>pruning</b> reduces the model size and associated computational demands, while token <b>pruning</b> further dynamically reduces the computation based on the input. Combining these two techniques should significantly reduce computation complexity and model size; however, naively integrating them results in irregular computation patterns, leading to significant accuracy drops and difficulties in hardware acceleration. Addressing the above challenges, we propose a comprehensive algorithm-hardware codesign for accelerating ViT on FPGA through simultaneous <b>pruning</b> -combining static weight <b>pruning</b> and dynamic token <b>pruning.</b> For algorithm design, we systematically combine a hardware-aware structured block-pruning method for <b>pruning</b> model parameters and a dynamic token <b>pruning</b> method for removing unimportant token vectors. Moreover, we design a novel training algorithm to recover the model's accuracy. For hardware design, we develop a novel hardware accelerator for executing the pruned model. The proposed hardware design employs multi-level parallelism with load balancing strategy to efficiently deal with the irregular computation pattern led by the two <b>pruning</b> approaches. Moreover, we develop an efficient hardware mechanism for efficiently executing the on-the-fly token <b>pruning.</b>

{{</citation>}}


## cs.SE (2)



### (1/2 | 206/241) Towards Single-System Illusion in Software-Defined Vehicles -- Automated, AI-Powered Workflow (Krzysztof Lebioda et al., 2024)

{{<citation>}}

Krzysztof Lebioda, Viktor Vorobev, Nenad Petrovic, Fengjunjie Pan, Vahid Zolfaghari, Alois Knoll. (2024)  
**Towards Single-System Illusion in Software-Defined Vehicles -- Automated, AI-Powered Workflow**
<br/>
<button class="copy-to-clipboard" title="Towards Single-System Illusion in Software-Defined Vehicles -- Automated, AI-Powered Workflow" index=206>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: D-2-1; D-2-2; D-2-4; I-2-7; I-2-2; I-7-0, cs-AI, cs-CL, cs-SE, cs.SE  
Keyword Score: 30  
Keywords: Generative AI, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14460v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14460v1.pdf" filename="2403.14460v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We propose a novel model- and feature-based approach to development of vehicle software systems, where the end architecture is not explicitly defined. Instead, it emerges from an iterative process of search and optimization given certain constraints, requirements and hardware architecture, while retaining the property of single-system illusion, where applications run in a logically uniform environment. One of the key points of the presented approach is the inclusion of modern <b>generative</b> <b>AI,</b> specifically <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> in the loop. With the recent advances in the field, we expect that the <b>LLMs</b> will be able to assist in processing of requirements, generation of formal system models, as well as generation of software deployment specification and test code. The resulting pipeline is automated to a <b>large</b> <b>extent,</b> <b>with</b> feedback being generated at each step.

{{</citation>}}


### (2/2 | 207/241) Multi-role Consensus through LLMs Discussions for Vulnerability Detection (Zhenyu Mao et al., 2024)

{{<citation>}}

Zhenyu Mao, Jialong Li, Munan Li, Kenji Tei. (2024)  
**Multi-role Consensus through LLMs Discussions for Vulnerability Detection**
<br/>
<button class="copy-to-clipboard" title="Multi-role Consensus through LLMs Discussions for Vulnerability Detection" index=207>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-AI, cs-SE, cs.SE  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14274v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14274v1.pdf" filename="2403.14274v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have highlighted the potential for vulnerability detection, a crucial component of software quality assurance. Despite this progress, most studies have been limited to the perspective of a single role, usually testers, lacking diverse viewpoints from different roles in a typical software development life-cycle, including both developers and testers. To this end, this paper introduces an approach to employ <b>LLMs</b> to act as different roles to simulate real-life code review process, engaging in discussions towards a consensus on the existence and classification of vulnerabilities in the code. Preliminary evaluation of the proposed approach indicates a 4.73% increase in the precision rate, 58.9% increase in the recall rate, and a 28.1% increase in the F1 score.

{{</citation>}}


## eess.SY (12)



### (1/12 | 208/241) Event-triggered Boundary Control of Mixed-autonomy Traffic (Yihuai Zhang et al., 2024)

{{<citation>}}

Yihuai Zhang, Huan Yu. (2024)  
**Event-triggered Boundary Control of Mixed-autonomy Traffic**
<br/>
<button class="copy-to-clipboard" title="Event-triggered Boundary Control of Mixed-autonomy Traffic" index=208>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY, math-AP  
Keyword Score: 30  
Keywords: Continuous Time, Continuous Time, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14194v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14194v1.pdf" filename="2403.14194v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Control problems of mixed-autonomy traffic system consisting of both Human-driven Vehicles (HV) and Autonomous Vehicles (AV) have gained increasing attention. This paper is focused on suppressing traffic oscillations of the mixed-autonomy traffic system using boundary control design. The mixed traffic dynamics are described by a 4 x 4 hyperbolic partial differential equations (PDE) which governs propagation of four properties in traffic including density of HV, density of AV, friction between two classes of vehicles from driving interactions, and averaged velocity. We propose event-triggered boundary control design since control signal of traffic light on ramp or varying speed limit cannot be updated in a <b>continuous</b> <b>time</b> fashion. We apply event-triggered mechanism for a PDE backstepping controller and obtain dynamic triggering condition. Lyapunov analysis is conducted to prove the exponential stability of the closed loop system with the event-triggered controller. Numerical <b>simulation</b> demonstrates how car-following spacing of AV affects event-triggering mechanism of control input in mixed-autonomy traffic.

{{</citation>}}


### (2/12 | 209/241) PE-GPT: A Physics-Informed Interactive Large Language Model for Power Converter Modulation Design (Fanfan Lin et al., 2024)

{{<citation>}}

Fanfan Lin, Junhua Liu, Xinze Li, Shuai Zhao, Bohui Zhao, Hao Ma, Xin Zhang. (2024)  
**PE-GPT: A Physics-Informed Interactive Large Language Model for Power Converter Modulation Design**
<br/>
<button class="copy-to-clipboard" title="PE-GPT: A Physics-Informed Interactive Large Language Model for Power Converter Modulation Design" index=209>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 30  
Keywords: In-context Learning, In-context Learning, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14059v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14059v1.pdf" filename="2403.14059v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper proposes PE-GPT, a custom-tailored <b>large</b> <b>language</b> <b>model</b> uniquely adapted for power converter modulation design. By harnessing <b>in-context</b> <b>learning</b> and specialized tiered physics-informed neural networks, PE-GPT guides users through text-based dialogues, recommending actionable modulation parameters. The effectiveness of PE-GPT is validated through a practical design case involving dual active bridge converters, supported by hardware experimentation. This research underscores the transformative potential of <b>large</b> <b>language</b> <b>models</b> in power converter modulation design, offering enhanced accessibility, explainability, and efficiency, thereby setting a new paradigm in the field.

{{</citation>}}


### (3/12 | 210/241) A Benchmark for the Application of Distributed Control Techniques to the Electricity Network of the European Economic Area (A. Riccardi et al., 2024)

{{<citation>}}

A. Riccardi, L. Laurenti, B. De Schutter. (2024)  
**A Benchmark for the Application of Distributed Control Techniques to the Electricity Network of the European Economic Area**
<br/>
<button class="copy-to-clipboard" title="A Benchmark for the Application of Distributed Control Techniques to the Electricity Network of the European Economic Area" index=210>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 23  
Keywords: Benchmarking, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14372v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14372v1.pdf" filename="2403.14372v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The European Economic Area Electricity Network <b>Benchmark</b> (EEA-ENB) is a multi-area power system representing the European network of transmission systems for electricity to facilitate the application of distributed control techniques. In the EEA-ENB we consider the Load Frequency Control (LFC) problem in the presence of renewable energy sources (RESs), and energy storage systems (ESSs). RESs are known to cause instability in power networks due to their inertia-less and intermittent characteristics, while ESSs are introduced as a resource to mitigate the problem. In the EEA-ENB, particular attention is dedicated to Distributed Model Predictive Control (DMPC), whose application is often limited to small and homogeneous test cases due to the lack of standardized large-scale scenarios for testing, and due to the large computation time required to obtain a centralized MPC action for performance comparison with DMPC strategies under consideration. The second problem is exacerbated when the scale of the system grows. To address these challenges and to provide a real-world-based and control-independent <b>benchmark,</b> the EEA-ENB has been developed. The <b>benchmark</b> includes a centralized MPC strategy providing performance and computation time metrics to compare distributed control within a repeatable and realistic <b>simulation</b> environment.

{{</citation>}}


### (4/12 | 211/241) Learning Hierarchical Control Systems for Autonomous Systems with Energy Constraints (Charlott Vallon et al., 2024)

{{<citation>}}

Charlott Vallon, Mark Pustilnik, Alessandro Pinto, Francesco Borrelli. (2024)  
**Learning Hierarchical Control Systems for Autonomous Systems with Energy Constraints**
<br/>
<button class="copy-to-clipboard" title="Learning Hierarchical Control Systems for Autonomous Systems with Energy Constraints" index=211>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14536v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14536v1.pdf" filename="2403.14536v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper focuses on the design of hierarchical control architectures for autonomous systems with energy constraints. We focus on systems where energy storage limitations and slow recharge rates drastically affect the way the autonomous systems are operated. Using examples from space robotics and public transportation, we motivate the need for formally designed learning hierarchical control systems. We propose a learning control architecture which incorporates learning mechanisms at various levels of the control hierarchy to improve performance and resource utilization. The proposed hierarchical control scheme relies on high-level energy-aware task planning and assignment, complemented by a low-level predictive control mechanism responsible for the autonomous execution of tasks, including motion control and energy management. <b>Simulation</b> examples show the benefits and the limitations of the proposed architecture when learning is used to obtain a more energy-efficient task allocation.

{{</citation>}}


### (5/12 | 212/241) Designing Robust Linear Output Feedback Controller based on CLF-CBF framework via Linear~Programming(LP-CLF-CBF) (Mahroo Bahreinian et al., 2024)

{{<citation>}}

Mahroo Bahreinian, Mehdi Kermanshah, Roberto Tron. (2024)  
**Designing Robust Linear Output Feedback Controller based on CLF-CBF framework via Linear~Programming(LP-CLF-CBF)**
<br/>
<button class="copy-to-clipboard" title="Designing Robust Linear Output Feedback Controller based on CLF-CBF framework via Linear~Programming(LP-CLF-CBF)" index=212>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14519v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14519v1.pdf" filename="2403.14519v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We consider the problem of designing output feedback controllers that use measurements from a set of landmarks to navigate through a cell-decomposable environment using duality, Control Lyapunov and Barrier Functions (CLF, CBF), and Linear Programming. We propose two objectives for navigating in an environment, one to traverse the environment by making loops and one by converging to a stabilization point while smoothing the transition between consecutive cells. We test our algorithms in a <b>simulation</b> environment, evaluating the robustness of the approach to practical conditions, such as bearing-only measurements, and measurements acquired with a camera with a limited field of view.

{{</citation>}}


### (6/12 | 213/241) Synthesizing Controller for Safe Navigation using Control Density Function (Joseph Moyalan et al., 2024)

{{<citation>}}

Joseph Moyalan, Sriram S. K. S Narayanan, Andrew Zheng, Umesh Vaidya. (2024)  
**Synthesizing Controller for Safe Navigation using Control Density Function**
<br/>
<button class="copy-to-clipboard" title="Synthesizing Controller for Safe Navigation using Control Density Function" index=213>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14464v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14464v1.pdf" filename="2403.14464v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We consider the problem of navigating a nonlinear dynamical system from some initial set to some target set while avoiding collision with an unsafe set. We extend the concept of density function to control density function (CDF) for solving navigation problems with safety constraints. The occupancy-based interpretation of the measure associated with the density function is instrumental in imposing the safety constraints. The navigation problem with safety constraints is formulated as a quadratic program (QP) using CDF. The existing approach using the control barrier function (CBF) also formulates the navigation problem with safety constraints as QP. One of the main advantages of the proposed QP using CDF compared to QP formulated using CBF is that both the convergence/stability and safety can be combined and imposed using the CDF. <b>Simulation</b> results involving the Duffing oscillator and safe navigation of Dubin car models are provided to verify the main findings of the paper.

{{</citation>}}


### (7/12 | 214/241) Lane level joint control of off-ramp and main line speed guidance on expressway in rainy weather (Boyao Peng et al., 2024)

{{<citation>}}

Boyao Peng, Lexing Zhang, Enkai Li. (2024)  
**Lane level joint control of off-ramp and main line speed guidance on expressway in rainy weather**
<br/>
<button class="copy-to-clipboard" title="Lane level joint control of off-ramp and main line speed guidance on expressway in rainy weather" index=214>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: 90-10, A-0, cs-SY, eess-SY, eess.SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14172v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14172v1.pdf" filename="2403.14172v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In the upstream of the exit ramp of the expressway, the speed limit difference leads to a significant deceleration of the vehicle in the area adjacent to the off-ramp. The friction coefficient of the road surface decreases under rainy weather, and the above deceleration process can easily lead to sideslip and rollover of the vehicle. Dynamic speed guidance is an effective way to improve the status quo. Currently, there is an emerging trend to utilize I2V technology and high-precision map technology for lane level speed guidance control. This paper presents an optimized joint control strategy for main line-off-ramp speed guidance, which can adjust the guidance speed in real time according to the rainfall intensity. At the same time, this paper designs a progressive deceleration strategy, which works together with the speed guidance control to ensure the safe deceleration of vehicles. The <b>simulation</b> results show that the proposed control strategy outperforms the fixed speed limit control in terms of improving the total traveled time (TTT), total traveled distance (TTD) and standard deviation of speed (SD). Sensitivity analysis shows that the proposed control strategy can improve performance with the increase of the compliance rate of drivers. The speed guidance control method established in this paper can improve the vehicle operation efficiency in the off-ramp area of the expressway and reduce the speed difference of each vehicle in rainy weather, which guarantee the safety of expressway driving in the rainy day.

{{</citation>}}


### (8/12 | 215/241) Optimizing queues with deadlines under infrequent monitoring (Faraz Farahvash et al., 2024)

{{<citation>}}

Faraz Farahvash, Ao Tang. (2024)  
**Optimizing queues with deadlines under infrequent monitoring**
<br/>
<button class="copy-to-clipboard" title="Optimizing queues with deadlines under infrequent monitoring" index=215>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-NI, cs-SY, eess-SY, eess.SY  
Keyword Score: 10  
Keywords: Discrete Time, Discrete Time  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14525v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14525v1.pdf" filename="2403.14525v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we aim to improve the percentage of packets meeting their deadline in <b>discrete-time</b> <b>M/M/1</b> queues with infrequent monitoring. More specifically, we look into policies that only monitor the system (and subsequently take actions) after a packet arrival. We model the system as an MDP and provide the optimal policy for some special cases. Furthermore, we introduce a heuristic algorithm called "AB-n" for general deadlines. Finally, we provide numerical results demonstrating the desirable performance of "AB-n" policies.

{{</citation>}}


### (9/12 | 216/241) Meta-learning of data-driven controllers with automatic model reference tuning: theory and experimental case study (Riccardo Busetto et al., 2024)

{{<citation>}}

Riccardo Busetto, Valentina Breschi, Federica Baracchi, Simone Formentin. (2024)  
**Meta-learning of data-driven controllers with automatic model reference tuning: theory and experimental case study**
<br/>
<button class="copy-to-clipboard" title="Meta-learning of data-driven controllers with automatic model reference tuning: theory and experimental case study" index=216>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 10  
Keywords: Meta Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14500v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14500v1.pdf" filename="2403.14500v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Data-driven control offers a viable option for control scenarios where constructing a system model is expensive or time-consuming. Nonetheless, many of these algorithms are not entirely automated, often necessitating the adjustment of multiple hyperparameters through cumbersome trial-and-error processes and demanding significant amounts of data. In this paper, we explore a <b>meta-learning</b> <b>approach</b> to leverage potentially existing prior knowledge about analogous (though not identical) systems, aiming to reduce both the experimental workload and ease the tuning of the available degrees of freedom. We validate this methodology through an experimental case study involving the tuning of proportional, integral (PI) controllers for brushless DC (BLDC) motors with variable loads and architectures.

{{</citation>}}


### (10/12 | 217/241) On the continuity and smoothness of the value function in reinforcement learning and optimal control (Hans Harder et al., 2024)

{{<citation>}}

Hans Harder, Sebastian Peitz. (2024)  
**On the continuity and smoothness of the value function in reinforcement learning and optimal control**
<br/>
<button class="copy-to-clipboard" title="On the continuity and smoothness of the value function in reinforcement learning and optimal control" index=217>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: 37H99, 37N35, 93E03, I-2-8, cs-AI, cs-SY, eess-SY, eess.SY  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14432v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14432v1.pdf" filename="2403.14432v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The value function plays a crucial role as a measure for the cumulative future reward an agent receives in both <b>reinforcement</b> <b>learning</b> and optimal control. It is therefore of interest to study how similar the values of neighboring states are, i.e., to investigate the continuity of the value function. We do so by providing and verifying upper bounds on the value function's modulus of continuity. Additionally, we show that the value function is always H\"older continuous under relatively weak assumptions on the underlying system and that non-differentiable value functions can be made differentiable by slightly "disturbing" the system.

{{</citation>}}


### (11/12 | 218/241) Exploiting Over-The-Air Consensus for Collision Avoidance and Formation Control in Multi-Agent Systems (Michael Epp et al., 2024)

{{<citation>}}

Michael Epp, Fabio Molinari, Joerg Raisch. (2024)  
**Exploiting Over-The-Air Consensus for Collision Avoidance and Formation Control in Multi-Agent Systems**
<br/>
<button class="copy-to-clipboard" title="Exploiting Over-The-Air Consensus for Collision Avoidance and Formation Control in Multi-Agent Systems" index=218>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14386v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14386v1.pdf" filename="2403.14386v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper introduces a distributed control method for multi-agent robotic systems employing Over the Air Consensus (OTA-Consensus). Designed for agents with decoupled single-integrator dynamics, this approach aims at efficient formation achievement and collision avoidance. As a distinctive feature, it leverages OTA's ability to exploit interference in wireless channels, a property traditionally considered a drawback, thus enhancing communication efficiency among robots. An analytical proof of asymptotic convergence is established for systems with time-varying communication topologies represented by sequences of strongly connected directed <b>graphs.</b> Comparative evaluations demonstrate significant efficiency improvements over current state-of-the-art methods, especially in scenarios with a large number of agents.

{{</citation>}}


### (12/12 | 219/241) Transformation-Free Fixed-Structure Model Reduction for LPV Systems (Lennart Heeren et al., 2024)

{{<citation>}}

Lennart Heeren, Adwait Datar, Antonio Mendez Gonzalez, Herbert Werner. (2024)  
**Transformation-Free Fixed-Structure Model Reduction for LPV Systems**
<br/>
<button class="copy-to-clipboard" title="Transformation-Free Fixed-Structure Model Reduction for LPV Systems" index=219>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14310v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14310v1.pdf" filename="2403.14310v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we propose a model reduction technique for linear parameter varying (LPV) systems based on available tools for fixed-structure controller synthesis. We start by transforming a model reduction problem into an equivalent controller synthesis problem by defining an appropriate generalized plant. The controller synthesis problem is then solved by using gradient-based tools available in the literature. Owing to the flexibility of the gradient-based synthesis tools, we are able to impose a desired structure on the obtained reduced model. Additionally, we obtain a bound on the approximation error as a direct output of the optimization problem. The proposed methods are applied on a <b>benchmark</b> mechanical system of interconnected masses, springs and dampers. To evaluate the effect of the proposed model-reduction approach on controller design, LPV controllers designed using the reduced models (with and without an imposed structure) are compared in closed-loop with the original model.

{{</citation>}}


## q-bio.BM (1)



### (1/1 | 220/241) Protein Conformation Generation via Force-Guided SE(3) Diffusion Models (Yan Wang et al., 2024)

{{<citation>}}

Yan Wang, Lihao Wang, Yuning Shen, Yiqun Wang, Huizhuo Yuan, Yue Wu, Quanquan Gu. (2024)  
**Protein Conformation Generation via Force-Guided SE(3) Diffusion Models**
<br/>
<button class="copy-to-clipboard" title="Protein Conformation Generation via Force-Guided SE(3) Diffusion Models" index=220>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: q-bio.BM  
Categories: cs-LG, q-bio-BM, q-bio.BM  
Keyword Score: 30  
Keywords: Diffusion Model, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14088v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14088v1.pdf" filename="2403.14088v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The conformational landscape of proteins is crucial to understanding their functionality in complex biological processes. Traditional physics-based computational methods, such as molecular dynamics (MD) <b>simulations,</b> suffer from rare event sampling and long equilibration time problems, hindering their applications in general protein systems. Recently, deep generative modeling techniques, especially <b>diffusion</b> <b>models,</b> have been employed to generate novel protein conformations. However, existing score-based <b>diffusion</b> <b>methods</b> cannot properly incorporate important physical prior knowledge to guide the generation process, causing large deviations in the sampled protein conformations from the equilibrium distribution. In this paper, to overcome these limitations, we propose a force-guided SE(3) <b>diffusion</b> <b>model,</b> ConfDiff, for protein conformation generation. By incorporating a force-guided network with a mixture of data-based score models, ConfDiff can can generate protein conformations with rich diversity while preserving high fidelity. Experiments on a variety of protein conformation prediction tasks, including 12 fast-folding proteins and the Bovine Pancreatic Trypsin Inhibitor (BPTI), demonstrate that our method surpasses the state-of-the-art method.

{{</citation>}}


## hep-ex (1)



### (1/1 | 221/241) Improving $Λ$ Signal Extraction with Domain Adaptation via Normalizing Flows (Rowan Kelleher et al., 2024)

{{<citation>}}

Rowan Kelleher, Matthew McEneaney, Anselm Vossen. (2024)  
**Improving $Λ$ Signal Extraction with Domain Adaptation via Normalizing Flows**
<br/>
<button class="copy-to-clipboard" title="Improving $Λ$ Signal Extraction with Domain Adaptation via Normalizing Flows" index=221>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: hep-ex  
Categories: cs-LG, hep-ex, hep-ex  
Keyword Score: 30  
Keywords: Simulation, Simulator, Domain Adaptation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14076v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14076v1.pdf" filename="2403.14076v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The present study presents a novel application for normalizing flows for <b>domain</b> <b>adaptation.</b> The study investigates the ability of flow based neural networks to improve signal extraction of $\Lambda$ Hyperons at CLAS12. Normalizing Flows can help model complex probability density functions that describe physics processes, enabling uses such as event generation. $\Lambda$ signal extraction has been improved through the use of classifier networks, but differences in <b>simulation</b> and data <b>domains</b> <b>limit</b> classifier performance; this study utilizes the flows for <b>domain</b> <b>adaptation</b> between Monte Carlo <b>simulation</b> and data. We were successful in training a flow network to transform between the latent physics space and a normal distribution. We also found that applying the flows lessened the dependence of the figure of merit on the cut on the classifier output, meaning that there was a broader range where the cut results in a similar figure of merit.

{{</citation>}}


## stat.ML (4)



### (1/4 | 222/241) Automatic Outlier Rectification via Optimal Transport (Jose Blanchet et al., 2024)

{{<citation>}}

Jose Blanchet, Jiajin Li, Markus Pelger, Greg Zanotti. (2024)  
**Automatic Outlier Rectification via Optimal Transport**
<br/>
<button class="copy-to-clipboard" title="Automatic Outlier Rectification via Optimal Transport" index=222>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: cs-LG, math-OC, stat-ME, stat-ML, stat.ML  
Keyword Score: 30  
Keywords: Outlier Detection, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14067v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14067v1.pdf" filename="2403.14067v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we propose a novel conceptual framework to detect <b>outliers</b> <b>using</b> optimal transport with a concave cost function. Conventional <b>outlier</b> <b>detection</b> approaches typically use a two-stage procedure: first, <b>outliers</b> <b>are</b> detected and removed, and then estimation is performed on the cleaned data. However, this approach does not inform <b>outlier</b> <b>removal</b> with the estimation task, leaving room for improvement. To address this limitation, we propose an automatic <b>outlier</b> <b>rectification</b> mechanism that integrates rectification and estimation within a joint optimization framework. We take the first step to utilize an optimal transport distance with a concave cost function to construct a rectification set in the space of probability distributions. Then, we select the best distribution within the rectification set to perform the estimation task. Notably, the concave cost function we introduced in this paper is the key to making our estimator effectively identify the <b>outlier</b> <b>during</b> the optimization process. We discuss the fundamental differences between our estimator and optimal transport-based distributionally robust optimization estimator. finally, we demonstrate the effectiveness and superiority of our approach over conventional approaches in extensive <b>simulation</b> and empirical analyses for mean estimation, least absolute regression, and the fitting of option implied volatility surfaces.

{{</citation>}}


### (2/4 | 223/241) Estimating Causal Effects with Double Machine Learning -- A Method Evaluation (Jonathan Fuhr et al., 2024)

{{<citation>}}

Jonathan Fuhr, Philipp Berens, Dominik Papies. (2024)  
**Estimating Causal Effects with Double Machine Learning -- A Method Evaluation**
<br/>
<button class="copy-to-clipboard" title="Estimating Causal Effects with Double Machine Learning -- A Method Evaluation" index=223>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: cs-LG, econ-EM, stat-ME, stat-ML, stat.ML  
Keyword Score: 10  
Keywords: Recommendation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14385v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14385v1.pdf" filename="2403.14385v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The estimation of causal effects with observational data continues to be a very active research area. In recent years, researchers have developed new frameworks which use machine learning to relax classical assumptions necessary for the estimation of causal effects. In this paper, we review one of the most prominent methods - "double/debiased machine learning" (DML) - and empirically evaluate it by comparing its performance on simulated data relative to more traditional statistical methods, before applying it to real-world data. Our findings indicate that the application of a suitably flexible machine learning algorithm within DML improves the adjustment for various nonlinear confounding relationships. This advantage enables a departure from traditional functional form assumptions typically necessary in causal effect estimation. However, we demonstrate that the method continues to critically depend on standard assumptions about causal structure and identification. When estimating the effects of air pollution on housing prices in our application, we find that DML estimates are consistently larger than estimates of less flexible methods. From our overall results, we provide actionable <b>recommendations</b> for specific choices researchers must make when applying DML in practice.

{{</citation>}}


### (3/4 | 224/241) Learning causal graphs using variable grouping according to ancestral relationship (Ming Cai et al., 2024)

{{<citation>}}

Ming Cai, Hisayuki Hara. (2024)  
**Learning causal graphs using variable grouping according to ancestral relationship**
<br/>
<button class="copy-to-clipboard" title="Learning causal graphs using variable grouping according to ancestral relationship" index=224>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: cs-LG, stat-ML, stat.ML  
Keyword Score: 6  
Keywords: Graph, Sample Size  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14125v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14125v1.pdf" filename="2403.14125v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Several causal discovery algorithms have been proposed. However, when the <b>sample</b> <b>size</b> is small relative to the number of variables, the accuracy of estimating causal <b>graphs</b> using existing methods decreases. And some methods are not feasible when the <b>sample</b> <b>size</b> is smaller than the number of variables. To circumvent these problems, some researchers proposed causal structure learning algorithms using divide-and-conquer approaches. For learning the entire causal <b>graph,</b> the approaches first split variables into several subsets according to the conditional independence relationships among the variables, then apply a conventional causal discovery algorithm to each subset and merge the estimated results. Since the divide-and-conquer approach reduces the number of variables to which a causal structure learning algorithm is applied, it is expected to improve the estimation accuracy of causal <b>graphs,</b> especially when the <b>sample</b> <b>size</b> is small relative to the number of variables and the model is sparse. However, existing methods are either computationally expensive or do not provide sufficient accuracy when the <b>sample</b> <b>size</b> is small. This paper proposes a new algorithm for grouping variables based the ancestral relationships among the variables, under the LiNGAM assumption, where the causal relationships are linear, and the mutually independent noise are distributed as continuous non-Gaussian distributions. We call the proposed algorithm CAG. The time complexity of the ancestor finding in CAG is shown to be cubic to the number of variables. Extensive computer experiments confirm that the proposed method outperforms the original DirectLiNGAM without grouping variables and other divide-and-conquer approaches not only in estimation accuracy but also in computation time when the <b>sample</b> <b>size</b> is small relative to the number of variables and the model is sparse.

{{</citation>}}


### (4/4 | 225/241) Recovering Latent Confounders from High-dimensional Proxy Variables (Nathan Mankovich et al., 2024)

{{<citation>}}

Nathan Mankovich, Homer Durand, Emiliano Diaz, Gherardo Varando, Gustau Camps-Valls. (2024)  
**Recovering Latent Confounders from High-dimensional Proxy Variables**
<br/>
<button class="copy-to-clipboard" title="Recovering Latent Confounders from High-dimensional Proxy Variables" index=225>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: cs-LG, stat-ML, stat.ML  
Keyword Score: 3  
Keywords: Sample Size  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14228v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14228v1.pdf" filename="2403.14228v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Detecting latent confounders from proxy variables is an essential problem in causal effect estimation. Previous approaches are limited to low-dimensional proxies, sorted proxies, and binary treatments. We remove these assumptions and present a novel Proxy Confounder Factorization (PCF) framework for continuous treatment effect estimation when latent confounders manifest through high-dimensional, mixed proxy variables. For specific <b>sample</b> <b>sizes,</b> our two-step PCF implementation, using Independent Component Analysis (ICA-PCF), and the end-to-end implementation, using Gradient Descent (GD-PCF), achieve high correlation with the latent confounder and low absolute error in causal effect estimation with synthetic datasets in the high <b>sample</b> <b>size</b> regime. Even when faced with climate data, ICA-PCF recovers four components that explain $75.9\%$ of the variance in the North Atlantic Oscillation, a known confounder of precipitation patterns in Europe. Code for our PCF implementations and experiments can be found here: https://github.com/IPL-UV/confound_it. The proposed methodology constitutes a stepping stone towards discovering latent confounders and can be applied to many problems in disciplines dealing with high-dimensional observed proxies, e.g., spatiotemporal fields.

{{</citation>}}


## quant-ph (3)



### (1/3 | 226/241) Polynomial-Time Classical Simulation of Noisy IQP Circuits with Constant Depth (Joel Rajakumar et al., 2024)

{{<citation>}}

Joel Rajakumar, James D. Watson, Yi-Kai Liu. (2024)  
**Polynomial-Time Classical Simulation of Noisy IQP Circuits with Constant Depth**
<br/>
<button class="copy-to-clipboard" title="Polynomial-Time Classical Simulation of Noisy IQP Circuits with Constant Depth" index=226>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: quant-ph  
Categories: cs-CC, quant-ph, quant-ph  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14607v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14607v1.pdf" filename="2403.14607v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Sampling from the output distributions of quantum computations comprising only commuting gates, known as instantaneous quantum polynomial (IQP) computations, is believed to be intractable for classical computers, and hence this task has become a leading candidate for testing the capabilities of quantum devices. Here we demonstrate that for an arbitrary IQP circuit undergoing dephasing or depolarizing noise, whose depth is greater than a critical $O(1)$ threshold, the output distribution can be efficiently sampled by a classical computer. Unlike other <b>simulation</b> algorithms for quantum supremacy tasks, we do not require assumptions on the circuit's architecture, on anti-concentration properties, nor do we require $\Omega(\log(n))$ circuit depth. We take advantage of the fact that IQP circuits have deep sections of diagonal gates, which allows the noise to build up predictably and induce a large-scale breakdown of entanglement within the circuit. Our results suggest that quantum supremacy experiments based on IQP circuits may be more susceptible to classical <b>simulation</b> than previously thought.

{{</citation>}}


### (2/3 | 227/241) Quantum Channel Simulation under Purified Distance is no more difficult than State Splitting (Michael X. Cao et al., 2024)

{{<citation>}}

Michael X. Cao, Rahul Jain, Marco Tomamichel. (2024)  
**Quantum Channel Simulation under Purified Distance is no more difficult than State Splitting**
<br/>
<button class="copy-to-clipboard" title="Quantum Channel Simulation under Purified Distance is no more difficult than State Splitting" index=227>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: quant-ph  
Categories: cs-IT, math-IT, quant-ph, quant-ph  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14416v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14416v1.pdf" filename="2403.14416v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Characterizing the minimal communication needed for the quantum channel <b>simulation</b> is a fundamental task in the quantum information theory. In this paper, we show that, under the purified distance, the quantum channel <b>simulation</b> can be directly achieved via quantum state splitting without using a technique known as the de Finetti reduction, and thus provide a pair of tighter one-shot bounds. Using the bounds, we also recover the quantum reverse Shannon theorem in a much simpler way.

{{</citation>}}


### (3/3 | 228/241) Optimal Second-Order Rates for Quantum Information Decoupling (Yu-Chen Shen et al., 2024)

{{<citation>}}

Yu-Chen Shen, Li Gao, Hao-Chung Cheng. (2024)  
**Optimal Second-Order Rates for Quantum Information Decoupling**
<br/>
<button class="copy-to-clipboard" title="Optimal Second-Order Rates for Quantum Information Decoupling" index=228>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: quant-ph  
Categories: cs-IT, math-IT, math-MP, math-ph, quant-ph, quant-ph  
Keyword Score: 10  
Keywords: Knowledge Distillation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14338v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14338v1.pdf" filename="2403.14338v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we consider the standard quantum information decoupling, in which Alice aims to decouple her system from the environment by local operations and discarding some of her systems. To achieve an $\varepsilon$-decoupling with trace distance as the error criterion, we establish a near-optimal one-shot characterization for the largest dimension of the remainder system in terms of the conditional $(1-\varepsilon)$-hypothesis-testing entropy. When the underlying system is independent and identically prepared, our result leads to the matched second-order rate as well as the matched moderate deviation rate. As an application, we find an achievability bound in entanglement <b>distillation</b> protocol, where the objective is for Alice and Bob to transform their quantum state to maximally entangled state with largest possible dimension using only local operations and one-way classical communications.

{{</citation>}}


## eess.AS (2)



### (1/2 | 229/241) Speech-Aware Neural Diarization with Encoder-Decoder Attractor Guided by Attention Constraints (PeiYing Lee et al., 2024)

{{<citation>}}

PeiYing Lee, HauYun Guo, Berlin Chen. (2024)  
**Speech-Aware Neural Diarization with Encoder-Decoder Attractor Guided by Attention Constraints**
<br/>
<button class="copy-to-clipboard" title="Speech-Aware Neural Diarization with Encoder-Decoder Attractor Guided by Attention Constraints" index=229>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.AS  
Categories: cs-SD, eess-AS, eess.AS  
Keyword Score: 20  
Keywords: Transformer, Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14268v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14268v1.pdf" filename="2403.14268v1.pdf">Download PDF</button>

---


**ABSTRACT**  
End-to-End Neural Diarization with Encoder-Decoder based Attractor (EEND-EDA) is an end-to-end neural model for automatic speaker segmentation and labeling. It achieves the capability to handle flexible number of speakers by estimating the number of attractors. EEND-EDA, however, struggles to accurately capture local speaker dynamics. This work proposes an auxiliary loss that aims to guide the <b>Transformer</b> encoders at the lower layer of EEND-EDA model to enhance the effect of <b>self-attention</b> modules using speaker activity information. The results evaluated on public dataset Mini LibriSpeech, demonstrates the effectiveness of the work, reducing Diarization Error Rate from 30.95% to 28.17%. We will release the source code on GitHub to allow further research and reproducibility.

{{</citation>}}


### (2/2 | 230/241) AdaProj: Adaptively Scaled Angular Margin Subspace Projections for Anomalous Sound Detection with Auxiliary Classification Tasks (Kevin Wilkinghoff, 2024)

{{<citation>}}

Kevin Wilkinghoff. (2024)  
**AdaProj: Adaptively Scaled Angular Margin Subspace Projections for Anomalous Sound Detection with Auxiliary Classification Tasks**
<br/>
<button class="copy-to-clipboard" title="AdaProj: Adaptively Scaled Angular Margin Subspace Projections for Anomalous Sound Detection with Auxiliary Classification Tasks" index=230>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.AS  
Categories: cs-SD, eess-AS, eess.AS  
Keyword Score: 20  
Keywords: Self-supervised Learning, Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14179v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14179v1.pdf" filename="2403.14179v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The state-of-the-art approach for semi-supervised anomalous sound detection is to first learn an embedding space by using auxiliary classification tasks based on meta information or <b>self-supervised</b> <b>learning</b> and then estimate the distribution of normal data. In this work, AdaProj a novel loss function is presented. In contrast to commonly used angular margin losses, which project data of each class as close as possible to their corresponding class centers, AdaProj learns to project data onto class-specific subspaces. By doing so, the resulting distributions of embeddings belonging to normal data are not required to be as restrictive as other loss functions allowing a more detailed view on the data. In experiments conducted on the DCASE2022 and DCASE2023 datasets, it is shown that using AdaProj to learn an embedding space significantly outperforms other commonly used loss functions and results in a state-of-the-art performance on the DCASE2023 dataset.

{{</citation>}}


## math.OC (1)



### (1/1 | 231/241) Reinforcement Learning Design for Quickest Change Detection (Austin Cooper et al., 2024)

{{<citation>}}

Austin Cooper, Sean Meyn. (2024)  
**Reinforcement Learning Design for Quickest Change Detection**
<br/>
<button class="copy-to-clipboard" title="Reinforcement Learning Design for Quickest Change Detection" index=231>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.OC  
Categories: cs-IT, math-IT, math-OC, math.OC  
Keyword Score: 20  
Keywords: Reinforcement Learning, Stochastic Gradient Descent  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14109v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14109v1.pdf" filename="2403.14109v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The field of quickest change detection (QCD) concerns design and analysis of algorithms to estimate in real time the time at which an important event takes place, and identify properties of the post-change behavior. It is shown in this paper that approaches based on <b>reinforcement</b> <b>learning</b> (RL) can be adapted based on any "surrogate information state" that is adapted to the observations. Hence we are left to choose both the surrogate information state process and the algorithm. For the former, it is argued that there are many choices available, based on a rich theory of asymptotic statistics for QCD. Two approaches to RL design are considered: (i) <b>Stochastic</b> <b>gradient</b> <b>descent</b> based on an actor-critic formulation. Theory is largely complete for this approach: the algorithm is unbiased, and will converge to a local minimum. However, it is shown that variance of <b>stochastic</b> <b>gradients</b> <b>can</b> be very large, necessitating the need for commensurately long run times; (ii) Q-learning algorithms based on a version of the projected Bellman equation. It is shown that the algorithm is stable, in the sense of bounded sample paths, and that a solution to the projected Bellman equation exists under mild conditions. Numerical experiments illustrate these findings, and provide a roadmap for algorithm design in more general settings.

{{</citation>}}


## cs.SI (4)



### (1/4 | 232/241) Random Graph Modeling: A survey of the concepts (Mikhail Drobyshevskiy et al., 2024)

{{<citation>}}

Mikhail Drobyshevskiy, Denis Turdakov. (2024)  
**Random Graph Modeling: A survey of the concepts**
<br/>
<button class="copy-to-clipboard" title="Random Graph Modeling: A survey of the concepts" index=232>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SI  
Categories: cs-SI, cs.SI  
Keyword Score: 11  
Keywords: Graph, Benchmarking, Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14415v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14415v1.pdf" filename="2403.14415v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Random <b>graph</b> (RG) models play a central role in the complex networks analysis. They help to understand, control, and predict phenomena occurring, for instance, in social networks, biological networks, the Internet, etc. Despite a large number of RG models presented in the literature, there are few concepts underlying them. Instead of trying to classify a wide variety of very dispersed models, we capture and describe concepts they exploit considering preferential attachment, copying principle, hyperbolic <b>geometry,</b> recursively defined structure, edge switching, Monte Carlo sampling, etc. We analyze RG models, extract their basic principles, and build a taxonomy of concepts they are based on. We also discuss how these concepts are combined in RG models and how they work in typical applications like <b>benchmarks,</b> null models, and data anonymization.

{{</citation>}}


### (2/4 | 233/241) From Perils to Possibilities: Understanding how Human (and AI) Biases affect Online Fora (Virginia Morini et al., 2024)

{{<citation>}}

Virginia Morini, Valentina Pansanella, Katherine Abramski, Erica Cau, Andrea Failla, Salvatore Citraro, Giulio Rossetti. (2024)  
**From Perils to Possibilities: Understanding how Human (and AI) Biases affect Online Fora**
<br/>
<button class="copy-to-clipboard" title="From Perils to Possibilities: Understanding how Human (and AI) Biases affect Online Fora" index=233>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SI  
Categories: cs-AI, cs-HC, cs-SI, cs.SI  
Keyword Score: 10  
Keywords: Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14298v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14298v1.pdf" filename="2403.14298v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Social media platforms are online fora where users engage in discussions, share content, and build connections. This review explores the dynamics of social interactions, user-generated contents, and biases within the context of social media analysis (analyzing works that use the tools offered by complex network analysis and natural language processing) through the lens of three key points of view: online debates, online support, and human-AI interactions. On the one hand, we delineate the phenomenon of online debates, where polarization, misinformation, and echo chamber formation often proliferate, driven by algorithmic biases and extreme mechanisms of homophily. On the other hand, we explore the emergence of online support groups through users' self-disclosure and social support mechanisms. Online debates and support mechanisms present a duality of both perils and possibilities within social media; perils of segregated communities and polarized debates, and possibilities of empathy narratives and self-help groups. This dichotomy also extends to a third perspective: users' reliance on AI-generated content, such as the ones produced by <b>Large</b> <b>Language</b> <b>Models,</b> which can manifest both human biases hidden in training sets and non-human biases that emerge from their artificial neural architectures. Analyzing interdisciplinary approaches, we aim to deepen the understanding of the complex interplay between social interactions, user-generated content, and biases within the realm of social media ecosystems.

{{</citation>}}


### (3/4 | 234/241) Dynamical importance and network perturbations (Ethan Young et al., 2024)

{{<citation>}}

Ethan Young, Mason A. Porter. (2024)  
**Dynamical importance and network perturbations**
<br/>
<button class="copy-to-clipboard" title="Dynamical importance and network perturbations" index=234>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SI  
Categories: cs-SI, cs.SI, physics-soc-ph  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14584v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14584v1.pdf" filename="2403.14584v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The leading eigenvalue $\lambda$ of the adjacency matrix of a <b>graph</b> exerts much influence on the behavior of dynamical processes on that <b>graph.</b> It is thus relevant to relate notions of the importance (specifically, centrality measures) of network structures to $\lambda$ and its associated eigenvector. We study a previously derived measure of edge importance known as "dynamical importance", which estimates how much $\lambda$ changes when one removes an edge from a <b>graph</b> or adds an edge to it. We examine the accuracy of this estimate for different network structures and compare it to the true change in $\lambda$ after an edge removal or edge addition. We then derive a first-order approximation of the change in the leading eigenvector. We also consider the effects of edge additions on Kuramoto dynamics on networks, and we express the Kuramoto order parameter in terms of dynamical importance. Through our analysis and computational experiments, we find that studying dynamical importance can improve understanding of the relationship between network perturbations and dynamical processes on networks.

{{</citation>}}


### (4/4 | 235/241) Collecting Influencers: A Comparative Study of Online Network Crawlers (Mikhail Drobyshevskiy et al., 2024)

{{<citation>}}

Mikhail Drobyshevskiy, Denis Aivazov, Denis Turdakov, Alexander Yatskov, Maksim Varlamov, Danil Shayhelislamov. (2024)  
**Collecting Influencers: A Comparative Study of Online Network Crawlers**
<br/>
<button class="copy-to-clipboard" title="Collecting Influencers: A Comparative Study of Online Network Crawlers" index=235>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SI  
Categories: cs-SI, cs.SI  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14351v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14351v1.pdf" filename="2403.14351v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Online network crawling tasks require a lot of efforts for the researchers to collect the data. One of them is identification of important nodes, which has many applications starting from viral marketing to the prevention of disease spread. Various crawling algorithms has been suggested but their efficiency is not studied well. In this paper we compared six known crawlers on the task of collecting the fraction of the most influential nodes of <b>graph.</b> We analyzed crawlers behavior for four measures of node influence: node degree, k-coreness, betweenness centrality, and eccentricity. The experiments confirmed that greedy methods perform the best in many settings, but the cases exist when they are very inefficient.

{{</citation>}}


## cs.NI (1)



### (1/1 | 236/241) A Mathematical Introduction to Deep Reinforcement Learning for 5G/6G Applications (Farhad Rezazadeh, 2024)

{{<citation>}}

Farhad Rezazadeh. (2024)  
**A Mathematical Introduction to Deep Reinforcement Learning for 5G/6G Applications**
<br/>
<button class="copy-to-clipboard" title="A Mathematical Introduction to Deep Reinforcement Learning for 5G/6G Applications" index=236>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NI  
Categories: cs-NI, cs.NI  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14516v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14516v1.pdf" filename="2403.14516v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Algorithmic innovation can unleash the potential of the beyond 5G (B5G)/6G communication systems. Artificial intelligence (AI)-driven zero-touch network slicing is envisaged as a promising cutting-edge technology to harness the full potential of heterogeneous 6G networks and enable the automation of demand-aware management and orchestration (MANO). The network slicing continues towards numerous slices with micro or macro services in 6G networks, and thereby, designing a robust, stable, and distributed learning mechanism is considered a necessity. In this regard, robust brain-inspired and dopamine-like learning methods, such as Actor-Critic approaches, can play a vital role. The tutorial begins with an introduction to network slicing, <b>reinforcement</b> <b>learning</b> (RL), and recent state-of-the-art (SoA) algorithms. Then, the paper elaborates on the combination of value-based and policy-based methods in the form of Actor-Critic techniques tailored to the needs of future wireless networks.

{{</citation>}}


## cond-mat.dis-nn (1)



### (1/1 | 237/241) Quantum-activated neural reservoirs on-chip open up large hardware security models for resilient authentication (Zhao He et al., 2024)

{{<citation>}}

Zhao He, Maxim S. Elizarov, Ning Li, Fei Xiang, Andrea Fratalocchi. (2024)  
**Quantum-activated neural reservoirs on-chip open up large hardware security models for resilient authentication**
<br/>
<button class="copy-to-clipboard" title="Quantum-activated neural reservoirs on-chip open up large hardware security models for resilient authentication" index=237>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cond-mat.dis-nn  
Categories: cond-mat-dis-nn, cond-mat.dis-nn, cs-AI, cs-CR  
Keyword Score: 10  
Keywords: Recurrent Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14188v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14188v1.pdf" filename="2403.14188v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Quantum artificial intelligence is a frontier of artificial intelligence research, pioneering quantum AI-powered circuits to address problems beyond the reach of deep learning with classical architectures. This work implements a large-scale quantum-activated <b>recurrent</b> <b>neural</b> <b>network</b> possessing more than 3 trillion hardware nodes/cm$^2$, originating from repeatable atomic-scale nucleation dynamics in an amorphous material integrated on-chip, controlled with 0.07 nW electric power per readout channel. Compared to the best-performing reservoirs currently reported, this implementation increases the scale of the network by two orders of magnitude and reduces the power consumption by six, reaching power efficiencies in the range of the human brain, dissipating 0.2 nW/neuron. When interrogated by a classical input, the chip implements a large-scale hardware security model, enabling dictionary-free authentication secure against statistical inference attacks, including AI's present and future development, even for an adversary with a copy of all the classical components available. Experimental tests report 99.6% reliability, 100% user authentication accuracy, and an ideal 50% key uniqueness. Due to its quantum nature, the chip supports a bit density per feature size area three times higher than the best technology available, with the capacity to store more than $2^{1104}$ keys in a footprint of 1 cm$^2$. Such a quantum-powered platform could help counteract the emerging form of warfare led by the cybercrime industry in breaching authentication to target small to large-scale facilities, from private users to intelligent energy grids.

{{</citation>}}


## cs.DS (2)



### (1/2 | 238/241) A Differentially Private Clustering Algorithm for Well-Clustered Graphs (Weiqiang He et al., 2024)

{{<citation>}}

Weiqiang He, Hendrik Fichtenberger, Pan Peng. (2024)  
**A Differentially Private Clustering Algorithm for Well-Clustered Graphs**
<br/>
<button class="copy-to-clipboard" title="A Differentially Private Clustering Algorithm for Well-Clustered Graphs" index=238>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DS  
Categories: cs-CR, cs-DS, cs-LG, cs.DS  
Keyword Score: 9  
Keywords: Graph, Benchmarking, Clustering  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14332v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14332v1.pdf" filename="2403.14332v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We study differentially private (DP) algorithms for recovering clusters in well-clustered <b>graphs,</b> which are <b>graphs</b> whose vertex set can be partitioned into a small number of sets, each inducing a subgraph of high inner conductance and small outer conductance. Such <b>graphs</b> have widespread application as a <b>benchmark</b> in the theoretical analysis of spectral <b>clustering.</b> We provide an efficient ($\epsilon$,$\delta$)-DP algorithm tailored specifically for such <b>graphs.</b> Our algorithm draws inspiration from the recent work of Chen et al., who developed DP algorithms for recovery of stochastic block models in cases where the <b>graph</b> comprises exactly two nearly-balanced clusters. Our algorithm works for well-clustered <b>graphs</b> with $k$ nearly-balanced clusters, and the misclassification ratio almost matches the one of the best-known non-private algorithms. We conduct experimental evaluations on datasets with known ground truth clusters to substantiate the prowess of our algorithm. We also show that any (pure) $\epsilon$-DP algorithm would result in substantial error.

{{</citation>}}


### (2/2 | 239/241) Induced Subforests and Superforests (Dieter Rautenbach et al., 2024)

{{<citation>}}

Dieter Rautenbach, Florian Werner. (2024)  
**Induced Subforests and Superforests**
<br/>
<button class="copy-to-clipboard" title="Induced Subforests and Superforests" index=239>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DS  
Categories: cs-DS, cs.DS, math-CO  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14492v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14492v1.pdf" filename="2403.14492v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Graph</b> isomorphism, subgraph isomorphism, and maximum common subgraphs are classical well-investigated objects. Their (parameterized) complexity and efficiently tractable cases have been studied. In the present paper, for a given set of forests, we study maximum common induced subforests and minimum common induced superforests. We show that finding a maximum subforest is NP-hard already for two subdivided stars while finding a minimum superforest is tractable for two trees but NP-hard for three trees. For a given set of $k$ trees, we present an efficient greedy $\left(\frac{k}{2}-\frac{1}{2}+\frac{1}{k}\right)$-approximation algorithm for the minimum superforest problem. Finally, we present a polynomial time approximation scheme for the maximum subforest problem for any given set of forests.

{{</citation>}}


## cs.DB (1)



### (1/1 | 240/241) Quantifying Semantic Query Similarity for Automated Linear SQL Grading: A Graph-based Approach (Leo Köberlein et al., 2024)

{{<citation>}}

Leo Köberlein, Dominik Probst, Richard Lenz. (2024)  
**Quantifying Semantic Query Similarity for Automated Linear SQL Grading: A Graph-based Approach**
<br/>
<button class="copy-to-clipboard" title="Quantifying Semantic Query Similarity for Automated Linear SQL Grading: A Graph-based Approach" index=240>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DB  
Categories: cs-DB, cs.DB  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14441v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14441v1.pdf" filename="2403.14441v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Quantifying the semantic similarity between database queries is a critical challenge with broad applications, ranging from query log analysis to automated educational assessment of SQL skills. Traditional methods often rely solely on syntactic comparisons or are limited to checking for semantic equivalence. This paper introduces a novel <b>graph-based</b> approach to measure the semantic dissimilarity between SQL queries. Queries are represented as nodes in an implicit <b>graph,</b> while the transitions between nodes are called edits, which are weighted by semantic dissimilarity. We employ shortest path algorithms to identify the lowest-cost edit sequence between two given queries, thereby defining a quantifiable measure of semantic distance. A prototype implementation of this technique has been evaluated through an empirical study, which strongly suggests that our method provides more accurate and comprehensible grading compared to existing techniques. Moreover, the results indicate that our approach comes close to the quality of manual grading, making it a robust tool for diverse database query comparison tasks.

{{</citation>}}


## cs.AR (1)



### (1/1 | 241/241) E-Syn: E-Graph Rewriting with Technology-Aware Cost Functions for Logic Synthesis (Chen Chen et al., 2024)

{{<citation>}}

Chen Chen, Guangyu Hu, Dongsheng Zuo, Cunxi Yu, Yuzhe Ma, Hongce Zhang. (2024)  
**E-Syn: E-Graph Rewriting with Technology-Aware Cost Functions for Logic Synthesis**
<br/>
<button class="copy-to-clipboard" title="E-Syn: E-Graph Rewriting with Technology-Aware Cost Functions for Logic Synthesis" index=241>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AR  
Categories: cs-AR, cs.AR  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14242v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14242v1.pdf" filename="2403.14242v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Logic synthesis plays a crucial role in the digital design flow. It has a decisive influence on the final Quality of Results (QoR) of the circuit implementations. However, existing multi-level logic optimization algorithms often employ greedy approaches with a series of local optimization steps. Each step breaks the circuit into small pieces (e.g., k-feasible cuts) and applies incremental changes to individual pieces separately. These local optimization steps could limit the exploration space and may miss opportunities for significant improvements. To address the limitation, this paper proposes using e-graph in logic synthesis. The new workflow, named Esyn, makes use of the well-established e-graph infrastructure to efficiently perform logic rewriting. It explores a diverse set of equivalent Boolean representations while allowing technology-aware cost functions to better support delay-oriented and area-oriented logic synthesis. Experiments over a wide range of <b>benchmark</b> designs show our proposed logic optimization approach reaches a wider design space compared to the commonly used AIG-based logic synthesis flow. It achieves on average 15.29% delay saving in delay-oriented synthesis and 6.42% area saving for area-oriented synthesis.

{{</citation>}}
