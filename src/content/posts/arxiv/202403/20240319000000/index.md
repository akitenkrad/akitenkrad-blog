---
draft: false
title: "arXiv @ 2024.03.19"
date: 2024-03-19
author: "akitenkrad"
description: ""
tags: ["arXiv", "Published:2024"]
menu:
  sidebar:
    name: "arXiv @ 2024.03.19"
    identifier: arxiv_20240319
    parent: 202403_arxiv
    weight: 10
math: true
---

<figure style="border:none; width:100%; display:flex; justify-content: center">
    <iframe src="pie.html" width=900 height=620 style="border:none"></iframe>
</figure>


## Primary Categories

- [cs.AI (2)](#csai-2)
- [cs.AR (1)](#csar-1)
- [cs.CL (25)](#cscl-25)
- [cs.CR (4)](#cscr-4)
- [cs.CV (57)](#cscv-57)
- [cs.CY (1)](#cscy-1)
- [cs.DB (2)](#csdb-2)
- [cs.DC (1)](#csdc-1)
- [cs.DS (1)](#csds-1)
- [cs.HC (2)](#cshc-2)
- [cs.IR (2)](#csir-2)
- [cs.IT (2)](#csit-2)
- [cs.LG (18)](#cslg-18)
- [cs.NE (3)](#csne-3)
- [cs.NI (3)](#csni-3)
- [cs.RO (8)](#csro-8)
- [cs.SD (1)](#cssd-1)
- [cs.SE (3)](#csse-3)
- [eess.IV (7)](#eessiv-7)
- [eess.SP (1)](#eesssp-1)
- [eess.SY (3)](#eesssy-3)
- [math.NA (1)](#mathna-1)
- [math.OC (2)](#mathoc-2)
- [physics.flu-dyn (1)](#physicsflu-dyn-1)
- [physics.geo-ph (1)](#physicsgeo-ph-1)
- [quant-ph (1)](#quant-ph-1)
- [stat.ME (1)](#statme-1)
- [stat.ML (2)](#statml-2)

## Keywords

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>keyword</th>
      <th>cs.CL</th>
      <th>cs.CV</th>
      <th>cs.LG</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Adversarial Attack</td>
      <td>2</td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Adversarial Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Anomaly Detection</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Autoencoder</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Automatic Evaluation</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>BART</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Bandit Algorithm</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Benchmarking</td>
      <td>6</td>
      <td>16</td>
      <td>3</td>
    </tr>
    <tr>
      <td>Black Box</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Chain-of-thought Prompt</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Clustering</td>
      <td></td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Continual Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Contrastive Learning</td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>ControlNet</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Convolution</td>
      <td>1</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Convolutional Neural Network</td>
      <td>1</td>
      <td></td>
      <td>2</td>
    </tr>
    <tr>
      <td>Data Augmentation</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Dialogue State Tracking</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Differential Privacy</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Diffusion Model</td>
      <td></td>
      <td>11</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Discrete Time</td>
      <td></td>
      <td></td>
      <td>2</td>
    </tr>
    <tr>
      <td>Domain Adaptation</td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Face Recognition</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Fact Verification</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Federated Learning</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Few-shot</td>
      <td>3</td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Few-shot Learning</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Fine-tuning</td>
      <td>3</td>
      <td>12</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Foundation Model</td>
      <td></td>
      <td>3</td>
      <td></td>
    </tr>
    <tr>
      <td>GPT</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>GPT-4</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Generative AI</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Generative Adversarial Network</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Geometry</td>
      <td></td>
      <td>5</td>
      <td></td>
    </tr>
    <tr>
      <td>Graph</td>
      <td>2</td>
      <td>4</td>
      <td>5</td>
    </tr>
    <tr>
      <td>Graph Convolutional Network</td>
      <td></td>
      <td></td>
      <td>2</td>
    </tr>
    <tr>
      <td>Graph Neural Network</td>
      <td></td>
      <td>1</td>
      <td>3</td>
    </tr>
    <tr>
      <td>Grounding</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Hate Speech Detection</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Image2text</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>In-context Learning</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Instruction Tuning</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Knowledge Distillation</td>
      <td></td>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <td>Knowledge Graph</td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Knowledge Transfer</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>LLaMA</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>LSTM</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Large Language Model</td>
      <td>20</td>
      <td>7</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Logistic Regression</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Low-Resource</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>MNIST</td>
      <td></td>
      <td></td>
      <td>2</td>
    </tr>
    <tr>
      <td>Message-Passing</td>
      <td></td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Morphological Analysis</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Multi-modal</td>
      <td>8</td>
      <td>8</td>
      <td></td>
    </tr>
    <tr>
      <td>Mutual Information</td>
      <td></td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Named Entity Recognition</td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Natural Language Inference</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Neural Machine Translation</td>
      <td>5</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Object Detection</td>
      <td></td>
      <td>6</td>
      <td></td>
    </tr>
    <tr>
      <td>Optical Character Recognition</td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Out-of-distribution</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Out-of-domain</td>
      <td></td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Pre-trained Language Model</td>
      <td>3</td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Prompt</td>
      <td>5</td>
      <td>12</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Pruning</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Quantization</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Question Answering</td>
      <td>2</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Reasoning</td>
      <td>2</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Recommendation</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Recurrent Neural Network</td>
      <td></td>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Reinforcement Learning</td>
      <td></td>
      <td></td>
      <td>3</td>
    </tr>
    <tr>
      <td>Representation Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Retrieval-Augmented Generation</td>
      <td></td>
      <td></td>
      <td>3</td>
    </tr>
    <tr>
      <td>Self-Attention</td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Self-supervised Learning</td>
      <td>1</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Sentence Embedding</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Sentiment Analysis</td>
      <td>3</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Simulation</td>
      <td></td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Simulator</td>
      <td></td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Stance Detection</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Stemming</td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Summarization</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Supervised Learning</td>
      <td></td>
      <td>9</td>
      <td></td>
    </tr>
    <tr>
      <td>TF-IDF</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Text Classification</td>
      <td>1</td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Text Embedding</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Text-to-speech</td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Text2image</td>
      <td>1</td>
      <td>6</td>
      <td></td>
    </tr>
    <tr>
      <td>Tokenization</td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Transfer Learning</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Transformer</td>
      <td>1</td>
      <td>4</td>
      <td>3</td>
    </tr>
    <tr>
      <td>Unsupervised Learning</td>
      <td></td>
      <td>4</td>
      <td></td>
    </tr>
    <tr>
      <td>Vision Transformer</td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Vision-and-Language</td>
      <td>1</td>
      <td>4</td>
      <td></td>
    </tr>
    <tr>
      <td>Visual Question Answering</td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Weakly-supervised Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Yolo</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Zero-shot</td>
      <td>2</td>
      <td>2</td>
      <td></td>
    </tr>
  </tbody>
</table>

<script>
$(function() {
  $("table").addClass("keyword-table table-bordered border-success");
  $("table thead").addClass("sticky-top");
  $("table tbody td").css("text-align", "");
});
</script>


## cs.AR (1)



### (1/1 | 1/156) Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework (Kaiyan Chang et al., 2024)

{{<citation>}}

Kaiyan Chang, Kun Wang, Nan Yang, Ying Wang, Dantong Jin, Wenlong Zhu, Zhirong Chen, Cangyuan Li, Hao Yan, Yunhao Zhou, Zhuoliang Zhao, Yuan Cheng, Yudong Pan, Yiqi Liu, Mengdi Wang, Shengwen Liang, yinhe han, Huawei Li, Xiaowei Li. (2024)  
**Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework**
<br/>
<button class="copy-to-clipboard" title="Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework" index=1>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AR  
Categories: cs-AI, cs-AR, cs-PL, cs.AR  
Keyword Score: 93  
Keywords: Benchmarking, Data Augmentation, Fine-tuning, Fine-tuning, GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11202v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11202v1.pdf" filename="2403.11202v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advances in <b>large</b> <b>language</b> <b>models</b> have demonstrated their potential for automated generation of hardware description language (HDL) code from high-level <b>prompts.</b> Researchers have utilized <b>fine-tuning</b> to enhance the ability of these <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in the field of Chip Design. However, the lack of Verilog <b>data</b> <b>hinders</b> further improvement in the quality of Verilog generation by <b>LLMs.</b> Additionally, the absence of a Verilog and Electronic Design Automation (EDA) script <b>data</b> <b>augmentation</b> framework significantly increases the time required to prepare the training dataset for <b>LLM</b> trainers. This paper proposes an automated design-data augmentation framework, which generates high-volume and high-quality natural language aligned with Verilog and EDA scripts. For Verilog generation, it translates Verilog files to an abstract syntax tree and then maps nodes to natural language with a predefined template. For Verilog repair, it uses predefined rules to generate the wrong verilog file and then pairs EDA Tool feedback with the right and wrong verilog file. For EDA Script generation, it uses existing LLM(GPT-3.5) to obtain the description of the Script. To evaluate the effectiveness of our <b>data</b> <b>augmentation</b> method, we <b>finetune</b> Llama2-13B and Llama2-7B models using the dataset generated by our augmentation framework. The results demonstrate a significant improvement in the Verilog generation tasks with <b>LLMs.</b> Moreover, the accuracy of Verilog generation surpasses that of the current state-of-the-art open-source Verilog generation model, increasing from 58.8% to 70.6% with the same <b>benchmark.</b> Our 13B model (ChipGPT-FT) has a pass rate improvement compared with <b>GPT-3.5</b> in Verilog generation and outperforms in EDA script (i.e., SiliconCompiler) generation with only 200 EDA script data.

{{</citation>}}


## cs.LG (18)



### (1/18 | 2/156) Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation (Silvia Corbara et al., 2024)

{{<citation>}}

Silvia Corbara, Alejandro Moreo. (2024)  
**Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation**
<br/>
<button class="copy-to-clipboard" title="Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation" index=2>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CL, cs-LG, cs.LG  
Keyword Score: 90  
Keywords: Convolution, Convolutional Neural Network, Data Augmentation, Generative Adversarial Network, GPT, Recurrent Neural Network, Transformer, Text Classification, Adversarial Attack  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11265v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11265v1.pdf" filename="2403.11265v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Authorship Verification (AV) is a <b>text</b> <b>classification</b> task concerned with inferring whether a candidate <b>text</b> <b>has</b> been written by one specific author or by someone else. It has been shown that many AV systems are vulnerable to <b>adversarial</b> <b>attacks,</b> where a malicious author actively tries to fool the classifier by either concealing their writing style, or by imitating the style of another author. In this paper, we investigate the potential benefits of augmenting the classifier training set with (negative) synthetic examples. These synthetic examples are generated to imitate the style of the author of interest. We analyze the improvements in classifier prediction that this augmentation brings to bear in the task of AV in an <b>adversarial</b> <b>setting.</b> In particular, we experiment with three different generator architectures (one based on <b>Recurrent</b> <b>Neural</b> <b>Networks,</b> another based on small-scale <b>transformers,</b> and another based on the popular <b>GPT</b> model) and with two training strategies (one inspired by standard Language Models, and another inspired by Wasserstein <b>Generative</b> <b>Adversarial</b> <b>Networks).</b> We evaluate our hypothesis on five datasets (three of which have been specifically collected to represent an <b>adversarial</b> <b>setting)</b> and using two learning algorithms for the AV classifier (Support Vector Machines and <b>Convolutional</b> <b>Neural</b> <b>Networks).</b> This experimentation has yielded negative results, revealing that, although our methodology proves effective in many <b>adversarial</b> <b>settings,</b> its benefits are too sporadic for a pragmatical application.

{{</citation>}}


### (2/18 | 3/156) JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning (Anique Tahir et al., 2024)

{{<citation>}}

Anique Tahir, Lu Cheng, Huan Liu. (2024)  
**JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning**
<br/>
<button class="copy-to-clipboard" title="JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning" index=3>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CL, cs-DC, cs-LG, cs.LG  
Keyword Score: 80  
Keywords: Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, LLaMA, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11366v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11366v1.pdf" filename="2403.11366v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The scaling of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for <b>retrieval-based</b> <b>tasks,</b> <b>particularly</b> in <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> <b>(RAG),</b> faces significant memory constraints, especially when <b>fine-tuning</b> extensive <b>prompt</b> sequences. Current open-source libraries support full-model inference and <b>fine-tuning</b> across multiple GPUs but fall short of accommodating the efficient parameter distribution required for retrieved context. Addressing this gap, we introduce a novel framework for PEFT-compatible <b>fine-tuning</b> of <b>Llama-2</b> models, leveraging distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT) compilation and tensor-sharding for efficient resource management, thereby enabling accelerated <b>fine-tuning</b> with reduced memory requirements. This advancement significantly improves the scalability and feasibility of <b>fine-tuning</b> <b>LLMs</b> for complex <b>RAG</b> applications, even on systems with limited GPU resources. Our experiments show more than 12x improvement in runtime compared to Hugging Face/DeepSpeed implementation with four GPUs while consuming less than half the VRAM per GPU. Our library will be open-sourced in due course.

{{</citation>}}


### (3/18 | 4/156) Incorporating Higher-order Structural Information for Graph Clustering (Qiankun Li et al., 2024)

{{<citation>}}

Qiankun Li, Haobing Liu, Ruobing Jiang, Tingting Wang. (2024)  
**Incorporating Higher-order Structural Information for Graph Clustering**
<br/>
<button class="copy-to-clipboard" title="Incorporating Higher-order Structural Information for Graph Clustering" index=4>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs-SI, cs.LG  
Keyword Score: 66  
Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Clustering, Convolution, Convolutional Neural Network, Mutual Information, Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11087v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11087v1.pdf" filename="2403.11087v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Clustering</b> holds profound significance in data mining. In recent years, <b>graph</b> <b>convolutional</b> <b>network</b> <b>(GCN)</b> has emerged as a powerful tool for deep <b>clustering,</b> integrating both <b>graph</b> <b>structural</b> <b>information</b> and node attributes. However, most existing methods ignore the higher-order structural information of the <b>graph.</b> <b>Evidently,</b> <b>nodes</b> within the same cluster can establish distant connections. Besides, recent deep <b>clustering</b> methods usually apply a <b>self-supervised</b> module to monitor the training process of their model, focusing solely on node attributes without paying attention to <b>graph</b> <b>structure.</b> <b>In</b> this paper, we propose a novel <b>graph</b> <b>clustering</b> <b>network</b> to make full use of <b>graph</b> <b>structural</b> <b>information.</b> To capture the higher-order structural information, we design a <b>graph</b> <b>mutual</b> <b>infomax</b> module, effectively maximizing <b>mutual</b> <b>information</b> between <b>graph-level</b> <b>and</b> <b>node-level</b> representations, and employ a trinary <b>self-supervised</b> module that includes modularity as a structural constraint. Our proposed model outperforms many state-of-the-art methods on various datasets, demonstrating its superiority.

{{</citation>}}


### (4/18 | 5/156) Graph Expansion in Pruned Recurrent Neural Network Layers Preserve Performance (Suryam Arnav Kalra et al., 2024)

{{<citation>}}

Suryam Arnav Kalra, Arindam Biswas, Pabitra Mitra, Biswajit Basu. (2024)  
**Graph Expansion in Pruned Recurrent Neural Network Layers Preserve Performance**
<br/>
<button class="copy-to-clipboard" title="Graph Expansion in Pruned Recurrent Neural Network Layers Preserve Performance" index=5>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: 05C68, I-2-6, cs-CV, cs-LG, cs-NE, cs.LG  
Keyword Score: 56  
Keywords: MNIST, Graph, Benchmarking, Pruning, LSTM, Recurrent Neural Network, Recurrent Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11100v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11100v1.pdf" filename="2403.11100v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Expansion property of a <b>graph</b> refers to its strong connectivity as well as sparseness. It has been reported that deep neural networks can be pruned to a high degree of sparsity while maintaining their performance. Such <b>pruning</b> is essential for performing real time sequence learning tasks using <b>recurrent</b> <b>neural</b> <b>networks</b> in resource constrained platforms. We prune <b>recurrent</b> <b>networks</b> <b>such</b> as <b>RNNs</b> and <b>LSTMs,</b> maintaining a large spectral gap of the underlying <b>graphs</b> and ensuring their layerwise expansion properties. We also study the time unfolded <b>recurrent</b> <b>network</b> <b>graphs</b> in terms of the properties of their bipartite layers. Experimental results for the <b>benchmark</b> sequence <b>MNIST,</b> CIFAR-10, and Google speech command data show that expander <b>graph</b> properties are key to preserving classification accuracy of <b>RNN</b> and <b>LSTM.</b>

{{</citation>}}


### (5/18 | 6/156) Self-Supervised Quantization-Aware Knowledge Distillation (Kaiqi Zhao et al., 2024)

{{<citation>}}

Kaiqi Zhao, Ming Zhao. (2024)  
**Self-Supervised Quantization-Aware Knowledge Distillation**
<br/>
<button class="copy-to-clipboard" title="Self-Supervised Quantization-Aware Knowledge Distillation" index=6>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CV, cs-LG, cs.LG  
Keyword Score: 50  
Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Quantization, Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11106v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11106v1.pdf" filename="2403.11106v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Quantization-aware</b> training (QAT) and <b>Knowledge</b> <b>Distillation</b> <b>(KD)</b> are combined to achieve competitive performance in creating low-bit deep learning models. However, existing works applying <b>KD</b> to QAT require tedious hyper-parameter tuning to balance the weights of different loss terms, assume the availability of labeled training data, and require complex, computationally intensive training procedures for good performance. To address these limitations, this paper proposes a novel <b>Self-Supervised</b> <b>Quantization-Aware</b> <b>Knowledge</b> <b>Distillation</b> (SQAKD) framework. SQAKD first unifies the forward and backward dynamics of various <b>quantization</b> functions, making it flexible for incorporating various QAT works. Then it formulates QAT as a co-optimization problem that simultaneously minimizes the KL-Loss between the full-precision and low-bit models for <b>KD</b> and the discretization error for <b>quantization,</b> without supervision from labels. A comprehensive evaluation shows that SQAKD substantially outperforms the state-of-the-art QAT and <b>KD</b> works for a variety of model architectures. Our code is at: https://github.com/kaiqi123/SQAKD.git.

{{</citation>}}


### (6/18 | 7/156) Federated Transfer Learning with Differential Privacy (Mengchu Li et al., 2024)

{{<citation>}}

Mengchu Li, Ye Tian, Yang Feng, Yi Yu. (2024)  
**Federated Transfer Learning with Differential Privacy**
<br/>
<button class="copy-to-clipboard" title="Federated Transfer Learning with Differential Privacy" index=7>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CR, cs-LG, cs.LG, math-ST, stat-ME, stat-ML, stat-TH  
Keyword Score: 40  
Keywords: Federated Learning, Knowledge Transfer, Transfer Learning, Differential Privacy  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11343v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11343v1.pdf" filename="2403.11343v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Federated</b> <b>learning</b> is gaining increasing popularity, with data heterogeneity and privacy being two prominent challenges. In this paper, we address both issues within a <b>federated</b> <b>transfer</b> <b>learning</b> framework, aiming to enhance learning on a target data set by leveraging information from multiple heterogeneous source data sets while adhering to privacy constraints. We rigorously formulate the notion of \textit{federated <b>differential</b> <b>privacy},</b> which offers privacy guarantees for each data set without assuming a trusted central server. Under this privacy constraint, we study three classical statistical problems, namely univariate mean estimation, low-dimensional linear regression, and high-dimensional linear regression. By investigating the minimax rates and identifying the costs of privacy for these problems, we show that <b>federated</b> <b>differential</b> <b>privacy</b> is an intermediate privacy model between the well-established local and central models of <b>differential</b> <b>privacy.</b> Our analyses incorporate data heterogeneity and privacy, highlighting the fundamental costs of both in <b>federated</b> <b>learning</b> and underscoring the benefit of <b>knowledge</b> <b>transfer</b> <b>across</b> data sets.

{{</citation>}}


### (7/18 | 8/156) Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts (Daniel Enström et al., 2024)

{{<citation>}}

Daniel Enström, Viktor Kjellberg, Moa Johansson. (2024)  
**Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts**
<br/>
<button class="copy-to-clipboard" title="Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts" index=8>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CL, cs-LG, cs.LG  
Keyword Score: 40  
Keywords: BART, Transformer, Reasoning, Pre-trained Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11314v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11314v1.pdf" filename="2403.11314v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Transformer</b> language models are neural networks used for a wide variety of tasks concerning natural language, including some that also require logical <b>reasoning.</b> However, a <b>transformer</b> model may easily learn spurious patterns in the data, short-circuiting actual <b>reasoning.</b> In this paper we investigate to what extent <b>transformers</b> can be trained to a) approximate <b>reasoning</b> in propositional logic while b) avoiding known <b>reasoning</b> shortcuts via spurious correlations in the training data. To do so, we use a dataset with known spurious correlation between truth and e.g. the number of rules in the problem. We augment the data with proofs, and train two models: a generative <b>transformer,</b> WP-BART, trained on problems and their whole proofs, and a neuro-symbolic model, SIP-BART, trained on individual proof steps and combining the generative <b>transformer</b> model <b>BART</b> with a symbolic proof checker. We find that SIP-BART succeeds in avoiding <b>reasoning</b> shortcuts, while WP-BART does not. For SIP-BART, we then identify a few remaining <b>reasoning</b> errors, not previously described in the literature, arising from using a <b>pre-trained</b> <b>language</b> <b>model.</b> These are qualitatively analysed to create a taxonomy of four different types of additional pitfalls.

{{</citation>}}


### (8/18 | 9/156) Phasic Diversity Optimization for Population-Based Reinforcement Learning (Jingcheng Jiang et al., 2024)

{{<citation>}}

Jingcheng Jiang, Haiyin Piao, Yu Fu, Yihang Hao, Chuanlu Jiang, Ziqi Wei, Xin Yang. (2024)  
**Phasic Diversity Optimization for Population-Based Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="Phasic Diversity Optimization for Population-Based Reinforcement Learning" index=9>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: 14J60 (Primary), I-2-9, cs-AI, cs-LG, cs.LG  
Keyword Score: 40  
Keywords: Bandit Algorithm, Reinforcement Learning, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11114v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11114v1.pdf" filename="2403.11114v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Reviewing the previous work of diversity Rein-forcement Learning,diversity is often obtained via an augmented loss function,which requires a balance between reward and diversity.Generally,diversity optimization algorithms use Multi-armed <b>Bandits</b> algorithms to select the coefficient in the pre-defined space. However, the dynamic distribution of reward signals for MABs or the conflict between quality and diversity limits the performance of these methods. We introduce the Phasic Diversity Optimization (PDO) algorithm, a Population-Based Training framework that separates reward and diversity training into distinct phases instead of optimizing a multi-objective function. In the auxiliary phase, agents with poor performance diversified via determinants will not replace the better agents in the archive. The decoupling of reward and diversity allows us to use an aggressive diversity optimization in the auxiliary phase without performance degradation. Furthermore, we construct a dogfight scenario for aerial agents to demonstrate the practicality of the PDO algorithm. We introduce two implementations of PDO archive and conduct tests in the newly proposed adversarial dogfight and MuJoCo <b>simulations.</b> The results show that our proposed algorithm achieves better performance than baselines.

{{</citation>}}


### (9/18 | 10/156) Graph Unitary Message Passing (Haiquan Qiu et al., 2024)

{{<citation>}}

Haiquan Qiu, Yatao Bian, Quanming Yao. (2024)  
**Graph Unitary Message Passing**
<br/>
<button class="copy-to-clipboard" title="Graph Unitary Message Passing" index=10>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 33  
Keywords: Message-Passing, Graph, Graph Neural Network, Recurrent Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11199v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11199v1.pdf" filename="2403.11199v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Message passing mechanism contributes to the success of <b>GNNs</b> in various applications, but also brings the oversquashing problem. Recent works combat oversquashing by improving the <b>graph</b> spectrums with rewiring techniques, disrupting the structural bias in <b>graphs,</b> and having limited improvement on oversquashing in terms of oversquashing measure. Motivated by unitary <b>RNN,</b> we propose <b>Graph</b> Unitary Message Passing (GUMP) to alleviate oversquashing in <b>GNNs</b> by applying unitary adjacency matrix for message passing. To design GUMP, a transformation is first proposed to make general <b>graphs</b> have unitary adjacency matrix and keep its structural bias. Then, unitary adjacency matrix is obtained with a unitary projection algorithm, which is implemented by utilizing the intrinsic structure of unitary adjacency matrix and allows GUMP to be permutation-equivariant. Experimental results show the effectiveness of GUMP in improving the performance on various <b>graph</b> learning tasks.

{{</citation>}}


### (10/18 | 11/156) CBR - Boosting Adaptive Classification By Retrieval of Encrypted Network Traffic with Out-of-distribution (Amir Lukach et al., 2024)

{{<citation>}}

Amir Lukach, Ran Dubin, Amit Dvir, Chen Hajaj. (2024)  
**CBR - Boosting Adaptive Classification By Retrieval of Encrypted Network Traffic with Out-of-distribution**
<br/>
<button class="copy-to-clipboard" title="CBR - Boosting Adaptive Classification By Retrieval of Encrypted Network Traffic with Out-of-distribution" index=11>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: ACM-class: F-2-2, I-2-7 ACM-class: F-2-2, I-2-7 ACM-class: F-2-2,
  I-2-7 ACM-class: F-2-2, I-2-7 ACM-class: I-2-6, cs-CR, cs-LG, cs-NI, cs.LG  
Keyword Score: 30  
Keywords: Few-shot, Out-of-distribution, Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11206v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11206v1.pdf" filename="2403.11206v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Encrypted network traffic Classification tackles the problem from different approaches and with different goals. One of the common approaches is using Machine learning or Deep Learning-based solutions on a fixed number of classes, leading to misclassification when an unknown class is given as input. One of the solutions for handling unknown classes is to retrain the model, however, retraining models every time they become obsolete is both resource and time-consuming. Therefore, there is a growing need to allow classification models to detect and adapt to new classes dynamically, without retraining, but instead able to detect new classes using few shots learning [1]. In this paper, we introduce Adaptive Classification By Retrieval CBR, a novel approach for encrypted network traffic classification. Our new approach is based on an ANN-based method, which allows us to effectively identify new and existing classes without retraining the model. The novel approach is simple, yet effective and achieved similar results to RF with up to 5% difference (usually less than that) in the classification tasks while having a slight decrease in the case of new samples (from new classes) without retraining. To <b>summarize,</b> the new method is a real-time classification, which can classify new classes without retraining. Furthermore, our solution can be used as a complementary solution alongside RF or any other machine/deep learning classification method, as an aggregated solution.

{{</citation>}}


### (11/18 | 12/156) Multi-Relational Graph Neural Network for Out-of-Domain Link Prediction (Asma Sattar et al., 2024)

{{<citation>}}

Asma Sattar, Georgios Deligiorgis, Marco Trincavelli, Davide Bacciu. (2024)  
**Multi-Relational Graph Neural Network for Out-of-Domain Link Prediction**
<br/>
<button class="copy-to-clipboard" title="Multi-Relational Graph Neural Network for Out-of-Domain Link Prediction" index=12>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 26  
Keywords: Graph, Graph Neural Network, Benchmarking, Out-of-domain  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11292v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11292v1.pdf" filename="2403.11292v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Dynamic multi-relational <b>graphs</b> <b>are</b> <b>an</b> expressive relational representation for data enclosing entities and relations of different types, and where relationships are allowed to vary in time. Addressing predictive tasks over such data requires the ability to find structure embeddings that capture the diversity of the relationships involved, as well as their dynamic evolution. In this work, we establish a novel class of challenging tasks for dynamic multi-relational <b>graphs</b> <b>involving</b> <b>out-of-domain</b> link prediction, where the relationship being predicted is not available in the input <b>graph.</b> <b>We</b> <b>then</b> introduce a novel <b>Graph</b> <b>Neural</b> <b>Network</b> model, named GOOD, designed specifically to tackle the <b>out-of-domain</b> generalization problem. GOOD introduces a novel design concept for multi-relation embedding aggregation, based on the idea that good representations are such when it is possible to disentangle the mixing proportions of the different relational embeddings that have produced it. We also propose five <b>benchmarks</b> based on two retail domains, where we show that GOOD can effectively generalize predictions out of known relationship types and achieve state-of-the-art results. Most importantly, we provide insights into problems where <b>out-of-domain</b> prediction might be preferred to an in-domain formulation, that is, where the relationship to be predicted has very few positive examples.

{{</citation>}}


### (12/18 | 13/156) Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective (Muhammad Aneeq uz Zaman et al., 2024)

{{<citation>}}

Muhammad Aneeq uz Zaman, Alec Koppel, Mathieu Laurière, Tamer Başar. (2024)  
**Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective**
<br/>
<button class="copy-to-clipboard" title="Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective" index=13>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-GT, cs-LG, cs-MA, cs.LG  
Keyword Score: 20  
Keywords: Discrete Time, Discrete Time, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11345v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11345v1.pdf" filename="2403.11345v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We address in this paper <b>Reinforcement</b> <b>Learning</b> (RL) among agents that are grouped into teams such that there is cooperation within each team but general-sum (non-zero sum) competition across different teams. To develop an RL method that provably achieves a Nash equilibrium, we focus on a linear-quadratic structure. Moreover, to tackle the non-stationarity induced by multi-agent interactions in the finite population setting, we consider the case where the number of agents within each team is infinite, i.e., the mean-field setting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTGs). We characterize the Nash equilibrium (NE) of the GS-MFTG, under a standard invertibility condition. This MFTG NE is then shown to be $\mathcal{O}(1/M)$-NE for the finite population game where $M$ is a lower bound on the number of agents in each team. These structural results motivate an algorithm called Multi-player Receding-horizon Natural Policy Gradient (MRPG), where each team minimizes its cumulative cost independently in a receding-horizon manner. Despite the non-convexity of the problem, we establish that the resulting algorithm converges to a global NE through a novel problem decomposition into sub-problems using backward recursive <b>discrete-time</b> <b>Hamilton-Jacobi-Isaacs</b> (HJI) equations, in which independent natural policy gradient is shown to exhibit linear convergence under time-independent diagonal dominance. Experiments illuminate the merits of this approach in practice.

{{</citation>}}


### (13/18 | 14/156) COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits (Mintong Kang et al., 2024)

{{<citation>}}

Mintong Kang, Nezihe Merve Gürel, Linyi Li, Bo Li. (2024)  
**COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits**
<br/>
<button class="copy-to-clipboard" title="COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits" index=14>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG, stat-ML  
Keyword Score: 15  
Keywords: Black Box, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11348v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11348v1.pdf" filename="2403.11348v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Conformal prediction has shown spurring performance in constructing statistically rigorous prediction sets for arbitrary <b>black-box</b> <b>machine</b> learning models, assuming the data is exchangeable. However, even small adversarial perturbations during the inference can violate the exchangeability assumption, challenge the coverage guarantees, and result in a subsequent decline in empirical coverage. In this work, we propose a certifiably robust learning-reasoning conformal prediction framework (COLEP) via probabilistic circuits, which comprise a data-driven learning component that trains statistical models to learn different semantic concepts, and a <b>reasoning</b> component that encodes knowledge and characterizes the relationships among the trained models for logic <b>reasoning.</b> To achieve exact and efficient <b>reasoning,</b> we employ probabilistic circuits (PCs) within the <b>reasoning</b> component. Theoretically, we provide end-to-end certification of prediction coverage for COLEP in the presence of bounded adversarial perturbations. We also provide certified coverage considering the finite size of the calibration set. Furthermore, we prove that COLEP achieves higher prediction coverage and accuracy over a single model as long as the utilities of knowledge models are non-trivial. Empirically, we show the validity and tightness of our certified coverage, demonstrating the robust conformal prediction of COLEP on various datasets, including GTSRB, CIFAR10, and AwA2. We show that COLEP achieves up to 12% improvement in certified coverage on GTSRB, 9% on CIFAR-10, and 14% on AwA2.

{{</citation>}}


### (14/18 | 15/156) Graph Neural Network based Double Machine Learning Estimator of Network Causal Effects (Seyedeh Baharan Khatami et al., 2024)

{{<citation>}}

Seyedeh Baharan Khatami, Harsh Parikh, Haowei Chen, Sudeepa Roy, Babak Salimi. (2024)  
**Graph Neural Network based Double Machine Learning Estimator of Network Causal Effects**
<br/>
<button class="copy-to-clipboard" title="Graph Neural Network based Double Machine Learning Estimator of Network Causal Effects" index=15>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs-SI, cs.LG, stat-ME  
Keyword Score: 13  
Keywords: Graph, Graph Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11332v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11332v1.pdf" filename="2403.11332v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Our paper addresses the challenge of inferring causal effects in social network data, characterized by complex interdependencies among individuals resulting in challenges such as non-independence of units, interference (where a unit's outcome is affected by neighbors' treatments), and introduction of additional confounding factors from neighboring units. We propose a novel methodology combining <b>graph</b> <b>neural</b> <b>networks</b> and double machine learning, enabling accurate and efficient estimation of direct and peer effects using a single observational social network. Our approach utilizes <b>graph</b> <b>isomorphism</b> <b>networks</b> in conjunction with double machine learning to effectively adjust for network confounders and consistently estimate the desired causal effects. We demonstrate that our estimator is both asymptotically normal and semiparametrically efficient. A comprehensive evaluation against four state-of-the-art baseline methods using three semi-synthetic social network datasets reveals our method's on-par or superior efficacy in precise causal effect estimation. Further, we illustrate the practical application of our method through a case study that investigates the impact of Self-Help Group participation on financial risk tolerance. The results indicate a significant positive direct effect, underscoring the potential of our approach in social network analysis. Additionally, we explore the effects of network sparsity on estimation performance.

{{</citation>}}


### (15/18 | 16/156) A Simple Mixture Policy Parameterization for Improving Sample Efficiency of CVaR Optimization (Yudong Luo et al., 2024)

{{<citation>}}

Yudong Luo, Yangchen Pan, Han Wang, Philip Torr, Pascal Poupart. (2024)  
**A Simple Mixture Policy Parameterization for Improving Sample Efficiency of CVaR Optimization**
<br/>
<button class="copy-to-clipboard" title="A Simple Mixture Policy Parameterization for Improving Sample Efficiency of CVaR Optimization" index=16>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, math-OC  
Keyword Score: 13  
Keywords: Benchmarking, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11062v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11062v1.pdf" filename="2403.11062v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Reinforcement</b> <b>learning</b> algorithms utilizing policy gradients (PG) to optimize Conditional Value at Risk (CVaR) face significant challenges with sample inefficiency, hindering their practical applications. This inefficiency stems from two main facts: a focus on tail-end performance that overlooks many sampled trajectories, and the potential of gradient vanishing when the lower tail of the return distribution is overly flat. To address these challenges, we propose a simple mixture policy parameterization. This method integrates a risk-neutral policy with an adjustable policy to form a risk-averse policy. By employing this strategy, all collected trajectories can be utilized for policy updating, and the issue of vanishing gradients is counteracted by stimulating higher returns through the risk-neutral component, thus lifting the tail and preventing flatness. Our empirical study reveals that this mixture parameterization is uniquely effective across a variety of <b>benchmark</b> domains. Specifically, it excels in identifying risk-averse CVaR policies in some Mujoco environments where the traditional CVaR-PG fails to learn a reasonable policy.

{{</citation>}}


### (16/18 | 17/156) Understanding Diffusion Models by Feynman's Path Integral (Yuji Hirono et al., 2024)

{{<citation>}}

Yuji Hirono, Akinori Tanaka, Kenji Fukushima. (2024)  
**Understanding Diffusion Models by Feynman's Path Integral**
<br/>
<button class="copy-to-clipboard" title="Understanding Diffusion Models by Feynman's Path Integral" index=17>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cond-mat-stat-mech, cs-AI, cs-LG, cs.LG, hep-th  
Keyword Score: 10  
Keywords: Diffusion Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11262v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11262v1.pdf" filename="2403.11262v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Score-based <b>diffusion</b> <b>models</b> have proven effective in image generation and have gained widespread usage; however, the underlying factors contributing to the performance disparity between stochastic and deterministic (i.e., the probability flow ODEs) sampling schemes remain unclear. We introduce a novel formulation of <b>diffusion</b> <b>models</b> using Feynman's path integral, which is a formulation originally developed for quantum physics. We find this formulation providing comprehensive descriptions of score-based generative models, and demonstrate the derivation of backward stochastic differential equations and loss functions.The formulation accommodates an interpolating parameter connecting stochastic and deterministic sampling schemes, and we identify this parameter as a counterpart of Planck's constant in quantum physics. This analogy enables us to apply the Wentzel-Kramers-Brillouin (WKB) expansion, a well-established technique in quantum physics, for evaluating the negative log-likelihood to assess the performance disparity between stochastic and deterministic sampling schemes.

{{</citation>}}


### (17/18 | 18/156) Partitioned Neural Network Training via Synthetic Intermediate Labels (Cevat Volkan Karadağ et al., 2024)

{{<citation>}}

Cevat Volkan Karadağ, Nezih Topaloğlu. (2024)  
**Partitioned Neural Network Training via Synthetic Intermediate Labels**
<br/>
<button class="copy-to-clipboard" title="Partitioned Neural Network Training via Synthetic Intermediate Labels" index=18>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: 68T01 (Primary) 68T07, 68T05 (Secondary), I-2-6; I-2-11, cs-AI, cs-DC, cs-LG, cs.LG  
Keyword Score: 10  
Keywords: MNIST  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11204v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11204v1.pdf" filename="2403.11204v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The proliferation of extensive neural network architectures, particularly deep learning models, presents a challenge in terms of resource-intensive training. GPU memory constraints have become a notable bottleneck in training such sizable models. Existing strategies, including data parallelism, model parallelism, pipeline parallelism, and fully sharded data parallelism, offer partial solutions. Model parallelism, in particular, enables the distribution of the entire model across multiple GPUs, yet the ensuing data communication between these partitions slows down training. Additionally, the substantial memory overhead required to store auxiliary parameters on each GPU compounds computational demands. Instead of using the entire model for training, this study advocates partitioning the model across GPUs and generating synthetic intermediate labels to train individual segments. These labels, produced through a random process, mitigate memory overhead and computational load. This approach results in a more efficient training process that minimizes data communication while maintaining model accuracy. To validate this method, a 6-layer fully connected neural network is partitioned into two parts and its performance is assessed on the extended <b>MNIST</b> dataset. Experimental results indicate that the proposed approach achieves similar testing accuracies to conventional training methods, while significantly reducing memory and computational requirements. This work contributes to mitigating the resource-intensive nature of training large neural networks, paving the way for more efficient deep learning model development.

{{</citation>}}


### (18/18 | 19/156) Is Mamba Effective for Time Series Forecasting? (Zihan Wang et al., 2024)

{{<citation>}}

Zihan Wang, Fanheng Kong, Shi Feng, Ming Wang, Han Zhao, Daling Wang, Yifei Zhang. (2024)  
**Is Mamba Effective for Time Series Forecasting?**
<br/>
<button class="copy-to-clipboard" title="Is Mamba Effective for Time Series Forecasting?" index=19>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11144v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11144v1.pdf" filename="2403.11144v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In the realm of time series forecasting (TSF), the <b>Transformer</b> has consistently demonstrated robust performance due to its ability to focus on the global context and effectively capture long-range dependencies within time, as well as discern correlations between multiple variables. However, due to the inefficiencies of the <b>Transformer</b> model and questions surrounding its ability to capture dependencies, ongoing efforts to refine the <b>Transformer</b> architecture persist. Recently, state space models (SSMs), e.g. Mamba, have gained traction due to their ability to capture complex dependencies in sequences, similar to the <b>Transformer,</b> while maintaining near-linear complexity. In text and image tasks, Mamba-based models can improve performance and cost savings, creating a win-win situation. This has piqued our interest in exploring SSM's potential in TSF tasks. In this paper, we introduce two straightforward SSM-based models for TSF, S-Mamba and D-Mamba, both employing the Mamba Block to extract variate correlations. Remarkably, S-Mamba and D-Mamba achieve superior performance while saving GPU memory and training time. Furthermore, we conduct extensive experiments to delve deeper into the potential of Mamba compared to the <b>Transformer</b> in the TSF, aiming to explore a new research direction for this field. Our code is available at https://github.com/wzhwzhwzh0921/S-D-Mamba.

{{</citation>}}


## cs.CL (25)



### (1/25 | 20/156) Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches (Igor Sterner et al., 2024)

{{<citation>}}

Igor Sterner, Weizhe Lin, Jinghong Chen, Bill Byrne. (2024)  
**Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches**
<br/>
<button class="copy-to-clipboard" title="Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches" index=20>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-CV, cs.CL  
Keyword Score: 86  
Keywords: Few-shot, Multi-modal, Multi-modal, Zero-shot, Question Answering, Visual Question Answering, Visual Question Answering, In-context Learning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11317v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11317v1.pdf" filename="2403.11317v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Two approaches have emerged to input images into <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> The first is to caption images into natural language. The second is to map image feature embeddings into the domain of the <b>LLM</b> and pass the mapped embeddings directly to the <b>LLM.</b> The majority of recent <b>few-shot</b> <b>multimodal</b> work reports performance using architectures that employ variations of one of these two approaches. But they overlook an important comparison between them. We design a controlled and focused experiment to compare these two approaches to <b>few-shot</b> <b>visual</b> <b>question</b> <b>answering</b> <b>(VQA)</b> with <b>LLMs.</b> Our findings indicate that for Flan-T5 XL, a 3B parameter <b>LLM,</b> connecting <b>visual</b> <b>embeddings</b> <b>directly</b> to the <b>LLM</b> embedding space does not guarantee improved performance over using image captions. In the <b>zero-shot</b> regime, we find using textual image captions is better. In the <b>few-shot</b> regimes, how the <b>in-context</b> examples are selected determines which is better.

{{</citation>}}


### (2/25 | 21/156) CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data (Kung Yin Hong et al., 2024)

{{<citation>}}

Kung Yin Hong, Lifeng Han, Riza Batista-Navarro, Goran Nenadic. (2024)  
**CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data**
<br/>
<button class="copy-to-clipboard" title="CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data" index=21>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 80  
Keywords: Automatic Evaluation, Data Augmentation, Fine-tuning, Low-Resource, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11346v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11346v1.pdf" filename="2403.11346v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Neural</b> <b>Machine</b> <b>Translation</b> <b>(NMT)</b> for <b>low-resource</b> languages is still a challenging task in front of NLP researchers. In this work, we deploy a standard <b>data</b> <b>augmentation</b> methodology by back-translation to a new language translation direction Cantonese-to-English. We present the models we <b>fine-tuned</b> using the limited amount of real <b>data</b> <b>and</b> the synthetic <b>data</b> <b>we</b> generated using back-translation including OpusMT, NLLB, and mBART. We carried out <b>automatic</b> <b>evaluation</b> using a range of different metrics including lexical-based and embedding-based. Furthermore. we create a user-friendly interface for the models we included in this\textsc{ CantonMT} research project and make it available to facilitate Cantonese-to-English <b>MT</b> research. Researchers can add more models into this platform via our open-source\textsc{ CantonMT} toolkit \url{https://github.com/kenrickkung/CantoneseTranslation}.

{{</citation>}}


### (3/25 | 22/156) Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding (Zichen Wu et al., 2024)

{{<citation>}}

Zichen Wu, HsiuYuan Huang, Fanyi Qu, Yunfang Wu. (2024)  
**Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding**
<br/>
<button class="copy-to-clipboard" title="Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding" index=22>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-MM, cs.CL  
Keyword Score: 66  
Keywords: Few-shot, Few-shot Learning, Multi-modal, Multi-modal, Transformer, Sentiment Analysis, Prompt, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11311v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11311v1.pdf" filename="2403.11311v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Deep <b>multimodal</b> semantic understanding that goes beyond the mere superficial content relation mining has received increasing attention in the realm of artificial intelligence. The challenges of collecting and annotating high-quality <b>multi-modal</b> data have underscored the significance of <b>few-shot</b> <b>learning.</b> In this paper, we focus on two critical tasks under this context: <b>few-shot</b> <b>multi-modal</b> sarcasm detection (MSD) and <b>multi-modal</b> <b>sentiment</b> <b>analysis</b> (MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware <b>Prompt</b> Fusion (MoPE-BAF), a novel <b>multi-modal</b> soft <b>prompt</b> framework based on the unified <b>vision-language</b> model (VLM). Specifically, we design three experts of soft <b>prompts:</b> a text <b>prompt</b> and an image <b>prompt</b> that extract modality-specific features to enrich the single-modal representation, and a unified <b>prompt</b> to assist <b>multi-modal</b> interaction. Additionally, we reorganize <b>Transformer</b> layers into several blocks and introduce cross-modal <b>prompt</b> attention between adjacent blocks, which smoothens the transition from single-modal representation to <b>multi-modal</b> fusion. On both MSD and MSA datasets in <b>few-shot</b> <b>setting,</b> our proposed model not only surpasses the 8.2B model InstructBLIP with merely 2% parameters (150M), but also significantly outperforms other widely-used <b>prompt</b> methods on VLMs or task-specific methods.

{{</citation>}}


### (4/25 | 23/156) Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models (Mohamed Taher Alrefaie et al., 2024)

{{<citation>}}

Mohamed Taher Alrefaie, Nour Eldin Morsy, Nada Samir. (2024)  
**Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models**
<br/>
<button class="copy-to-clipboard" title="Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models" index=23>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 60  
Keywords: Recommendation, Hate Speech Detection, Morphological Analysis, Natural Language Inference, Sentiment Analysis, Tokenization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11130v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11130v1.pdf" filename="2403.11130v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents a comprehensive examination of the impact of <b>tokenization</b> strategies and vocabulary sizes on the performance of Arabic language models in downstream <b>natural</b> <b>language</b> <b>processing</b> tasks. Our investigation focused on the effectiveness of four tokenizers across various tasks, including News Classification, <b>Hate</b> <b>Speech</b> <b>Detection,</b> <b>Sentiment</b> <b>Analysis,</b> and <b>Natural</b> <b>Language</b> <b>Inference.</b> Leveraging a diverse set of vocabulary sizes, we scrutinize the intricate interplay between <b>tokenization</b> approaches and model performance. The results reveal that Byte Pair Encoding (BPE) with Farasa outperforms other strategies in multiple tasks, underscoring the significance of <b>morphological</b> <b>analysis</b> in capturing the nuances of the Arabic language. However, challenges arise in <b>sentiment</b> <b>analysis,</b> where dialect specific segmentation issues impact model efficiency. Computational efficiency analysis demonstrates the stability of BPE with Farasa, suggesting its practical viability. Our study uncovers limited impacts of vocabulary size on model performance while keeping the model size unchanged. This is challenging the established beliefs about the relationship between vocabulary, model size, and downstream tasks, emphasizing the need for the study of models' size and their corresponding vocabulary size to generalize across domains and mitigate biases, particularly in dialect based datasets. Paper's <b>recommendations</b> include refining <b>tokenization</b> strategies to address dialect challenges, enhancing model robustness across diverse linguistic contexts, and expanding datasets to encompass the rich dialect based Arabic. This work not only advances our understanding of Arabic language models but also lays the foundation for responsible and ethical developments in <b>natural</b> <b>language</b> <b>processing</b> technologies tailored to the intricacies of the Arabic language.

{{</citation>}}


### (5/25 | 24/156) RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning (Javad Rafiei Asl et al., 2024)

{{<citation>}}

Javad Rafiei Asl, Prajwal Panzade, Eduardo Blanco, Daniel Takabi, Zhipeng Cai. (2024)  
**RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning**
<br/>
<button class="copy-to-clipboard" title="RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning" index=24>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-LG, cs.CL  
Keyword Score: 60  
Keywords: Contrastive Learning, Self-supervised Learning, Sentence Embedding, Pre-trained Language Model, Pre-trained Language Model, Adversarial Attack  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11082v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11082v1.pdf" filename="2403.11082v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> have consistently demonstrated outstanding performance across a diverse spectrum of natural language processing tasks. Nevertheless, despite their success with unseen data, current <b>PLM-based</b> representations often exhibit poor robustness in <b>adversarial</b> <b>settings.</b> In this paper, we introduce RobustSentEmbed, a <b>self-supervised</b> <b>sentence</b> <b>embedding</b> framework designed to improve both generalization and robustness in diverse text representation tasks and against a diverse set of <b>adversarial</b> <b>attacks.</b> Through the generation of high-risk <b>adversarial</b> <b>perturbations</b> and their utilization in a novel objective function, RobustSentEmbed adeptly learns high-quality and robust <b>sentence</b> <b>embeddings.</b> Our experiments confirm the superiority of RobustSentEmbed over state-of-the-art representations. Specifically, Our framework achieves a significant reduction in the success rate of various <b>adversarial</b> <b>attacks,</b> notably reducing the BERTAttack success rate by almost half (from 75.51\% to 38.81\%). The framework also yields improvements of 1.59\% and 0.23\% in semantic textual similarity tasks and various transfer tasks, respectively.

{{</citation>}}


### (6/25 | 25/156) Correcting misinformation on social media with a large language model (Xinyi Zhou et al., 2024)

{{<citation>}}

Xinyi Zhou, Ashish Sharma, Amy X. Zhang, Tim Althoff. (2024)  
**Correcting misinformation on social media with a large language model**
<br/>
<button class="copy-to-clipboard" title="Correcting misinformation on social media with a large language model" index=25>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 56  
Keywords: Multi-modal, Multi-modal, GPT, GPT-4, Fact Verification, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11169v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11169v1.pdf" filename="2403.11169v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Misinformation undermines public trust in science and democracy, particularly on social media where inaccuracies can spread rapidly. Experts and laypeople have shown to be effective in correcting misinformation by manually identifying and explaining inaccuracies. Nevertheless, this approach is difficult to scale, a concern as technologies like <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> make misinformation easier to produce. <b>LLMs</b> also have versatile capabilities that could accelerate misinformation correction; however, they struggle due to a lack of recent information, a tendency to produce plausible but false content and references, and limitations in addressing <b>multimodal</b> information. To address these issues, we propose MUSE, an <b>LLM</b> augmented with access to and credibility evaluation of up-to-date information. By retrieving contextual evidence and refutations, MUSE can provide accurate and trustworthy explanations and references. It also describes visuals and conducts <b>multimodal</b> searches for correcting <b>multimodal</b> misinformation. We recruit <b>fact-checking</b> <b>and</b> journalism experts to evaluate corrections to real social media posts across 13 dimensions, ranging from the factuality of explanation to the relevance of references. The results demonstrate MUSE's ability to correct misinformation promptly after appearing on social media; overall, MUSE outperforms <b>GPT-4</b> by 37% and even high-quality corrections from laypeople by 29%. This work underscores the potential of <b>LLMs</b> to combat real-world misinformation effectively and efficiently.

{{</citation>}}


### (7/25 | 26/156) Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment (Feifan Song et al., 2024)

{{<citation>}}

Feifan Song, Bowen Yu, Hao Lang, Haiyang Yu, Fei Huang, Houfeng Wang, Yongbin Li. (2024)  
**Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment**
<br/>
<button class="copy-to-clipboard" title="Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment" index=26>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 50  
Keywords: Data Augmentation, Fine-tuning, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11124v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11124v1.pdf" filename="2403.11124v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Alignment with human preference prevents <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> from generating misleading or toxic content while requiring high-cost human feedback. Assuming resources of human annotation are limited, there are two different ways of allocating considered: more diverse <b>PROMPTS</b> or more diverse RESPONSES to be labeled. Nonetheless, a straightforward comparison between their impact is absent. In this work, we first control the diversity of both sides according to the number of samples for <b>fine-tuning,</b> which can directly reflect their influence. We find that instead of numerous <b>prompts,</b> more responses but fewer <b>prompts</b> better trigger <b>LLMs</b> for human alignment. Additionally, the concept of diversity for <b>prompts</b> can be more complex than responses that are typically quantified by single digits. Consequently, a new formulation of <b>prompt</b> diversity is proposed, further implying a linear correlation with the final performance of <b>LLMs</b> after <b>fine-tuning.</b> We also leverage it on <b>data</b> <b>augmentation</b> and conduct experiments to show its effect on different algorithms.

{{</citation>}}


### (8/25 | 27/156) ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models (Yuzhao Heng et al., 2024)

{{<citation>}}

Yuzhao Heng, Chunyuan Deng, Yitong Li, Yue Yu, Yinghao Li, Rongzhi Zhang, Chao Zhang. (2024)  
**ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models**
<br/>
<button class="copy-to-clipboard" title="ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models" index=27>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-LG, cs.CL  
Keyword Score: 50  
Keywords: Named Entity Recognition, Named Entity Recognition, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11103v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11103v1.pdf" filename="2403.11103v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Although <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as <b>named</b> <b>entity</b> <b>recognition</b> <b>(NER).</b> This paper explores an innovative, cost-efficient strategy to harness <b>LLMs</b> with modest <b>NER</b> capabilities for producing superior <b>NER</b> datasets. Our approach diverges from the basic class-conditional <b>prompts</b> by instructing <b>LLMs</b> to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data. Furthermore, we preemptively generate entity terms and then develop <b>NER</b> context data around these entities, effectively bypassing the <b>LLMs'</b> challenges with complex structures. Our experiments across both general and niche domains reveal significant performance enhancements over conventional data generation methods while being more cost-effective than existing alternatives.

{{</citation>}}


### (9/25 | 28/156) Enhancing Event Causality Identification with Rationale and Structure-Aware Causal Question Answering (Baiyan Zhang et al., 2024)

{{<citation>}}

Baiyan Zhang, Qin Chen, Jie Zhou, Jian Jin, Liang He. (2024)  
**Enhancing Event Causality Identification with Rationale and Structure-Aware Causal Question Answering**
<br/>
<button class="copy-to-clipboard" title="Enhancing Event Causality Identification with Rationale and Structure-Aware Causal Question Answering" index=28>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 46  
Keywords: Graph, Benchmarking, Question Answering, Reasoning, Large Language Model, Pre-trained Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11129v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11129v1.pdf" filename="2403.11129v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Document-level Event Causality Identification (DECI) aims to identify causal relations between two events in documents. Recent research tends to use <b>pre-trained</b> <b>language</b> <b>models</b> to generate the event causal relations. Whereas, these methods are prone to the errors of sequential generation due to multiple events in a document. Moreover, the potential structures such as event coreference and related causal chain are neglected. In this paper, we propose a multi-task learning framework to enhance event causality identification with rationale and structure-aware causal <b>question</b> <b>answering.</b> Specifically, the DECI task is transformed into multiple-choice <b>question</b> <b>answering,</b> and the causes and effects of the <b>questioned</b> <b>event</b> are generated with <b>large</b> <b>language</b> <b>models.</b> In addition, we generate the rationales to explain why these events have causal relations. Moreover, we construct an event structure <b>graph,</b> which models the multi-hop potential relations for causal <b>reasoning</b> of the current event. Experiments on two <b>benchmark</b> datasets show the great advantages of our proposed approach compared to the state-of-the-art methods. Moreover, we conduct both quantitative and qualitative analyses, which shed light on why each component of our approach can lead to great improvements.

{{</citation>}}


### (10/25 | 29/156) Cheap Ways of Extracting Clinical Markers from Texts (Anastasia Sandu et al., 2024)

{{<citation>}}

Anastasia Sandu, Teodor Mihailescu, Sergiu Nisioi. (2024)  
**Cheap Ways of Extracting Clinical Markers from Texts**
<br/>
<button class="copy-to-clipboard" title="Cheap Ways of Extracting Clinical Markers from Texts" index=29>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-LG, cs.CL  
Keyword Score: 40  
Keywords: Logistic Regression, Large Language Model, Large Language Model, TF-IDF  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11227v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11227v1.pdf" filename="2403.11227v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper describes the work of the UniBuc Archaeology team for CLPsych's 2024 Shared Task, which involved finding evidence within the text supporting the assigned suicide risk level. Two types of evidence were required: highlights (extracting relevant spans within the text) and summaries (aggregating evidence into a synthesis). Our work focuses on evaluating <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> as opposed to an alternative method that is much more memory and resource efficient. The first approach employs a good old-fashioned machine learning (GOML) pipeline consisting of a <b>tf-idf</b> vectorizer with a <b>logistic</b> <b>regression</b> classifier, whose representative features are used to extract relevant highlights. The second, more resource intensive, uses an <b>LLM</b> for generating the summaries and is guided by chain-of-thought to provide sequences of text indicating clinical markers.

{{</citation>}}


### (11/25 | 30/156) Granular Change Accuracy: A More Accurate Performance Metric for Dialogue State Tracking (Taha Aksu et al., 2024)

{{<citation>}}

Taha Aksu, Nancy F. Chen. (2024)  
**Granular Change Accuracy: A More Accurate Performance Metric for Dialogue State Tracking**
<br/>
<button class="copy-to-clipboard" title="Granular Change Accuracy: A More Accurate Performance Metric for Dialogue State Tracking" index=30>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 36  
Keywords: Benchmarking, Benchmarking, Few-shot, Zero-shot, Dialogue State Tracking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11123v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11123v1.pdf" filename="2403.11123v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Current metrics for evaluating <b>Dialogue</b> <b>State</b> <b>Tracking</b> (DST) systems exhibit three primary limitations. They: i) erroneously presume a uniform distribution of slots throughout the dialog, ii) neglect to assign partial scores for individual turns, iii) frequently overestimate or underestimate performance by repeatedly counting the models' successful or failed predictions. To address these shortcomings, we introduce a novel metric: Granular Change Accuracy (GCA). GCA focuses on evaluating the predicted changes in <b>dialogue</b> <b>state</b> <b>over</b> the entire <b>dialogue</b> <b>history.</b> <b>Benchmarking</b> reveals that GCA effectively reduces biases arising from distribution uniformity and the positioning of errors across turns, resulting in a more precise evaluation. Notably, we find that these biases are particularly pronounced when evaluating <b>few-shot</b> or <b>zero-shot</b> trained models, becoming even more evident as the model's error rate increases. Hence, GCA offers significant promise, particularly for assessing models trained with limited resources. Our GCA implementation is a useful addition to the pool of DST metrics.

{{</citation>}}


### (12/25 | 31/156) StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows (Yiran Wu et al., 2024)

{{<citation>}}

Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, Qingyun Wu. (2024)  
**StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows**
<br/>
<button class="copy-to-clipboard" title="StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows" index=31>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 33  
Keywords: Benchmarking, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11322v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11322v1.pdf" filename="2403.11322v1.pdf">Download PDF</button>

---


**ABSTRACT**  
It is a notable trend to use <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments. In this paper, we propose StateFlow, a novel <b>LLM-based</b> task-solving paradigm that conceptualizes complex task-solving processes backed by <b>LLMs</b> as state machines. With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of <b>LLMs'</b> responses throughout the task-solving process. Within each state, StateFlow allows execution of a series of actions, involving not only the generation of <b>LLM's</b> responses guided by a specific <b>prompt,</b> but also the utilization of external tools as needed. State transitions are controlled by specific rules or decisions made by the <b>LLM,</b> allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model. Evaluations on the InterCode SQL and Bash <b>benchmarks</b> show that StateFlow significantly enhances <b>LLMs'</b> efficiency.

{{</citation>}}


### (13/25 | 32/156) ChartThinker: A Contextual Chain-of-Thought Approach to Optimized Chart Summarization (Mengsha Liu et al., 2024)

{{<citation>}}

Mengsha Liu, Daoyuan Chen, Yaliang Li, Guian Fang, Ying Shen. (2024)  
**ChartThinker: A Contextual Chain-of-Thought Approach to Optimized Chart Summarization**
<br/>
<button class="copy-to-clipboard" title="ChartThinker: A Contextual Chain-of-Thought Approach to Optimized Chart Summarization" index=32>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 30  
Keywords: Fine-tuning, Reasoning, Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11236v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11236v1.pdf" filename="2403.11236v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Data visualization serves as a critical means for presenting data and mining its valuable insights. The task of chart <b>summarization,</b> through natural language processing techniques, facilitates in-depth data analysis of charts. However, there still are notable deficiencies in terms of visual-language matching and <b>reasoning</b> ability for existing approaches. To address these limitations, this study constructs a large-scale dataset of comprehensive chart-caption pairs and <b>fine-tuning</b> instructions on each chart. Thanks to the broad coverage of various topics and visual styles within this dataset, better matching degree can be achieved from the view of training data. Moreover, we propose an innovative chart <b>summarization</b> method, ChartThinker, which synthesizes deep analysis based on chains of thought and strategies of context retrieval, aiming to improve the logical coherence and accuracy of the generated summaries. Built upon the curated datasets, our trained model consistently exhibits superior performance in chart <b>summarization</b> tasks, surpassing 8 state-of-the-art models over 7 evaluation metrics. Our dataset and codes are publicly accessible.

{{</citation>}}


### (14/25 | 33/156) Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants' API Invocation Capabilities (Honglin Mu et al., 2024)

{{<citation>}}

Honglin Mu, Yang Xu, Yunlong Feng, Xiaofeng Han, Yitong Li, Yutai Hou, Wanxiang Che. (2024)  
**Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants' API Invocation Capabilities**
<br/>
<button class="copy-to-clipboard" title="Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants' API Invocation Capabilities" index=33>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 23  
Keywords: Benchmarking, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11128v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11128v1.pdf" filename="2403.11128v1.pdf">Download PDF</button>

---


**ABSTRACT**  
With the rise of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> AI assistants' ability to utilize tools, especially through API calls, has advanced notably. This progress has necessitated more accurate evaluation methods. Many existing studies adopt static evaluation, where they assess AI assistants' API call based on pre-defined dialogue histories. However, such evaluation method can be misleading, as an AI assistant might fail in generating API calls from preceding human interaction in real cases. Instead of the resource-intensive method of direct human-machine interactions, we propose Automated Dynamic Evaluation (AutoDE) to assess an assistant's API call capability without human involvement. In our framework, we endeavor to closely mirror genuine human conversation patterns in human-machine interactions, using a <b>LLM-based</b> user agent, equipped with a user script to ensure human alignment. Experimental results highlight that AutoDE uncovers errors overlooked by static evaluations, aligning more closely with human assessment. Testing four AI assistants using our crafted <b>benchmark,</b> our method mirrored human evaluation with an correlation of 0.99, marking an 8% enhancement compared to conventional static evaluations.

{{</citation>}}


### (15/25 | 34/156) Lost in Translation? Translation Errors and Challenges for Fair Assessment of Text-to-Image Models on Multilingual Concepts (Michael Saxon et al., 2024)

{{<citation>}}

Michael Saxon, Yiran Luo, Sharon Levy, Chitta Baral, Yezhou Yang, William Yang Wang. (2024)  
**Lost in Translation? Translation Errors and Challenges for Fair Assessment of Text-to-Image Models on Multilingual Concepts**
<br/>
<button class="copy-to-clipboard" title="Lost in Translation? Translation Errors and Challenges for Fair Assessment of Text-to-Image Models on Multilingual Concepts" index=34>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-CV, cs-CY, cs.CL, eess-IV  
Keyword Score: 23  
Keywords: Benchmarking, Text2image, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11092v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11092v1.pdf" filename="2403.11092v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Benchmarks</b> of the multilingual capabilities of <b>text-to-image</b> (T2I) models compare generated images <b>prompted</b> in a test language to an expected image distribution over a concept set. One such <b>benchmark,</b> "Conceptual Coverage Across Languages" (CoCo-CroLa), assesses the tangible noun inventory of T2I models by <b>prompting</b> them to generate pictures from a concept list translated to seven languages and comparing the output image populations. Unfortunately, we find that this <b>benchmark</b> contains translation errors of varying severity in Spanish, Japanese, and Chinese. We provide corrections for these errors and analyze how impactful they are on the utility and validity of CoCo-CroLa as a <b>benchmark.</b> We reassess multiple baseline T2I models with the revisions, compare the outputs elicited under the new translations to those conditioned on the old, and show that a correction's impactfulness on the image-domain <b>benchmark</b> results can be predicted in the text domain with similarity scores. Our findings will guide the future development of T2I multilinguality metrics by providing analytical tools for practical translation decisions.

{{</citation>}}


### (16/25 | 35/156) What Makes Math Word Problems Challenging for LLMs? (KV Aditya Srivatsa et al., 2024)

{{<citation>}}

KV Aditya Srivatsa, Ekaterina Kochmar. (2024)  
**What Makes Math Word Problems Challenging for LLMs?**
<br/>
<button class="copy-to-clipboard" title="What Makes Math Word Problems Challenging for LLMs?" index=35>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11369v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11369v1.pdf" filename="2403.11369v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper investigates the question of what makes math word problems (MWPs) challenging for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> We conduct an in-depth analysis of the key linguistic and mathematical characteristics of MWPs. In addition, we train feature-based classifiers to better understand the impact of each feature on the overall difficulty of MWPs for prominent <b>LLMs</b> and investigate whether this helps predict how well <b>LLMs</b> fare against specific categories of MWPs.

{{</citation>}}


### (17/25 | 36/156) A Modified Word Saliency-Based Adversarial Attack on Text Classification Models (Hetvi Waghela et al., 2024)

{{<citation>}}

Hetvi Waghela, Sneha Rakshit, Jaydip Sen. (2024)  
**A Modified Word Saliency-Based Adversarial Attack on Text Classification Models**
<br/>
<button class="copy-to-clipboard" title="A Modified Word Saliency-Based Adversarial Attack on Text Classification Models" index=36>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-CR, cs-LG, cs.CL  
Keyword Score: 20  
Keywords: Text Classification, Adversarial Attack  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11297v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11297v1.pdf" filename="2403.11297v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper introduces a novel <b>adversarial</b> <b>attack</b> method targeting <b>text</b> <b>classification</b> models, termed the Modified Word Saliency-based <b>Adversarial</b> <b>At-tack</b> (MWSAA). The technique builds upon the concept of word saliency to strategically perturb input <b>texts,</b> <b>aiming</b> to mislead classification models while preserving semantic coherence. By refining the traditional <b>adversarial</b> <b>attack</b> approach, MWSAA significantly enhances its efficacy in evading detection by classification systems. The methodology involves first identifying salient words in the input <b>text</b> <b>through</b> a saliency estimation process, which prioritizes words most influential to the model's decision-making process. Subsequently, these salient words are subjected to carefully crafted modifications, guided by semantic similarity metrics to ensure that the altered <b>text</b> <b>remains</b> coherent and retains its original meaning. Empirical evaluations conducted on diverse <b>text</b> <b>classification</b> datasets demonstrate the effectiveness of the proposed method in generating <b>adversarial</b> <b>examples</b> capable of successfully deceiving state-of-the-art classification models. Comparative analyses with existing <b>adversarial</b> <b>attack</b> techniques further indicate the superiority of the proposed approach in terms of both attack success rate and preservation of <b>text</b> <b>coherence.</b>

{{</citation>}}


### (18/25 | 37/156) Creating an African American-Sounding TTS: Guidelines, Technical Challenges,and Surprising Evaluations (Claudio Pinhanez et al., 2024)

{{<citation>}}

Claudio Pinhanez, Raul Fernandez, Marcelo Grave, Julio Nogima, Ron Hoory. (2024)  
**Creating an African American-Sounding TTS: Guidelines, Technical Challenges,and Surprising Evaluations**
<br/>
<button class="copy-to-clipboard" title="Creating an African American-Sounding TTS: Guidelines, Technical Challenges,and Surprising Evaluations" index=37>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-HC, cs.CL  
Keyword Score: 20  
Keywords: Text-to-speech, Text-to-speech  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11209v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11209v1.pdf" filename="2403.11209v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Representations of AI agents in user interfaces and robotics are predominantly White, not only in terms of facial and skin features, but also in the synthetic voices they use. In this paper we explore some unexpected challenges in the representation of race we found in the process of developing an U.S. English <b>Text-to-Speech</b> <b>(TTS)</b> system aimed to sound like an educated, professional, regional accent-free African American woman. The paper starts by presenting the results of focus groups with African American IT professionals where guidelines and challenges for the creation of a representative and appropriate <b>TTS</b> system were discussed and gathered, followed by a discussion about some of the technical difficulties faced by the <b>TTS</b> system developers. We then describe two studies with U.S. English speakers where the participants were not able to attribute the correct race to the African American <b>TTS</b> voice while overwhelmingly correctly recognizing the race of a White <b>TTS</b> system of similar quality. A focus group with African American IT workers not only confirmed the representativeness of the African American voice we built, but also suggested that the surprising recognition results may have been caused by the inability or the latent prejudice from non-African Americans to associate educated, non-vernacular, professionally-sounding voices to African American people.

{{</citation>}}


### (19/25 | 38/156) Decoding Continuous Character-based Language from Non-invasive Brain Recordings (Cenyuan Zhang et al., 2024)

{{<citation>}}

Cenyuan Zhang, Xiaoqing Zheng, Ruicheng Yin, Shujie Geng, Jianhan Xu, Xuan Gao, Changze Lv, Zixuan Ling, Xuanjing Huang, Miao Cao, Jianfeng Feng. (2024)  
**Decoding Continuous Character-based Language from Non-invasive Brain Recordings**
<br/>
<button class="copy-to-clipboard" title="Decoding Continuous Character-based Language from Non-invasive Brain Recordings" index=38>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 20  
Keywords: Convolution, Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11183v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11183v1.pdf" filename="2403.11183v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Deciphering natural language from brain activity through non-invasive devices remains a formidable challenge. Previous non-invasive decoders either require multiple experiments with identical stimuli to pinpoint cortical regions and enhance signal-to-noise ratios in brain activity, or they are limited to discerning basic linguistic elements such as letters and words. We propose a novel approach to decoding continuous language from single-trial non-invasive fMRI recordings, in which a three-dimensional <b>convolutional</b> <b>network</b> augmented with information bottleneck is developed to automatically identify responsive voxels to stimuli, and a character-based decoder is designed for the semantic reconstruction of continuous language characterized by inherent character structures. The resulting decoder can produce intelligible textual sequences that faithfully capture the meaning of perceived speech both within and across subjects, while existing decoders exhibit significantly inferior performance in cross-subject contexts. The ability to decode continuous language from single trials across subjects demonstrates the promising applications of non-invasive language brain-computer interfaces in both healthcare and neuroscience.

{{</citation>}}


### (20/25 | 39/156) Evaluation Ethics of LLMs in Legal Domain (Ruizhe Zhang et al., 2024)

{{<citation>}}

Ruizhe Zhang, Haitao Li, Yueyue Wu, Qingyao Ai, Yiqun Liu, Min Zhang, Shaoping Ma. (2024)  
**Evaluation Ethics of LLMs in Legal Domain**
<br/>
<button class="copy-to-clipboard" title="Evaluation Ethics of LLMs in Legal Domain" index=39>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11152v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11152v1.pdf" filename="2403.11152v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In recent years, the utilization of <b>large</b> <b>language</b> <b>models</b> for natural language dialogue has gained momentum, leading to their widespread adoption across various domains. However, their universal competence in addressing challenges specific to specialized fields such as law remains a subject of scrutiny. The incorporation of legal ethics into the model has been overlooked by researchers. We asserts that rigorous ethic evaluation is essential to ensure the effective integration of <b>large</b> <b>language</b> <b>models</b> in legal domains, emphasizing the need to assess domain-specific proficiency and domain-specific ethic. To address this, we propose a novelty evaluation methodology, utilizing authentic legal cases to evaluate the fundamental language abilities, specialized legal knowledge and legal robustness of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> The findings from our comprehensive evaluation contribute significantly to the academic discourse surrounding the suitability and performance of <b>large</b> <b>language</b> <b>models</b> in legal domains.

{{</citation>}}


### (21/25 | 40/156) A Challenge Dataset and Effective Models for Conversational Stance Detection (Fuqiang Niu et al., 2024)

{{<citation>}}

Fuqiang Niu, Min Yang, Ang Li, Baoquan Zhang, Xiaojiang Peng, Bowen Zhang. (2024)  
**A Challenge Dataset and Effective Models for Conversational Stance Detection**
<br/>
<button class="copy-to-clipboard" title="A Challenge Dataset and Effective Models for Conversational Stance Detection" index=40>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 20  
Keywords: Neural Machine Translation, Stance Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11145v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11145v1.pdf" filename="2403.11145v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Previous <b>stance</b> <b>detection</b> studies typically concentrate on evaluating <b>stances</b> <b>within</b> individual instances, thereby exhibiting limitations in effectively modeling multi-party discussions concerning the same specific topic, as naturally transpire in authentic social media interactions. This constraint arises primarily due to the scarcity of datasets that authentically replicate real social media contexts, hindering the research progress of conversational <b>stance</b> <b>detection.</b> In this paper, we introduce a new multi-turn conversation <b>stance</b> <b>detection</b> dataset (called \textbf{MT-CSD}), which encompasses multiple targets for conversational <b>stance</b> <b>detection.</b> To derive <b>stances</b> <b>from</b> this challenging dataset, we propose a global-local attention network (\textbf{GLAN}) to address both long and short-range dependencies inherent in conversational data. Notably, even state-of-the-art <b>stance</b> <b>detection</b> methods, exemplified by GLAN, exhibit an accuracy of only 50.47\%, highlighting the persistent challenges in conversational <b>stance</b> <b>detection.</b> Furthermore, our <b>MT-CSD</b> dataset serves as a valuable resource to catalyze advancements in cross-domain <b>stance</b> <b>detection,</b> where a classifier is adapted from a different yet related target. We believe that <b>MT-CSD</b> will contribute to advancing real-world applications of <b>stance</b> <b>detection</b> research. Our source code, data, and models are available at \url{https://github.com/nfq729/MT-CSD}.

{{</citation>}}


### (22/25 | 41/156) Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback (Dong Won Lee et al., 2024)

{{<citation>}}

Dong Won Lee, Hae Won Park, Yoon Kim, Cynthia Breazeal, Louis-Philippe Morency. (2024)  
**Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback**
<br/>
<button class="copy-to-clipboard" title="Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback" index=41>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-HC, cs-LG, cs.CL  
Keyword Score: 16  
Keywords: Multi-modal, Multi-modal, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11330v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11330v1.pdf" filename="2403.11330v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We describe an approach for aligning an <b>LLM-based</b> dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring <b>multimodal</b> signals. At a high level, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI} <b>multimodal</b> reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the standard RHLF pipeline improve an <b>LLM-based</b> dialog agent. We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods.

{{</citation>}}


### (23/25 | 42/156) HarmPot: An Annotation Framework for Evaluating Offline Harm Potential of Social Media Text (Ritesh Kumar et al., 2024)

{{<citation>}}

Ritesh Kumar, Ojaswee Bhalla, Madhu Vanthi, Shehlat Maknoon Wani, Siddharth Singh. (2024)  
**HarmPot: An Annotation Framework for Evaluating Offline Harm Potential of Social Media Text**
<br/>
<button class="copy-to-clipboard" title="HarmPot: An Annotation Framework for Evaluating Offline Harm Potential of Social Media Text" index=42>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 10  
Keywords: Grounding  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11108v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11108v1.pdf" filename="2403.11108v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we discuss the development of an annotation schema to build datasets for evaluating the offline harm potential of social media texts. We define "harm potential" as the potential for an online public post to cause real-world physical harm (i.e., violence). Understanding that real-world violence is often spurred by a web of triggers, often combining several online tactics and pre-existing intersectional fissures in the social milieu, to result in targeted physical violence, we do not focus on any single divisive aspect (i.e., caste, gender, religion, or other identities of the victim and perpetrators) nor do we focus on just hate speech or mis/dis-information. Rather, our understanding of the intersectional causes of such triggers focuses our attempt at measuring the harm potential of online content, irrespective of whether it is hateful or not. In this paper, we discuss the development of a framework/annotation schema that allows annotating the data with different aspects of the text including its socio-political <b>grounding</b> and intent of the speaker (as expressed through mood and modality) that together contribute to it being a trigger for offline harm. We also give a comparative analysis and mapping of our framework with some of the existing frameworks.

{{</citation>}}


### (24/25 | 43/156) Deep Learning-based Sentiment Analysis in Persian Language (Mohammad Heydari et al., 2024)

{{<citation>}}

Mohammad Heydari, Mohsen Khazeni, Mohammad Ali Soltanshahi. (2024)  
**Deep Learning-based Sentiment Analysis in Persian Language**
<br/>
<button class="copy-to-clipboard" title="Deep Learning-based Sentiment Analysis in Persian Language" index=43>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 10  
Keywords: Sentiment Analysis  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11069v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11069v1.pdf" filename="2403.11069v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recently, there has been a growing interest in the use of deep learning techniques for tasks in natural language processing (NLP), with <b>sentiment</b> <b>analysis</b> being one of the most challenging areas, particularly in the Persian language. The vast amounts of content generated by Persian users on thousands of websites, blogs, and social networks such as Telegram, Instagram, and Twitter present a rich resource of information. Deep learning techniques have become increasingly favored for extracting insights from this extensive pool of raw data, although they face several challenges. In this study, we introduced and implemented a hybrid deep learning-based model for <b>sentiment</b> <b>analysis,</b> using customer review data from the Digikala Online Retailer website. We employed a variety of deep learning networks and regularization techniques as classifiers. Ultimately, our hybrid approach yielded an impressive performance, achieving an F1 score of 78.3 across three <b>sentiment</b> <b>categories:</b> positive, negative, and neutral.

{{</citation>}}


### (25/25 | 44/156) TRELM: Towards Robust and Efficient Pre-training for Knowledge-Enhanced Language Models (Junbing Yan et al., 2024)

{{<citation>}}

Junbing Yan, Chengyu Wang, Taolin Zhang, Xiaofeng He, Jun Huang, Longtao Huang, Hui Xue, Wei Zhang. (2024)  
**TRELM: Towards Robust and Efficient Pre-training for Knowledge-Enhanced Language Models**
<br/>
<button class="copy-to-clipboard" title="TRELM: Towards Robust and Efficient Pre-training for Knowledge-Enhanced Language Models" index=44>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 8  
Keywords: Graph, Knowledge Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11203v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11203v1.pdf" filename="2403.11203v1.pdf">Download PDF</button>

---


**ABSTRACT**  
KEPLMs are pre-trained models that utilize external <b>knowledge</b> <b>to</b> enhance language understanding. Previous language models facilitated <b>knowledge</b> <b>acquisition</b> by incorporating <b>knowledge-related</b> <b>pre-training</b> tasks learned from relation triples in <b>knowledge</b> <b>graphs.</b> However, these models do not prioritize learning embeddings for entity-related tokens. Moreover, updating the entire set of parameters in KEPLMs is computationally demanding. This paper introduces TRELM, a Robust and Efficient Pre-training framework for <b>Knowledge-Enhanced</b> <b>Language</b> Models. We observe that entities in text corpora usually follow the long-tail distribution, where the representations of some entities are suboptimally optimized and hinder the pre-training process for KEPLMs. To tackle this, we employ a robust approach to inject <b>knowledge</b> <b>triples</b> and employ a <b>knowledge-augmented</b> <b>memory</b> bank to capture valuable information. Furthermore, updating a small subset of neurons in the feed-forward networks (FFNs) that store factual <b>knowledge</b> <b>is</b> both sufficient and efficient. Specifically, we utilize dynamic <b>knowledge</b> <b>routing</b> to identify <b>knowledge</b> <b>paths</b> in FFNs and selectively update parameters during pre-training. Experimental results show that TRELM reduces pre-training time by at least 50% and outperforms other KEPLMs in <b>knowledge</b> <b>probing</b> tasks and multiple <b>knowledge-aware</b> <b>language</b> understanding tasks.

{{</citation>}}


## cs.IR (2)



### (1/2 | 45/156) ConvSDG: Session Data Generation for Conversational Search (Fengran Mo et al., 2024)

{{<citation>}}

Fengran Mo, Bole Yi, Kelong Mao, Chen Qu, Kaiyu Huang, Jian-Yun Nie. (2024)  
**ConvSDG: Session Data Generation for Conversational Search**
<br/>
<button class="copy-to-clipboard" title="ConvSDG: Session Data Generation for Conversational Search" index=45>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-CL, cs-IR, cs.IR  
Keyword Score: 80  
Keywords: Dense Retrieval, Fine-tuning, Fine-tuning, Semi-Supervised Learning, Unsupervised Learning, Text Generation, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11335v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11335v1.pdf" filename="2403.11335v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Conversational search provides a more convenient interface for users to search by allowing multi-turn interaction with the search engine. However, the effectiveness of the conversational <b>dense</b> <b>retrieval</b> methods is limited by the scarcity of training data required for their <b>fine-tuning.</b> Thus, generating more training conversational sessions with relevant labels could potentially improve search performance. Based on the promising capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> on <b>text</b> <b>generation,</b> we propose ConvSDG, a simple yet effective framework to explore the feasibility of boosting conversational search by using <b>LLM</b> for session data generation. Within this framework, we design dialogue/session-level and query-level data generation with <b>unsupervised</b> and <b>semi-supervised</b> <b>learning,</b> according to the availability of relevance judgments. The generated data are used to <b>fine-tune</b> the conversational <b>dense</b> <b>retriever.</b> Extensive experiments on four widely used datasets demonstrate the effectiveness and broad applicability of our ConvSDG framework compared with several strong baselines.

{{</citation>}}


### (2/2 | 46/156) Is Contrastive Learning Necessary? A Study of Data Augmentation vs Contrastive Learning in Sequential Recommendation (Peilin Zhou et al., 2024)

{{<citation>}}

Peilin Zhou, You-Liang Huang, Yueqi Xie, Jingqi Gao, Shoujin Wang, Jae Boum Kim, Sunghun Kim. (2024)  
**Is Contrastive Learning Necessary? A Study of Data Augmentation vs Contrastive Learning in Sequential Recommendation**
<br/>
<button class="copy-to-clipboard" title="Is Contrastive Learning Necessary? A Study of Data Augmentation vs Contrastive Learning in Sequential Recommendation" index=46>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-IR, cs.IR  
Keyword Score: 53  
Keywords: Benchmarking, Contrastive Learning, Data Augmentation, Recommendation, Recommender System, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11136v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11136v1.pdf" filename="2403.11136v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Sequential <b>recommender</b> <b>systems</b> (SRS) are designed to predict users' future behaviors based on their historical interaction <b>data.</b> <b>Recent</b> research has increasingly utilized <b>contrastive</b> <b>learning</b> (CL) to leverage <b>unsupervised</b> signals to alleviate the <b>data</b> <b>sparsity</b> issue in SRS. In general, CL-based SRS first augments the raw sequential interaction <b>data</b> <b>by</b> using <b>data</b> <b>augmentation</b> strategies and employs a <b>contrastive</b> <b>training</b> scheme to enforce the representations of those sequences from the same raw interaction <b>data</b> <b>to</b> be similar. Despite the growing popularity of CL, <b>data</b> <b>augmentation,</b> as a basic component of CL, has not received sufficient attention. This raises the question: Is it possible to achieve superior <b>recommendation</b> results solely through <b>data</b> <b>augmentation?</b> To answer this question, we <b>benchmark</b> eight widely used <b>data</b> <b>augmentation</b> strategies, as well as state-of-the-art CL-based SRS methods, on four real-world datasets under both warm- and cold-start settings. Intriguingly, the conclusion drawn from our study is that, certain <b>data</b> <b>augmentation</b> strategies can achieve similar or even superior performance compared with some CL-based methods, demonstrating the potential to significantly alleviate the <b>data</b> <b>sparsity</b> issue with fewer computational overhead. We hope that our study can further inspire more fundamental studies on the key functional components of complex CL techniques. Our processed datasets and codes are available at https://github.com/AIM-SE/DA4Rec.

{{</citation>}}


## cs.RO (8)



### (1/8 | 47/156) ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models (Siyuan Huang et al., 2024)

{{<citation>}}

Siyuan Huang, Iaroslav Ponomarenko, Zhengkai Jiang, Xiaoqi Li, Xiaobin Hu, Peng Gao, Hongsheng Li, Hao Dong. (2024)  
**ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models**
<br/>
<button class="copy-to-clipboard" title="ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models" index=47>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 79  
Keywords: Object Detection, Benchmarking, Fine-tuning, Multi-modal, Multi-modal, Image2text, Question Answering, Visual Question Answering, Visual Question Answering, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11289v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11289v1.pdf" filename="2403.11289v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The integration of <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) with robotic systems has significantly enhanced the ability of robots to interpret and act upon natural language instructions. Despite these advancements, conventional MLLMs are typically trained on generic <b>image-text</b> pairs, lacking essential robotics knowledge such as affordances and physical knowledge, which hampers their efficacy in manipulation tasks. To bridge this gap, we introduce ManipVQA, a novel framework designed to endow MLLMs with Manipulation-centric knowledge through a <b>Visual</b> <b>Question-Answering</b> <b>format.</b> This approach not only encompasses tool detection and affordance recognition but also extends to a comprehensive understanding of physical concepts. Our approach starts with collecting a varied set of images displaying interactive <b>objects,</b> <b>which</b> presents a broad range of challenges in tool <b>object</b> <b>detection,</b> affordance, and physical concept predictions. To seamlessly integrate this robotic-specific knowledge with the inherent vision-reasoning capabilities of MLLMs, we adopt a unified <b>VQA</b> format and devise a <b>fine-tuning</b> strategy that preserves the original vision-reasoning abilities while incorporating the new robotic insights. Empirical evaluations conducted in robotic simulators and across various vision task <b>benchmarks</b> demonstrate the robust performance of ManipVQA. Code and dataset will be made publicly available at https://github.com/SiyuanHuang95/ManipVQA.

{{</citation>}}


### (2/8 | 48/156) Driving Style Alignment for LLM-powered Driver Agent (Ruoxuan Yang et al., 2024)

{{<citation>}}

Ruoxuan Yang, Xinyue Zhang, Anais Fernandez-Laaksonen, Xin Ding, Jiangtao Gong. (2024)  
**Driving Style Alignment for LLM-powered Driver Agent**
<br/>
<button class="copy-to-clipboard" title="Driving Style Alignment for LLM-powered Driver Agent" index=48>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: 68T42, cs-AI, cs-RO, cs.RO  
Keyword Score: 40  
Keywords: Simulation, Simulator, Reasoning, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11368v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11368v1.pdf" filename="2403.11368v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recently, <b>LLM-powered</b> driver agents have demonstrated considerable potential in the field of autonomous driving, showcasing human-like <b>reasoning</b> and decision-making abilities.However, current research on aligning driver agent behaviors with human driving styles remains limited, partly due to the scarcity of high-quality natural language data from human driving behaviors.To address this research gap, we propose a multi-alignment framework designed to align driver agents with human driving styles through demonstrations and feedback. Notably, we construct a natural language dataset of human driver behaviors through naturalistic driving experiments and post-driving interviews, offering high-quality human demonstrations for <b>LLM</b> alignment. The framework's effectiveness is validated through <b>simulation</b> experiments in the CARLA urban traffic simulator and further corroborated by human evaluations. Our research offers valuable insights into designing driving agents with diverse driving styles.The implementation of the framework and details of the dataset can be found at the link.

{{</citation>}}


### (3/8 | 49/156) Leveraging Simulation-Based Model Preconditions for Fast Action Parameter Optimization with Multiple Models (M. Yunus Seker et al., 2024)

{{<citation>}}

M. Yunus Seker, Oliver Kroemer. (2024)  
**Leveraging Simulation-Based Model Preconditions for Fast Action Parameter Optimization with Multiple Models**
<br/>
<button class="copy-to-clipboard" title="Leveraging Simulation-Based Model Preconditions for Fast Action Parameter Optimization with Multiple Models" index=49>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 30  
Keywords: Fine-tuning, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11313v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11313v1.pdf" filename="2403.11313v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Optimizing robotic action parameters is a significant challenge for manipulation tasks that demand high levels of precision and generalization. Using a model-based approach, the robot must quickly reason about the outcomes of different actions using a predictive model to find a set of parameters that will have the desired effect. The model may need to capture the behaviors of rigid and deformable objects, as well as objects of various shapes and sizes. Predictive models often need to trade-off speed for prediction accuracy and generalization. This paper proposes a framework that leverages the strengths of multiple predictive models, including analytical, learned, and <b>simulation-based</b> models, to enhance the efficiency and accuracy of action parameter optimization. Our approach uses Model Deviation Estimators (MDEs) to determine the most suitable predictive model for any given state-action parameters, allowing the robot to select models to make fast and precise predictions. We extend the MDE framework by not only learning sim-to-real MDEs, but also sim-to-sim MDEs. Our experiments show that these sim-to-sim MDEs provide significantly faster parameter optimization as well as a basis for efficiently learning sim-to-real MDEs through <b>finetuning.</b> The ease of collecting sim-to-sim training data also allows the robot to learn MDEs based directly on visual inputs and local material properties.

{{</citation>}}


### (4/8 | 50/156) Continuous Jumping of a Parallel Wire-Driven Monopedal Robot RAMIEL Using Reinforcement Learning (Kento Kawaharazuka et al., 2024)

{{<citation>}}

Kento Kawaharazuka, Temma Suzuki, Kei Okada, Masayuki Inaba. (2024)  
**Continuous Jumping of a Parallel Wire-Driven Monopedal Robot RAMIEL Using Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="Continuous Jumping of a Parallel Wire-Driven Monopedal Robot RAMIEL Using Reinforcement Learning" index=50>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 30  
Keywords: Reinforcement Learning, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11205v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11205v1.pdf" filename="2403.11205v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We have developed a parallel wire-driven monopedal robot, RAMIEL, which has both speed and power due to the parallel wire mechanism and a long acceleration distance. RAMIEL is capable of jumping high and continuously, and so has high performance in traveling. On the other hand, one of the drawbacks of a minimal parallel wire-driven robot without joint encoders is that the current joint velocities estimated from the wire lengths oscillate due to the elongation of the wires, making the values unreliable. Therefore, despite its high performance, the control of the robot is unstable, and in 10 out of 16 jumps, the robot could only jump up to two times continuously. In this study, we propose a method to realize a continuous jumping motion by <b>reinforcement</b> <b>learning</b> in <b>simulation,</b> and its application to the actual robot. Because the joint velocities oscillate with the elongation of the wires, they are not used directly, but instead are inferred from the time series of joint angles. At the same time, noise that imitates the vibration caused by the elongation of the wires is added for transfer to the actual robot. The results show that the system can be applied to the actual robot RAMIEL as well as to the stable continuous jumping motion in <b>simulation.</b>

{{</citation>}}


### (5/8 | 51/156) Hybrid Feedback for Three-dimensional Convex Obstacle Avoidance (Mayur Sawant et al., 2024)

{{<citation>}}

Mayur Sawant, Ilia Polushin, Abdelhamid Tayebi. (2024)  
**Hybrid Feedback for Three-dimensional Convex Obstacle Avoidance**
<br/>
<button class="copy-to-clipboard" title="Hybrid Feedback for Three-dimensional Convex Obstacle Avoidance" index=51>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs-SY, cs.RO, eess-SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11279v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11279v1.pdf" filename="2403.11279v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We propose a hybrid feedback control scheme for the autonomous robot navigation problem in three-dimensional environments with arbitrarily-shaped convex obstacles. The proposed hybrid control strategy, which consists in switching between the move-to-target mode and the obstacle-avoidance mode, guarantees global asymptotic stability of the target location in the obstacle-free workspace. We also provide a procedure for the implementation of the proposed hybrid controller in a priori unknown environments and validate its effectiveness through <b>simulation</b> results.

{{</citation>}}


### (6/8 | 52/156) PyroTrack: Belief-Based Deep Reinforcement Learning Path Planning for Aerial Wildfire Monitoring in Partially Observable Environments (Sahand Khoshdel et al., 2024)

{{<citation>}}

Sahand Khoshdel, Qi Luo, Fatemeh Afghah. (2024)  
**PyroTrack: Belief-Based Deep Reinforcement Learning Path Planning for Aerial Wildfire Monitoring in Partially Observable Environments**
<br/>
<button class="copy-to-clipboard" title="PyroTrack: Belief-Based Deep Reinforcement Learning Path Planning for Aerial Wildfire Monitoring in Partially Observable Environments" index=52>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: 68T40, cs-RO, cs-SY, cs.RO, eess-SY  
Keyword Score: 20  
Keywords: Markov Decision Process, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11095v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11095v1.pdf" filename="2403.11095v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Motivated by agility, 3D mobility, and low-risk operation compared to human-operated management systems of autonomous unmanned aerial vehicles (UAVs), this work studies UAV-based active wildfire monitoring where a UAV detects fire incidents in remote areas and tracks the fire frontline. A UAV path planning solution is proposed considering realistic wildfire management missions, where a single low-altitude drone with limited power and flight time is available. Noting the limited field of view of commercial low-altitude UAVs, the problem formulates as a partially observable <b>Markov</b> <b>decision</b> <b>process</b> (POMDP), in which wildfire progression outside the field of view causes inaccurate state representation that prevents the UAV from finding the optimal path to track the fire front in limited time. Common deep <b>reinforcement</b> <b>learning</b> (DRL)-based trajectory planning solutions require diverse drone-recorded wildfire data to generalize pre-trained models to real-time systems, which is not currently available at a diverse and standard scale. To narrow down the gap caused by partial observability in the space of possible policies, a belief-based state representation with broad, extensive simulated data is proposed where the beliefs (i.e., ignition probabilities of different grid areas) are updated using a Bayesian framework for the cells within the field of view. The performance of the proposed solution in terms of the ratio of detected fire cells and monitored ignited area (MIA) is evaluated in a complex fire scenario with multiple rapidly growing fire batches, indicating that the belief state representation outperforms the observation state representation both in fire coverage and the distance to fire frontline.

{{</citation>}}


### (7/8 | 53/156) Learning-Based Wiping Behavior of Low-Rigidity Robots Considering Various Surface Materials and Task Definitions (Kento Kawaharazuka et al., 2024)

{{<citation>}}

Kento Kawaharazuka, Naoaki Kanazawa, Kei Okada, Masayuki Inaba. (2024)  
**Learning-Based Wiping Behavior of Low-Rigidity Robots Considering Various Surface Materials and Task Definitions**
<br/>
<button class="copy-to-clipboard" title="Learning-Based Wiping Behavior of Low-Rigidity Robots Considering Various Surface Materials and Task Definitions" index=53>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 10  
Keywords: PaLM  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11198v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11198v1.pdf" filename="2403.11198v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Wiping behavior is a task of tracing the surface of an object while feeling the force with the <b>palm</b> of the hand. It is necessary to adjust the force and posture appropriately considering the various contact conditions felt by the hand. Several studies have been conducted on the wiping motion, however, these studies have only dealt with a single surface material, and have only considered the application of the amount of appropriate force, lacking intelligent movements to ensure that the force is applied either evenly to the entire surface or to a certain area. Depending on the surface material, the hand posture and pressing force should be varied appropriately, and this is highly dependent on the definition of the task. Also, most of the movements are executed by high-rigidity robots that are easy to model, and few movements are executed by robots that are low-rigidity but therefore have a small risk of damage due to excessive contact. So, in this study, we develop a method of motion generation based on the learned prediction of contact force during the wiping motion of a low-rigidity robot. We show that MyCobot, which is made of low-rigidity resin, can appropriately perform wiping behaviors on a plane with multiple surface materials based on various task definitions.

{{</citation>}}


### (8/8 | 54/156) Pioneering SE(2)-Equivariant Trajectory Planning for Automated Driving (Steffen Hagedorn et al., 2024)

{{<citation>}}

Steffen Hagedorn, Marcel Milich, Alexandru P. Condurache. (2024)  
**Pioneering SE(2)-Equivariant Trajectory Planning for Automated Driving**
<br/>
<button class="copy-to-clipboard" title="Pioneering SE(2)-Equivariant Trajectory Planning for Automated Driving" index=54>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-AI, cs-MA, cs-RO, cs.RO  
Keyword Score: 3  
Keywords: Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11304v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11304v1.pdf" filename="2403.11304v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Planning the trajectory of the controlled ego vehicle is a key challenge in automated driving. As for human drivers, predicting the motions of surrounding vehicles is important to plan the own actions. Recent motion prediction methods utilize equivariant neural networks to exploit geometric symmetries in the scene. However, no existing method combines motion prediction and trajectory planning in a joint step while guaranteeing equivariance under roto-translations of the input space. We address this gap by proposing a lightweight equivariant planning model that generates <b>multi-modal</b> joint predictions for all vehicles and selects one mode as the ego plan. The equivariant network design improves sample efficiency, guarantees output stability, and reduces model parameters. We further propose equivariant route attraction to guide the ego vehicle along a high-level route provided by an off-the-shelf GPS navigation system. This module creates a momentum from embedded vehicle positions toward the route in latent space while keeping the equivariance property. Route attraction enables goal-oriented behavior without forcing the vehicle to stick to the exact route. We conduct experiments on the challenging nuScenes dataset to investigate the capability of our planner. The results show that the planned trajectory is stable under roto-translations of the input scene which demonstrates the equivariance of our model. Despite using only a small split of the dataset for training, our method improves L2 distance at 3 s by 20.6 % and surpasses the state of the art.

{{</citation>}}


## cs.CV (57)



### (1/57 | 55/156) SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant (Guohao Sun et al., 2024)

{{<citation>}}

Guohao Sun, Can Qin, Jiamian Wang, Zeyuan Chen, Ran Xu, Zhiqiang Tao. (2024)  
**SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant**
<br/>
<button class="copy-to-clipboard" title="SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant" index=55>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV  
Keyword Score: 70  
Keywords: Fine-tuning, Fine-tuning, Self-supervised Learning, Question Answering, Instruction Tuning, Large Language Model, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11299v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11299v1.pdf" filename="2403.11299v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advancements in the <b>vision-language</b> model have shown notable generalization in <b>vision-language</b> tasks after visual <b>instruction</b> <b>tuning.</b> However, bridging the gap between the pre-trained vision encoder and the <b>large</b> <b>language</b> <b>models</b> becomes the whole network's bottleneck. To improve cross-modality alignment, existing works usually consider more visual <b>instruction</b> <b>data</b> covering a broader range of vision tasks to <b>fine-tune</b> the model for <b>question-answering,</b> <b>which</b> are costly to obtain. However, the image contains rich contextual information that has been largely under-explored. This paper first attempts to harness this overlooked context within visual <b>instruction</b> <b>data,</b> training the model to <b>self-supervised</b> `learning' how to ask high-quality <b>questions.</b> <b>In</b> this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for <b>Large</b> <b>Vision-Language</b> <b>Assistant.</b> SQ-LLaVA exhibits proficiency in generating flexible and meaningful image-related <b>questions</b> <b>while</b> analyzing the visual clue and prior language knowledge, signifying an advanced level of generalized visual understanding. Moreover, <b>fine-tuning</b> SQ-LLaVA on higher-quality <b>instruction</b> <b>data</b> shows a consistent performance improvement compared with traditional visual-instruction tuning methods. This improvement highlights the efficacy of self-questioning techniques in achieving a deeper and more nuanced comprehension of visual content across various contexts.

{{</citation>}}


### (2/57 | 56/156) Fast Personalized Text-to-Image Syntheses With Attention Injection (Yuxuan Zhang et al., 2024)

{{<citation>}}

Yuxuan Zhang, Yiren Song, Jinpeng Yu, Han Pan, Zhongliang Jing. (2024)  
**Fast Personalized Text-to-Image Syntheses With Attention Injection**
<br/>
<button class="copy-to-clipboard" title="Fast Personalized Text-to-Image Syntheses With Attention Injection" index=56>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 70  
Keywords: Diffusion Model, Fine-tuning, Fine-tuning, Text2image, Text2image, Prompt, Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11284v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11284v1.pdf" filename="2403.11284v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Currently, personalized image generation methods mostly require considerable time to <b>finetune</b> and often overfit the concept resulting in generated images that are similar to custom concepts but difficult to edit by <b>prompts.</b> We propose an effective and fast approach that could balance the <b>text-image</b> consistency and identity consistency of the generated image and reference image. Our method can generate personalized images without any <b>fine-tuning</b> while maintaining the inherent <b>text-to-image</b> generation ability of <b>diffusion</b> <b>models.</b> Given a <b>prompt</b> and a reference image, we merge the custom concept into generated images by manipulating cross-attention and <b>self-attention</b> layers of the original <b>diffusion</b> <b>model</b> to generate personalized images that match the text description. Comprehensive experiments highlight the superiority of our method.

{{</citation>}}


### (3/57 | 57/156) Endora: Video Generation Models as Endoscopy Simulators (Chenxin Li et al., 2024)

{{<citation>}}

Chenxin Li, Hengyu Liu, Yifan Liu, Brandon Y. Feng, Wuyang Li, Xinyu Liu, Zhen Chen, Jing Shao, Yixuan Yuan. (2024)  
**Endora: Video Generation Models as Endoscopy Simulators**
<br/>
<button class="copy-to-clipboard" title="Endora: Video Generation Models as Endoscopy Simulators" index=57>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 63  
Keywords: Benchmarking, Data Augmentation, Foundation Model, Generative AI, Simulation, Simulator, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11050v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11050v1.pdf" filename="2403.11050v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Generative</b> <b>models</b> hold promise for revolutionizing medical education, robot-assisted surgery, and <b>data</b> <b>augmentation</b> for machine learning. Despite progress in generating 2D medical images, the complex domain of clinical video generation has largely remained untapped.This paper introduces \model, an innovative approach to generate medical videos that simulate clinical endoscopy scenes. We present a novel <b>generative</b> <b>model</b> design that integrates a meticulously crafted spatial-temporal video <b>transformer</b> with advanced 2D vision <b>foundation</b> <b>model</b> priors, explicitly modeling spatial-temporal dynamics during video generation. We also pioneer the first public <b>benchmark</b> for endoscopy <b>simulation</b> with video generation models, adapting existing state-of-the-art methods for this endeavor.Endora demonstrates exceptional visual quality in generating endoscopy videos, surpassing state-of-the-art methods in extensive testing. Moreover, we explore how this endoscopy simulator can empower downstream video analysis tasks and even generate 3D medical scenes with multi-view consistency. In a nutshell, Endora marks a notable breakthrough in the deployment of <b>generative</b> <b>AI</b> for clinical endoscopy research, setting a substantial stage for further advances in medical content generation. For more details, please visit our project page: https://endora-medvidgen.github.io/.

{{</citation>}}


### (4/57 | 58/156) Tokensome: Towards a Genetic Vision-Language GPT for Explainable and Cognitive Karyotyping (Haoxi Zhang et al., 2024)

{{<citation>}}

Haoxi Zhang, Xinxu Zhang, Yuanxin Lin, Maiqi Wang, Yi Lai, Yu Wang, Linfeng Yu, Yufeng Xu, Ran Cheng, Edward Szczerbicki. (2024)  
**Tokensome: Towards a Genetic Vision-Language GPT for Explainable and Cognitive Karyotyping**
<br/>
<button class="copy-to-clipboard" title="Tokensome: Towards a Genetic Vision-Language GPT for Explainable and Cognitive Karyotyping" index=58>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 58  
Keywords: Graph, Knowledge Graph, GPT, Reasoning, Tokenization, Large Language Model, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11073v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11073v1.pdf" filename="2403.11073v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Automatic karyotype analysis is often defined as a visual perception task focused solely on chromosomal object-level modeling. This definition has led most existing methods to overlook componential and holistic information, significantly constraining model performance. Moreover, the lack of interpretability in current technologies hinders clinical adoption. In this paper, we introduce Tokensome, a novel <b>vision-language</b> model based on chromosome <b>tokenization</b> for explainable and cognitive karyotyping. Tokensome elevates the method from the conventional visual perception layer to the cognitive decision-making layer. This elevation enables the integration of domain <b>knowledge</b> <b>and</b> cognitive <b>reasoning</b> via <b>knowledge</b> <b>graphs</b> and <b>LLMs,</b> markedly enhancing model's explainability and facilitating abnormality detection.

{{</citation>}}


### (5/57 | 59/156) A Versatile Framework for Multi-scene Person Re-identification (Wei-Shi Zheng et al., 2024)

{{<citation>}}

Wei-Shi Zheng, Junkai Yan, Yi-Xing Peng. (2024)  
**A Versatile Framework for Multi-scene Person Re-identification**
<br/>
<button class="copy-to-clipboard" title="A Versatile Framework for Multi-scene Person Re-identification" index=59>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 50  
Keywords: Data Augmentation, Knowledge Distillation, Self-supervised Learning, Self-supervised Learning, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11121v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11121v1.pdf" filename="2403.11121v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Person Re-identification (ReID) has been extensively developed for a decade in order to learn the association of images of the same person across non-overlapping camera views. To overcome significant variations between images across camera views, mountains of variants of ReID models were developed for solving a number of challenges, such as resolution change, clothing change, occlusion, modality change, and so on. Despite the impressive performance of many ReID variants, these variants typically function distinctly and cannot be applied to other challenges. To our best knowledge, there is no versatile ReID model that can handle various ReID challenges at the same time. This work contributes to the first attempt at learning a versatile ReID model to solve such a problem. Our main idea is to form a two-stage <b>prompt-based</b> twin modeling framework called VersReID. Our VersReID firstly leverages the scene label to train a ReID Bank that contains abundant knowledge for handling various scenes, where several groups of scene-specific <b>prompts</b> are used to encode different scene-specific knowledge. In the second stage, we <b>distill</b> a V-Branch model with versatile <b>prompts</b> from the ReID Bank for adaptively solving the ReID of different scenes, eliminating the demand for scene labels during the inference stage. To facilitate training VersReID, we further introduce the multi-scene properties into <b>self-supervised</b> <b>learning</b> of ReID via a multi-scene prioris <b>data</b> <b>augmentation</b> (MPDA) strategy. Through extensive experiments, we demonstrate the success of learning an effective and versatile ReID model for handling ReID tasks under multi-scene conditions without manual assignment of scene labels in the inference stage, including general, low-resolution, clothing change, occlusion, and cross-modality scenes. Codes and models are available at https://github.com/iSEE-Laboratory/VersReID.

{{</citation>}}


### (6/57 | 60/156) PhD: A Prompted Visual Hallucination Evaluation Dataset (Jiazhen Liu et al., 2024)

{{<citation>}}

Jiazhen Liu, Yuhan Fu, Ruobing Xie, Runquan Xie, Xingwu Sun, Fengzong Lian, Zhanhui Kang, Xirong Li. (2024)  
**PhD: A Prompted Visual Hallucination Evaluation Dataset**
<br/>
<button class="copy-to-clipboard" title="PhD: A Prompted Visual Hallucination Evaluation Dataset" index=60>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 46  
Keywords: Benchmarking, Multi-modal, Large Language Model, Large Language Model, Prompt, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11116v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11116v1.pdf" filename="2403.11116v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The rapid growth of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has driven the development of <b>Large</b> <b>Vision-Language</b> <b>Models</b> (LVLMs). The challenge of hallucination, prevalent in <b>LLMs,</b> also emerges in LVLMs. However, most existing efforts mainly focus on object hallucination in LVLM, ignoring diverse types of LVLM hallucinations. In this study, we delve into the Intrinsic <b>Vision-Language</b> Hallucination (IVL-Hallu) issue, thoroughly analyzing different types of IVL-Hallu on their causes and reflections. Specifically, we propose several novel IVL-Hallu tasks and categorize them into four types: (a) object hallucination, which arises from the misidentification of objects, (b) attribute hallucination, which is caused by the misidentification of attributes, (c) <b>multi-modal</b> conflicting hallucination, which derives from the contradictions between textual and visual information, and (d) counter-common-sense hallucination, which owes to the contradictions between the LVLM knowledge and actual images. Based on these taxonomies, we propose a more challenging <b>benchmark</b> named PhD to evaluate and explore IVL-Hallu. An automated pipeline is proposed for generating different types of IVL-Hallu data. Extensive experiments on five SOTA LVLMs reveal their inability to effectively tackle our proposed IVL-Hallu tasks, with detailed analyses and insights on the origins and possible solutions of these new challenging IVL-Hallu tasks, facilitating future researches on IVL-Hallu and LVLM. The <b>benchmark</b> can be accessed at https://github.com/jiazhen-code/IntrinsicHallu

{{</citation>}}


### (7/57 | 61/156) Self-supervised co-salient object detection via feature correspondence at multiple scales (Souradeep Chakraborty et al., 2024)

{{<citation>}}

Souradeep Chakraborty, Dimitris Samaras. (2024)  
**Self-supervised co-salient object detection via feature correspondence at multiple scales**
<br/>
<button class="copy-to-clipboard" title="Self-supervised co-salient object detection via feature correspondence at multiple scales" index=61>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 46  
Keywords: Object Detection, Benchmarking, Clustering, Self-supervised Learning, Supervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11107v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11107v1.pdf" filename="2403.11107v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Our paper introduces a novel two-stage <b>self-supervised</b> approach for detecting co-occurring salient <b>objects</b> <b>(CoSOD)</b> in image groups without requiring segmentation annotations. Unlike existing <b>unsupervised</b> methods that rely solely on patch-level information (e.g. <b>clustering</b> patch descriptors) or on computation heavy off-the-shelf components for CoSOD, our lightweight model leverages feature correspondences at both patch and region levels, significantly improving prediction performance. In the first stage, we train a <b>self-supervised</b> network that detects co-salient regions by computing local patch-level feature correspondences across images. We obtain the segmentation predictions using confidence-based adaptive thresholding. In the next stage, we refine these intermediate segmentations by eliminating the detected regions (within each image) whose averaged feature representations are dissimilar to the foreground feature representation averaged across all the cross-attention maps (from the previous stage). Extensive experiments on three CoSOD <b>benchmark</b> datasets show that our <b>self-supervised</b> model outperforms the corresponding state-of-the-art models by a huge margin (e.g. on the CoCA dataset, our model has a 13.7% F-measure gain over the SOTA <b>unsupervised</b> CoSOD model). Notably, our <b>self-supervised</b> model also outperforms several recent fully <b>supervised</b> CoSOD models on the three test datasets (e.g., on the CoCA dataset, our model has a 4.6% F-measure gain over a recent <b>supervised</b> CoSOD model).

{{</citation>}}


### (8/57 | 62/156) Omni-Recon: Towards General-Purpose Neural Radiance Fields for Versatile 3D Applications (Yonggan Fu et al., 2024)

{{<citation>}}

Yonggan Fu, Huaizhi Qu, Zhifan Ye, Chaojian Li, Kevin Zhao, Yingyan Lin. (2024)  
**Omni-Recon: Towards General-Purpose Neural Radiance Fields for Versatile 3D Applications**
<br/>
<button class="copy-to-clipboard" title="Omni-Recon: Towards General-Purpose Neural Radiance Fields for Versatile 3D Applications" index=62>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 45  
Keywords: Diffusion Model, Foundation Model, Geometry, Zero-shot, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11131v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11131v1.pdf" filename="2403.11131v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent breakthroughs in Neural Radiance Fields (NeRFs) have sparked significant demand for their integration into real-world 3D applications. However, the varied functionalities required by different 3D applications often necessitate diverse NeRF models with various pipelines, leading to tedious NeRF training for each target task and cumbersome trial-and-error experiments. Drawing inspiration from the generalization capability and adaptability of emerging <b>foundation</b> <b>models,</b> our work aims to develop one general-purpose NeRF for handling diverse 3D tasks. We achieve this by proposing a framework called Omni-Recon, which is capable of (1) generalizable 3D reconstruction and <b>zero-shot</b> multitask scene understanding, and (2) adaptability to diverse downstream 3D applications such as real-time rendering and scene editing. Our key insight is that an image-based rendering pipeline, with accurate <b>geometry</b> and appearance estimation, can lift 2D image features into their 3D counterparts, thus extending widely explored 2D tasks to the 3D world in a generalizable manner. Specifically, our Omni-Recon features a general-purpose NeRF model using image-based rendering with two decoupled branches: one complex <b>transformer-based</b> branch that progressively fuses <b>geometry</b> and appearance features for accurate <b>geometry</b> estimation, and one lightweight branch for predicting blending weights of source views. This design achieves state-of-the-art (SOTA) generalizable 3D surface reconstruction quality with blending weights reusable across diverse tasks for <b>zero-shot</b> multitask scene understanding. In addition, it can enable real-time rendering after baking the complex <b>geometry</b> branch into meshes, swift adaptation to achieve SOTA generalizable 3D understanding performance, and seamless integration with 2D <b>diffusion</b> <b>models</b> for text-guided 3D editing.

{{</citation>}}


### (9/57 | 63/156) Uncertainty-Aware Pseudo-Label Filtering for Source-Free Unsupervised Domain Adaptation (Xi Chen et al., 2024)

{{<citation>}}

Xi Chen, Haosen Yang, Huicong Zhang, Hongxun Yao, Xiatian Zhu. (2024)  
**Uncertainty-Aware Pseudo-Label Filtering for Source-Free Unsupervised Domain Adaptation**
<br/>
<button class="copy-to-clipboard" title="Uncertainty-Aware Pseudo-Label Filtering for Source-Free Unsupervised Domain Adaptation" index=63>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 43  
Keywords: Benchmarking, Contrastive Learning, Supervised Learning, Unsupervised Learning, Domain Adaptation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11256v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11256v1.pdf" filename="2403.11256v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Source-free <b>unsupervised</b> <b>domain</b> <b>adaptation</b> (SFUDA) aims to enable the utilization of a pre-trained source model in an unlabeled target <b>domain</b> <b>without</b> access to source data. Self-training is a way to solve SFUDA, where confident target samples are iteratively selected as pseudo-labeled samples to guide target model learning. However, prior heuristic noisy pseudo-label filtering methods all involve introducing extra models, which are sensitive to model assumptions and may introduce additional errors or mislabeling. In this work, we propose a method called Uncertainty-aware Pseudo-label-filtering Adaptation (UPA) to efficiently address this issue in a coarse-to-fine manner. Specially, we first introduce a sample selection module named Adaptive Pseudo-label Selection (APS), which is responsible for filtering noisy pseudo labels. The APS utilizes a simple sample uncertainty estimation method by aggregating knowledge from neighboring samples and confident samples are selected as clean pseudo-labeled. Additionally, we incorporate Class-Aware <b>Contrastive</b> <b>Learning</b> (CACL) to mitigate the memorization of pseudo-label noise by learning robust pair-wise representation <b>supervised</b> by pseudo labels. Through extensive experiments conducted on three widely used <b>benchmarks,</b> we demonstrate that our proposed method achieves competitive performance on par with state-of-the-art SFUDA methods. Code is available at https://github.com/chenxi52/UPA.

{{</citation>}}


### (10/57 | 64/156) Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning (Xiaohao Xu et al., 2024)

{{<citation>}}

Xiaohao Xu, Yunkang Cao, Yongqi Chen, Weiming Shen, Xiaonan Huang. (2024)  
**Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning**
<br/>
<button class="copy-to-clipboard" title="Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning" index=64>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CL, cs-CV, cs.CV  
Keyword Score: 43  
Keywords: Anomaly Detection, Foundation Model, Multi-modal, Reasoning, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11083v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11083v1.pdf" filename="2403.11083v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Anomaly</b> <b>detection</b> is vital in various industrial scenarios, including the identification of unusual patterns in production lines and the detection of manufacturing defects for quality control. Existing techniques tend to be specialized in individual scenarios and lack generalization capacities. In this study, we aim to develop a generic <b>anomaly</b> <b>detection</b> model applicable across multiple scenarios. To achieve this, we customize generic visual-language <b>foundation</b> <b>models</b> that possess extensive knowledge and robust <b>reasoning</b> abilities into <b>anomaly</b> <b>detectors</b> and reasoners. Specifically, we introduce a <b>multi-modal</b> <b>prompting</b> strategy that incorporates domain knowledge from experts as conditions to guide the models. Our approach considers <b>multi-modal</b> <b>prompt</b> types, including task descriptions, class context, normality rules, and reference images. In addition, we unify the input representation of multi-modality into a 2D image format, enabling <b>multi-modal</b> <b>anomaly</b> <b>detection</b> and <b>reasoning.</b> Our preliminary studies demonstrate that combining visual and language <b>prompts</b> as conditions for customizing the models enhances <b>anomaly</b> <b>detection</b> performance. The customized models showcase the ability to detect anomalies across different data modalities such as images and point clouds. Qualitative case studies further highlight the <b>anomaly</b> <b>detection</b> and <b>reasoning</b> capabilities, particularly for multi-object scenes and temporal data. Our code is available at https://github.com/Xiaohao-Xu/Customizable-VLM.

{{</citation>}}


### (11/57 | 65/156) MaskDiffusion: Exploiting Pre-trained Diffusion Models for Semantic Segmentation (Yasufumi Kawano et al., 2024)

{{<citation>}}

Yasufumi Kawano, Yoshimitsu Aoki. (2024)  
**MaskDiffusion: Exploiting Pre-trained Diffusion Models for Semantic Segmentation**
<br/>
<button class="copy-to-clipboard" title="MaskDiffusion: Exploiting Pre-trained Diffusion Models for Semantic Segmentation" index=65>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Diffusion Model, Supervised Learning, Supervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11194v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11194v1.pdf" filename="2403.11194v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Semantic segmentation is essential in computer vision for various applications, yet traditional approaches face significant challenges, including the high cost of annotation and extensive training for <b>supervised</b> <b>learning.</b> Additionally, due to the limited predefined categories in <b>supervised</b> <b>learning,</b> models typically struggle with infrequent classes and are unable to predict novel classes. To address these limitations, we propose MaskDiffusion, an innovative approach that leverages pretrained frozen Stable <b>Diffusion</b> <b>to</b> achieve open-vocabulary semantic segmentation without the need for additional training or annotation, leading to improved performance compared to similar methods. We also demonstrate the superior performance of MaskDiffusion in handling open vocabularies, including fine-grained and proper noun-based categories, thus expanding the scope of segmentation applications. Overall, our MaskDiffusion shows significant qualitative and quantitative improvements in contrast to other comparable <b>unsupervised</b> segmentation methods, i.e. on the Potsdam dataset (+10.5 mIoU compared to GEM) and COCO-Stuff (+14.8 mIoU compared to DiffSeg). All code and data will be released at https://github.com/Valkyrja3607/MaskDiffusion.

{{</citation>}}


### (12/57 | 66/156) Quality-Aware Image-Text Alignment for Real-World Image Quality Assessment (Lorenzo Agnolucci et al., 2024)

{{<citation>}}

Lorenzo Agnolucci, Leonardo Galteri, Marco Bertini. (2024)  
**Quality-Aware Image-Text Alignment for Real-World Image Quality Assessment**
<br/>
<button class="copy-to-clipboard" title="Quality-Aware Image-Text Alignment for Real-World Image Quality Assessment" index=66>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CL, cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Self-supervised Learning, Supervised Learning, Image2text, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11176v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11176v1.pdf" filename="2403.11176v1.pdf">Download PDF</button>

---


**ABSTRACT**  
No-Reference Image Quality Assessment (NR-IQA) focuses on designing methods to measure image quality in alignment with human perception when a high-quality reference image is unavailable. The reliance on annotated Mean Opinion Scores (MOS) in the majority of state-of-the-art NR-IQA approaches limits their scalability and broader applicability to real-world scenarios. To overcome this limitation, we propose QualiCLIP (Quality-aware CLIP), a CLIP-based <b>self-supervised</b> opinion-unaware method that does not require labeled MOS. In particular, we introduce a quality-aware <b>image-text</b> alignment strategy to make CLIP generate representations that correlate with the inherent quality of the images. Starting from pristine images, we synthetically degrade them with increasing levels of intensity. Then, we train CLIP to rank these degraded images based on their similarity to quality-related antonym text <b>prompts,</b> while guaranteeing consistent representations for images with comparable quality. Our method achieves state-of-the-art performance on several datasets with authentic distortions. Moreover, despite not requiring MOS, QualiCLIP outperforms <b>supervised</b> methods when their training dataset differs from the testing one, thus proving to be more suitable for real-world scenarios. Furthermore, our approach demonstrates greater robustness and improved explainability than competing methods. The code and the model are publicly available at https://github.com/miccunifi/QualiCLIP.

{{</citation>}}


### (13/57 | 67/156) CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion (Xiaoyu Wu et al., 2024)

{{<citation>}}

Xiaoyu Wu, Yang Hua, Chumeng Liang, Jiaru Zhang, Hao Wang, Tao Song, Haibing Guan. (2024)  
**CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion**
<br/>
<button class="copy-to-clipboard" title="CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion" index=67>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CR, cs-CV, cs-CY, cs-LG, cs.CV  
Keyword Score: 40  
Keywords: Diffusion Model, Few-shot, Fine-tuning, Stemming  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11162v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11162v1.pdf" filename="2403.11162v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Diffusion</b> <b>Models</b> (DMs) have evolved into advanced image generation tools, especially for <b>few-shot</b> generation where a pretrained model is <b>fine-tuned</b> on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations <b>stemming</b> from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for <b>Diffusion</b> <b>Models</b> (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and <b>fine-tuned</b> models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and recovered images serves as a strong indicator of potential infringements. Extensive experiments on the WikiArt and Dreambooth datasets demonstrate the high accuracy of CGI-DM in digital copyright authentication, surpassing alternative validation techniques. Code implementation is available at https://github.com/Nicholas0228/Revelio.

{{</citation>}}


### (14/57 | 68/156) Reconstruct before Query: Continual Missing Modality Learning with Decomposed Prompt Collaboration (Shu Zhao et al., 2024)

{{<citation>}}

Shu Zhao, Xiaohan Zou, Tan Yu, Huijuan Xu. (2024)  
**Reconstruct before Query: Continual Missing Modality Learning with Decomposed Prompt Collaboration**
<br/>
<button class="copy-to-clipboard" title="Reconstruct before Query: Continual Missing Modality Learning with Decomposed Prompt Collaboration" index=68>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 36  
Keywords: Benchmarking, Continual Learning, Fine-tuning, Multi-modal, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11373v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11373v1.pdf" filename="2403.11373v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Pre-trained large <b>multi-modal</b> models (LMMs) exploit <b>fine-tuning</b> to adapt diverse user applications. Nevertheless, <b>fine-tuning</b> may face challenges due to deactivated sensors (e.g., cameras turned off for privacy or technical issues), yielding modality-incomplete data and leading to inconsistency in training data and the data for inference. Additionally, continuous training leads to catastrophic forgetting, diluting the knowledge in pre-trained LMMs. To overcome these challenges, we introduce a novel task, <b>Continual</b> <b>Missing</b> Modality Learning (CMML), to investigate how models can generalize when data of certain modalities is missing during <b>continual</b> <b>fine-tuning.</b> Our preliminary <b>benchmarks</b> reveal that existing methods suffer from a significant performance drop in CMML, even with the aid of advanced <b>continual</b> <b>learning</b> techniques. Therefore, we devise a framework termed Reconstruct before Query (RebQ). It decomposes <b>prompts</b> into modality-specific ones and breaks them into components stored in pools accessible via a key-query mechanism, which facilitates ParameterEfficient <b>Fine-Tuning</b> and enhances knowledge transferability for subsequent tasks. Meanwhile, our RebQ leverages extensive <b>multi-modal</b> knowledge from pre-trained LMMs to reconstruct the data of missing modality. Comprehensive experiments demonstrate that RebQ effectively reconstructs the missing modality information and retains pre-trained knowledge. Specifically, compared with the baseline, RebQ improves average precision from 20.00 to 50.92 and decreases average forgetting from 75.95 to 8.56. Code and datasets are available on https://github.com/Tree-Shu-Zhao/RebQ.pytorch

{{</citation>}}


### (15/57 | 69/156) From Pixels to Predictions: Spectrogram and Vision Transformer for Better Time Series Forecasting (Zhen Zeng et al., 2024)

{{<citation>}}

Zhen Zeng, Rachneet Kaur, Suchetha Siddagangappa, Tucker Balch, Manuela Veloso. (2024)  
**From Pixels to Predictions: Spectrogram and Vision Transformer for Better Time Series Forecasting**
<br/>
<button class="copy-to-clipboard" title="From Pixels to Predictions: Spectrogram and Vision Transformer for Better Time Series Forecasting" index=69>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CE, cs-CV, cs.CV  
Keyword Score: 36  
Keywords: Vision Transformer, Multi-modal, Multi-modal, Transformer, Vision Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11047v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11047v1.pdf" filename="2403.11047v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Time series forecasting plays a crucial role in decision-making across various domains, but it presents significant challenges. Recent studies have explored image-driven approaches using computer <b>vision</b> <b>models</b> to address these challenges, often employing lineplots as the visual representation of time series data. In this paper, we propose a novel approach that uses time-frequency spectrograms as the visual representation of time series data. We introduce the use of a <b>vision</b> <b>transformer</b> for <b>multimodal</b> learning, showcasing the advantages of our approach across diverse datasets from different domains. To evaluate its effectiveness, we compare our method against statistical baselines (EMA and ARIMA), a state-of-the-art deep learning-based approach (DeepAR), other visual representations of time series data (lineplot images), and an ablation study on using only the time series as input. Our experiments demonstrate the benefits of utilizing spectrograms as a visual representation for time series data, along with the advantages of employing a <b>vision</b> <b>transformer</b> for simultaneous learning in both the time and frequency domains.

{{</citation>}}


### (16/57 | 70/156) Selective Hourglass Mapping for Universal Image Restoration Based on Diffusion Model (Dian Zheng et al., 2024)

{{<citation>}}

Dian Zheng, Xiao-Ming Wu, Shuzhou Yang, Jian Zhang, Jian-Fang Hu, Wei-Shi Zheng. (2024)  
**Selective Hourglass Mapping for Universal Image Restoration Based on Diffusion Model**
<br/>
<button class="copy-to-clipboard" title="Selective Hourglass Mapping for Universal Image Restoration Based on Diffusion Model" index=70>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 33  
Keywords: Diffusion Model, Benchmarking, Zero-shot, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11157v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11157v1.pdf" filename="2403.11157v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Universal image restoration is a practical and potential computer vision task for real-world applications. The main challenge of this task is handling the different degradation distributions at once. Existing methods mainly utilize task-specific conditions (e.g., <b>prompt)</b> to guide the model to learn different distributions separately, named multi-partite mapping. However, it is not suitable for universal model learning as it ignores the shared information between different tasks. In this work, we propose an advanced selective hourglass mapping strategy based on <b>diffusion</b> <b>model,</b> termed DiffUIR. Two novel considerations make our DiffUIR non-trivial. Firstly, we equip the model with strong condition guidance to obtain accurate generation direction of <b>diffusion</b> <b>model</b> (selective). More importantly, DiffUIR integrates a flexible shared distribution term (SDT) into the <b>diffusion</b> <b>algorithm</b> elegantly and naturally, which gradually maps different distributions into a shared one. In the reverse process, combined with SDT and strong condition guidance, DiffUIR iteratively guides the shared distribution to the task-specific distribution with high image quality (hourglass). Without bells and whistles, by only modifying the mapping strategy, we achieve state-of-the-art performance on five image restoration tasks, 22 <b>benchmarks</b> in the universal setting and <b>zero-shot</b> generalization setting. Surprisingly, by only using a lightweight model (only 0.89M), we could achieve outstanding performance. The source code and pre-trained models are available at https://github.com/iSEE-Laboratory/DiffUIR

{{</citation>}}


### (17/57 | 71/156) Advanced Knowledge Extraction of Physical Design Drawings, Translation and conversion to CAD formats using Deep Learning (Jesher Joshua M et al., 2024)

{{<citation>}}

Jesher Joshua M, Ragav V, Syed Ibrahim S P. (2024)  
**Advanced Knowledge Extraction of Physical Design Drawings, Translation and conversion to CAD formats using Deep Learning**
<br/>
<button class="copy-to-clipboard" title="Advanced Knowledge Extraction of Physical Design Drawings, Translation and conversion to CAD formats using Deep Learning" index=71>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 30  
Keywords: Object Detection, Optical Character Recognition, Optical Character Recognition  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11291v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11291v1.pdf" filename="2403.11291v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The maintenance, archiving and usage of the design drawings is cumbersome in physical form in different industries for longer period. It is hard to extract information by simple scanning of drawing sheets. Converting them to their digital formats such as Computer-Aided Design (CAD), with needed knowledge extraction can solve this problem. The conversion of these machine drawings to its digital form is a crucial challenge which requires advanced techniques. This research proposes an innovative methodology utilizing Deep Learning methods. The approach employs <b>object</b> <b>detection</b> model, such as Yolov7, Faster R-CNN, to detect physical drawing <b>objects</b> <b>present</b> in the images followed by, edge detection algorithms such as canny filter to extract and refine the identified lines from the drawing region and curve detection techniques to detect circle. Also ornaments (complex shapes) within the drawings are extracted. To ensure comprehensive conversion, an <b>Optical</b> <b>Character</b> <b>Recognition</b> <b>(OCR)</b> tool is integrated to identify and extract the text elements from the drawings. The extracted data which includes the lines, shapes and text is consolidated and stored in a structured comma separated values(.csv) file format. The accuracy and the efficiency of conversion is evaluated. Through this, conversion can be automated to help organizations enhance their productivity, facilitate seamless collaborations and preserve valuable design information in a digital format easily accessible. Overall, this study contributes to the advancement of CAD conversions, providing accurate results from the translating process. Future research can focus on handling diverse drawing types, enhanced accuracy in shape and line detection and extraction.

{{</citation>}}


### (18/57 | 72/156) BrightDreamer: Generic 3D Gaussian Generative Framework for Fast Text-to-3D Synthesis (Lutao Jiang et al., 2024)

{{<citation>}}

Lutao Jiang, Lin Wang. (2024)  
**BrightDreamer: Generic 3D Gaussian Generative Framework for Fast Text-to-3D Synthesis**
<br/>
<button class="copy-to-clipboard" title="BrightDreamer: Generic 3D Gaussian Generative Framework for Fast Text-to-3D Synthesis" index=72>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Knowledge Distillation, Text2image, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11273v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11273v1.pdf" filename="2403.11273v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Text-to-3D synthesis has recently seen intriguing advances by combining the <b>text-to-image</b> models with 3D representation methods, e.g., Gaussian Splatting (GS), via Score <b>Distillation</b> Sampling (SDS). However, a hurdle of existing methods is the low efficiency, per-prompt optimization for a single 3D object. Therefore, it is imperative for a paradigm shift from per-prompt optimization to one-stage generation for any unseen text <b>prompts,</b> which yet remains challenging. A hurdle is how to directly generate a set of millions of 3D Gaussians to represent a 3D object. This paper presents BrightDreamer, an end-to-end single-stage approach that can achieve generalizable and fast (77 ms) text-to-3D generation. Our key idea is to formulate the generation process as estimating the 3D deformation from an anchor shape with predefined positions. For this, we first propose a Text-guided Shape Deformation (TSD) network to predict the deformed shape and its new positions, used as the centers (one attribute) of 3D Gaussians. To estimate the other four attributes (i.e., scaling, rotation, opacity, and SH coefficient), we then design a novel Text-guided Triplane Generator (TTG) to generate a triplane representation for a 3D object. The center of each Gaussian enables us to transform the triplane feature into the four attributes. The generated 3D Gaussians can be finally rendered at 705 frames per second. Extensive experiments demonstrate the superiority of our method over existing methods. Also, BrightDreamer possesses a strong semantic understanding capability even for complex text <b>prompts.</b> The project code is available at https://vlislab22.github.io/BrightDreamer.

{{</citation>}}


### (19/57 | 73/156) CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations (Yuwei Zhang et al., 2024)

{{<citation>}}

Yuwei Zhang, Yan Wu, Yanming Liu, Xinyue Peng. (2024)  
**CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations**
<br/>
<button class="copy-to-clipboard" title="CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations" index=73>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs.CV  
Keyword Score: 30  
Keywords: Object Detection, Chain-of-thought Prompt, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11220v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11220v1.pdf" filename="2403.11220v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Object</b> <b>detection</b> methods under known single degradations have been extensively investigated. However, existing approaches require prior knowledge of the degradation type and train a separate model for each, limiting their practical applications in unpredictable environments. To address this challenge, we propose a <b>chain-of-thought</b> <b>(CoT)</b> <b>prompted</b> adaptive enhancer, CPA-Enhancer, for <b>object</b> <b>detection</b> under unknown degradations. Specifically, CPA-Enhancer progressively adapts its enhancement strategy under the step-by-step guidance of CoT <b>prompts,</b> that encode degradation-related information. To the best of our knowledge, it's the first work that exploits CoT <b>prompting</b> for <b>object</b> <b>detection</b> tasks. Overall, CPA-Enhancer is a plug-and-play enhancement model that can be integrated into any generic detectors to achieve substantial gains on degraded images, without knowing the degradation type priorly. Experimental results demonstrate that CPA-Enhancer not only sets the new state of the art for <b>object</b> <b>detection</b> but also boosts the performance of other downstream vision tasks under unknown degradations.

{{</citation>}}


### (20/57 | 74/156) TAG: Guidance-free Open-Vocabulary Semantic Segmentation (Yasufumi Kawano et al., 2024)

{{<citation>}}

Yasufumi Kawano, Yoshimitsu Aoki. (2024)  
**TAG: Guidance-free Open-Vocabulary Semantic Segmentation**
<br/>
<button class="copy-to-clipboard" title="TAG: Guidance-free Open-Vocabulary Semantic Segmentation" index=74>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Supervised Learning, Supervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11197v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11197v1.pdf" filename="2403.11197v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Semantic segmentation is a crucial task in computer vision, where each pixel in an image is classified into a category. However, traditional methods face significant challenges, including the need for pixel-level annotations and extensive training. Furthermore, because <b>supervised</b> <b>learning</b> uses a limited set of predefined categories, models typically struggle with rare classes and cannot recognize new ones. <b>Unsupervised</b> and open-vocabulary segmentation, proposed to tackle these issues, faces challenges, including the inability to assign specific class labels to clusters and the necessity of user-provided text queries for guidance. In this context, we propose a novel approach, TAG which achieves Training, Annotation, and Guidance-free open-vocabulary semantic segmentation. TAG utilizes pre-trained models such as CLIP and DINO to segment images into meaningful categories without additional training or dense annotations. It retrieves class labels from an external database, providing flexibility to adapt to new scenarios. Our TAG achieves state-of-the-art results on PascalVOC, PascalContext and ADE20K for open-vocabulary segmentation without given class names, i.e. improvement of +15.3 mIoU on PascalVOC. All code and data will be released at https://github.com/Valkyrja3607/TAG.

{{</citation>}}


### (21/57 | 75/156) Artifact Feature Purification for Cross-domain Detection of AI-generated Images (Zheling Meng et al., 2024)

{{<citation>}}

Zheling Meng, Bo Peng, Jing Dong, Tieniu Tan. (2024)  
**Artifact Feature Purification for Cross-domain Detection of AI-generated Images**
<br/>
<button class="copy-to-clipboard" title="Artifact Feature Purification for Cross-domain Detection of AI-generated Images" index=75>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Diffusion Model, Mutual Information, Out-of-domain  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11172v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11172v1.pdf" filename="2403.11172v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In the era of AIGC, the fast development of visual content generation technologies, such as <b>diffusion</b> <b>models,</b> bring potential security risks to our society. Existing generated image detection methods suffer from performance drop when faced with <b>out-of-domain</b> generators and image scenes. To relieve this problem, we propose Artifact Purification Network (APN) to facilitate the artifact extraction from generated images through the explicit and implicit purification processes. For the explicit one, a suspicious frequency-band proposal method and a spatial feature decomposition method are proposed to extract artifact-related features. For the implicit one, a training strategy based on <b>mutual</b> <b>information</b> estimation is proposed to further purify the artifact-related features. Experiments show that for cross-generator detection, the average accuracy of APN is 5.6% ~ 16.4% higher than the previous 10 methods on GenImage dataset and 1.7% ~ 50.1% on DiffusionForensics dataset. For cross-scene detection, APN maintains its high performance. Via visualization analysis, we find that the proposed method extracts flexible forgery patterns and condenses the forgery information diluted in irrelevant features. We also find that the artifact features APN focuses on across generators and scenes are global and diverse. The code will be available on GitHub.

{{</citation>}}


### (22/57 | 76/156) Training A Small Emotional Vision Language Model for Visual Art Comprehension (Jing Zhang et al., 2024)

{{<citation>}}

Jing Zhang, Liang Zheng, Dan Guo, Meng Wang. (2024)  
**Training A Small Emotional Vision Language Model for Visual Art Comprehension**
<br/>
<button class="copy-to-clipboard" title="Training A Small Emotional Vision Language Model for Visual Art Comprehension" index=76>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Fine-tuning, Text Embedding, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11150v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11150v1.pdf" filename="2403.11150v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper develops small vision language models to understand visual art, which, given an art work, aims to identify its emotion category and explain this prediction with natural language. While small models are computationally efficient, their capacity is much limited compared with large models. To break this trade-off, this paper builds a small emotional vision language model (SEVLM) by emotion modeling and input-output feature alignment. On the one hand, based on valence-arousal-dominance (VAD) knowledge annotated by psychology experts, we introduce and fuse emotional features derived through VAD dictionary and a VAD head to align VAD vectors of predicted emotion explanation and the ground truth. This allows the vision language model to better understand and generate emotional <b>texts,</b> <b>compared</b> with using traditional <b>text</b> <b>embeddings</b> alone. On the other hand, we design a contrastive head to pull close embeddings of the image, its emotion class, and explanation, which aligns model outputs and inputs. On two public affective explanation datasets, we show that the proposed techniques consistently improve the visual art understanding performance of baseline SEVLMs. Importantly, the proposed model can be trained and evaluated on a single RTX 2080 Ti while exhibiting very strong performance: it not only outperforms the state-of-the-art small models but is also competitive compared with LLaVA 7B after <b>fine-tuning</b> and GPT4(V).

{{</citation>}}


### (23/57 | 77/156) Large Language Models Powered Context-aware Motion Prediction (Xiaoji Zheng et al., 2024)

{{<citation>}}

Xiaoji Zheng, Lixiu Wu, Zhijie Yan, Yuanrong Tang, Hao Zhao, Chen Zhong, Bokui Chen, Jiangtao Gong. (2024)  
**Large Language Models Powered Context-aware Motion Prediction**
<br/>
<button class="copy-to-clipboard" title="Large Language Models Powered Context-aware Motion Prediction" index=77>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: 68T45, cs-CV, cs-RO, cs.CV  
Keyword Score: 30  
Keywords: Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11057v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11057v1.pdf" filename="2403.11057v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Motion prediction is among the most fundamental tasks in autonomous driving. Traditional methods of motion forecasting primarily encode vector information of maps and historical trajectory data of traffic participants, lacking a comprehensive understanding of overall traffic semantics, which in turn affects the performance of prediction tasks. In this paper, we utilized <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to enhance the global traffic context understanding for motion prediction tasks. We first conducted systematic <b>prompt</b> engineering, visualizing complex traffic environments and historical trajectory information of traffic participants into image <b>prompts</b> -- Transportation Context Map (TC-Map), accompanied by corresponding text <b>prompts.</b> Through this approach, we obtained rich traffic context information from the <b>LLM.</b> By integrating this information into the motion prediction model, we demonstrate that such context can enhance the accuracy of motion predictions. Furthermore, considering the cost associated with <b>LLMs,</b> we propose a cost-effective deployment strategy: enhancing the accuracy of motion prediction tasks at scale with 0.7\% <b>LLM-augmented</b> datasets. Our research offers valuable insights into enhancing the understanding of traffic scenes of <b>LLMs</b> and the motion prediction performance of autonomous driving.

{{</citation>}}


### (24/57 | 78/156) Unveiling and Mitigating Memorization in Text-to-image Diffusion Models through Cross Attention (Jie Ren et al., 2024)

{{<citation>}}

Jie Ren, Yaxin Li, Shenglai Zen, Han Xu, Lingjuan Lyu, Yue Xing, Jiliang Tang. (2024)  
**Unveiling and Mitigating Memorization in Text-to-image Diffusion Models through Cross Attention**
<br/>
<button class="copy-to-clipboard" title="Unveiling and Mitigating Memorization in Text-to-image Diffusion Models through Cross Attention" index=78>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CR, cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Diffusion Model, Text2image, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11052v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11052v1.pdf" filename="2403.11052v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advancements in <b>text-to-image</b> <b>diffusion</b> <b>models</b> have demonstrated their remarkable capability to generate high-quality images from textual <b>prompts.</b> However, increasing research indicates that these models memorize and replicate images from their training data, raising tremendous concerns about potential copyright infringement and privacy risks. In our study, we provide a novel perspective to understand this memorization phenomenon by examining its relationship with cross-attention mechanisms. We reveal that during memorization, the cross-attention tends to focus disproportionately on the embeddings of specific tokens. The <b>diffusion</b> <b>model</b> is overfitted to these token embeddings, memorizing corresponding training images. To elucidate this phenomenon, we further identify and discuss various intrinsic findings of cross-attention that contribute to memorization. Building on these insights, we introduce an innovative approach to detect and mitigate memorization in <b>diffusion</b> <b>models.</b> The advantage of our proposed method is that it will not compromise the speed of either the training or the inference processes in these models while preserving the quality of generated images. Our code is available at https://github.com/renjie3/MemAttn .

{{</citation>}}


### (25/57 | 79/156) LERENet: Eliminating Intra-class Differences for Metal Surface Defect Few-shot Semantic Segmentation (Hanze Ding et al., 2024)

{{<citation>}}

Hanze Ding, Zhangkai Wu, Jiyan Zhang, Ming Ping, Yanfang Liu. (2024)  
**LERENet: Eliminating Intra-class Differences for Metal Surface Defect Few-shot Semantic Segmentation**
<br/>
<button class="copy-to-clipboard" title="LERENet: Eliminating Intra-class Differences for Metal Surface Defect Few-shot Semantic Segmentation" index=79>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 26  
Keywords: Graph, Benchmarking, Few-shot, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11122v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11122v1.pdf" filename="2403.11122v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Few-shot</b> segmentation models excel in metal defect detection due to their rapid generalization ability to new classes and pixel-level segmentation, rendering them ideal for addressing data scarcity issues and achieving refined object delineation in industrial applications. Existing works neglect the \textit{Intra-Class Differences}, inherent in metal surface defect data, which hinders the model from learning sufficient knowledge from the support set to guide the query set segmentation. Specifically, it can be categorized into two types: the \textit{Semantic Difference} induced by internal factors in metal samples and the \textit{Distortion Difference} caused by external factors of surroundings. To address these differences, we introduce a \textbf{L}ocal d\textbf{E}scriptor based \textbf{R}easoning and \textbf{E}xcitation \textbf{Net}work (\textbf{LERENet}) to learn the two-view guidance, i.e., local and global information from the <b>graph</b> and feature space, and fuse them to segment precisely. Since the relation structure of local features embedded in <b>graph</b> space will help to eliminate \textit{Semantic Difference}, we employ Multi-Prototype <b>Reasoning</b> (MPR) module, extracting local descriptors based prototypes and analyzing local-view feature relevance in support-query pairs. Besides, due to the global information that will assist in countering the \textit{Distortion Difference} in observations, we utilize Multi-Prototype Excitation (MPE) module to capture the global-view relations in support-query pairs. Finally, we employ an Information Fusion Module (IFM) to fuse learned prototypes in local and global views to generate pixel-level masks. Our comprehensive experiments on defect datasets demonstrate that it outperforms existing <b>benchmarks,</b> establishing a new state-of-the-art.

{{</citation>}}


### (26/57 | 80/156) m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks (Zixian Ma et al., 2024)

{{<citation>}}

Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, Ranjay Krishna. (2024)  
**m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks**
<br/>
<button class="copy-to-clipboard" title="m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks" index=80>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CL, cs-CV, cs.CV  
Keyword Score: 26  
Keywords: Benchmarking, Multi-modal, Large Language Model, Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11085v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11085v1.pdf" filename="2403.11085v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Real-world <b>multi-modal</b> problems are rarely solved by a single machine learning model, and often require multi-step computational plans that involve stitching several models. Tool-augmented <b>LLMs</b> hold tremendous promise for automating the generation of such computational plans. However, the lack of standardized <b>benchmarks</b> for evaluating <b>LLMs</b> as planners for multi-step <b>multi-modal</b> tasks has prevented a systematic study of planner design decisions. Should <b>LLMs</b> generate a full plan in a single shot or step-by-step? Should they invoke tools directly with Python code or through structured data formats like JSON? Does feedback improve planning? To answer these questions and more, we introduce m&m's: a <b>benchmark</b> containing 4K+ multi-step <b>multi-modal</b> tasks involving 33 tools that include <b>multi-modal</b> models, (free) public APIs, and image processing modules. For each of these task queries, we provide automatically generated plans using this realistic toolset. We further provide a high-quality subset of 1,565 task plans that are human-verified and correctly executable. With m&m's, we evaluate 6 popular <b>LLMs</b> with 2 planning strategies (multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3 types of feedback (parsing/verification/execution). Finally, we <b>summarize</b> takeaways from our extensive experiments. Our dataset and code are available on HuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github (https://github.com/RAIVNLab/mnms).

{{</citation>}}


### (27/57 | 81/156) Recent Advances in 3D Gaussian Splatting (Tong Wu et al., 2024)

{{<citation>}}

Tong Wu, Yu-Jie Yuan, Ling-Xiao Zhang, Jie Yang, Yan-Pei Cao, Ling-Qi Yan, Lin Gao. (2024)  
**Recent Advances in 3D Gaussian Splatting**
<br/>
<button class="copy-to-clipboard" title="Recent Advances in 3D Gaussian Splatting" index=81>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-GR, cs.CV  
Keyword Score: 25  
Keywords: Geometry, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11134v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11134v1.pdf" filename="2403.11134v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The emergence of 3D Gaussian Splatting (3DGS) has greatly accelerated the rendering speed of novel view synthesis. Unlike neural implicit representations like Neural Radiance Fields (NeRF) that represent a 3D scene with position and viewpoint-conditioned neural networks, 3D Gaussian Splatting utilizes a set of Gaussian ellipsoids to model the scene so that efficient rendering can be accomplished by rasterizing Gaussian ellipsoids into images. Apart from the fast rendering speed, the explicit representation of 3D Gaussian Splatting facilitates editing tasks like dynamic reconstruction, <b>geometry</b> editing, and physical <b>simulation.</b> Considering the rapid change and growing number of works in this field, we present a literature review of recent 3D Gaussian Splatting methods, which can be roughly classified into 3D reconstruction, 3D editing, and other downstream applications by functionality. Traditional point-based rendering methods and the rendering formulation of 3D Gaussian Splatting are also illustrated for a better understanding of this technique. This survey aims to help beginners get into this field quickly and provide experienced researchers with a comprehensive overview, which can stimulate the future development of the 3D Gaussian Splatting representation.

{{</citation>}}


### (28/57 | 82/156) DynamicGlue: Epipolar and Time-Informed Data Association in Dynamic Environments using Graph Neural Networks (Theresa Huber et al., 2024)

{{<citation>}}

Theresa Huber, Simon Schaefer, Stefan Leutenegger. (2024)  
**DynamicGlue: Epipolar and Time-Informed Data Association in Dynamic Environments using Graph Neural Networks**
<br/>
<button class="copy-to-clipboard" title="DynamicGlue: Epipolar and Time-Informed Data Association in Dynamic Environments using Graph Neural Networks" index=82>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-RO, cs.CV  
Keyword Score: 23  
Keywords: Graph, Graph Neural Network, Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11370v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11370v1.pdf" filename="2403.11370v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The assumption of a static environment is common in many geometric computer vision tasks like SLAM but limits their applicability in highly dynamic scenes. Since these tasks rely on identifying point correspondences between input images within the static part of the environment, we propose a <b>graph</b> <b>neural</b> <b>network-based</b> sparse feature matching network designed to perform robust matching under challenging conditions while excluding keypoints on moving objects. We employ a similar scheme of attentional aggregation over <b>graph</b> <b>edges</b> <b>to</b> enhance keypoint representations as state-of-the-art feature-matching networks but augment the <b>graph</b> <b>with</b> <b>epipolar</b> and temporal information and vastly reduce the number of <b>graph</b> <b>edges.</b> <b>Furthermore,</b> we introduce a <b>self-supervised</b> training scheme to extract pseudo labels for image pairs in dynamic environments from exclusively unprocessed visual-inertial data. A series of experiments show the superior performance of our network as it excludes keypoints on moving objects compared to state-of-the-art feature matching networks while still achieving similar results regarding conventional matching metrics. When integrated into a SLAM system, our network significantly improves performance, especially in highly dynamic scenes.

{{</citation>}}


### (29/57 | 83/156) GRA: Detecting Oriented Objects through Group-wise Rotating and Attention (Jiangshan Wang et al., 2024)

{{<citation>}}

Jiangshan Wang, Yifan Pu, Yizeng Han, Jiayi Guo, Yiru Wang, Xiu Li, Gao Huang. (2024)  
**GRA: Detecting Oriented Objects through Group-wise Rotating and Attention**
<br/>
<button class="copy-to-clipboard" title="GRA: Detecting Oriented Objects through Group-wise Rotating and Attention" index=83>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Object Detection, Benchmarking, Convolution  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11127v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11127v1.pdf" filename="2403.11127v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Oriented <b>object</b> <b>detection,</b> an emerging task in recent years, aims to identify and locate <b>objects</b> <b>across</b> varied orientations. This requires the detector to accurately capture the orientation information, which varies significantly within and across images. Despite the existing substantial efforts, simultaneously ensuring model effectiveness and parameter efficiency remains challenging in this scenario. In this paper, we propose a lightweight yet effective \textbf{G}roup-wise \textbf{R}otating and \textbf{A}ttention (GRA) module to replace the <b>convolution</b> operations in backbone networks for oriented <b>object</b> <b>detection.</b> GRA can adaptively capture fine-grained features of <b>objects</b> <b>with</b> diverse orientations, comprising two key components: Group-wise Rotating and Group-wise Attention. Group-wise Rotating first divides the <b>convolution</b> kernel into groups, where each group extracts different <b>object</b> <b>features</b> by rotating at a specific angle according to the <b>object</b> <b>orientation.</b> Subsequently, Group-wise Attention is employed to adaptively enhance the <b>object-related</b> <b>regions</b> in the feature. The collaborative effort of these components enables GRA to effectively capture the various orientation information while maintaining parameter efficiency. Extensive experimental results demonstrate the superiority of our method. For example, GRA achieves a new state-of-the-art (SOTA) on the DOTA-v2.0 <b>benchmark,</b> while saving the parameters by nearly 50\% compared to the previous SOTA method. Code will be released.

{{</citation>}}


### (30/57 | 84/156) Unifying Feature and Cost Aggregation with Transformers for Semantic and Visual Correspondence (Sunghwan Hong et al., 2024)

{{<citation>}}

Sunghwan Hong, Seokju Cho, Seungryong Kim, Stephen Lin. (2024)  
**Unifying Feature and Cost Aggregation with Transformers for Semantic and Visual Correspondence**
<br/>
<button class="copy-to-clipboard" title="Unifying Feature and Cost Aggregation with Transformers for Semantic and Visual Correspondence" index=84>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Benchmarking, Transformer, Stemming  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11120v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11120v1.pdf" filename="2403.11120v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper introduces a <b>Transformer-based</b> integrative feature and cost aggregation network designed for dense matching tasks. In the context of dense matching, many works benefit from one of two forms of aggregation: feature aggregation, which pertains to the alignment of similar features, or cost aggregation, a procedure aimed at instilling coherence in the flow estimates across neighboring pixels. In this work, we first show that feature aggregation and cost aggregation exhibit distinct characteristics and reveal the potential for substantial benefits <b>stemming</b> from the judicious use of both aggregation processes. We then introduce a simple yet effective architecture that harnesses self- and cross-attention mechanisms to show that our approach unifies feature aggregation and cost aggregation and effectively harnesses the strengths of both techniques. Within the proposed attention layers, the features and cost volume both complement each other, and the attention layers are interleaved through a coarse-to-fine design to further promote accurate correspondence estimation. Finally at inference, our network produces multi-scale predictions, computes their confidence scores, and selects the most confident flow for final prediction. Our framework is evaluated on standard <b>benchmarks</b> for semantic matching, and also applied to geometric matching, where we show that our approach achieves significant improvements compared to existing methods.

{{</citation>}}


### (31/57 | 85/156) 3D Human Reconstruction in the Wild with Synthetic Data Using Generative Models (Yongtao Ge et al., 2024)

{{<citation>}}

Yongtao Ge, Wenjia Wang, Yongfan Chen, Hao Chen, Chunhua Shen. (2024)  
**3D Human Reconstruction in the Wild with Synthetic Data Using Generative Models**
<br/>
<button class="copy-to-clipboard" title="3D Human Reconstruction in the Wild with Synthetic Data Using Generative Models" index=85>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: ControlNet, Diffusion Model, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11111v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11111v1.pdf" filename="2403.11111v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this work, we show that synthetic data created by generative models is complementary to computer graphics (CG) rendered data for achieving remarkable generalization performance on diverse real-world scenes for 3D human pose and shape estimation (HPS). Specifically, we propose an effective approach based on recent <b>diffusion</b> <b>models,</b> termed HumanWild, which can effortlessly generate human images and corresponding 3D mesh annotations. We first collect a large-scale human-centric dataset with comprehensive annotations, e.g., text captions and surface normal images. Then, we train a customized <b>ControlNet</b> model upon this dataset to generate diverse human images and initial ground-truth labels. At the core of this step is that we can easily obtain numerous surface normal images from a 3D human parametric model, e.g., SMPL-X, by rendering the 3D mesh onto the image plane. As there exists inevitable noise in the initial labels, we then apply an off-the-shelf foundation segmentation model, i.e., SAM, to filter negative data samples. Our data generation pipeline is flexible and customizable to facilitate different real-world tasks, e.g., ego-centric scenes and perspective-distortion scenes. The generated dataset comprises 0.79M images with corresponding 3D annotations, covering versatile viewpoints, scenes, and human identities. We train various HPS regressors on top of the generated data and evaluate them on a wide range of <b>benchmarks</b> (3DPW, RICH, EgoBody, AGORA, SSP-3D) to verify the effectiveness of the generated data. By exclusively employing generative models, we generate large-scale in-the-wild human images and high-quality annotations, eliminating the need for real-world data collection.

{{</citation>}}


### (32/57 | 86/156) Enhancing Bandwidth Efficiency for Video Motion Transfer Applications using Deep Learning Based Keypoint Prediction (Xue Bai et al., 2024)

{{<citation>}}

Xue Bai, Tasmiah Haque, Sumit Mohan, Yuliang Cai, Byungheon Jeong, Adam Halasz, Srinjoy Das. (2024)  
**Enhancing Bandwidth Efficiency for Video Motion Transfer Applications using Deep Learning Based Keypoint Prediction**
<br/>
<button class="copy-to-clipboard" title="Enhancing Bandwidth Efficiency for Video Motion Transfer Applications using Deep Learning Based Keypoint Prediction" index=86>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Self-supervised Learning, Recurrent Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11337v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11337v1.pdf" filename="2403.11337v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We propose a deep learning based novel prediction framework for enhanced bandwidth reduction in motion transfer enabled video applications such as video conferencing, virtual reality gaming and privacy preservation for patient health monitoring. To model complex motion, we use the First Order Motion Model (FOMM) that represents dynamic objects using learned keypoints along with their local affine transformations. Keypoints are extracted by a <b>self-supervised</b> keypoint detector and organized in a time series corresponding to the video frames. Prediction of keypoints, to enable transmission using lower frames per second on the source device, is performed using a Variational <b>Recurrent</b> <b>Neural</b> <b>Network</b> (VRNN). The predicted keypoints are then synthesized to video frames using an optical flow estimator and a generator network. This efficacy of leveraging keypoint based representations in conjunction with VRNN based prediction for both video animation and reconstruction is demonstrated on three diverse datasets. For real-time applications, our results show the effectiveness of our proposed architecture by enabling up to 2x additional bandwidth reduction over existing keypoint based video motion transfer frameworks without significantly compromising video quality.

{{</citation>}}


### (33/57 | 87/156) NeoNeXt: Novel neural network operator and architecture based on the patch-wise matrix multiplications (Vladimir Korviakov et al., 2024)

{{<citation>}}

Vladimir Korviakov, Denis Koposov. (2024)  
**NeoNeXt: Novel neural network operator and architecture based on the patch-wise matrix multiplications**
<br/>
<button class="copy-to-clipboard" title="NeoNeXt: Novel neural network operator and architecture based on the patch-wise matrix multiplications" index=87>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Convolution, Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11251v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11251v1.pdf" filename="2403.11251v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Most of the computer vision architectures nowadays are built upon the well-known foundation operations: fully-connected layers, <b>convolutions</b> and multi-head <b>self-attention</b> blocks. In this paper we propose a novel foundation operation - NeoCell - which learns matrix patterns and performs patchwise matrix multiplications with the input data. The main advantages of the proposed operator are (1) simple implementation without need in operations like im2col, (2) low computational complexity (especially for large matrices) and (3) simple and flexible implementation of up-/down-sampling. We validate NeoNeXt family of models based on this operation on ImageNet-1K classification task and show that they achieve competitive quality.

{{</citation>}}


### (34/57 | 88/156) Concatenate, Fine-tuning, Re-training: A SAM-enabled Framework for Semi-supervised 3D Medical Image Segmentation (Shumeng Li et al., 2024)

{{<citation>}}

Shumeng Li, Lei Qi, Qian Yu, Jing Huo, Yinghuan Shi, Yang Gao. (2024)  
**Concatenate, Fine-tuning, Re-training: A SAM-enabled Framework for Semi-supervised 3D Medical Image Segmentation**
<br/>
<button class="copy-to-clipboard" title="Concatenate, Fine-tuning, Re-training: A SAM-enabled Framework for Semi-supervised 3D Medical Image Segmentation" index=88>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Fine-tuning, Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11229v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11229v1.pdf" filename="2403.11229v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Segment Anything Model (SAM) <b>fine-tuning</b> has shown remarkable performance in medical image segmentation in a fully <b>supervised</b> manner, but requires precise annotations. To reduce the annotation cost and maintain satisfactory performance, in this work, we leverage the capabilities of SAM for establishing semi-supervised medical image segmentation models. Rethinking the requirements of effectiveness, efficiency, and compatibility, we propose a three-stage framework, i.e., Concatenate, <b>Fine-tuning,</b> and Re-training (CFR). The current <b>fine-tuning</b> approaches mostly involve 2D slice-wise <b>fine-tuning</b> that disregards the contextual information between adjacent slices. Our concatenation strategy mitigates the mismatch between natural and 3D medical images. The concatenated images are then used for <b>fine-tuning</b> SAM, providing robust initialization pseudo-labels. Afterwards, we train a 3D semi-supervised segmentation model while maintaining the same parameter size as the conventional segmenter such as V-Net. Our CFR framework is plug-and-play, and easily compatible with various popular semi-supervised methods. Extensive experiments validate that our CFR achieves significant improvements in both moderate annotation and scarce annotation across four datasets. In particular, CFR framework improves the Dice score of Mean Teacher from 29.68% to 74.40% with only one labeled data of LA dataset.

{{</citation>}}


### (35/57 | 89/156) MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data (Paul S. Scotti et al., 2024)

{{<citation>}}

Paul S. Scotti, Mihir Tripathy, Cesar Kadir Torrico Villanueva, Reese Kneeland, Tong Chen, Ashutosh Narang, Charan Santhirasegaran, Jonathan Xu, Thomas Naselaris, Kenneth A. Norman, Tanishq Mathew Abraham. (2024)  
**MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data**
<br/>
<button class="copy-to-clipboard" title="MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data" index=89>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV, q-bio-NC  
Keyword Score: 20  
Keywords: Fine-tuning, Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11207v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11207v1.pdf" filename="2403.11207v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Reconstructions of visual perception from brain activity have improved tremendously, but the practical utility of such methods has been limited. This is because such models are trained independently per subject where each subject requires dozens of hours of expensive fMRI training data to attain high-quality results. The present work showcases high-quality reconstructions using only 1 hour of fMRI training data. We pretrain our model across 7 subjects and then <b>fine-tune</b> on minimal data from a new subject. Our novel functional alignment procedure linearly maps all brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space. We then map from CLIP space to pixel space by <b>fine-tuning</b> Stable Diffusion XL to accept CLIP latents as inputs instead of text. This approach improves out-of-subject generalization with limited training data and also attains state-of-the-art image retrieval and reconstruction metrics compared to single-subject approaches. MindEye2 demonstrates how accurate reconstructions of perception are possible from a single visit to the MRI facility. All code is available on GitHub.

{{</citation>}}


### (36/57 | 90/156) DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation (Yuanchen Wu et al., 2024)

{{<citation>}}

Yuanchen Wu, Xichen Ye, Kequan Yang, Jide Li, Xiaoqiang Li. (2024)  
**DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation**
<br/>
<button class="copy-to-clipboard" title="DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation" index=90>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Supervised Learning, Weakly-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11184v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11184v1.pdf" filename="2403.11184v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recently, One-stage Weakly <b>Supervised</b> Semantic Segmentation (WSSS) with image-level labels has gained increasing interest due to simplification over its cumbersome multi-stage counterpart. Limited by the inherent ambiguity of Class Activation Map (CAM), we observe that one-stage pipelines often encounter confirmation bias caused by incorrect CAM pseudo-labels, impairing their final segmentation performance. Although recent works discard many unreliable pseudo-labels to implicitly alleviate this issue, they fail to exploit sufficient supervision for their models. To this end, we propose a dual student framework with trustworthy progressive learning (DuPL). Specifically, we propose a dual student network with a discrepancy loss to yield diverse CAMs for each sub-net. The two sub-nets generate supervision for each other, mitigating the confirmation bias caused by learning their own incorrect pseudo-labels. In this process, we progressively introduce more trustworthy pseudo-labels to be involved in the supervision through dynamic threshold adjustment with an adaptive noise filtering strategy. Moreover, we believe that every pixel, even discarded from supervision due to its unreliability, is important for WSSS. Thus, we develop consistency regularization on these discarded regions, providing supervision of every pixel. Experiment results demonstrate the superiority of the proposed DuPL over the recent state-of-the-art alternatives on PASCAL VOC 2012 and MS COCO datasets. Code is available at https://github.com/Wu0409/DuPL.

{{</citation>}}


### (37/57 | 91/156) Source Prompt Disentangled Inversion for Boosting Image Editability with Diffusion Models (Ruibin Li et al., 2024)

{{<citation>}}

Ruibin Li, Ruihuang Li, Song Guo, Lei Zhang. (2024)  
**Source Prompt Disentangled Inversion for Boosting Image Editability with Diffusion Models**
<br/>
<button class="copy-to-clipboard" title="Source Prompt Disentangled Inversion for Boosting Image Editability with Diffusion Models" index=91>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Diffusion Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11105v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11105v1.pdf" filename="2403.11105v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Text-driven <b>diffusion</b> <b>models</b> have significantly advanced the image editing performance by using text <b>prompts</b> as inputs. One crucial step in text-driven image editing is to invert the original image into a latent noise code conditioned on the source <b>prompt.</b> While previous methods have achieved promising results by refactoring the image synthesizing process, the inverted latent noise code is tightly coupled with the source <b>prompt,</b> limiting the image editability by target text <b>prompts.</b> To address this issue, we propose a novel method called Source <b>Prompt</b> Disentangled Inversion (SPDInv), which aims at reducing the impact of source <b>prompt,</b> thereby enhancing the text-driven image editing performance by employing <b>diffusion</b> <b>models.</b> To make the inverted noise code be independent of the given source <b>prompt</b> as much as possible, we indicate that the iterative inversion process should satisfy a fixed-point constraint. Consequently, we transform the inversion problem into a searching problem to find the fixed-point solution, and utilize the pre-trained <b>diffusion</b> <b>models</b> to facilitate the searching process. The experimental results show that our proposed SPDInv method can effectively mitigate the conflicts between the target editing <b>prompt</b> and the source <b>prompt,</b> leading to a significant decrease in editing artifacts. In addition to text-driven image editing, with SPDInv we can easily adapt customized image generation models to localized editing tasks and produce promising performance. The source code are available at https://github.com/leeruibin/SPDInv.

{{</citation>}}


### (38/57 | 92/156) Zippo: Zipping Color and Transparency Distributions into a Single Diffusion Model (Kangyang Xie et al., 2024)

{{<citation>}}

Kangyang Xie, Binbin Yang, Hao Chen, Meng Wang, Cheng Zou, Hui Xue, Ming Yang, Chunhua Shen. (2024)  
**Zippo: Zipping Color and Transparency Distributions into a Single Diffusion Model**
<br/>
<button class="copy-to-clipboard" title="Zippo: Zipping Color and Transparency Distributions into a Single Diffusion Model" index=92>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Diffusion Model, Text2image  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11077v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11077v1.pdf" filename="2403.11077v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Beyond the superiority of the <b>text-to-image</b> <b>diffusion</b> <b>model</b> in generating high-quality images, recent studies have attempted to uncover its potential for adapting the learned semantic knowledge to visual perception tasks. In this work, instead of translating a generative <b>diffusion</b> <b>model</b> into a visual perception model, we explore to retain the generative ability with the perceptive adaptation. To accomplish this, we present Zippo, a unified framework for zipping the color and transparency distributions into a single <b>diffusion</b> <b>model</b> by expanding the <b>diffusion</b> <b>latent</b> into a joint representation of RGB images and alpha mattes. By alternatively selecting one modality as the condition and then applying the <b>diffusion</b> <b>process</b> to the counterpart modality, Zippo is capable of generating RGB images from alpha mattes and predicting transparency from input images. In addition to single-modality prediction, we propose a modality-aware noise reassignment strategy to further empower Zippo with jointly generating RGB images and its corresponding alpha mattes under the text guidance. Our experiments showcase Zippo's ability of efficient text-conditioned transparent image generation and present plausible results of Matte-to-RGB and RGB-to-Matte translation.

{{</citation>}}


### (39/57 | 93/156) Intelligent Railroad Grade Crossing: Leveraging Semantic Segmentation and Object Detection for Enhanced Safety (Al Amin et al., 2024)

{{<citation>}}

Al Amin, Deo Chimba, Kamrul Hasan, Emmanuel Samson. (2024)  
**Intelligent Railroad Grade Crossing: Leveraging Semantic Segmentation and Object Detection for Enhanced Safety**
<br/>
<button class="copy-to-clipboard" title="Intelligent Railroad Grade Crossing: Leveraging Semantic Segmentation and Object Detection for Enhanced Safety" index=93>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Yolo, Object Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11060v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11060v1.pdf" filename="2403.11060v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Crashes and delays at Railroad Highway Grade Crossings (RHGC), where highways and railroads intersect, pose significant safety concerns for the U.S. Federal Railroad Administration (FRA). Despite the critical importance of addressing accidents and traffic delays at highway-railroad intersections, there is a notable dearth of research on practical solutions for managing these issues. In response to this gap in the literature, our study introduces an intelligent system that leverages machine learning and computer vision techniques to enhance safety at Railroad Highway Grade crossings (RHGC). This research proposed a Non-Maximum Suppression (NMS)- based ensemble model that integrates a variety of <b>YOLO</b> variants, specifically YOLOv5S, YOLOv5M, and YOLOv5L, for grade-crossing <b>object</b> <b>detection,</b> utilizes segmentation techniques from the UNet architecture for detecting approaching rail at a grade crossing. Both methods are implemented on a Raspberry Pi. Moreover, the strategy employs high-definition cameras installed at the RHGC. This framework enables the system to monitor <b>objects</b> <b>within</b> the Region of Interest (ROI) at crossings, detect the approach of trains, and clear the crossing area before a train arrives. Regarding accuracy, precision, recall, and Intersection over Union (IoU), the proposed state-of-the-art NMS-based <b>object</b> <b>detection</b> ensemble model achieved 96% precision. In addition, the UNet segmentation model obtained a 98% IoU value. This automated railroad grade crossing system powered by artificial intelligence represents a promising solution for enhancing safety at highway-railroad intersections.

{{</citation>}}


### (40/57 | 94/156) OSTAF: A One-Shot Tuning Method for Improved Attribute-Focused T2I Personalization (Ye Wang et al., 2024)

{{<citation>}}

Ye Wang, Zili Yi, Rui Ma. (2024)  
**OSTAF: A One-Shot Tuning Method for Improved Attribute-Focused T2I Personalization**
<br/>
<button class="copy-to-clipboard" title="OSTAF: A One-Shot Tuning Method for Improved Attribute-Focused T2I Personalization" index=94>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Fine-tuning, Text2image  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11053v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11053v1.pdf" filename="2403.11053v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Personalized <b>text-to-image</b> (T2I) models not only produce lifelike and varied visuals but also allow users to tailor the images to fit their personal taste. These personalization techniques can grasp the essence of a concept through a collection of images, or adjust a pre-trained <b>text-to-image</b> model with a specific image input for subject-driven or attribute-aware guidance. Yet, accurately capturing the distinct visual attributes of an individual image poses a challenge for these methods. To address this issue, we introduce OSTAF, a novel parameter-efficient one-shot <b>fine-tuning</b> method which only utilizes one reference image for T2I personalization. A novel hypernetwork-powered attribute-focused <b>fine-tuning</b> mechanism is employed to achieve the precise learning of various attribute features (e.g., appearance, shape or drawing style) from the reference image. Comparing to existing image customization methods, our method shows significant superiority in attribute identification and application, as well as achieves a good balance between efficiency and output quality.

{{</citation>}}


### (41/57 | 95/156) V2X-DGW: Domain Generalization for Multi-agent Perception under Adverse Weather Conditions (Baolu Li et al., 2024)

{{<citation>}}

Baolu Li, Jinlong Li, Xinyu Liu, Runsheng Xu, Zhengzhong Tu, Jiacheng Guo, Xiaopeng Li, Hongkai Yu. (2024)  
**V2X-DGW: Domain Generalization for Multi-agent Perception under Adverse Weather Conditions**
<br/>
<button class="copy-to-clipboard" title="V2X-DGW: Domain Generalization for Multi-agent Perception under Adverse Weather Conditions" index=95>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 18  
Keywords: Object Detection, Benchmarking, Representation Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11371v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11371v1.pdf" filename="2403.11371v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Current LiDAR-based Vehicle-to-Everything (V2X) multi-agent perception systems have shown the significant success on 3D <b>object</b> <b>detection.</b> While these models perform well in the trained clean weather, they struggle in unseen adverse weather conditions with the real-world domain gap. In this paper, we propose a domain generalization approach, named V2X-DGW, for LiDAR-based 3D <b>object</b> <b>detection</b> on multi-agent perception system under adverse weather conditions. Not only in the clean weather does our research aim to ensure favorable multi-agent performance, but also in the unseen adverse weather conditions by learning only on the clean weather data. To advance research in this area, we have simulated the impact of three prevalent adverse weather conditions on two widely-used multi-agent datasets, resulting in the creation of two novel <b>benchmark</b> datasets: OPV2V-w and V2XSet-w. To this end, we first introduce the Adaptive Weather Augmentation (AWA) to mimic the unseen adverse weather conditions, and then propose two alignments for generalizable <b>representation</b> <b>learning:</b> Trust-region Weather-invariant Alignment (TWA) and Agent-aware Contrastive Alignment (ACA). Extensive experimental results demonstrate that our V2X-DGW achieved improvements in the unseen adverse weather conditions.

{{</citation>}}


### (42/57 | 96/156) Bilateral Propagation Network for Depth Completion (Jie Tang et al., 2024)

{{<citation>}}

Jie Tang, Fei-Peng Tian, Boshi An, Jian Li, Ping Tan. (2024)  
**Bilateral Propagation Network for Depth Completion**
<br/>
<button class="copy-to-clipboard" title="Bilateral Propagation Network for Depth Completion" index=96>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 16  
Keywords: Benchmarking, Convolution, Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11270v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11270v1.pdf" filename="2403.11270v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Depth completion aims to derive a dense depth map from sparse depth measurements with a synchronized color image. Current state-of-the-art (SOTA) methods are predominantly propagation-based, which work as an iterative refinement on the initial estimated dense depth. However, the initial depth estimations mostly result from direct applications of <b>convolutional</b> layers on the sparse depth map. In this paper, we present a Bilateral Propagation Network (BP-Net), that propagates depth at the earliest stage to avoid directly convolving on sparse data. Specifically, our approach propagates the target depth from nearby depth measurements via a non-linear model, whose coefficients are generated through a multi-layer perceptron conditioned on both \emph{radiometric difference} and \emph{spatial distance}. By integrating bilateral propagation with <b>multi-modal</b> fusion and depth refinement in a multi-scale framework, our BP-Net demonstrates outstanding performance on both indoor and outdoor scenes. It achieves SOTA on the NYUv2 dataset and ranks 1st on the KITTI depth completion <b>benchmark</b> at the time of submission. Experimental results not only show the effectiveness of bilateral propagation but also emphasize the significance of early-stage propagation in contrast to the refinement stage. Our code and trained models will be available on the project page.

{{</citation>}}


### (43/57 | 97/156) A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose Estimation (Qucheng Peng et al., 2024)

{{<citation>}}

Qucheng Peng, Ce Zheng, Chen Chen. (2024)  
**A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose Estimation**
<br/>
<button class="copy-to-clipboard" title="A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose Estimation" index=97>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 13  
Keywords: Adversarial Learning, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11310v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11310v1.pdf" filename="2403.11310v1.pdf">Download PDF</button>

---


**ABSTRACT**  
3D human pose data collected in controlled laboratory settings present challenges for pose estimators that generalize across diverse scenarios. To address this, domain generalization is employed. Current methodologies in domain generalization for 3D human pose estimation typically utilize <b>adversarial</b> <b>training</b> to generate synthetic poses for training. Nonetheless, these approaches exhibit several limitations. First, the lack of prior information about the target domain complicates the application of suitable augmentation through a single pose augmentor, affecting generalization on target domains. Moreover, <b>adversarial</b> <b>training's</b> discriminator tends to enforce similarity between source and synthesized poses, impeding the exploration of out-of-source distributions. Furthermore, the pose estimator's optimization is not exposed to domain shifts, limiting its overall generalization ability. To address these limitations, we propose a novel framework featuring two pose augmentors: the weak and the strong augmentors. Our framework employs differential strategies for generation and discrimination processes, facilitating the preservation of knowledge related to source poses and the exploration of out-of-source distributions without prior information about target poses. Besides, we leverage meta-optimization to simulate domain shifts in the optimization process of the pose estimator, thereby improving its generalization ability. Our proposed approach significantly outperforms existing methods, as demonstrated through comprehensive experiments on various <b>benchmark</b> datasets.

{{</citation>}}


### (44/57 | 98/156) Universal Semi-Supervised Domain Adaptation by Mitigating Common-Class Bias (Wenyu Zhang et al., 2024)

{{<citation>}}

Wenyu Zhang, Qingmu Liu, Felix Ong Wei Cong, Mohamed Ragab, Chuan-Sheng Foo. (2024)  
**Universal Semi-Supervised Domain Adaptation by Mitigating Common-Class Bias**
<br/>
<button class="copy-to-clipboard" title="Universal Semi-Supervised Domain Adaptation by Mitigating Common-Class Bias" index=98>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 13  
Keywords: Benchmarking, Domain Adaptation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11234v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11234v1.pdf" filename="2403.11234v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Domain</b> <b>adaptation</b> is a critical task in machine learning that aims to improve model performance on a target <b>domain</b> <b>by</b> leveraging knowledge from a related source <b>domain.</b> <b>In</b> this work, we introduce Universal Semi-Supervised <b>Domain</b> <b>Adaptation</b> (UniSSDA), a practical yet challenging setting where the target <b>domain</b> <b>is</b> partially labeled, and the source and target label space may not strictly match. UniSSDA is at the intersection of Universal <b>Domain</b> <b>Adaptation</b> (UniDA) and Semi-Supervised <b>Domain</b> <b>Adaptation</b> (SSDA): the UniDA setting does not allow for fine-grained categorization of target private classes not represented in the source <b>domain,</b> <b>while</b> SSDA focuses on the restricted closed-set setting where source and target label spaces match exactly. Existing UniDA and SSDA methods are susceptible to common-class bias in UniSSDA settings, where models overfit to data distributions of classes common to both <b>domains</b> <b>at</b> the expense of private classes. We propose a new prior-guided pseudo-label refinement strategy to reduce the reinforcement of common-class bias due to pseudo-labeling, a common label propagation strategy in <b>domain</b> <b>adaptation.</b> We demonstrate the effectiveness of the proposed strategy on <b>benchmark</b> datasets Office-Home, DomainNet, and VisDA. The proposed strategy attains the best performance across UniSSDA adaptation settings and establishes a new baseline for UniSSDA.

{{</citation>}}


### (45/57 | 99/156) Neural Markov Random Field for Stereo Matching (Tongfan Guan et al., 2024)

{{<citation>}}

Tongfan Guan, Chen Wang, Yun-Hui Liu. (2024)  
**Neural Markov Random Field for Stereo Matching**
<br/>
<button class="copy-to-clipboard" title="Neural Markov Random Field for Stereo Matching" index=99>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 13  
Keywords: Message-Passing, Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11193v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11193v1.pdf" filename="2403.11193v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Stereo matching is a core task for many computer vision and robotics applications. Despite their dominance in traditional stereo methods, the hand-crafted Markov Random Field (MRF) models lack sufficient modeling accuracy compared to end-to-end deep models. While deep learning representations have greatly improved the unary terms of the MRF models, the overall accuracy is still severely limited by the hand-crafted pairwise terms and message passing. To address these issues, we propose a neural MRF model, where both potential functions and message passing are designed using data-driven neural networks. Our fully data-driven model is built on the foundation of variational inference theory, to prevent convergence issues and retain stereo MRF's <b>graph</b> inductive bias. To make the inference tractable and scale well to high-resolution images, we also propose a Disparity Proposal Network (DPN) to adaptively prune the search space of disparity. The proposed approach ranks $1^{st}$ on both KITTI 2012 and 2015 leaderboards among all published methods while running faster than 100 ms. This approach significantly outperforms prior global methods, e.g., lowering D1 metric by more than 50% on KITTI 2015. In addition, our method exhibits strong cross-domain generalization and can recover sharp edges. The codes at https://github.com/aeolusguan/NMRF .

{{</citation>}}


### (46/57 | 100/156) NetTrack: Tracking Highly Dynamic Objects with a Net (Guangze Zheng et al., 2024)

{{<citation>}}

Guangze Zheng, Shijie Lin, Haobo Zuo, Changhong Fu, Jia Pan. (2024)  
**NetTrack: Tracking Highly Dynamic Objects with a Net**
<br/>
<button class="copy-to-clipboard" title="NetTrack: Tracking Highly Dynamic Objects with a Net" index=100>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 13  
Keywords: Benchmarking, Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11186v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11186v1.pdf" filename="2403.11186v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The complex dynamicity of open-world objects presents non-negligible challenges for multi-object tracking (MOT), often manifested as severe deformations, fast motion, and occlusions. Most methods that solely depend on coarse-grained object cues, such as boxes and the overall appearance of the object, are susceptible to degradation due to distorted internal relationships of dynamic objects. To address this problem, this work proposes NetTrack, an efficient, generic, and affordable tracking framework to introduce fine-grained learning that is robust to dynamicity. Specifically, NetTrack constructs a dynamicity-aware association with a fine-grained Net, leveraging point-level visual cues. Correspondingly, a fine-grained sampler and matching method have been incorporated. Furthermore, NetTrack learns object-text correspondence for fine-grained localization. To evaluate MOT in extremely dynamic open-world scenarios, a bird flock tracking (BFT) dataset is constructed, which exhibits high dynamicity with diverse species and open-world scenarios. Comprehensive evaluation on BFT validates the effectiveness of fine-grained learning on object dynamicity, and thorough transfer experiments on challenging open-world <b>benchmarks,</b> i.e., TAO, TAO-OW, AnimalTrack, and GMOT-40, validate the strong generalization ability of NetTrack even without <b>finetuning.</b> Project page: https://george-zhuang.github.io/nettrack/.

{{</citation>}}


### (47/57 | 101/156) 3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual ReLocalization (Peng Jiang et al., 2024)

{{<citation>}}

Peng Jiang, Gaurav Pandey, Srikanth Saripalli. (2024)  
**3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual ReLocalization**
<br/>
<button class="copy-to-clipboard" title="3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual ReLocalization" index=101>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-GR, cs-RO, cs.CV  
Keyword Score: 10  
Keywords: Knowledge Distillation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11367v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11367v1.pdf" filename="2403.11367v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents a novel system designed for 3D mapping and visual relocalization using 3D Gaussian Splatting. Our proposed method uses LiDAR and camera data to create accurate and visually plausible representations of the environment. By leveraging LiDAR data to initiate the training of the 3D Gaussian Splatting map, our system constructs maps that are both detailed and geometrically accurate. To mitigate excessive GPU memory usage and facilitate rapid spatial queries, we employ a combination of a 2D voxel map and a <b>KD-tree.</b> This preparation makes our method well-suited for visual localization tasks, enabling efficient identification of correspondences between the query image and the rendered image from the Gaussian Splatting map via normalized cross-correlation (NCC). Additionally, we refine the camera pose of the query image using feature-based matching and the Perspective-n-Point (PnP) technique. The effectiveness, adaptability, and precision of our system are demonstrated through extensive evaluation on the KITTI360 dataset.

{{</citation>}}


### (48/57 | 102/156) Domain-Guided Masked Autoencoders for Unique Player Identification (Bavesh Balaji et al., 2024)

{{<citation>}}

Bavesh Balaji, Jerrin Bright, Sirisha Rambhatla, Yuhao Chen, Alexander Wong, John Zelek, David A Clausi. (2024)  
**Domain-Guided Masked Autoencoders for Unique Player Identification**
<br/>
<button class="copy-to-clipboard" title="Domain-Guided Masked Autoencoders for Unique Player Identification" index=102>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Autoencoder  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11328v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11328v1.pdf" filename="2403.11328v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Unique player identification is a fundamental module in vision-driven sports analytics. Identifying players from broadcast videos can aid with various downstream tasks such as player assessment, in-game analysis, and broadcast production. However, automatic detection of jersey numbers using deep features is challenging primarily due to: a) motion blur, b) low resolution video feed, and c) occlusions. With their recent success in various vision tasks, masked <b>autoencoders</b> (MAEs) have emerged as a superior alternative to conventional feature extractors. However, most MAEs simply zero-out image patches either randomly or focus on where to mask rather than how to mask. Motivated by human vision, we devise a novel domain-guided masking policy for MAEs termed d-MAE to facilitate robust feature extraction in the presence of motion blur for player identification. We further introduce a new spatio-temporal network leveraging our novel d-MAE for unique player identification. We conduct experiments on three large-scale sports datasets, including a curated baseball dataset, the SoccerNet dataset, and an in-house ice hockey dataset. We preprocess the datasets using an upgraded keyframe identification (KfID) module by focusing on frames containing jersey numbers. Additionally, we propose a keyframe-fusion technique to augment keyframes, preserving spatial and temporal context. Our spatio-temporal network showcases significant improvements, surpassing the current state-of-the-art by 8.58%, 4.29%, and 1.20% in the test set accuracies, respectively. Rigorous ablations highlight the effectiveness of our domain-guided masking approach and the refined KfID module, resulting in performance enhancements of 1.48% and 1.84% respectively, compared to original architectures.

{{</citation>}}


### (49/57 | 103/156) Stylized Face Sketch Extraction via Generative Prior with Limited Data (Kwan Yun et al., 2024)

{{<citation>}}

Kwan Yun, Kwanggyoon Seo, Chang Wook Seo, Soyeon Yoon, Seongcheol Kim, Soohyun Ji, Amirsaman Ashtari, Junyong Noh. (2024)  
**Stylized Face Sketch Extraction via Generative Prior with Limited Data**
<br/>
<button class="copy-to-clipboard" title="Stylized Face Sketch Extraction via Generative Prior with Limited Data" index=103>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: 68T45, I-4-9, cs-CV, cs-GR, cs.CV  
Keyword Score: 10  
Keywords: Few-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11263v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11263v1.pdf" filename="2403.11263v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Facial sketches are both a concise way of showing the identity of a person and a means to express artistic intention. While a few techniques have recently emerged that allow sketches to be extracted in different styles, they typically rely on a large amount of data that is difficult to obtain. Here, we propose StyleSketch, a method for extracting high-resolution stylized sketches from a face image. Using the rich semantics of the deep features from a pretrained StyleGAN, we are able to train a sketch generator with 16 pairs of face and the corresponding sketch images. The sketch generator utilizes part-based losses with two-stage learning for fast convergence during training for high-quality sketch extraction. Through a set of comparisons, we show that StyleSketch outperforms existing state-of-the-art sketch extraction methods and <b>few-shot</b> image adaptation methods for the task of extracting high-resolution abstract face sketches. We further demonstrate the versatility of StyleSketch by extending its use to other domains and explore the possibility of semantic editing. The project page can be found in https://kwanyun.github.io/stylesketch_project.

{{</citation>}}


### (50/57 | 104/156) THOR: Text to Human-Object Interaction Diffusion via Relation Intervention (Qianyang Wu et al., 2024)

{{<citation>}}

Qianyang Wu, Ye Shi, Xiaoshui Huang, Jingyi Yu, Lan Xu, Jingya Wang. (2024)  
**THOR: Text to Human-Object Interaction Diffusion via Relation Intervention**
<br/>
<button class="copy-to-clipboard" title="THOR: Text to Human-Object Interaction Diffusion via Relation Intervention" index=104>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Diffusion Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11208v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11208v1.pdf" filename="2403.11208v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper addresses new methodologies to deal with the challenging task of generating dynamic Human-Object Interactions from textual descriptions (Text2HOI). While most existing works assume interactions with limited body parts or static objects, our task involves addressing the variation in human motion, the diversity of object shapes, and the semantic vagueness of object motion simultaneously. To tackle this, we propose a novel Text-guided Human-Object Interaction <b>diffusion</b> <b>model</b> with Relation Intervention (THOR). THOR is a cohesive <b>diffusion</b> <b>model</b> equipped with a relation intervention mechanism. In each <b>diffusion</b> <b>step,</b> we initiate text-guided human and object motion and then leverage human-object relations to intervene in object motion. This intervention enhances the spatial-temporal relations between humans and objects, with human-centric interaction representation providing additional guidance for synthesizing consistent motion from text. To achieve more reasonable and realistic results, interaction losses is introduced at different levels of motion granularity. Moreover, we construct Text-BEHAVE, a Text2HOI dataset that seamlessly integrates textual descriptions with the currently largest publicly available 3D HOI dataset. Both quantitative and qualitative experiments demonstrate the effectiveness of our proposed model.

{{</citation>}}


### (51/57 | 105/156) Self-Supervised Video Desmoking for Laparoscopic Surgery (Renlong Wu et al., 2024)

{{<citation>}}

Renlong Wu, Zhilu Zhang, Shuohao Zhang, Longfei Gou, Haobin Chen, Lei Zhang, Hao Chen, Wangmeng Zuo. (2024)  
**Self-Supervised Video Desmoking for Laparoscopic Surgery**
<br/>
<button class="copy-to-clipboard" title="Self-Supervised Video Desmoking for Laparoscopic Surgery" index=105>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11192v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11192v1.pdf" filename="2403.11192v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Due to the difficulty of collecting real paired data, most existing desmoking methods train the models by synthesizing smoke, generalizing poorly to real surgical scenarios. Although a few works have explored single-image real-world desmoking in unpaired learning manners, they still encounter challenges in handling dense smoke. In this work, we address these issues together by introducing the <b>self-supervised</b> surgery video desmoking (SelfSVD). On the one hand, we observe that the frame captured before the activation of high-energy devices is generally clear (named pre-smoke frame, PS frame), thus it can serve as supervision for other smoky frames, making real-world <b>self-supervised</b> video desmoking practically feasible. On the other hand, in order to enhance the desmoking performance, we further feed the valuable information from PS frame into models, where a masking strategy and a regularization term are presented to avoid trivial solutions. In addition, we construct a real surgery video dataset for desmoking, which covers a variety of smoky scenes. Extensive experiments on the dataset show that our SelfSVD can remove smoke more effectively and efficiently while recovering more photo-realistic details than the state-of-the-art methods. The dataset, codes, and pre-trained models are available at \url{https://github.com/ZcsrenlongZ/SelfSVD}.

{{</citation>}}


### (52/57 | 106/156) Hierarchical Generative Network for Face Morphing Attacks (Zuyuan He et al., 2024)

{{<citation>}}

Zuyuan He, Zongyong Deng, Qiaoyun He, Qijun Zhao. (2024)  
**Hierarchical Generative Network for Face Morphing Attacks**
<br/>
<button class="copy-to-clipboard" title="Hierarchical Generative Network for Face Morphing Attacks" index=106>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Face Recognition  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11101v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11101v1.pdf" filename="2403.11101v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Face</b> <b>morphing</b> attacks circumvent <b>face</b> <b>recognition</b> systems (FRSs) by creating a morphed image that contains multiple identities. However, existing <b>face</b> <b>morphing</b> attack methods either sacrifice image quality or compromise the identity preservation capability. Consequently, these attacks fail to bypass FRSs verification well while still managing to deceive human observers. These methods typically rely on global information from contributing images, ignoring the detailed information from effective facial regions. To address the above issues, we propose a novel morphing attack method to improve the quality of morphed images and better preserve the contributing identities. Our proposed method leverages the hierarchical generative network to capture both local detailed and global consistency information. Additionally, a mask-guided image blending module is dedicated to removing artifacts from areas outside the <b>face</b> <b>to</b> improve the image's visual quality. The proposed attack method is compared to state-of-the-art methods on three public datasets in terms of FRSs' vulnerability, attack detectability, and image quality. The results show our method's potential threat of deceiving FRSs while being capable of passing multiple morphing attack detection (MAD) scenarios.

{{</citation>}}


### (53/57 | 107/156) Controllable Relation Disentanglement for Few-Shot Class-Incremental Learning (Yuan Zhou et al., 2024)

{{<citation>}}

Yuan Zhou, Richang Hong, Yanrong Guo, Lin Liu, Shijie Hao, Hanwang Zhang. (2024)  
**Controllable Relation Disentanglement for Few-Shot Class-Incremental Learning**
<br/>
<button class="copy-to-clipboard" title="Controllable Relation Disentanglement for Few-Shot Class-Incremental Learning" index=107>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Few-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11070v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11070v1.pdf" filename="2403.11070v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we propose to tackle <b>Few-Shot</b> Class-Incremental Learning (FSCIL) from a new perspective, i.e., relation disentanglement, which means enhancing FSCIL via disentangling spurious relation between categories. The challenge of disentangling spurious correlations lies in the poor controllability of FSCIL. On one hand, an FSCIL model is required to be trained in an incremental manner and thus it is very hard to directly control relationships between categories of different sessions. On the other hand, training samples per novel category are only in the <b>few-shot</b> setting, which increases the difficulty of alleviating spurious relation issues as well. To overcome this challenge, in this paper, we propose a new simple-yet-effective method, called ConTrollable Relation-disentangLed <b>Few-Shot</b> Class-Incremental Learning (CTRL-FSCIL). Specifically, during the base session, we propose to anchor base category embeddings in feature space and construct disentanglement proxies to bridge gaps between the learning for category representations in different sessions, thereby making category relation controllable. During incremental learning, the parameters of the backbone network are frozen in order to relieve the negative impact of data scarcity. Moreover, a disentanglement loss is designed to effectively guide a relation disentanglement controller to disentangle spurious correlations between the embeddings encoded by the backbone. In this way, the spurious correlation issue in FSCIL can be suppressed. Extensive experiments on CIFAR-100, mini-ImageNet, and CUB-200 datasets demonstrate the effectiveness of our CTRL-FSCIL method.

{{</citation>}}


### (54/57 | 108/156) GeoGaussian: Geometry-aware Gaussian Splatting for Scene Rendering (Yanyan Li et al., 2024)

{{<citation>}}

Yanyan Li, Chenyu Lyu, Yan Di, Guangyao Zhai, Gim Hee Lee, Federico Tombari. (2024)  
**GeoGaussian: Geometry-aware Gaussian Splatting for Scene Rendering**
<br/>
<button class="copy-to-clipboard" title="GeoGaussian: Geometry-aware Gaussian Splatting for Scene Rendering" index=108>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 5  
Keywords: Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11324v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11324v1.pdf" filename="2403.11324v1.pdf">Download PDF</button>

---


**ABSTRACT**  
During the Gaussian Splatting optimization process, the scene's <b>geometry</b> can gradually deteriorate if its structure is not deliberately preserved, especially in non-textured regions such as walls, ceilings, and furniture surfaces. This degradation significantly affects the rendering quality of novel views that deviate significantly from the viewpoints in the training data. To mitigate this issue, we propose a novel approach called GeoGaussian. Based on the smoothly connected areas observed from point clouds, this method introduces a novel pipeline to initialize thin Gaussians aligned with the surfaces, where the characteristic can be transferred to new generations through a carefully designed densification strategy. Finally, the pipeline ensures that the scene's <b>geometry</b> and texture are maintained through constrained optimization processes with explicit <b>geometry</b> constraints. Benefiting from the proposed architecture, the generative ability of 3D Gaussians is enhanced, especially in structured regions. Our proposed pipeline achieves state-of-the-art performance in novel view synthesis and geometric reconstruction, as evaluated qualitatively and quantitatively on public datasets.

{{</citation>}}


### (55/57 | 109/156) Compact 3D Gaussian Splatting For Dense Visual SLAM (Tianchen Deng et al., 2024)

{{<citation>}}

Tianchen Deng, Yaohui Chen, Leyan Zhang, Jianfei Yang, Shenghai Yuan, Danwei Wang, Weidong Chen. (2024)  
**Compact 3D Gaussian Splatting For Dense Visual SLAM**
<br/>
<button class="copy-to-clipboard" title="Compact 3D Gaussian Splatting For Dense Visual SLAM" index=109>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-RO, cs.CV  
Keyword Score: 5  
Keywords: Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11247v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11247v1.pdf" filename="2403.11247v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent work has shown that 3D Gaussian-based SLAM enables high-quality reconstruction, accurate pose estimation, and real-time rendering of scenes. However, these approaches are built on a tremendous number of redundant 3D Gaussian ellipsoids, leading to high memory and storage costs, and slow training speed. To address the limitation, we propose a compact 3D Gaussian Splatting SLAM system that reduces the number and the parameter size of Gaussian ellipsoids. A sliding window-based masking strategy is first proposed to reduce the redundant ellipsoids. Then we observe that the covariance matrix <b>(geometry)</b> of most 3D Gaussian ellipsoids are extremely similar, which motivates a novel <b>geometry</b> codebook to compress 3D Gaussian geometric attributes, i.e., the parameters. Robust and accurate pose estimation is achieved by a global bundle adjustment method with reprojection loss. Extensive experiments demonstrate that our method achieves faster training and rendering speed while maintaining the state-of-the-art (SOTA) quality of the scene representation.

{{</citation>}}


### (56/57 | 110/156) Local-consistent Transformation Learning for Rotation-invariant Point Cloud Analysis (Yiyang Chen et al., 2024)

{{<citation>}}

Yiyang Chen, Lunhao Duan, Shanshan Zhao, Changxing Ding, Dacheng Tao. (2024)  
**Local-consistent Transformation Learning for Rotation-invariant Point Cloud Analysis**
<br/>
<button class="copy-to-clipboard" title="Local-consistent Transformation Learning for Rotation-invariant Point Cloud Analysis" index=110>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 5  
Keywords: Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11113v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11113v1.pdf" filename="2403.11113v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Rotation invariance is an important requirement for point shape analysis. To achieve this, current state-of-the-art methods attempt to construct the local rotation-invariant representation through learning or defining the local reference frame (LRF). Although efficient, these LRF-based methods suffer from perturbation of local geometric relations, resulting in suboptimal local rotation invariance. To alleviate this issue, we propose a Local-consistent Transformation (LocoTrans) learning strategy. Specifically, we first construct the local-consistent reference frame (LCRF) by considering the symmetry of the two axes in LRF. In comparison with previous LRFs, our LCRF is able to preserve local geometric relationships better through performing local-consistent transformation. However, as the consistency only exists in local regions, the relative pose information is still lost in the intermediate layers of the network. We mitigate such a relative pose issue by developing a relative pose recovery (RPR) module. RPR aims to restore the relative pose between adjacent transformed patches. Equipped with LCRF and RPR, our LocoTrans is capable of learning local-consistent transformation and preserving local <b>geometry,</b> which benefits rotation invariance learning. Competitive performance under arbitrary rotations on both shape classification and part segmentation tasks and ablations can demonstrate the effectiveness of our method. Code will be available publicly at https://github.com/wdttt/LocoTrans.

{{</citation>}}


### (57/57 | 111/156) FORCE: Dataset and Method for Intuitive Physics Guided Human-object Interaction (Xiaohan Zhang et al., 2024)

{{<citation>}}

Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke, Ilya Petrov, Vladimir Guzov, Helisa Dhamo, Eduardo Pérez-Pellitero, Gerard Pons-Moll. (2024)  
**FORCE: Dataset and Method for Intuitive Physics Guided Human-object Interaction**
<br/>
<button class="copy-to-clipboard" title="FORCE: Dataset and Method for Intuitive Physics Guided Human-object Interaction" index=111>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 3  
Keywords: Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11237v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11237v1.pdf" filename="2403.11237v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Interactions between human and objects are influenced not only by the object's pose and shape, but also by physical attributes such as object mass and surface friction. They introduce important motion nuances that are essential for diversity and realism. Despite advancements in recent kinematics-based methods, this aspect has been overlooked. Generating nuanced human motion presents two challenges. First, it is non-trivial to learn from <b>multi-modal</b> human and object information derived from both the physical and non-physical attributes. Second, there exists no dataset capturing nuanced human interactions with objects of varying physical properties, hampering model development. This work addresses the gap by introducing the FORCE model, a kinematic approach for synthesizing diverse, nuanced human-object interactions by modeling physical attributes. Our key insight is that human motion is dictated by the interrelation between the force exerted by the human and the perceived resistance. Guided by a novel intuitive physics encoding, the model captures the interplay between human force and resistance. Experiments also demonstrate incorporating human force facilitates learning multi-class motion. Accompanying our model, we contribute the FORCE dataset. It features diverse, different-styled motion through interactions with varying resistances.

{{</citation>}}


## cs.SE (3)



### (1/3 | 112/156) An Empirical Study on JIT Defect Prediction Based on BERT-style Model (Yuxiang Guo et al., 2024)

{{<citation>}}

Yuxiang Guo, Xiaopeng Gao, Bo Jiang. (2024)  
**An Empirical Study on JIT Defect Prediction Based on BERT-style Model**
<br/>
<button class="copy-to-clipboard" title="An Empirical Study on JIT Defect Prediction Based on BERT-style Model" index=112>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-SE, cs.SE  
Keyword Score: 50  
Keywords: Convolutional Neural Network, Fine-tuning, BERT, LSTM, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11158v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11158v1.pdf" filename="2403.11158v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Previous works on Just-In-Time (JIT) defect prediction tasks have primarily applied pre-trained models directly, neglecting the configurations of their <b>fine-tuning</b> process. In this study, we perform a systematic empirical study to understand the impact of the settings of the <b>fine-tuning</b> process on <b>BERT-style</b> pre-trained model for JIT defect prediction. Specifically, we explore the impact of different parameter freezing settings, parameter initialization settings, and optimizer strategies on the performance of <b>BERT-style</b> models for JIT defect prediction. Our findings reveal the crucial role of the first encoder layer in the <b>BERT-style</b> model and the project sensitivity to parameter initialization settings. Another notable finding is that the addition of a weight decay strategy in the Adam optimizer can slightly improve model performance. Additionally, we compare performance using different feature extractors (FCN, <b>CNN,</b> <b>LSTM,</b> <b>transformer)</b> and find that a simple network can achieve great performance. These results offer new insights for <b>fine-tuning</b> pre-trained models for JIT defect prediction. We combine these findings to find a cost-effective <b>fine-tuning</b> method based on LoRA, which achieve a comparable performance with only one-third memory consumption than original <b>fine-tuning</b> process.

{{</citation>}}


### (2/3 | 113/156) Java JIT Testing with Template Extraction (Zhiqiang Zang et al., 2024)

{{<citation>}}

Zhiqiang Zang, Fu-Yao Yu, Aditya Thimmaiah, August Shi, Milos Gligoric. (2024)  
**Java JIT Testing with Template Extraction**
<br/>
<button class="copy-to-clipboard" title="Java JIT Testing with Template Extraction" index=113>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-SE, cs.SE  
Keyword Score: 10  
Keywords: GLUE  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11281v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11281v1.pdf" filename="2403.11281v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present LeJit, a template-based framework for testing Java just-in-time (JIT) compilers. Like recent template-based frameworks, LeJit executes a template -- a program with holes to be filled -- to generate concrete programs given as inputs to Java JIT compilers. LeJit automatically generates template programs from existing Java code by converting expressions to holes, as well as generating necessary <b>glue</b> code (i.e., code that generates instances of non-primitive types) to make generated templates executable. We have successfully used LeJit to test a range of popular Java JIT compilers, revealing five bugs in HotSpot, nine bugs in OpenJ9, and one bug in GraalVM. All of these bugs have been confirmed by Oracle and IBM developers, and 11 of these bugs were previously unknown, including two CVEs (Common Vulnerabilities and Exposures). Our comparison with several existing approaches shows that LeJit is complementary to them and is a powerful technique for ensuring Java JIT compiler correctness.

{{</citation>}}


### (3/3 | 114/156) Efficiently Detecting Reentrancy Vulnerabilities in Complex Smart Contracts (Zexu Wang et al., 2024)

{{<citation>}}

Zexu Wang, Jiachi Chen, Yanlin Wang, Yu Zhang, Weizhe Zhang, Zibin Zheng. (2024)  
**Efficiently Detecting Reentrancy Vulnerabilities in Complex Smart Contracts**
<br/>
<button class="copy-to-clipboard" title="Efficiently Detecting Reentrancy Vulnerabilities in Complex Smart Contracts" index=114>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-SE, cs.SE  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11254v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11254v1.pdf" filename="2403.11254v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Reentrancy vulnerability as one of the most notorious vulnerabilities, has been a prominent topic in smart contract security research. Research shows that existing vulnerability detection presents a range of challenges, especially as smart contracts continue to increase in complexity. Existing tools perform poorly in terms of efficiency and successful detection rates for vulnerabilities in complex contracts. To effectively detect reentrancy vulnerabilities in contracts with complex logic, we propose a tool named SliSE. SliSE's detection process consists of two stages: Warning Search and Symbolic Execution Verification. In Stage I, SliSE utilizes program slicing to analyze the Inter-contract Program Dependency <b>Graph</b> (I-PDG) of the contract, and collects suspicious vulnerability information as warnings. In Stage II, symbolic execution is employed to verify the reachability of these warnings, thereby enhancing vulnerability detection accuracy. SliSE obtained the best performance compared with eight state-of-the-art detection tools. It achieved an F1 score of 78.65%, surpassing the highest score recorded by an existing tool of 9.26%. Additionally, it attained a recall rate exceeding 90% for detection of contracts on Ethereum. Overall, SliSE provides a robust and efficient method for detection of Reentrancy vulnerabilities for complex contracts.

{{</citation>}}


## eess.SY (3)



### (1/3 | 115/156) Unsupervised Learning for Equitable DER Control (Zhenyi Yuan et al., 2024)

{{<citation>}}

Zhenyi Yuan, Guido Cavraro, Ahmed S. Zamzam, Jorge Cortés. (2024)  
**Unsupervised Learning for Equitable DER Control**
<br/>
<button class="copy-to-clipboard" title="Unsupervised Learning for Equitable DER Control" index=115>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 50  
Keywords: Fairness, Simulation, Simulator, Unsupervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11068v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11068v1.pdf" filename="2403.11068v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In the context of managing distributed energy resources (DERs) within distribution networks (DNs), this work focuses on the task of developing local controllers. We propose an <b>unsupervised</b> <b>learning</b> framework to train functions that can closely approximate optimal power flow (OPF) solutions. The primary aim is to establish specific conditions under which these learned functions can collectively guide the network towards desired configurations asymptotically, leveraging an incremental control approach. The flexibility of the proposed methodology allows to integrate <b>fairness-driven</b> components into the cost function associated with the OPF problem. This addition seeks to mitigate power curtailment disparities among DERs, thereby promoting equitable power injections across the network. To demonstrate the effectiveness of the proposed approach, power flow <b>simulations</b> are conducted using the IEEE 37-bus feeder. The findings not only showcase the guaranteed system stability but also underscore its improved overall performance.

{{</citation>}}


### (2/3 | 116/156) Toward Adaptive Cooperation: Model-Based Shared Control Using LQ-Differential Games (Balint Varga, 2024)

{{<citation>}}

Balint Varga. (2024)  
**Toward Adaptive Cooperation: Model-Based Shared Control Using LQ-Differential Games**
<br/>
<button class="copy-to-clipboard" title="Toward Adaptive Cooperation: Model-Based Shared Control Using LQ-Differential Games" index=116>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-RO, cs-SY, eess-SY, eess.SY  
Keyword Score: 30  
Keywords: Simulation, Simulator, human-in-the-loop  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11146v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11146v1.pdf" filename="2403.11146v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper introduces a novel model-based adaptive shared control to allow for the identification and design challenge for shared-control systems, in which humans and automation share control tasks. The main challenge is the adaptive behavior of the human in such shared control interactions. Consequently, merely identifying human behavior without considering automation is insufficient and often leads to inadequate automation design. Therefore, this paper proposes a novel solution involving online identification of the human and the adaptation of shared control using Linear-Quadratic differential games. The effectiveness of the proposed online adaptation is analyzed in <b>simulations</b> and compared with a non-adaptive shared control from the state of the art. Finally, the proposed approach is tested through <b>human-in-the-loop</b> experiments, highlighting its suitability for real-time applications.

{{</citation>}}


### (3/3 | 117/156) Deep Neural Network NMPC for Computationally Tractable Optimal Power Management of Hybrid Electric Vehicle (Suyong Park et al., 2024)

{{<citation>}}

Suyong Park, Duc Giap Nguyen, Jinrak Park, Dohee Kim, Jeong Soo Eo, Kyoungseok Han. (2024)  
**Deep Neural Network NMPC for Computationally Tractable Optimal Power Management of Hybrid Electric Vehicle**
<br/>
<button class="copy-to-clipboard" title="Deep Neural Network NMPC for Computationally Tractable Optimal Power Management of Hybrid Electric Vehicle" index=117>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11104v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11104v1.pdf" filename="2403.11104v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This study presents a method for deep neural network nonlinear model predictive control (DNN-MPC) to reduce computational complexity, and we show its practical utility through its application in optimizing the energy management of hybrid electric vehicles (HEVs). For optimal power management of HEVs, we first design the online NMPC to collect the data set, and the deep neural network is trained to approximate the NMPC solutions. We assess the effectiveness of our approach by conducting comparative <b>simulations</b> with rule and online NMPC-based power management strategies for HEV, evaluating both fuel consumption and computational complexity. Lastly, we verify the real-time feasibility of our approach through process-in-the-loop (PIL) testing. The test results demonstrate that the proposed method closely approximates the NMPC performance while substantially reducing the computational burden.

{{</citation>}}


## cs.CR (4)



### (1/4 | 118/156) usfAD Based Effective Unknown Attack Detection Focused IDS Framework (Md. Ashraf Uddin et al., 2024)

{{<citation>}}

Md. Ashraf Uddin, Sunil Aryal, Mohamed Reda Bouadjenek, Muna Al-Hawawreh, Md. Alamin Talukder. (2024)  
**usfAD Based Effective Unknown Attack Detection Focused IDS Framework**
<br/>
<button class="copy-to-clipboard" title="usfAD Based Effective Unknown Attack Detection Focused IDS Framework" index=118>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs-LG, cs.CR  
Keyword Score: 43  
Keywords: Anomaly Detection, Benchmarking, Semi-Supervised Learning, Supervised Learning, Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11180v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11180v1.pdf" filename="2403.11180v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The rapid expansion of varied network systems, including the Internet of Things (IoT) and Industrial Internet of Things (IIoT), has led to an increasing range of cyber threats. Ensuring robust protection against these threats necessitates the implementation of an effective Intrusion Detection System (IDS). For more than a decade, researchers have delved into <b>supervised</b> <b>machine</b> learning techniques to develop IDS to classify normal and attack traffic. However, building effective IDS models using <b>supervised</b> <b>learning</b> requires a substantial number of benign and attack samples. To collect a sufficient number of attack samples from real-life scenarios is not possible since cyber attacks occur occasionally. Further, IDS trained and tested on known datasets fails in detecting zero-day or unknown attacks due to the swift evolution of attack patterns. To address this challenge, we put forth two strategies for <b>semi-supervised</b> <b>learning</b> based IDS where training samples of attacks are not required: 1) training a <b>supervised</b> <b>machine</b> learning model using randomly and uniformly dispersed synthetic attack samples; 2) building a One Class Classification (OCC) model that is trained exclusively on benign network traffic. We have implemented both approaches and compared their performances using 10 recent <b>benchmark</b> IDS datasets. Our findings demonstrate that the OCC model based on the state-of-art <b>anomaly</b> <b>detection</b> technique called usfAD significantly outperforms conventional <b>supervised</b> <b>classification</b> and other OCC based techniques when trained and tested considering real-life scenarios, particularly to detect previously unseen attacks.

{{</citation>}}


### (2/4 | 119/156) Pencil: Private and Extensible Collaborative Learning without the Non-Colluding Assumption (Xuanqi Liu et al., 2024)

{{<citation>}}

Xuanqi Liu, Zhuotao Liu, Qi Li, Ke Xu, Mingwei Xu. (2024)  
**Pencil: Private and Extensible Collaborative Learning without the Non-Colluding Assumption**
<br/>
<button class="copy-to-clipboard" title="Pencil: Private and Extensible Collaborative Learning without the Non-Colluding Assumption" index=119>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs-LG, cs.CR  
Keyword Score: 10  
Keywords: Federated Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11166v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11166v1.pdf" filename="2403.11166v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The escalating focus on data privacy poses significant challenges for collaborative neural network training, where data ownership and model training/deployment responsibilities reside with distinct entities. Our community has made substantial contributions to addressing this challenge, proposing various approaches such as <b>federated</b> <b>learning</b> (FL) and privacy-preserving machine learning based on cryptographic constructs like homomorphic encryption (HE) and secure multiparty computation (MPC). However, FL completely overlooks model privacy, and HE has limited extensibility (confined to only one data provider). While the state-of-the-art MPC frameworks provide reasonable throughput and simultaneously ensure model/data privacy, they rely on a critical non-colluding assumption on the computing servers, and relaxing this assumption is still an open problem. In this paper, we present Pencil, the first private training framework for collaborative learning that simultaneously offers data privacy, model privacy, and extensibility to multiple data providers, without relying on the non-colluding assumption. Our fundamental design principle is to construct the n-party collaborative training protocol based on an efficient two-party protocol, and meanwhile ensuring that switching to different data providers during model training introduces no extra cost. We introduce several novel cryptographic protocols to realize this design principle and conduct a rigorous security and privacy analysis. Our comprehensive evaluations of Pencil demonstrate that (i) models trained in plaintext and models trained privately using Pencil exhibit nearly identical test accuracies; (ii) The training overhead of Pencil is greatly reduced: Pencil achieves 10 ~ 260x higher throughput and 2 orders of magnitude less communication than prior art; (iii) Pencil is resilient against both existing and adaptive (white-box) attacks.

{{</citation>}}


### (3/4 | 120/156) Programming Frameworks for Differential Privacy (Marco Gaboardi et al., 2024)

{{<citation>}}

Marco Gaboardi, Michael Hay, Salil Vadhan. (2024)  
**Programming Frameworks for Differential Privacy**
<br/>
<button class="copy-to-clipboard" title="Programming Frameworks for Differential Privacy" index=120>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs-DB, cs-PL, cs.CR  
Keyword Score: 10  
Keywords: Differential Privacy  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11088v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11088v1.pdf" filename="2403.11088v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Many programming frameworks have been introduced to support the development of differentially private software applications. In this chapter, we survey some of the conceptual ideas underlying these frameworks in a way that we hope will be helpful for both practitioners and researchers. For practitioners, the survey can provide a starting point for understanding what features may be valuable when selecting a programming framework. For researchers, it can help organize existing work in a unified way and provide context for understanding new features in future frameworks.

{{</citation>}}


### (4/4 | 121/156) A Tip for IOTA Privacy: IOTA Light Node Deanonymization via Tip Selection (Hojung Yang et al., 2024)

{{<citation>}}

Hojung Yang, Suhyeon Lee, Seungjoo Kim. (2024)  
**A Tip for IOTA Privacy: IOTA Light Node Deanonymization via Tip Selection**
<br/>
<button class="copy-to-clipboard" title="A Tip for IOTA Privacy: IOTA Light Node Deanonymization via Tip Selection" index=121>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs.CR  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11171v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11171v1.pdf" filename="2403.11171v1.pdf">Download PDF</button>

---


**ABSTRACT**  
IOTA is a distributed ledger technology that uses a Directed Acyclic <b>Graph</b> (DAG) structure called the Tangle. It is known for its efficiency and is widely used in the Internet of Things (IoT) environment. Tangle can be configured by utilizing the tip selection process. Due to performance issues with light nodes, full nodes are being asked to perform the tip selections of light nodes. However, in this paper, we demonstrate that tip selection can be exploited to compromise users' privacy. An adversary full node can associate a transaction with the identity of a light node by comparing the light node's request with its ledger. We show that these types of attacks are not only viable in the current IOTA environment but also in IOTA 2.0 and the privacy improvement being studied. We also provide solutions to mitigate these attacks and propose ways to enhance anonymity in the IOTA network while maintaining efficiency and scalability.

{{</citation>}}


## cs.NI (3)



### (1/3 | 122/156) Jointly Optimizing Terahertz based Sensing and Communications in Vehicular Networks: A Dynamic Graph Neural Network Approach (Xuefei Li et al., 2024)

{{<citation>}}

Xuefei Li, Mingzhe Chen, Ye Hu, Zhilong Zhang, Danpu Liu, Shiwen Mao. (2024)  
**Jointly Optimizing Terahertz based Sensing and Communications in Vehicular Networks: A Dynamic Graph Neural Network Approach**
<br/>
<button class="copy-to-clipboard" title="Jointly Optimizing Terahertz based Sensing and Communications in Vehicular Networks: A Dynamic Graph Neural Network Approach" index=122>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NI  
Categories: cs-NI, cs.NI, eess-SP  
Keyword Score: 43  
Keywords: Graph, Graph Neural Network, Graph Neural Network, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11102v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11102v1.pdf" filename="2403.11102v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, the problem of vehicle service mode selection (sensing, communication, or both) and vehicle connections within terahertz (THz) enabled joint sensing and communications over vehicular networks is studied. The considered network consists of several service provider vehicles (SPVs) that can provide: 1) only sensing service, 2) only communication service, and 3) both services, sensing service request vehicles, and communication service request vehicles. Based on the vehicle network topology and their service accessibility, SPVs strategically select service request vehicles to provide sensing, communication, or both services. This problem is formulated as an optimization problem, aiming to maximize the number of successfully served vehicles by jointly determining the service mode of each SPV and its associated vehicles. To solve this problem, we propose a dynamic <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> model that selects appropriate <b>graph</b> <b>information</b> <b>aggregation</b> functions according to the vehicle network topology, thus extracting more vehicle network information compared to traditional static <b>GNNs</b> that use fixed aggregation functions for different vehicle network topologies. Using the extracted vehicle network information, the service mode of each SPV and its served service request vehicles will be determined. <b>Simulation</b> results show that the proposed dynamic <b>GNN</b> based method can improve the number of successfully served vehicles by up to 17% and 28% compared to a <b>GNN</b> based algorithm with a fixed neural network model and a conventional optimization algorithm without using <b>GNNs.</b>

{{</citation>}}


### (2/3 | 123/156) Brain-on-Switch: Towards Advanced Intelligent Network Data Plane via NN-Driven Traffic Analysis at Line-Speed (Jinzhu Yan et al., 2024)

{{<citation>}}

Jinzhu Yan, Haotian Xu, Zhuotao Liu, Qi Li, Ke Xu, Mingwei Xu, Jianping Wu. (2024)  
**Brain-on-Switch: Towards Advanced Intelligent Network Data Plane via NN-Driven Traffic Analysis at Line-Speed**
<br/>
<button class="copy-to-clipboard" title="Brain-on-Switch: Towards Advanced Intelligent Network Data Plane via NN-Driven Traffic Analysis at Line-Speed" index=123>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NI  
Categories: cs-LG, cs-NI, cs.NI  
Keyword Score: 30  
Keywords: Recurrent Neural Network, Recurrent Neural Network, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11090v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11090v1.pdf" filename="2403.11090v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The emerging programmable networks sparked significant research on Intelligent Network Data Plane (INDP), which achieves learning-based traffic analysis at line-speed. Prior art in INDP focus on deploying tree/forest models on the data plane. We observe a fundamental limitation in tree-based INDP approaches: although it is possible to represent even larger tree/forest tables on the data plane, the flow features that are computable on the data plane are fundamentally limited by hardware constraints. In this paper, we present BoS to push the boundaries of INDP by enabling Neural Network (NN) driven traffic analysis at line-speed. Many types of NNs (such as <b>Recurrent</b> <b>Neural</b> <b>Network</b> <b>(RNN),</b> and <b>transformers)</b> that are designed to work with sequential data have advantages over tree-based models, because they can take raw network data as input without complex feature computations on the fly. However, the challenge is significant: the <b>recurrent</b> <b>computation</b> <b>scheme</b> used in <b>RNN</b> inference is fundamentally different from the match-action paradigm used on the network data plane. BoS addresses this challenge by (i) designing a novel data plane friendly <b>RNN</b> architecture that can execute unlimited <b>RNN</b> time steps with limited data plane stages, effectively achieving line-speed <b>RNN</b> inference; and (ii) complementing the on-switch <b>RNN</b> model with an off-switch <b>transformer-based</b> traffic analysis module to further boost the overall performance. We implement a prototype of BoS using a P4 programmable switch as our data plane, and extensively evaluate it over multiple traffic analysis tasks. The results show that BoS outperforms state-of-the-art in both analysis accuracy and scalability.

{{</citation>}}


### (3/3 | 124/156) Non-Primary Channel Access in IEEE 802.11 UHR: Comprehensive Analysis and Evaluation (Dongyu Wei et al., 2024)

{{<citation>}}

Dongyu Wei, Liu Cao, Xiangyu Gao, Hao Yin. (2024)  
**Non-Primary Channel Access in IEEE 802.11 UHR: Comprehensive Analysis and Evaluation**
<br/>
<button class="copy-to-clipboard" title="Non-Primary Channel Access in IEEE 802.11 UHR: Comprehensive Analysis and Evaluation" index=124>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NI  
Categories: cs-NI, cs.NI  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11300v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11300v1.pdf" filename="2403.11300v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The evolution of the IEEE 802.11 standards marks a significant throughput advancement in wireless access technologies, progressively increasing bandwidth capacities from 20 MHz in the IEEE 802.11a to up to 320 MHz in the latest IEEE 802.11be (Wi-Fi 7). However, the increased bandwidth capacities may not be well exploited due to inefficient bandwidth utilization incorporating primary channel and secondary channels. Particularly, when primary channel is busy, secondary channels (also known as non-primary channels) are prevented from being utilized even if they are idle, thereby wasting the available bandwidth. This paper investigates the fundamentals of the Non-Primary Channel Access (NPCA) protocol that was introduced in IEEE 802.11 Ultra-High Reliability (UHR) group to cope with the above issue. We propose a novel analytical model to assess NPCA protocol performance in terms of the average throughput and delay. Via <b>simulation,</b> we verify that the NPCA network outperforms the legacy network by increasing by at least 50% average throughput while reducing by at least 40% average delay.

{{</citation>}}


## eess.IV (7)



### (1/7 | 125/156) Simple 2D Convolutional Neural Network-based Approach for COVID-19 Detection (Chih-Chung Hsu et al., 2024)

{{<citation>}}

Chih-Chung Hsu, Chia-Ming Lee, Yang Fan Chiang, Yi-Shiuan Chou, Chih-Yu Jiang, Shen-Chieh Tai, Chi-Han Tsai. (2024)  
**Simple 2D Convolutional Neural Network-based Approach for COVID-19 Detection**
<br/>
<button class="copy-to-clipboard" title="Simple 2D Convolutional Neural Network-based Approach for COVID-19 Detection" index=125>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, cs-LG, eess-IV, eess.IV  
Keyword Score: 40  
Keywords: Convolution, Convolutional Neural Network, Knowledge Distillation, Out-of-distribution  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11230v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11230v1.pdf" filename="2403.11230v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This study explores the use of deep learning techniques for analyzing lung Computed Tomography (CT) images. Classic deep learning approaches face challenges with varying slice counts and resolutions in CT images, a diversity arising from the utilization of assorted scanning equipment. Typically, predictions are made on single slices which are then combined for a comprehensive outcome. Yet, this method does not incorporate learning features specific to each slice, leading to a compromise in effectiveness. To address these challenges, we propose an advanced Spatial-Slice Feature Learning (SSFL++) framework specifically tailored for CT scans. It aims to filter out <b>out-of-distribution</b> (OOD) data within the entire CT scan, allowing us to select essential spatial-slice features for analysis by reducing data redundancy by 70\%. Additionally, we introduce a Kernel-Density-based slice Sampling <b>(KDS)</b> method to enhance stability during training and inference phases, thereby accelerating convergence and enhancing overall performance. Remarkably, our experiments reveal that our model achieves promising results with a simple EfficientNet-2D (E2D) model. The effectiveness of our approach is confirmed on the COVID-19-CT-DB datasets provided by the DEF-AI-MIA workshop.

{{</citation>}}


### (2/7 | 126/156) Ensembling and Test Augmentation for Covid-19 Detection and Covid-19 Domain Adaptation from 3D CT-Scans (Fares Bougourzi et al., 2024)

{{<citation>}}

Fares Bougourzi, Feryal Windal Moula, Halim Benhabiles, Fadi Dornaika, Abdelmalik Taleb-Ahmed. (2024)  
**Ensembling and Test Augmentation for Covid-19 Detection and Covid-19 Domain Adaptation from 3D CT-Scans**
<br/>
<button class="copy-to-clipboard" title="Ensembling and Test Augmentation for Covid-19 Detection and Covid-19 Domain Adaptation from 3D CT-Scans" index=126>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, cs-LG, eess-IV, eess.IV  
Keyword Score: 20  
Keywords: Convolutional Neural Network, Domain Adaptation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11338v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11338v1.pdf" filename="2403.11338v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Since the emergence of Covid-19 in late 2019, medical image analysis using artificial intelligence (AI) has emerged as a crucial research area, particularly with the utility of CT-scan imaging for disease diagnosis. This paper contributes to the 4th COV19D competition, focusing on Covid-19 Detection and Covid-19 <b>Domain</b> <b>Adaptation</b> Challenges. Our approach centers on lung segmentation and Covid-19 infection segmentation employing the recent <b>CNN-based</b> segmentation architecture PDAtt-Unet, which simultaneously segments lung regions and infections. Departing from traditional methods, we concatenate the input slice (grayscale) with segmented lung and infection, generating three input channels akin to color channels. Additionally, we employ three 3D <b>CNN</b> backbones Customized Hybrid-DeCoVNet, along with pretrained 3D-Resnet-18 and 3D-Resnet-50 models to train Covid-19 recognition for both challenges. Furthermore, we explore ensemble approaches and testing augmentation to enhance performance. Comparison with baseline results underscores the substantial efficiency of our approach, with a significant margin in terms of F1-score (14 %). This study advances the field by presenting a comprehensive methodology for accurate Covid-19 detection and adaptation, leveraging cutting-edge AI techniques in medical image analysis.

{{</citation>}}


### (3/7 | 127/156) YOLOv9 for Fracture Detection in Pediatric Wrist Trauma X-ray Images (Chun-Tse Chien et al., 2024)

{{<citation>}}

Chun-Tse Chien, Rui-Yang Ju, Kuang-Yi Chou, Jen-Shiun Chiang. (2024)  
**YOLOv9 for Fracture Detection in Pediatric Wrist Trauma X-ray Images**
<br/>
<button class="copy-to-clipboard" title="YOLOv9 for Fracture Detection in Pediatric Wrist Trauma X-ray Images" index=127>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 20  
Keywords: Yolo, Data Augmentation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11249v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11249v1.pdf" filename="2403.11249v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The introduction of YOLOv9, the latest version of the You Only Look Once <b>(YOLO)</b> series, has led to its widespread adoption across various scenarios. This paper is the first to apply the YOLOv9 algorithm model to the fracture detection task as computer-assisted diagnosis (CAD) to help radiologists and surgeons to interpret X-ray images. Specifically, this paper trained the model on the GRAZPEDWRI-DX dataset and extended the training set using <b>data</b> <b>augmentation</b> techniques to improve the model performance. Experimental results demonstrate that compared to the mAP 50-95 of the current state-of-the-art (SOTA) model, the YOLOv9 model increased the value from 42.16% to 43.73%, with an improvement of 3.7%. The implementation code is publicly available at https://github.com/RuiyangJu/YOLOv9-Fracture-Detection.

{{</citation>}}


### (4/7 | 128/156) Adaptive Semantic-Enhanced Denoising Diffusion Probabilistic Model for Remote Sensing Image Super-Resolution (Jialu Sui et al., 2024)

{{<citation>}}

Jialu Sui, Xianping Ma, Xiaokang Zhang, Man-On Pun. (2024)  
**Adaptive Semantic-Enhanced Denoising Diffusion Probabilistic Model for Remote Sensing Image Super-Resolution**
<br/>
<button class="copy-to-clipboard" title="Adaptive Semantic-Enhanced Denoising Diffusion Probabilistic Model for Remote Sensing Image Super-Resolution" index=128>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 20  
Keywords: Probabilistic Model, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11078v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11078v1.pdf" filename="2403.11078v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Remote sensing image super-resolution (SR) is a crucial task to restore high-resolution (HR) images from low-resolution (LR) observations. Recently, the Denoising Diffusion <b>Probabilistic</b> <b>Model</b> (DDPM) has shown promising performance in image reconstructions by overcoming problems inherent in generative models, such as over-smoothing and mode collapse. However, the high-frequency details generated by DDPM often suffer from misalignment with HR images due to the model's tendency to overlook long-range semantic contexts. This is attributed to the widely used U-Net decoder in the conditional noise predictor, which tends to overemphasize local information, leading to the generation of noises with significant variances during the prediction process. To address these issues, an adaptive semantic-enhanced DDPM (ASDDPM) is proposed to enhance the detail-preserving capability of the DDPM by incorporating low-frequency semantic information provided by the <b>Transformer.</b> Specifically, a novel adaptive diffusion <b>Transformer</b> decoder (ADTD) is developed to bridge the semantic gap between the encoder and decoder through regulating the noise prediction with the global contextual relationships and long-range dependencies in the diffusion process. Additionally, a residual feature fusion strategy establishes information exchange between the two decoders at multiple levels. As a result, the predicted noise generated by our approach closely approximates that of the real noise distribution.Extensive experiments on two SR and two semantic segmentation datasets confirm the superior performance of the proposed ASDDPM in both SR and the subsequent downstream applications. The source code will be available at https://github.com/littlebeen/ASDDPM-Adaptive-Semantic-Enhanced-DDPM.

{{</citation>}}


### (5/7 | 129/156) Interactive $360^{\circ}$ Video Streaming Using FoV-Adaptive Coding with Temporal Prediction (Yixiang Mao et al., 2024)

{{<citation>}}

Yixiang Mao, Liyang Sun, Yong Liu, Yao Wang. (2024)  
**Interactive $360^{\circ}$ Video Streaming Using FoV-Adaptive Coding with Temporal Prediction**
<br/>
<button class="copy-to-clipboard" title="Interactive $360^{\circ}$ Video Streaming Using FoV-Adaptive Coding with Temporal Prediction" index=129>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-MM, eess-IV, eess.IV  
Keyword Score: 13  
Keywords: Benchmarking, LSTM  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11155v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11155v1.pdf" filename="2403.11155v1.pdf">Download PDF</button>

---


**ABSTRACT**  
For $360^{\circ}$ video streaming, FoV-adaptive coding that allocates more bits for the predicted user's field of view (FoV) is an effective way to maximize the rendered video quality under the limited bandwidth. We develop a low-latency FoV-adaptive coding and streaming system for interactive applications that is robust to bandwidth variations and FoV prediction errors. To minimize the end-to-end delay and yet maximize the coding efficiency, we propose a frame-level FoV-adaptive inter-coding structure. In each frame, regions that are in or near the predicted FoV are coded using temporal and spatial prediction, while a small rotating region is coded with spatial prediction only. This rotating intra region periodically refreshes the entire frame, thereby providing robustness to both FoV prediction errors and frame losses due to transmission errors. The system adapts the sizes and rates of different regions for each video segment to maximize the rendered video quality under the predicted bandwidth constraint. Integrating such frame-level FoV adaptation with temporal prediction is challenging due to the temporal variations of the FoV. We propose novel ways for modeling the influence of FoV dynamics on the quality-rate performance of temporal predictive coding.We further develop <b>LSTM-based</b> machine learning models to predict the user's FoV and network bandwidth.The proposed system is compared with three <b>benchmark</b> systems, using real-world network bandwidth traces and FoV traces, and is shown to significantly improve the rendered video quality, while achieving very low end-to-end delay and low frame-freeze probability.

{{</citation>}}


### (6/7 | 130/156) StainDiffuser: MultiTask Dual Diffusion Model for Virtual Staining (Tushar Kataria et al., 2024)

{{<citation>}}

Tushar Kataria, Beatrice Knudsen, Shireen Y. Elhabian. (2024)  
**StainDiffuser: MultiTask Dual Diffusion Model for Virtual Staining**
<br/>
<button class="copy-to-clipboard" title="StainDiffuser: MultiTask Dual Diffusion Model for Virtual Staining" index=130>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 10  
Keywords: Diffusion Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11340v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11340v1.pdf" filename="2403.11340v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Hematoxylin and Eosin (H&E) staining is the most commonly used for disease diagnosis and tumor recurrence tracking. Hematoxylin excels at highlighting nuclei, whereas eosin stains the cytoplasm. However, H&E stain lacks details for differentiating different types of cells relevant to identifying the grade of the disease or response to specific treatment variations. Pathologists require special immunohistochemical (IHC) stains that highlight different cell types. These stains help in accurately identifying different regions of disease growth and their interactions with the cell's microenvironment. The advent of deep learning models has made Image-to-Image (I2I) translation a key research area, reducing the need for expensive physical staining processes. Pix2Pix and CycleGAN are still the most commonly used methods for virtual staining applications. However, both suffer from hallucinations or staining irregularities when H&E stain has less discriminate information about the underlying cells IHC needs to highlight (e.g.,CD3 lymphocytes). <b>Diffusion</b> <b>models</b> are currently the state-of-the-art models for image generation and conditional generation tasks. However, they require extensive and diverse datasets (millions of samples) to converge, which is less feasible for virtual staining applications.Inspired by the success of multitask deep learning models for limited dataset size, we propose StainDiffuser, a novel multitask dual <b>diffusion</b> <b>architecture</b> for virtual staining that converges under a limited training budget. StainDiffuser trains two <b>diffusion</b> <b>processes</b> simultaneously: (a) generation of cell-specific IHC stain from H&E and (b) H&E-based cell segmentation using coarse segmentation only during training. Our results show that StainDiffuser produces high-quality results for easier (CK8/18,epithelial marker) and difficult stains(CD3, Lymphocytes).

{{</citation>}}


### (7/7 | 131/156) A lightweight deep learning pipeline with DRDA-Net and MobileNet for breast cancer classification (Mahdie Ahmadi et al., 2024)

{{<citation>}}

Mahdie Ahmadi, Nader Karimi, Shadrokh Samavi. (2024)  
**A lightweight deep learning pipeline with DRDA-Net and MobileNet for breast cancer classification**
<br/>
<button class="copy-to-clipboard" title="A lightweight deep learning pipeline with DRDA-Net and MobileNet for breast cancer classification" index=131>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11135v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11135v1.pdf" filename="2403.11135v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Accurate and early detection of breast cancer is essential for successful treatment. This paper introduces a novel deep-learning approach for improved breast cancer classification in histopathological images, a crucial step in diagnosis. Our method hinges on the Dense Residual Dual-Shuffle Attention Network (DRDA-Net), inspired by ShuffleNet's efficient architecture. DRDA-Net achieves exceptional accuracy across various magnification levels on the BreaKHis dataset, a breast cancer histopathology analysis <b>benchmark.</b> However, for real-world deployment, computational efficiency is paramount. We integrate a pre-trained MobileNet model renowned for its lightweight design to address computational. MobileNet ensures fast execution even on devices with limited resources without sacrificing performance. This combined approach offers a promising solution for accurate breast cancer diagnosis, paving the way for faster and more accessible screening procedures.

{{</citation>}}


## cs.AI (2)



### (1/2 | 132/156) Causality from Bottom to Top: A Survey (Abraham Itzhak Weinberg et al., 2024)

{{<citation>}}

Abraham Itzhak Weinberg, Cristiano Premebida, Diego Resende Faria. (2024)  
**Causality from Bottom to Top: A Survey**
<br/>
<button class="copy-to-clipboard" title="Causality from Bottom to Top: A Survey" index=132>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 40  
Keywords: Anomaly Detection, Generative AI, Recommender System, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11219v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11219v1.pdf" filename="2403.11219v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Causality has become a fundamental approach for explaining the relationships between events, phenomena, and outcomes in various fields of study. It has invaded various fields and applications, such as medicine, healthcare, economics, finance, fraud detection, cybersecurity, education, public policy, <b>recommender</b> <b>systems,</b> <b>anomaly</b> <b>detection,</b> robotics, control, sociology, marketing, and advertising. In this paper, we survey its development over the past five decades, shedding light on the differences between causality and other approaches, as well as the preconditions for using it. Furthermore, the paper illustrates how causality interacts with new approaches such as Artificial Intelligence (AI), <b>Generative</b> <b>AI</b> (GAI), Machine and Deep Learning, <b>Reinforcement</b> <b>Learning</b> (RL), and Fuzzy Logic. We study the impact of causality on various fields, its contribution, and its interaction with state-of-the-art approaches. Additionally, the paper exemplifies the trustworthiness and explainability of causality models. We offer several ways to evaluate causality models and discuss future directions.

{{</citation>}}


### (2/2 | 133/156) Research on Personal Credit Risk Assessment Methods Based on Causal Inference (Jiaxin Wang et al., 2024)

{{<citation>}}

Jiaxin Wang, YiLong Ma. (2024)  
**Research on Personal Credit Risk Assessment Methods Based on Causal Inference**
<br/>
<button class="copy-to-clipboard" title="Research on Personal Credit Risk Assessment Methods Based on Causal Inference" index=133>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI, math-CT  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11217v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11217v1.pdf" filename="2403.11217v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The discussion on causality in human history dates back to ancient Greece, yet to this day, there is still no consensus. Fundamentally, this stems from the nature of human cognition, as understanding causality requires abstract tools to transcend the limitations of human cognition. In recent decades, the rapid development of mathematical and computational tools has provided new theoretical and technical means for exploring causality, creating more avenues for investigation. Based on this, this paper introduces a new definition of causality using category theory, proposed by Samuel Eilenberg and Saunders Mac Lane in 1945 to avoid the self-referential contradictions in set theory, notably the Russell paradox. Within this framework, the feasibility of indicator synthesis in causal inference is demonstrated. Due to the limitations in the development of category theory-related technical tools, this paper adopts the widely-used probabilistic causal <b>graph</b> tool proposed by Judea Pearl in 1995 to study the application of causal inference in personal credit risk management. The specific work includes: research on the construction method of causal inference index system, definition of causality and feasibility proof of indicator synthesis causal inference within this framework, application methods of causal <b>graph</b> model and intervention alternative criteria in personal credit risk management, and so on.

{{</citation>}}


## cs.NE (3)



### (1/3 | 134/156) Deep Neural Crossover (Eliad Shem-Tov et al., 2024)

{{<citation>}}

Eliad Shem-Tov, Achiya Elyasaf. (2024)  
**Deep Neural Crossover**
<br/>
<button class="copy-to-clipboard" title="Deep Neural Crossover" index=134>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NE  
Categories: cs-NE, cs.NE  
Keyword Score: 36  
Keywords: Graph, Benchmarking, Reinforcement Learning, Recurrent Neural Network, Recurrent Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11159v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11159v1.pdf" filename="2403.11159v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present a novel multi-parent crossover operator in genetic algorithms (GAs) called ``Deep Neural Crossover'' (DNC). Unlike conventional GA crossover operators that rely on a random selection of parental genes, DNC leverages the capabilities of deep <b>reinforcement</b> <b>learning</b> (DRL) and an encoder-decoder architecture to select the genes. Specifically, we use DRL to learn a policy for selecting promising genes. The policy is stochastic, to maintain the stochastic nature of GAs, representing a distribution for selecting genes with a higher probability of improving fitness. Our architecture features a <b>recurrent</b> <b>neural</b> <b>network</b> <b>(RNN)</b> to encode the parental genomes into latent memory states, and a decoder <b>RNN</b> that utilizes an attention-based pointing mechanism to generate a distribution over the next selected gene in the offspring. To improve the training time, we present a pre-training approach, wherein the architecture is initially trained on a single problem within a specific domain and then applied to solving other problems of the same domain. We compare DNC to known operators from the literature over two <b>benchmark</b> domains -- bin packing and <b>graph</b> coloring. We compare with both two- and three-parent crossover, outperforming all baselines. DNC is domain-independent and can be easily applied to other problem domains.

{{</citation>}}


### (2/3 | 135/156) Spiking Wavelet Transformer (Yuetong Fang et al., 2024)

{{<citation>}}

Yuetong Fang, Ziqing Wang, Lingfeng Zhang, Jiahang Cao, Honglei Chen, Renjing Xu. (2024)  
**Spiking Wavelet Transformer**
<br/>
<button class="copy-to-clipboard" title="Spiking Wavelet Transformer" index=135>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NE  
Categories: cs-NE, cs.NE  
Keyword Score: 30  
Keywords: Convolution, Transformer, Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11138v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11138v1.pdf" filename="2403.11138v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Spiking neural networks (SNNs) offer an energy-efficient alternative to conventional deep learning by mimicking the event-driven processing of the brain. Incorporating the <b>Transformers</b> with SNNs has shown promise for accuracy, yet it is incompetent to capture high-frequency patterns like moving edge and pixel-level brightness changes due to their reliance on global <b>self-attention</b> operations. Porting frequency representations in SNN is challenging yet crucial for event-driven vision. To address this issue, we propose the Spiking Wavelet <b>Transformer</b> (SWformer), an attention-free architecture that effectively learns comprehensive spatial-frequency features in a spike-driven manner by leveraging the sparse wavelet transform. The critical component is a Frequency-Aware Token Mixer (FATM) with three branches: 1) spiking wavelet learner for spatial-frequency domain learning, 2) <b>convolution-based</b> learner for spatial feature extraction, and 3) spiking pointwise <b>convolution</b> for cross-channel information aggregation. We also adopt negative spike dynamics to strengthen the frequency representation further. This enables the SWformer to outperform vanilla Spiking <b>Transformers</b> in capturing high-frequency visual components, as evidenced by our empirical results. Experiments on both static and neuromorphic datasets demonstrate SWformer's effectiveness in capturing spatial-frequency patterns in a multiplication-free, event-driven fashion, outperforming state-of-the-art SNNs. SWformer achieves an over 50% reduction in energy consumption, a 21.1% reduction in parameter count, and a 2.40% performance improvement on the ImageNet dataset compared to vanilla Spiking <b>Transformers.</b>

{{</citation>}}


### (3/3 | 136/156) Multi-Objective Evolutionary Neural Architecture Search for Recurrent Neural Networks (Reinhard Booysen et al., 2024)

{{<citation>}}

Reinhard Booysen, Anna Sergeevna Bosman. (2024)  
**Multi-Objective Evolutionary Neural Architecture Search for Recurrent Neural Networks**
<br/>
<button class="copy-to-clipboard" title="Multi-Objective Evolutionary Neural Architecture Search for Recurrent Neural Networks" index=136>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NE  
Categories: cs-LG, cs-NE, cs.NE  
Keyword Score: 20  
Keywords: Recurrent Neural Network, Recurrent Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11173v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11173v1.pdf" filename="2403.11173v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Artificial neural network (NN) architecture design is a nontrivial and time-consuming task that often requires a high level of human expertise. Neural architecture search (NAS) serves to automate the design of NN architectures and has proven to be successful in automatically finding NN architectures that outperform those manually designed by human experts. NN architecture performance can be quantified based on multiple objectives, which include model accuracy and some NN architecture complexity objectives, among others. The majority of modern NAS methods that consider multiple objectives for NN architecture performance evaluation are concerned with automated feed forward NN architecture design, which leaves multi-objective automated <b>recurrent</b> <b>neural</b> <b>network</b> <b>(RNN)</b> architecture design unexplored. <b>RNNs</b> are important for modeling sequential datasets, and prominent within the natural language processing domain. It is often the case in real world implementations of machine learning and NNs that a reasonable trade-off is accepted for marginally reduced model accuracy in favour of lower computational resources demanded by the model. This paper proposes a multi-objective evolutionary algorithm-based <b>RNN</b> architecture search method. The proposed method relies on approximate network morphisms for <b>RNN</b> architecture complexity optimisation during evolution. The results show that the proposed method is capable of finding novel <b>RNN</b> architectures with comparable performance to state-of-the-art manually designed <b>RNN</b> architectures, but with reduced computational demand.

{{</citation>}}


## stat.ML (2)



### (1/2 | 137/156) Machine learning-based system reliability analysis with Gaussian Process Regression (Lisang Zhou et al., 2024)

{{<citation>}}

Lisang Zhou, Ziqian Luo, Xueting Pan. (2024)  
**Machine learning-based system reliability analysis with Gaussian Process Regression**
<br/>
<button class="copy-to-clipboard" title="Machine learning-based system reliability analysis with Gaussian Process Regression" index=137>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: cs-LG, math-PR, stat-ML, stat.ML  
Keyword Score: 30  
Keywords: Gaussian Process, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11125v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11125v1.pdf" filename="2403.11125v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Machine learning-based reliability analysis methods have shown great advancements for their computational efficiency and accuracy. Recently, many efficient learning strategies have been proposed to enhance the computational performance. However, few of them explores the theoretical optimal learning strategy. In this article, we propose several theorems that facilitates such exploration. Specifically, cases that considering and neglecting the correlations among the candidate design samples are well elaborated. Moreover, we prove that the well-known U learning function can be reformulated to the optimal learning function for the case neglecting the Kriging correlation. In addition, the theoretical optimal learning strategy for sequential multiple training samples enrichment is also mathematically explored through the Bayesian estimate with the corresponding lost functions. <b>Simulation</b> results show that the optimal learning strategy considering the Kriging correlation works better than that neglecting the Kriging correlation and other state-of-the art learning functions from the literatures in terms of the reduction of number of evaluations of performance function. However, the implementation needs to investigate very large computational resource.

{{</citation>}}


### (2/2 | 138/156) Prior-dependent analysis of posterior sampling reinforcement learning with function approximation (Yingru Li et al., 2024)

{{<citation>}}

Yingru Li, Zhi-Quan Luo. (2024)  
**Prior-dependent analysis of posterior sampling reinforcement learning with function approximation**
<br/>
<button class="copy-to-clipboard" title="Prior-dependent analysis of posterior sampling reinforcement learning with function approximation" index=138>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: cs-AI, cs-IT, cs-LG, math-IT, math-ST, stat-ML, stat-TH, stat.ML  
Keyword Score: 23  
Keywords: Benchmarking, Markov Decision Process, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11175v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11175v1.pdf" filename="2403.11175v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This work advances randomized exploration in <b>reinforcement</b> <b>learning</b> (RL) with function approximation modeled by linear mixture <b>MDPs.</b> We establish the first prior-dependent Bayesian regret bound for RL with function approximation; and refine the Bayesian regret analysis for posterior sampling <b>reinforcement</b> <b>learning</b> (PSRL), presenting an upper bound of ${\mathcal{O}}(d\sqrt{H^3 T \log T})$, where $d$ represents the dimensionality of the transition kernel, $H$ the planning horizon, and $T$ the total number of interactions. This signifies a methodological enhancement by optimizing the $\mathcal{O}(\sqrt{\log T})$ factor over the previous <b>benchmark</b> (Osband and Van Roy, 2014) specified to linear mixture <b>MDPs.</b> Our approach, leveraging a value-targeted model learning perspective, introduces a decoupling argument and a variance reduction technique, moving beyond traditional analyses reliant on confidence sets and concentration inequalities to formalize Bayesian regret bounds more effectively.

{{</citation>}}


## cs.SD (1)



### (1/1 | 139/156) Multitask frame-level learning for few-shot sound event detection (Liang Zou et al., 2024)

{{<citation>}}

Liang Zou, Genwei Yan, Ruoyu Wang, Jun Du, Meng Lei, Tian Gao, Xin Fang. (2024)  
**Multitask frame-level learning for few-shot sound event detection**
<br/>
<button class="copy-to-clipboard" title="Multitask frame-level learning for few-shot sound event detection" index=139>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SD  
Categories: cs-CV, cs-SD, cs.SD, eess-AS  
Keyword Score: 30  
Keywords: Data Augmentation, Few-shot, Event Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11091v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11091v1.pdf" filename="2403.11091v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper focuses on <b>few-shot</b> Sound <b>Event</b> <b>Detection</b> (SED), which aims to automatically recognize and classify sound <b>events</b> <b>with</b> limited samples. However, prevailing methods methods in <b>few-shot</b> SED predominantly rely on segment-level predictions, which often providing detailed, fine-grained predictions, particularly for <b>events</b> <b>of</b> brief duration. Although frame-level prediction strategies have been proposed to overcome these limitations, these strategies commonly face difficulties with prediction truncation caused by background noise. To alleviate this issue, we introduces an innovative multitask frame-level SED framework. In addition, we introduce TimeFilterAug, a linear timing mask for <b>data</b> <b>augmentation,</b> to increase the model's robustness and adaptability to diverse acoustic environments. The proposed method achieves a F-score of 63.8%, securing the 1st rank in the <b>few-shot</b> bioacoustic <b>event</b> <b>detection</b> category of the Detection and Classification of Acoustic Scenes and <b>Events</b> <b>Challenge</b> 2023.

{{</citation>}}


## cs.CY (1)



### (1/1 | 140/156) Regulating Chatbot Output via Inter-Informational Competition (Jiawei Zhang, 2024)

{{<citation>}}

Jiawei Zhang. (2024)  
**Regulating Chatbot Output via Inter-Informational Competition**
<br/>
<button class="copy-to-clipboard" title="Regulating Chatbot Output via Inter-Informational Competition" index=140>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CY  
Categories: cs-AI, cs-CY, cs-ET, cs-IT, cs-LG, cs.CY, math-IT  
Keyword Score: 30  
Keywords: Generative AI, ChatGPT, Chatbot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11046v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11046v1.pdf" filename="2403.11046v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The advent of <b>ChatGPT</b> has sparked over a year of regulatory frenzy. However, few existing studies have rigorously questioned the assumption that, if left unregulated, AI <b>chatbot's</b> output would inflict tangible, severe real harm on human affairs. Most researchers have overlooked the critical possibility that the information market itself can effectively mitigate these risks and, as a result, they tend to use regulatory tools to address the issue directly. This Article develops a yardstick for reevaluating both AI-related content risks and corresponding regulatory proposals by focusing on inter-informational competition among various outlets. The decades-long history of regulating information and communications technologies indicates that regulators tend to err too much on the side of caution and to put forward excessive regulatory measures when encountering the uncertainties brought about by new technologies. In fact, a trove of empirical evidence has demonstrated that market competition among information outlets can effectively mitigate most risks and that overreliance on regulation is not only unnecessary but detrimental, as well. This Article argues that sufficient competition among <b>chatbots</b> and other information outlets in the information marketplace can sufficiently mitigate and even resolve most content risks posed by <b>generative</b> <b>AI</b> technologies. This renders certain loudly advocated regulatory strategies, like mandatory prohibitions, licensure, curation of datasets, and notice-and-response regimes, truly unnecessary and even toxic to desirable competition and innovation throughout the AI industry. Ultimately, the ideas that I advance in this Article should pour some much-needed cold water on the regulatory frenzy over <b>generative</b> <b>AI</b> and steer the issue back to a rational track.

{{</citation>}}


## math.NA (1)



### (1/1 | 141/156) Quasi-Monte Carlo and importance sampling methods for Bayesian inverse problems (Zhijian He et al., 2024)

{{<citation>}}

Zhijian He, Hejin Wang, Xiaoqun Wang. (2024)  
**Quasi-Monte Carlo and importance sampling methods for Bayesian inverse problems**
<br/>
<button class="copy-to-clipboard" title="Quasi-Monte Carlo and importance sampling methods for Bayesian inverse problems" index=141>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.NA  
Categories: 35R60, 62F15, 65C05, 65N21, cs-NA, math-NA, math-ST, math.NA, stat-TH  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11374v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11374v1.pdf" filename="2403.11374v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Importance Sampling (IS), an effective variance reduction strategy in Monte Carlo (MC) <b>simulation,</b> is frequently utilized for Bayesian inference and other statistical challenges. Quasi-Monte Carlo (QMC) replaces the random samples in MC with low discrepancy points and has the potential to substantially enhance error rates. In this paper, we integrate IS with a randomly shifted rank-1 lattice rule, a widely used QMC method, to approximate posterior expectations arising from Bayesian Inverse Problems (BIPs) where the posterior density tends to concentrate as the intensity of noise diminishes. Within the framework of weighted Hilbert spaces, we first establish the convergence rate of the lattice rule for a large class of unbounded integrands. This method extends to the analysis of QMC combined with IS in BIPs. Furthermore, we explore the robustness of the IS-based randomly shifted rank-1 lattice rule by determining the quadrature error rate with respect to the noise level. The effects of using Gaussian distributions and $t$-distributions as the proposal distributions on the error rate of QMC are comprehensively investigated. We find that the error rate may deteriorate at low intensity of noise when using improper proposals, such as the prior distribution. To reclaim the effectiveness of QMC, we propose a new IS method such that the lattice rule with $N$ quadrature points achieves an optimal error rate close to $O(N^{-1})$, which is insensitive to the noise level. Numerical experiments are conducted to support the theoretical results.

{{</citation>}}


## cs.HC (2)



### (1/2 | 142/156) The Effects of Generative AI on Design Fixation and Divergent Thinking (Samangi Wadinambiarachchi et al., 2024)

{{<citation>}}

Samangi Wadinambiarachchi, Ryan M. Kelly, Saumya Pareek, Qiushi Zhou, Eduardo Velloso. (2024)  
**The Effects of Generative AI on Design Fixation and Divergent Thinking**
<br/>
<button class="copy-to-clipboard" title="The Effects of Generative AI on Design Fixation and Divergent Thinking" index=142>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-HC, cs.HC  
Keyword Score: 20  
Keywords: Generative AI, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11164v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11164v1.pdf" filename="2403.11164v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Generative</b> <b>AI</b> systems have been heralded as tools for augmenting human creativity and inspiring divergent thinking, though with little empirical evidence for these claims. This paper explores the effects of exposure to AI-generated images on measures of design fixation and divergent thinking in a visual ideation task. Through a between-participants experiment (N=60), we found that support from an AI image generator during ideation leads to higher fixation on an initial example. Participants who used AI produced fewer ideas, with less variety and lower originality compared to a baseline. Our qualitative analysis suggests that the effectiveness of co-ideation with AI rests on participants' chosen approach to <b>prompt</b> creation and on the strategies used by participants to generate ideas in response to the AI's suggestions. We discuss opportunities for designing <b>generative</b> <b>AI</b> systems for ideation support and incorporating these AI tools into ideation workflows.

{{</citation>}}


### (2/2 | 143/156) GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment (Lance Ying et al., 2024)

{{<citation>}}

Lance Ying, Kunal Jha, Shivam Aarya, Joshua B. Tenenbaum, Antonio Torralba, Tianmin Shu. (2024)  
**GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment**
<br/>
<button class="copy-to-clipboard" title="GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment" index=143>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-AI, cs-HC, cs-MA, cs.HC  
Keyword Score: 10  
Keywords: Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11075v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11075v1.pdf" filename="2403.11075v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Verbal communication plays a crucial role in human cooperation, particularly when the partners only have incomplete information about the task, environment, and each other's mental state. In this paper, we propose a novel cooperative communication framework, Goal-Oriented Mental Alignment (GOMA). GOMA formulates verbal communication as a planning problem that minimizes the misalignment between the parts of agents' mental states that are relevant to the goals. This approach enables an embodied assistant to reason about when and how to proactively initialize communication with humans verbally using natural language to help achieve better cooperation. We evaluate our approach against strong baselines in two challenging environments, Overcooked (a multiplayer game) and VirtualHome (a household simulator). Our experimental results demonstrate that <b>large</b> <b>language</b> <b>models</b> struggle with generating meaningful communication that is grounded in the social and physical context. In contrast, our approach can successfully generate concise verbal communication for the embodied assistant to effectively boost the performance of the cooperation as well as human users' perception of the assistant.

{{</citation>}}


## physics.flu-dyn (1)



### (1/1 | 144/156) Journey into SPH Simulation: A Comprehensive Framework and Showcase (Haofeng Huang et al., 2024)

{{<citation>}}

Haofeng Huang, Li Yi. (2024)  
**Journey into SPH Simulation: A Comprehensive Framework and Showcase**
<br/>
<button class="copy-to-clipboard" title="Journey into SPH Simulation: A Comprehensive Framework and Showcase" index=144>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: physics.flu-dyn  
Categories: I-3-6; I-3-7, cs-GR, physics-flu-dyn, physics.flu-dyn  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11156v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11156v1.pdf" filename="2403.11156v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This report presents the development and results of an advanced SPH (Smoothed Particle Hydrodynamics) <b>simulation</b> framework, designed for high fidelity fluid dynamics modeling. Our framework, accessible at https://github.com/jason-huang03/SPH_Project, integrates various SPH algorithms including WCSPH, PCISPH, and DFSPH, alongside techniques for rigid-fluid coupling and high viscosity fluid <b>simulations.</b> Leveraging the computational power of CUDA and the versatility of Taichi, the framework excels in handling large-scale <b>simulations</b> with millions of particles. We demonstrate the capability of our framework through a series of <b>simulations</b> showcasing rigid-fluid coupling, high viscosity fluids, and large-scale fluid dynamics. Furthermore, a detailed performance analysis reveals CUDA's superior efficiency across different hardware platforms. This work is an exploraion into modern SPH <b>simulation</b> techniques, showcasing their practical implementation and capabilities.

{{</citation>}}


## cs.IT (2)



### (1/2 | 145/156) Enhanced Index Modulation Aided Non-Orthogonal Multiple Access via Constellation Rotation (Ronglan Huang et al., 2024)

{{<citation>}}

Ronglan Huang, Fei ji, Zeng Hu, Dehuan Wan, Pengcheng Xu, Yun Liu. (2024)  
**Enhanced Index Modulation Aided Non-Orthogonal Multiple Access via Constellation Rotation**
<br/>
<button class="copy-to-clipboard" title="Enhanced Index Modulation Aided Non-Orthogonal Multiple Access via Constellation Rotation" index=145>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs-NI, cs.IT, eess-SP, math-IT  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11081v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11081v1.pdf" filename="2403.11081v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Non-orthogonal multiple access (NOMA) has been widely nominated as an emerging spectral efficiency (SE) multiple access technique for the next generation of wireless communication network. To meet the growing demands in massive connectivity and huge data in transmission, a novel index modulation aided NOMA with the rotation of signal constellation of low power users (IM-NOMA-RC) is developed to the downlink transmission. In the proposed IM-NOMA-RC system, the users are classified into far-user group and near-user group according to their channel conditions, where the rotation constellation based IM operation is performed only on the users who belong to the near-user group that are allocated lower power compared with the far ones to transmit extra information. In the proposed IM-NOMA-RC, all the subcarriers are activated to transmit information to multiple users to achieve higher SE. With the aid of the multiple dimension modulation in IM-NOMA-RC, more users can be supported over an orthogonal resource block. Then, both maximum likelihood (ML) detector and successive interference cancellation (SIC) detector are studied for all the user. Numerical <b>simulation</b> results of the proposed IM-NOMARC scheme are investigate for the ML detector and the SIC detector for each users, which shows that proposed scheme can outperform conventional NOMA.

{{</citation>}}


### (2/2 | 146/156) Task-Based Quantizer Design for Sensing With Random Signals (Hang Ruan et al., 2024)

{{<citation>}}

Hang Ruan, Fan Liu. (2024)  
**Task-Based Quantizer Design for Sensing With Random Signals**
<br/>
<button class="copy-to-clipboard" title="Task-Based Quantizer Design for Sensing With Random Signals" index=146>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, eess-SP, math-IT  
Keyword Score: 10  
Keywords: Quantization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11187v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11187v1.pdf" filename="2403.11187v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In integrated sensing and communication (ISAC) systems, random signaling is used to convey useful information as well as sense the environment. Such randomness poses challenges in various components in sensing signal processing. In this paper, we investigate quantizer design for sensing in ISAC systems. Unlike quantizers for channel estimation in massive multiple-input-multiple-out (MIMO) communication systems, sensing in ISAC systems needs to deal with random nonorthogonal transmitted signals rather than a fixed orthogonal pilot. Considering sensing performance and hardware implementation, we focus on task-based hardware-limited <b>quantization</b> with spatial analog combining. We propose two strategies of quantizer optimization, i.e., data-dependent (DD) and data-independent (DI). The former achieves optimized sensing performance with high implementation overhead. To reduce hardware complexity, the latter optimizes the quantizer with respect to the random signal from a stochastic perspective. We derive the optimal quantizers for both strategies and formulate an algorithm based on sample average approximation (SAA) to solve the optimization in the DI strategy. Numerical results show that the optimized quantizers outperform digital-only quantizers in terms of sensing performance. Additionally, the DI strategy, despite its lower computational complexity compared to the DD strategy, achieves near-optimal sensing performance.

{{</citation>}}


## quant-ph (1)



### (1/1 | 147/156) JustQ: Automated Deployment of Fair and Accurate Quantum Neural Networks (Ruhan Wang et al., 2024)

{{<citation>}}

Ruhan Wang, Fahiz Baba-Yara, Fan Chen. (2024)  
**JustQ: Automated Deployment of Fair and Accurate Quantum Neural Networks**
<br/>
<button class="copy-to-clipboard" title="JustQ: Automated Deployment of Fair and Accurate Quantum Neural Networks" index=147>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: quant-ph  
Categories: cs-CY, cs-LG, quant-ph, quant-ph  
Keyword Score: 20  
Keywords: Fairness, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11048v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11048v1.pdf" filename="2403.11048v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Despite the success of Quantum Neural Networks (QNNs) in decision-making systems, their <b>fairness</b> remains unexplored, as the focus primarily lies on accuracy. This work conducts a design space exploration, unveiling QNN unfairness, and highlighting the significant influence of QNN deployment and quantum noise on accuracy and <b>fairness.</b> To effectively navigate the vast QNN deployment design space, we propose JustQ, a framework for deploying fair and accurate QNNs on NISQ computers. It includes a complete NISQ error model, <b>reinforcement</b> <b>learning-based</b> deployment, and a flexible optimization objective incorporating both <b>fairness</b> and accuracy. Experimental results show JustQ outperforms previous methods, achieving superior accuracy and <b>fairness.</b> This work pioneers fair QNN design on NISQ computers, paving the way for future investigations.

{{</citation>}}


## physics.geo-ph (1)



### (1/1 | 148/156) Potential of Domain Adaptation in Machine Learning in Ecology and Hydrology to Improve Model Extrapolability (Haiyang Shi, 2024)

{{<citation>}}

Haiyang Shi. (2024)  
**Potential of Domain Adaptation in Machine Learning in Ecology and Hydrology to Improve Model Extrapolability**
<br/>
<button class="copy-to-clipboard" title="Potential of Domain Adaptation in Machine Learning in Ecology and Hydrology to Improve Model Extrapolability" index=148>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: physics.geo-ph  
Categories: cs-LG, physics-data-an, physics-geo-ph, physics.geo-ph  
Keyword Score: 10  
Keywords: Domain Adaptation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11331v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11331v1.pdf" filename="2403.11331v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Due to the heterogeneity of the global distribution of ecological and hydrological ground-truth observations, machine learning models can have limited adaptability when applied to unknown locations, which is referred to as weak extrapolability. <b>Domain</b> <b>adaptation</b> techniques have been widely used in machine learning <b>domains</b> <b>such</b> as image classification, which can improve the model generalization ability by adjusting the difference or inconsistency of the <b>domain</b> <b>distribution</b> between the training and test sets. However, this approach has rarely been used explicitly in machine learning models in ecology and hydrology at the global scale, although these models have often been questioned due to geographic extrapolability issues. This paper briefly describes the shortcomings of current machine learning models of ecology and hydrology in terms of the global representativeness of the distribution of observations and the resulting limitations of the lack of extrapolability and suggests that future related modelling efforts should consider the use of <b>domain</b> <b>adaptation</b> techniques to improve extrapolability.

{{</citation>}}


## cs.DC (1)



### (1/1 | 149/156) Lion: Minimizing Distributed Transactions through Adaptive Replica Provision (Extended Version) (Qiushi Zheng et al., 2024)

{{<citation>}}

Qiushi Zheng, Zhanhao Zhao, Wei Lu, Chang Yao, Yuxing Chen, Anqun Pan, Xiaoyong Du. (2024)  
**Lion: Minimizing Distributed Transactions through Adaptive Replica Provision (Extended Version)**
<br/>
<button class="copy-to-clipboard" title="Lion: Minimizing Distributed Transactions through Adaptive Replica Provision (Extended Version)" index=149>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DC  
Categories: cs-DB, cs-DC, cs.DC  
Keyword Score: 10  
Keywords: LSTM  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11221v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11221v1.pdf" filename="2403.11221v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Distributed transaction processing often involves multiple rounds of cross-node communications, and therefore tends to be slow. To improve performance, existing approaches convert distributed transactions into single-node transactions by either migrating co-accessed partitions onto the same nodes or establishing a super node housing replicas of the entire database. However, migration-based methods might cause transactions to be blocked due to waiting for data migration, while the super node can become a bottleneck. In this paper, we present Lion, a novel transaction processing protocol that utilizes partition-based replication to reduce the occurrence of distributed transactions. Lion aims to assign a node with one replica from each partition involved in a given transaction's read or write operations. To ensure such a node is available, we propose an adaptive replica provision mechanism, enhanced with an <b>LSTM-based</b> workload prediction algorithm, to determine the appropriate node for locating replicas of co-accessed partitions. The adaptation of replica placement is conducted preemptively and asynchronously, thereby minimizing its impact on performance. By employing this adaptive replica placement strategy, we ensure that the majority of transactions can be efficiently processed on a single node without additional overhead. Only a small fraction of transactions will need to be treated as regular distributed transactions when such a node is unavailable. Consequently, Lion effectively minimizes distributed transactions while avoiding any disruption caused by data migration or the creation of a super node. We conduct extensive experiments to compare Lion against various transaction processing protocols. The results show that Lion achieves up to 2.7x higher throughput and 76.4% better scalability against these state-of-the-art approaches.

{{</citation>}}


## cs.DB (2)



### (1/2 | 150/156) Wait to be Faster: a Smart Pooling Framework for Dynamic Ridesharing (Xiaoyao Zhong et al., 2024)

{{<citation>}}

Xiaoyao Zhong, Jiabao Jin, Peng Cheng, Wangze Ni, Libin Zheng, Lei Chen, Xuemin Lin. (2024)  
**Wait to be Faster: a Smart Pooling Framework for Dynamic Ridesharing**
<br/>
<button class="copy-to-clipboard" title="Wait to be Faster: a Smart Pooling Framework for Dynamic Ridesharing" index=150>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DB  
Categories: cs-DB, cs.DB  
Keyword Score: 10  
Keywords: Markov Decision Process  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11099v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11099v1.pdf" filename="2403.11099v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Ridesharing services, such as Uber or Didi, have attracted considerable attention in recent years due to their positive impact on environmental protection and the economy. Existing studies require quick responses to orders, which lack the flexibility to accommodate longer wait times for better grouping opportunities. In this paper, we address a NP-hard ridesharing problem, called Minimal Extra Time RideSharing (METRS), which balances waiting time and group quality (i.e., detour time) to improve riders' satisfaction. To tackle this problem, we propose a novel approach called WATTER (WAit To be fasTER), which leverages an order pooling management algorithm allowing orders to wait until they can be matched with suitable groups. The key challenge is to customize the extra time threshold for each order by reducing the original optimization objective into a convex function of threshold, thus offering a theoretical guarantee to be optimized efficiently. We model the dispatch process using a <b>Markov</b> <b>Decision</b> <b>Process</b> (MDP) with a carefully designed value function to learn the threshold. Through extensive experiments on three real datasets, we demonstrate the efficiency and effectiveness of our proposed approaches.

{{</citation>}}


### (2/2 | 151/156) Graph Theory for Consent Management: A New Approach for Complex Data Flows (Dorota Filipczuk et al., 2024)

{{<citation>}}

Dorota Filipczuk, Enrico H. Gerding, George Konstantinidis. (2024)  
**Graph Theory for Consent Management: A New Approach for Complex Data Flows**
<br/>
<button class="copy-to-clipboard" title="Graph Theory for Consent Management: A New Approach for Complex Data Flows" index=151>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DB  
Categories: cs-DB, cs.DB  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11361v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11361v1.pdf" filename="2403.11361v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Through legislation and technical advances users gain more control over how their data is processed, and they expect online services to respect their privacy choices and preferences. However, data may be processed for many different purposes by several layers of algorithms that create complex data workflows. To date, there is no existing approach to automatically satisfy fine-grained privacy constraints of a user in a way which optimises the service provider's gains from processing. In this article, we propose a solution to this problem by modelling a data flow as a <b>graph.</b> User constraints and processing purposes are pairs of vertices which need to be disconnected in this <b>graph.</b> In general, this problem is NP-hard, thus, we propose several heuristics and algorithms. We discuss the optimality versus efficiency of our algorithms and evaluate them using synthetically generated data. On the practical side, our algorithms can provide nearly optimal solutions for tens of constraints and <b>graphs</b> of thousands of nodes, in a few seconds.

{{</citation>}}


## math.OC (2)



### (1/2 | 152/156) An SDP-based Branch-and-Cut Algorithm for Biclustering (Antonio M. Sudoso, 2024)

{{<citation>}}

Antonio M. Sudoso. (2024)  
**An SDP-based Branch-and-Cut Algorithm for Biclustering**
<br/>
<button class="copy-to-clipboard" title="An SDP-based Branch-and-Cut Algorithm for Biclustering" index=152>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.OC  
Categories: cs-LG, math-OC, math.OC, stat-ML  
Keyword Score: 6  
Keywords: Graph, Clustering  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11351v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11351v1.pdf" filename="2403.11351v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Biclustering, also called co-clustering, block <b>clustering,</b> or two-way <b>clustering,</b> involves the simultaneous <b>clustering</b> of both the rows and columns of a data matrix into distinct groups, such that the rows and columns within a group display similar patterns. As a model problem for biclustering, we consider the $k$-densest-disjoint biclique problem, whose goal is to identify $k$ disjoint complete bipartite subgraphs (called bicliques) of a given weighted complete bipartite <b>graph</b> such that the sum of their densities is maximized. To address this problem, we present a tailored branch-and-cut algorithm. For the upper bound routine, we consider a semidefinite programming relaxation and propose valid inequalities to strengthen the bound. We solve this relaxation in a cutting-plane fashion using a first-order method. For the lower bound, we design a maximum weight matching rounding procedure that exploits the solution of the relaxation solved at each node. Computational results on both synthetic and real-world instances show that the proposed algorithm can solve instances approximately 20 times larger than those handled by general-purpose solvers.

{{</citation>}}


### (2/2 | 153/156) Learning-Based Pricing and Matching for Two-Sided Queues (Zixian Yang et al., 2024)

{{<citation>}}

Zixian Yang, Lei Ying. (2024)  
**Learning-Based Pricing and Matching for Two-Sided Queues**
<br/>
<button class="copy-to-clipboard" title="Learning-Based Pricing and Matching for Two-Sided Queues" index=153>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.OC  
Categories: cs-LG, math-OC, math-PR, math.OC  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11093v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11093v1.pdf" filename="2403.11093v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We consider a dynamic system with multiple types of customers and servers. Each type of waiting customer or server joins a separate queue, forming a bipartite <b>graph</b> with customer-side queues and server-side queues. The platform can match the servers and customers if their types are compatible. The matched pairs then leave the system. The platform will charge a customer a price according to their type when they arrive and will pay a server a price according to their type. The arrival rate of each queue is determined by the price according to some unknown demand or supply functions. Our goal is to design pricing and matching algorithms to maximize the profit of the platform with unknown demand and supply functions, while keeping queue lengths of both customers and servers below a predetermined threshold. This system can be used to model two-sided markets such as ride-sharing markets with passengers and drivers. The difficulties of the problem include simultaneous learning and decision making, and the tradeoff between maximizing profit and minimizing queue length. We use a longest-queue-first matching algorithm and propose a learning-based pricing algorithm, which combines gradient-free stochastic projected gradient ascent with bisection search. We prove that our proposed algorithm yields a sublinear regret $\tilde{O}(T^{5/6})$ and queue-length bound $\tilde{O}(T^{2/3})$, where $T$ is the time horizon. We further establish a tradeoff between the regret bound and the queue-length bound: $\tilde{O}(T^{1-\gamma/4})$ versus $\tilde{O}(T^{\gamma})$ for $\gamma \in (0, 2/3].$

{{</citation>}}


## cs.DS (1)



### (1/1 | 154/156) A constant time complexity algorithm for the unbounded knapsack problem with bounded coefficients (Yang Yang, 2024)

{{<citation>}}

Yang Yang. (2024)  
**A constant time complexity algorithm for the unbounded knapsack problem with bounded coefficients**
<br/>
<button class="copy-to-clipboard" title="A constant time complexity algorithm for the unbounded knapsack problem with bounded coefficients" index=154>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DS  
Categories: cs-CC, cs-DS, cs.DS  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11320v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11320v1.pdf" filename="2403.11320v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Benchmark</b> instances for the unbounded knapsack problem are typically generated according to specific criteria within a given constant range $R$, and these instances can be referred to as the unbounded knapsack problem with bounded coefficients (UKPB). In order to increase the difficulty of solving these instances, the knapsack capacity $C$ is usually set to a very large value. Therefore, an exact algorithm that neither time complexity nor space complexity includes the capacity coefficient $C$ is highly anticipated. In this paper, we propose an exact algorithm with time complexity of $O(R^4)$ and space complexity of $O(R^3)$. The algorithm initially divides the multiset $N$ into two multisubsets, $N_1$ and $N_2$, based on the profit density of their types. For the multisubset $N_2$ composed of types with profit density lower than the maximum profit density type, we utilize a recent branch and bound (B\&B) result by Dey et al. (Math. Prog., pp 569-587, 2023) to determine the maximum selection number for types in $N_2$. We then employ the Unbounded-DP algorithm to exactly solve for the types in $N_2$. For the multisubset $N_1$ composed of the maximum profit density type and its counterparts with the same profit density, we transform it into a linear Diophantine equation and leverage relevant conclusions from the Frobenius problem to solve it efficiently. In particular, the proof techniques required by the algorithm are primarily covered in the first-year mathematics curriculum, which is convenient for subsequent researchers to grasp.

{{</citation>}}


## stat.ME (1)



### (1/1 | 155/156) A Selective Review on Statistical Methods for Massive Data Computation: Distributed Computing, Subsampling, and Minibatch Techniques (Xuetong Li et al., 2024)

{{<citation>}}

Xuetong Li, Yuan Gao, Hong Chang, Danyang Huang, Yingying Ma, Rui Pan, Haobo Qi, Feifei Wang, Shuyuan Wu, Ke Xu, Jing Zhou, Xuening Zhu, Yingqiu Zhu, Hansheng Wang. (2024)  
**A Selective Review on Statistical Methods for Massive Data Computation: Distributed Computing, Subsampling, and Minibatch Techniques**
<br/>
<button class="copy-to-clipboard" title="A Selective Review on Statistical Methods for Massive Data Computation: Distributed Computing, Subsampling, and Minibatch Techniques" index=155>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ME  
Categories: cs-LG, math-ST, stat-CO, stat-ME, stat-TH, stat.ME  
Keyword Score: 3  
Keywords: Sample Size  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11163v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11163v1.pdf" filename="2403.11163v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents a selective review of statistical computation methods for massive data analysis. A huge amount of statistical methods for massive data computation have been rapidly developed in the past decades. In this work, we focus on three categories of statistical computation methods: (1) distributed computing, (2) subsampling methods, and (3) minibatch gradient techniques. The first class of literature is about distributed computing and focuses on the situation, where the dataset size is too huge to be comfortably handled by one single computer. In this case, a distributed computation system with multiple computers has to be utilized. The second class of literature is about subsampling methods and concerns about the situation, where the <b>sample</b> <b>size</b> of dataset is small enough to be placed on one single computer but too large to be easily processed by its memory as a whole. The last class of literature studies those minibatch gradient related optimization techniques, which have been extensively used for optimizing various deep learning models.

{{</citation>}}


## eess.SP (1)



### (1/1 | 156/156) Wavenumber Domain Sparse Channel Estimation in Holographic MIMO (Xufeng Guo et al., 2024)

{{<citation>}}

Xufeng Guo, Yuanbin Chen, Ying Wang, Zhaocheng Wang, Zhu Han. (2024)  
**Wavenumber Domain Sparse Channel Estimation in Holographic MIMO**
<br/>
<button class="copy-to-clipboard" title="Wavenumber Domain Sparse Channel Estimation in Holographic MIMO" index=156>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SP  
Categories: cs-IT, eess-SP, eess.SP, math-IT  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.11071v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.11071v1.pdf" filename="2403.11071v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we investigate the sparse channel estimation in holographic multiple-input multiple-output (HMIMO) systems. The conventional angular-domain representation fails to capture the continuous angular power spectrum characterized by the spatially-stationary electromagnetic random field, thus leading to the ambiguous detection of the significant angular power, which is referred to as the power leakage. To tackle this challenge, the HMIMO channel is represented in the wavenumber domain for exploring its cluster-dominated sparsity. Specifically, a finite set of Fourier harmonics acts as a series of sampling probes to encapsulate the integral of the power spectrum over specific angular regions. This technique effectively eliminates power leakage resulting from power mismatches induced by the use of discrete angular-domain probes. Next, the channel estimation problem is recast as a sparse recovery of the significant angular power spectrum over the continuous integration region. We then propose an accompanying <b>graph-cut-based</b> swap expansion (GCSE) algorithm to extract beneficial sparsity inherent in HMIMO channels. Numerical results demonstrate that this wavenumber-domainbased GCSE approach achieves robust performance with rapid convergence.

{{</citation>}}
