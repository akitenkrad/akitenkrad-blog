---
draft: false
title: "arXiv @ 2024.03.16"
date: 2024-03-16
author: "akitenkrad"
description: ""
tags: ["arXiv", "Published:2024"]
menu:
  sidebar:
    name: "arXiv @ 2024.03.16"
    identifier: arxiv_20240316
    parent: 202403_arxiv
    weight: 10
math: true
---

<figure style="border:none; width:100%; display:flex; justify-content: center">
    <iframe src="pie.html" width=900 height=620 style="border:none"></iframe>
</figure>


## Primary Categories

- [cs.AI (6)](#csai-6)
- [cs.AR (2)](#csar-2)
- [cs.CE (2)](#csce-2)
- [cs.CL (32)](#cscl-32)
- [cs.CR (7)](#cscr-7)
- [cs.CV (91)](#cscv-91)
- [cs.CY (2)](#cscy-2)
- [cs.DB (1)](#csdb-1)
- [cs.DC (2)](#csdc-2)
- [cs.DM (2)](#csdm-2)
- [cs.GR (2)](#csgr-2)
- [cs.GT (1)](#csgt-1)
- [cs.HC (8)](#cshc-8)
- [cs.IR (5)](#csir-5)
- [cs.IT (6)](#csit-6)
- [cs.LG (31)](#cslg-31)
- [cs.LO (1)](#cslo-1)
- [cs.NI (2)](#csni-2)
- [cs.RO (10)](#csro-10)
- [cs.SD (6)](#cssd-6)
- [cs.SE (6)](#csse-6)
- [cs.SI (3)](#cssi-3)
- [eess.IV (8)](#eessiv-8)
- [eess.SY (9)](#eesssy-9)
- [math.CO (1)](#mathco-1)
- [math.LO (1)](#mathlo-1)
- [math.NA (2)](#mathna-2)
- [math.OC (1)](#mathoc-1)
- [physics.optics (1)](#physicsoptics-1)
- [quant-ph (1)](#quant-ph-1)
- [stat.ML (2)](#statml-2)

## Keywords

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>keyword</th>
      <th>cs.CL</th>
      <th>cs.CV</th>
      <th>cs.LG</th>
      <th>cs.RO</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Active Learning</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Adversarial Attack</td>
      <td></td>
      <td>1</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Adversarial Learning</td>
      <td></td>
      <td>1</td>
      <td>3</td>
      <td></td>
    </tr>
    <tr>
      <td>Anomaly Detection</td>
      <td></td>
      <td>1</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Autoencoder</td>
      <td></td>
      <td>1</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Benchmarking</td>
      <td>6</td>
      <td>21</td>
      <td>7</td>
      <td>3</td>
    </tr>
    <tr>
      <td>Black Box</td>
      <td></td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Chain-of-thought Prompt</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>ChatGPT</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Chatbot</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Clustering</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Cohere</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Continual Learning</td>
      <td></td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Continuous Time</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Contrastive Learning</td>
      <td>1</td>
      <td>6</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>ControlNet</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Convolution</td>
      <td></td>
      <td>6</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Convolutional Neural Network</td>
      <td></td>
      <td>12</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Counter-factual</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Curriculum Learning</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Data Augmentation</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Diffusion Model</td>
      <td></td>
      <td>15</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Distribution Shift</td>
      <td></td>
      <td>4</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Domain Adaptation</td>
      <td>2</td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Emotion Recognition</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Event Detection</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Face Recognition</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Fact Verification</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Fairness</td>
      <td>2</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Fake News Detection</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Federated Learning</td>
      <td></td>
      <td>1</td>
      <td>3</td>
      <td></td>
    </tr>
    <tr>
      <td>Few-shot</td>
      <td>3</td>
      <td>1</td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Few-shot Learning</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Fine-tuning</td>
      <td>16</td>
      <td>14</td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Foundation Model</td>
      <td></td>
      <td>12</td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>GPT</td>
      <td>8</td>
      <td>3</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>GPT-2</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>GPT-3</td>
      <td>3</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>GPT-3.5</td>
      <td>3</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>GPT-4</td>
      <td>4</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Gaussian Process</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Generative Adversarial Network</td>
      <td></td>
      <td>3</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Geometry</td>
      <td></td>
      <td>7</td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Graph</td>
      <td>1</td>
      <td>6</td>
      <td>4</td>
      <td></td>
    </tr>
    <tr>
      <td>Graph Attention Networks</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Graph Contrastive Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Graph Convolutional Network</td>
      <td></td>
      <td>4</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Graph Embedding</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Graph Neural Network</td>
      <td></td>
      <td></td>
      <td>4</td>
      <td></td>
    </tr>
    <tr>
      <td>Grounding</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Human Intervention</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Image2text</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>In-context Learning</td>
      <td>8</td>
      <td>4</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Information Retrieval</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Instruction Following</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Instruction Tuning</td>
      <td>3</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Knowledge Distillation</td>
      <td>3</td>
      <td>13</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Knowledge Graph</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Knowledge Transfer</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>LLaMA</td>
      <td>3</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Large Language Model</td>
      <td>37</td>
      <td>15</td>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <td>Logistic Regression</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Message-Passing</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Meta Learning</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Model Compression</td>
      <td></td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Model Distillation</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Multi-modal</td>
      <td>2</td>
      <td>37</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Named Entity Recognition</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Natural Language Explanation</td>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Natural Language Understanding</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Neural Machine Translation</td>
      <td>7</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Node Embedding</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Object Detection</td>
      <td></td>
      <td>10</td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Offline Reinforcement Learning</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Optical Character Recognition</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Out-of-distribution</td>
      <td></td>
      <td>2</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Perplexity</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Pre-trained Language Model</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Prompt</td>
      <td>6</td>
      <td>18</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Prompt Learning</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Pruning</td>
      <td>1</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Quantization</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Question Answering</td>
      <td>5</td>
      <td>3</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Reasoning</td>
      <td>7</td>
      <td>5</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Recommendation</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Recurrent Neural Network</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Reinforcement Learning</td>
      <td></td>
      <td></td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Relation Extraction</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Representation Learning</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Retrieval-Augmented Generation</td>
      <td>6</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Sample Size</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Self-Attention</td>
      <td></td>
      <td>3</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Self-supervised Learning</td>
      <td></td>
      <td>5</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Sentence Embedding</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Simulation</td>
      <td></td>
      <td>2</td>
      <td>2</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Simulator</td>
      <td></td>
      <td>2</td>
      <td>2</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Speech-to-Speech Translation</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Stemming</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Stochastic Gradient Descent</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Summarization</td>
      <td>2</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Supervised Learning</td>
      <td></td>
      <td>12</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>T5</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Text Generation</td>
      <td>1</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Text Summarization</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Text2SQL</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Text2image</td>
      <td></td>
      <td>3</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Transfer Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Transformer</td>
      <td>4</td>
      <td>9</td>
      <td>3</td>
      <td></td>
    </tr>
    <tr>
      <td>Unsupervised Learning</td>
      <td>1</td>
      <td>7</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Vision Transformer</td>
      <td></td>
      <td>4</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Vision-and-Language</td>
      <td>1</td>
      <td>17</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Visual Question Answering</td>
      <td></td>
      <td>4</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Weakly Supervised Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Weakly-supervised Learning</td>
      <td></td>
      <td>3</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Zero-shot</td>
      <td>4</td>
      <td>9</td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Zero-shot Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
  </tbody>
</table>

<script>
$(function() {
  $("table").addClass("keyword-table table-bordered border-success");
  $("table thead").addClass("sticky-top");
  $("table tbody td").css("text-align", "");
});
</script>


## cs.CV (91)



### (1/91 | 1/254) MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training (Brandon McKinzie et al., 2024)

{{<citation>}}

Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang. (2024)  
**MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training**
<br/>
<button class="copy-to-clipboard" title="MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training" index=1>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CL, cs-CV, cs-LG, cs.CV  
Keyword Score: 129  
Keywords: Benchmarking, Few-shot, Fine-tuning, Multi-modal, Multi-modal, Supervised Learning, Image2text, Reasoning, Chain-of-thought Prompt, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09611v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09611v1.pdf" filename="2403.09611v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this work, we discuss building performant <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for <b>large-scale</b> <b>multimodal</b> <b>pre-training</b> using a careful mix of image-caption, interleaved <b>image-text,</b> and text-only data is crucial for achieving state-of-the-art (SOTA) <b>few-shot</b> results across multiple <b>benchmarks,</b> compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the <b>vision-language</b> connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of <b>multimodal</b> models up to 30B parameters, consisting of both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after <b>supervised</b> <b>fine-tuning</b> on a range of established <b>multimodal</b> <b>benchmarks.</b> Thanks to <b>large-scale</b> <b>pre-training,</b> <b>MM1</b> enjoys appealing properties such as enhanced <b>in-context</b> <b>learning,</b> and multi-image <b>reasoning,</b> enabling <b>few-shot</b> <b>chain-of-thought</b> <b>prompting.</b>

{{</citation>}}


### (2/91 | 2/254) GiT: Towards Generalist Vision Transformer through Universal Language Interface (Haiyang Wang et al., 2024)

{{<citation>}}

Haiyang Wang, Hao Tang, Li Jiang, Shaoshuai Shi, Muhammad Ferjad Naeem, Hongsheng Li, Bernt Schiele, Liwei Wang. (2024)  
**GiT: Towards Generalist Vision Transformer through Universal Language Interface**
<br/>
<button class="copy-to-clipboard" title="GiT: Towards Generalist Vision Transformer through Universal Language Interface" index=2>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 103  
Keywords: Vision Transformer, Benchmarking, Fine-tuning, Foundation Model, Zero-shot, GPT, Transformer, Large Language Model, Large Language Model, Vision Transformer, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09394v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09394v1.pdf" filename="2403.09394v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper proposes a simple, yet effective framework, called GiT, simultaneously applicable for various <b>vision</b> <b>tasks</b> only with a vanilla ViT. Motivated by the universality of the Multi-layer <b>Transformer</b> architecture (e.g, <b>GPT)</b> widely used in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> we seek to broaden its scope to serve as a powerful <b>vision</b> <b>foundation</b> <b>model</b> (VFM). However, unlike language modeling, visual tasks typically require specific modules, such as bounding box heads for detection and pixel decoders for segmentation, greatly hindering the application of powerful multi-layer <b>transformers</b> in the <b>vision</b> <b>domain.</b> To solve this, we design a universal language interface that empowers the successful auto-regressive decoding to adeptly unify various visual tasks, from image-level understanding (e.g., captioning), over sparse perception (e.g., detection), to dense prediction (e.g., segmentation). Based on the above designs, the entire model is composed solely of a ViT, without any specific additions, offering a remarkable architectural simplification. GiT is a multi-task visual model, jointly trained across five representative <b>benchmarks</b> without task-specific <b>fine-tuning.</b> Interestingly, our GiT builds a new <b>benchmark</b> in generalist performance, and fosters mutual enhancement across tasks, leading to significant improvements compared to isolated training. This reflects a similar impact observed in <b>LLMs.</b> Further enriching training with 27 datasets, GiT achieves strong <b>zero-shot</b> results over various tasks. Due to its simple design, this paradigm holds promise for narrowing the architectural gap between <b>vision</b> <b>and</b> language. Code and models will be available at \url{https://github.com/Haiyang-W/GiT}.

{{</citation>}}


### (3/91 | 3/254) Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models (Yu-Chu Yu et al., 2024)

{{<citation>}}

Yu-Chu Yu, Chi-Pin Huang, Jr-Jen Chen, Kai-Po Chang, Yung-Hsuan Lai, Fu-En Yang, Yu-Chiang Frank Wang. (2024)  
**Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models**
<br/>
<button class="copy-to-clipboard" title="Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models" index=3>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 83  
Keywords: Benchmarking, Continual Learning, Fine-tuning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Knowledge Transfer, Zero-shot, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09296v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09296v1.pdf" filename="2403.09296v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Large-scale <b>vision-language</b> models (VLMs) have shown a strong <b>zero-shot</b> generalization capability on unseen-domain data. However, when adapting pre-trained VLMs to a sequence of downstream tasks, they are prone to forgetting previously learned <b>knowledge</b> <b>and</b> degrade their <b>zero-shot</b> classification capability. To tackle this problem, we propose a unique Selective Dual-Teacher <b>Knowledge</b> <b>Transfer</b> framework that leverages the most recent <b>fine-tuned</b> and the original pre-trained VLMs as dual teachers to preserve the previously learned <b>knowledge</b> <b>and</b> <b>zero-shot</b> capabilities, respectively. With only access to an unlabeled reference dataset, our proposed framework performs a selective <b>knowledge</b> <b>distillation</b> mechanism by measuring the feature discrepancy from the dual teacher VLMs. Consequently, our selective dual-teacher <b>knowledge</b> <b>distillation</b> would mitigate catastrophic forgetting of previously learned <b>knowledge</b> <b>while</b> preserving the <b>zero-shot</b> capabilities from pre-trained VLMs. Through extensive experiments on <b>benchmark</b> datasets, we show that our proposed framework is favorable against state-of-the-art <b>continual</b> <b>learning</b> approaches for preventing catastrophic forgetting and <b>zero-shot</b> degradation.

{{</citation>}}


### (4/91 | 4/254) Adversarial Training with OCR Modality Perturbation for Scene-Text Visual Question Answering (Zhixuan Shen et al., 2024)

{{<citation>}}

Zhixuan Shen, Haonan Luo, Sijia Li, Tianrui Li. (2024)  
**Adversarial Training with OCR Modality Perturbation for Scene-Text Visual Question Answering**
<br/>
<button class="copy-to-clipboard" title="Adversarial Training with OCR Modality Perturbation for Scene-Text Visual Question Answering" index=4>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 76  
Keywords: Optical Character Recognition, Optical Character Recognition, Adversarial Learning, Fine-tuning, Multi-modal, Multi-modal, Question Answering, Visual Question Answering, Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09288v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09288v1.pdf" filename="2403.09288v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Scene-Text <b>Visual</b> <b>Question</b> <b>Answering</b> (ST-VQA) aims to understand scene text in images and answer <b>questions</b> <b>related</b> to the text content. Most existing methods heavily rely on the accuracy of <b>Optical</b> <b>Character</b> <b>Recognition</b> <b>(OCR)</b> systems, and aggressive <b>fine-tuning</b> based on limited spatial location information and erroneous <b>OCR</b> text information often leads to inevitable overfitting. In this paper, we propose a <b>multimodal</b> <b>adversarial</b> <b>training</b> architecture with spatial awareness capabilities. Specifically, we introduce an <b>Adversarial</b> <b>OCR</b> Enhancement (AOE) module, which leverages <b>adversarial</b> <b>training</b> in the embedding space of <b>OCR</b> modality to enhance fault-tolerant representation of <b>OCR</b> texts, thereby reducing noise caused by <b>OCR</b> errors. Simultaneously, We add a Spatial-Aware <b>Self-Attention</b> (SASA) mechanism to help the model better capture the spatial relationships among <b>OCR</b> tokens. Various experiments demonstrate that our method achieves significant performance improvements on both the ST-VQA and TextVQA datasets and provides a novel paradigm for <b>multimodal</b> <b>adversarial</b> <b>training.</b>

{{</citation>}}


### (5/91 | 5/254) VisionGPT: Vision-Language Understanding Agent Using Generalized Multimodal Framework (Chris Kelly et al., 2024)

{{<citation>}}

Chris Kelly, Luhui Hu, Bang Yang, Yu Tian, Deshun Yang, Cindy Yang, Zaoshan Huang, Zihao Li, Jiayin Hu, Yuexian Zou. (2024)  
**VisionGPT: Vision-Language Understanding Agent Using Generalized Multimodal Framework**
<br/>
<button class="copy-to-clipboard" title="VisionGPT: Vision-Language Understanding Agent Using Generalized Multimodal Framework" index=5>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 76  
Keywords: Foundation Model, Multi-modal, Multi-modal, LLaMA, Question Answering, Visual Question Answering, Large Language Model, Large Language Model, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09027v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09027v1.pdf" filename="2403.09027v1.pdf">Download PDF</button>

---


**ABSTRACT**  
With the emergence of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and vision <b>foundation</b> <b>models,</b> how to combine the intelligence and capacity of these open-sourced or API-available models to achieve open-world <b>visual</b> <b>perception</b> <b>remains</b> an open <b>question.</b> <b>In</b> this paper, we introduce VisionGPT to consolidate and automate the integration of state-of-the-art <b>foundation</b> <b>models,</b> thereby facilitating <b>vision-language</b> understanding and the development of vision-oriented AI. VisionGPT builds upon a generalized <b>multimodal</b> framework that distinguishes itself through three key features: (1) utilizing <b>LLMs</b> (e.g., <b>LLaMA-2)</b> as the pivot to break down users' requests into detailed action proposals to call suitable <b>foundation</b> <b>models;</b> (2) integrating multi-source outputs from <b>foundation</b> <b>models</b> automatically and generating comprehensive responses for users; (3) adaptable to a wide range of applications such as text-conditioned image understanding/generation/editing and <b>visual</b> <b>question</b> <b>answering.</b> This paper outlines the architecture and capabilities of VisionGPT, demonstrating its potential to revolutionize the field of computer vision through enhanced efficiency, versatility, and generalization, and performance. Our code and models will be made publicly available. Keywords: VisionGPT, Open-world <b>visual</b> <b>perception,</b> <b>Vision-language</b> understanding, <b>Large</b> <b>language</b> <b>model,</b> and <b>Foundation</b> <b>model</b>

{{</citation>}}


### (6/91 | 6/254) Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation (Yunhao Gou et al., 2024)

{{<citation>}}

Yunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James T. Kwok, Yu Zhang. (2024)  
**Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation**
<br/>
<button class="copy-to-clipboard" title="Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation" index=6>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 69  
Keywords: Benchmarking, Human Intervention, Multi-modal, Multi-modal, Supervised Learning, Image2text, Reasoning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09572v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09572v1.pdf" filename="2403.09572v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) have shown impressive <b>reasoning</b> abilities, which, however, are also more vulnerable to jailbreak attacks than their <b>LLM</b> predecessors. Although still capable of detecting unsafe responses, we observe that safety mechanisms of the pre-aligned <b>LLMs</b> in MLLMs can be easily bypassed due to the introduction of image features. To construct robust MLLMs, we propose ECSO(Eyes Closed, Safety On), a novel training-free protecting approach that exploits the inherent safety awareness of MLLMs, and generates safer responses via adaptively transforming unsafe images into texts to activate intrinsic safety mechanism of pre-aligned <b>LLMs</b> in MLLMs. Experiments on five state-of-the-art (SoTA) MLLMs demonstrate that our ECSO enhances model safety significantly (e.g., a 37.6% improvement on the MM-SafetyBench (SD+OCR), and 71.3% on VLSafe for the LLaVA-1.5-7B), while consistently maintaining utility results on common MLLM <b>benchmarks.</b> Furthermore, we show that ECSO can be used as a data engine to generate <b>supervised-finetuning</b> (SFT) data for MLLM alignment without extra <b>human</b> <b>intervention.</b>

{{</citation>}}


### (7/91 | 7/254) 3D-VLA: A 3D Vision-Language-Action Generative World Model (Haoyu Zhen et al., 2024)

{{<citation>}}

Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, Chuang Gan. (2024)  
**3D-VLA: A 3D Vision-Language-Action Generative World Model**
<br/>
<button class="copy-to-clipboard" title="3D-VLA: A 3D Vision-Language-Action Generative World Model" index=7>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CL, cs-CV, cs-RO, cs.CV  
Keyword Score: 66  
Keywords: Diffusion Model, Foundation Model, Multi-modal, Multi-modal, Reasoning, Large Language Model, Large Language Model, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09631v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09631v1.pdf" filename="2403.09631v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent <b>vision-language-action</b> (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied <b>foundation</b> <b>models</b> that seamlessly link 3D perception, <b>reasoning,</b> and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based <b>large</b> <b>language</b> <b>model</b> <b>(LLM),</b> and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied <b>diffusion</b> <b>models</b> and align them into the <b>LLM</b> for predicting the goal images and point clouds. To train our 3D-VLA, we curate a <b>large-scale</b> <b>3D</b> <b>embodied</b> instruction dataset by extracting vast 3D-related information from existing robotics datasets. Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the <b>reasoning,</b> <b>multimodal</b> generation, and planning capabilities in embodied environments, showcasing its potential in real-world applications.

{{</citation>}}


### (8/91 | 8/254) VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding (Chris Kelly et al., 2024)

{{<citation>}}

Chris Kelly, Luhui Hu, Jiayin Hu, Yu Tian, Deshun Yang, Bang Yang, Cindy Yang, Zihao Li, Zaoshan Huang, Yuexian Zou. (2024)  
**VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding**
<br/>
<button class="copy-to-clipboard" title="VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding" index=8>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CL, cs-CV, cs-GR, cs.CV  
Keyword Score: 66  
Keywords: Foundation Model, Multi-modal, Multi-modal, GPT, GPT-4, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09530v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09530v1.pdf" filename="2403.09530v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The evolution of text to visual components facilitates people's daily lives, such as generating image, videos from text and identifying the desired elements within the images. Computer vision models involving the <b>multimodal</b> abilities in the previous days are focused on image detection, classification based on well-defined objects. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> introduces the transformation from nature language to visual objects, which present the visual layout for text contexts. OpenAI <b>GPT-4</b> has emerged as the pinnacle in <b>LLMs,</b> while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models and algorithms to convert 2D images to their 3D representations. However, the mismatching between the algorithms with the problem could lead to undesired results. In response to this challenge, we propose an unified VisionGPT-3D framework to consolidate the state-of-the-art vision models, thereby facilitating the development of vision-oriented AI. VisionGPT-3D provides a versatile <b>multimodal</b> framework building upon the strengths of <b>multimodal</b> <b>foundation</b> <b>models.</b> It seamlessly integrates various SOTA vision models and brings the automation in the selection of SOTA vision models, identifies the suitable 3D mesh creation algorithms corresponding to 2D depth maps analysis, generates optimal results based on diverse <b>multimodal</b> inputs such as text <b>prompts.</b> Keywords: VisionGPT-3D, 3D vision understanding, <b>Multimodal</b> agent

{{</citation>}}


### (9/91 | 9/254) SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition (Jeonghyeok Do et al., 2024)

{{<citation>}}

Jeonghyeok Do, Munchurl Kim. (2024)  
**SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition**
<br/>
<button class="copy-to-clipboard" title="SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition" index=9>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 66  
Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Benchmarking, Convolution, Convolutional Neural Network, Transformer, Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09508v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09508v1.pdf" filename="2403.09508v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Skeleton-based action recognition, which classifies human actions based on the coordinates of joints and their connectivity within skeleton data, is widely utilized in various scenarios. While <b>Graph</b> <b>Convolutional</b> <b>Networks</b> <b>(GCNs)</b> have been proposed for skeleton data represented as <b>graphs,</b> <b>they</b> <b>suffer</b> from limited receptive fields constrained by joint connectivity. To address this limitation, recent advancements have introduced <b>transformer-based</b> methods. However, capturing correlations between all joints in all frames requires substantial memory resources. To alleviate this, we propose a novel approach called Skeletal-Temporal <b>Transformer</b> (SkateFormer) that partitions joints and frames based on different types of skeletal-temporal relation (Skate-Type) and performs skeletal-temporal <b>self-attention</b> (Skate-MSA) within each partition. We categorize the key skeletal-temporal relations for action recognition into a total of four distinct types. These types combine (i) two skeletal relation types based on physically neighboring and distant joints, and (ii) two temporal relation types based on neighboring and distant frames. Through this partition-specific attention strategy, our SkateFormer can selectively focus on key joints and frames crucial for action recognition in an action-adaptive manner with efficient computation. Extensive experiments on various <b>benchmark</b> datasets validate that our SkateFormer outperforms recent state-of-the-art methods.

{{</citation>}}


### (10/91 | 10/254) DF4LCZ: A SAM-Empowered Data Fusion Framework for Scene-Level Local Climate Zone Classification (Qianqian Wu et al., 2024)

{{<citation>}}

Qianqian Wu, Xianping Ma, Jialu Sui, Man-On Pun. (2024)  
**DF4LCZ: A SAM-Empowered Data Fusion Framework for Scene-Level Local Climate Zone Classification**
<br/>
<button class="copy-to-clipboard" title="DF4LCZ: A SAM-Empowered Data Fusion Framework for Scene-Level Local Climate Zone Classification" index=10>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 63  
Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Convolutional Neural Network, Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09367v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09367v1.pdf" filename="2403.09367v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advancements in remote sensing (RS) technologies have shown their potential in accurately classifying local climate zones (LCZs). However, traditional scene-level methods using <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> often struggle to integrate prior knowledge of ground objects effectively. Moreover, commonly utilized data sources like Sentinel-2 encounter difficulties in capturing detailed ground object information. To tackle these challenges, we propose a data fusion method that integrates ground object priors extracted from high-resolution Google imagery with Sentinel-2 multispectral imagery. The proposed method introduces a novel Dual-stream Fusion framework for LCZ classification (DF4LCZ), integrating instance-based location features from Google imagery with the scene-level spatial-spectral features extracted from Sentinel-2 imagery. The framework incorporates a <b>Graph</b> <b>Convolutional</b> <b>Network</b> <b>(GCN)</b> module empowered by the Segment Anything Model (SAM) to enhance feature extraction from Google imagery. Simultaneously, the framework employs a 3D-CNN architecture to learn the spectral-spatial features of Sentinel-2 imagery. Experiments are conducted on a multi-source remote sensing image dataset specifically designed for LCZ classification, validating the effectiveness of the proposed DF4LCZ. The related code and dataset are available at https://github.com/ctrlovefly/DF4LCZ.

{{</citation>}}


### (11/91 | 11/254) SCP-Diff: Photo-Realistic Semantic Image Synthesis with Spatial-Categorical Joint Prior (Huan-ang Gao et al., 2024)

{{<citation>}}

Huan-ang Gao, Mingju Gao, Jiaju Li, Wenyi Li, Rong Zhi, Hao Tang, Hao Zhao. (2024)  
**SCP-Diff: Photo-Realistic Semantic Image Synthesis with Spatial-Categorical Joint Prior**
<br/>
<button class="copy-to-clipboard" title="SCP-Diff: Photo-Realistic Semantic Image Synthesis with Spatial-Categorical Joint Prior" index=11>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 60  
Keywords: ControlNet, Diffusion Model, Generative Adversarial Network, Simulation, Simulator, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09638v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09638v1.pdf" filename="2403.09638v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Semantic image synthesis (SIS) shows good promises for sensor <b>simulation.</b> However, current best practices in this field, based on <b>GANs,</b> have not yet reached the desired level of quality. As latent <b>diffusion</b> <b>models</b> make significant strides in image generation, we are <b>prompted</b> to evaluate <b>ControlNet,</b> a notable method for its dense control capabilities. Our investigation uncovered two primary issues with its results: the presence of weird sub-structures within large semantic areas and the misalignment of content with the semantic mask. Through empirical study, we pinpointed the cause of these problems as a mismatch between the noised training data distribution and the standard normal prior applied at the inference stage. To address this challenge, we developed specific noise priors for SIS, encompassing spatial, categorical, and a novel spatial-categorical joint prior for inference. This approach, which we have named SCP-Diff, has yielded exceptional results, achieving an FID of 10.53 on Cityscapes and 12.66 on ADE20K.The code and models can be accessed via the project page.

{{</citation>}}


### (12/91 | 12/254) Cloud gap-filling with deep learning for improved grassland monitoring (Iason Tsardanidis et al., 2024)

{{<citation>}}

Iason Tsardanidis, Alkiviadis Koukos, Vasileios Sitokonstantinou, Thanassis Drivas, Charalampos Kontoes. (2024)  
**Cloud gap-filling with deep learning for improved grassland monitoring**
<br/>
<button class="copy-to-clipboard" title="Cloud gap-filling with deep learning for improved grassland monitoring" index=12>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV, eess-IV  
Keyword Score: 60  
Keywords: Continuous Time, Continuous Time, Convolution, Convolutional Neural Network, Convolutional Neural Network, Recurrent Neural Network, Event Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09554v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09554v1.pdf" filename="2403.09554v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Uninterrupted optical image time series are crucial for the timely monitoring of agricultural land changes. However, the continuity of such time series is often disrupted by clouds. In response to this challenge, we propose a deep learning method that integrates cloud-free optical (Sentinel-2) observations and weather-independent (Sentinel-1) Synthetic Aperture Radar (SAR) data, using a combined <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)-Recurrent</b> Neural Network <b>(RNN)</b> architecture to generate <b>continuous</b> <b>Normalized</b> Difference Vegetation Index (NDVI) time series. We emphasize the significance of observation continuity by assessing the impact of the generated time series on the detection of grassland mowing <b>events.</b> <b>We</b> focus on Lithuania, a country characterized by extensive cloud coverage, and compare our approach with alternative interpolation techniques (i.e., linear, Akima, quadratic). Our method surpasses these techniques, with an average MAE of 0.024 and R^2 of 0.92. It not only improves the accuracy of <b>event</b> <b>detection</b> tasks by employing a <b>continuous</b> <b>time</b> series, but also effectively filters out sudden shifts and noise originating from cloudy observations that cloud masks often fail to detect.

{{</citation>}}


### (13/91 | 13/254) Anomaly Detection by Adapting a pre-trained Vision Language Model (Yuxuan Cai et al., 2024)

{{<citation>}}

Yuxuan Cai, Xinwei He, Dingkang Liang, Ao Tong, Xiang Bai. (2024)  
**Anomaly Detection by Adapting a pre-trained Vision Language Model**
<br/>
<button class="copy-to-clipboard" title="Anomaly Detection by Adapting a pre-trained Vision Language Model" index=13>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 60  
Keywords: Anomaly Detection, Self-supervised Learning, Self-supervised Learning, Prompt, Vision-and-Language, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09493v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09493v1.pdf" filename="2403.09493v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recently, large vision and language models have shown their success when adapting them to many downstream tasks. In this paper, we present a unified framework named CLIP-ADA for <b>Anomaly</b> <b>Detection</b> by Adapting a pre-trained CLIP model. To this end, we make two important improvements: 1) To acquire unified <b>anomaly</b> <b>detection</b> across industrial images of multiple categories, we introduce the learnable <b>prompt</b> and propose to associate it with abnormal patterns through <b>self-supervised</b> <b>learning.</b> 2) To fully exploit the representation power of CLIP, we introduce an <b>anomaly</b> <b>region</b> refinement strategy to refine the localization quality. During testing, the anomalies are localized by directly calculating the similarity between the representation of the learnable <b>prompt</b> and the image. Comprehensive experiments demonstrate the superiority of our framework, e.g., we achieve the state-of-the-art 97.5/55.6 and 89.3/33.1 on MVTec-AD and VisA for <b>anomaly</b> <b>detection</b> and localization. In addition, the proposed method also achieves encouraging performance with marginal training data, which is more challenging.

{{</citation>}}


### (14/91 | 14/254) LocalMamba: Visual State Space Model with Windowed Selective Scan (Tao Huang et al., 2024)

{{<citation>}}

Tao Huang, Xiaohuan Pei, Shan You, Fei Wang, Chen Qian, Chang Xu. (2024)  
**LocalMamba: Visual State Space Model with Windowed Selective Scan**
<br/>
<button class="copy-to-clipboard" title="LocalMamba: Visual State Space Model with Windowed Selective Scan" index=14>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 60  
Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Vision Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09338v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09338v1.pdf" filename="2403.09338v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advancements in state space models, notably Mamba, have demonstrated significant progress in modeling long sequences for tasks like language understanding. Yet, their application in <b>vision</b> <b>tasks</b> has not markedly surpassed the performance of traditional <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> and <b>Vision</b> <b>Transformers</b> (ViTs). This paper posits that the key to enhancing <b>Vision</b> <b>Mamba</b> (ViM) lies in optimizing scan directions for sequence modeling. Traditional ViM approaches, which flatten spatial tokens, overlook the preservation of local 2D dependencies, thereby elongating the distance between adjacent tokens. We introduce a novel local scanning strategy that divides images into distinct windows, effectively capturing local dependencies while maintaining a global perspective. Additionally, acknowledging the varying preferences for scan patterns across different network layers, we propose a dynamic method to independently search for the optimal scan choices for each layer, substantially improving performance. Extensive experiments across both plain and hierarchical models underscore our approach's superiority in effectively capturing image representations. For example, our model significantly outperforms Vim-Ti by 3.1% on ImageNet with the same 1.5G FLOPs. Code is available at: https://github.com/hunto/LocalMamba.

{{</citation>}}


### (15/91 | 15/254) UniCode: Learning a Unified Codebook for Multimodal Large Language Models (Sipeng Zheng et al., 2024)

{{<citation>}}

Sipeng Zheng, Bohan Zhou, Yicheng Feng, Ye Wang, Zongqing Lu. (2024)  
**UniCode: Learning a Unified Codebook for Multimodal Large Language Models**
<br/>
<button class="copy-to-clipboard" title="UniCode: Learning a Unified Codebook for Multimodal Large Language Models" index=15>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CL, cs-CV, cs.CV  
Keyword Score: 59  
Keywords: Benchmarking, Multi-modal, Multi-modal, Quantization, Visual Question Answering, In-context Learning, Instruction Tuning, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09072v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09072v1.pdf" filename="2403.09072v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we propose \textbf{UniCode}, a novel approach within the domain of <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) that learns a unified codebook to efficiently tokenize visual, text, and potentially other types of signals. This innovation addresses a critical limitation in existing MLLMs: their reliance on a text-only codebook, which restricts MLLM's ability to generate images and texts in a <b>multimodal</b> context. Towards this end, we propose a language-driven iterative training paradigm, coupled with an <b>in-context</b> pre-training task we term ``image decompression'', enabling our model to interpret compressed visual data and generate high-quality images.The unified codebook empowers our model to extend visual <b>instruction</b> <b>tuning</b> to non-linguistic generation tasks. Moreover, UniCode is adaptable to diverse stacked <b>quantization</b> approaches in order to compress visual signals into a more compact token representation. Despite using significantly fewer parameters and less data during training, Unicode demonstrates promising capabilities in visual reconstruction and generation. It also achieves performances comparable to leading MLLMs across a spectrum of <b>VQA</b> <b>benchmarks.</b>

{{</citation>}}


### (16/91 | 16/254) Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring (Yufei Zhan et al., 2024)

{{<citation>}}

Yufei Zhan, Yousong Zhu, Hongyin Zhao, Fan Yang, Ming Tang, Jinqiao Wang. (2024)  
**Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring**
<br/>
<button class="copy-to-clipboard" title="Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring" index=16>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 56  
Keywords: Object Detection, Multi-modal, Multi-modal, Grounding, Large Language Model, Prompt, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09333v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09333v1.pdf" filename="2403.09333v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>Vision</b> <b>Language</b> Models have achieved fine-grained <b>object</b> <b>perception,</b> but the limitation of image resolution remains a significant obstacle to surpass the performance of task-specific experts in complex and dense scenarios. Such limitation further restricts the model's potential to achieve nuanced visual and language referring in domains such as GUI Agents, Counting and \etc. To address this issue, we introduce a unified high-resolution generalist model, Griffon v2, enabling flexible <b>object</b> <b>referring</b> with visual and textual <b>prompts.</b> To efficiently scaling up image resolution, we design a simple and lightweight down-sampling projector to overcome the input tokens constraint in <b>Large</b> <b>Language</b> <b>Models.</b> This design inherently preserves the complete contexts and fine details, and significantly improves <b>multimodal</b> perception ability especially for small <b>objects.</b> <b>Building</b> upon this, we further equip the model with visual-language co-referring capabilities through a plug-and-play visual tokenizer. It enables user-friendly interaction with flexible target images, free-form texts and even coordinates. Experiments demonstrate that Griffon v2 can localize any <b>objects</b> <b>of</b> interest with visual and textual referring, achieve state-of-the-art performance on REC, phrase <b>grounding,</b> and REG tasks, and outperform expert models in <b>object</b> <b>detection</b> and <b>object</b> <b>counting.</b> Data, codes and models will be released at https://github.com/jefferyZhan/Griffon.

{{</citation>}}


### (17/91 | 17/254) Are Vision Language Models Texture or Shape Biased and Can We Steer Them? (Paul Gavrikov et al., 2024)

{{<citation>}}

Paul Gavrikov, Jovita Lukasik, Steffen Jung, Robert Geirhos, Bianca Lamm, Muhammad Jehanzeb Mirza, Margret Keuper, Janis Keuper. (2024)  
**Are Vision Language Models Texture or Shape Biased and Can We Steer Them?**
<br/>
<button class="copy-to-clipboard" title="Are Vision Language Models Texture or Shape Biased and Can We Steer Them?" index=17>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs.CV, q-bio-NC  
Keyword Score: 56  
Keywords: Multi-modal, Multi-modal, Zero-shot, Question Answering, Visual Question Answering, Prompt, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09193v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09193v1.pdf" filename="2403.09193v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from <b>zero-shot</b> image classification, over to image captioning, and <b>visual</b> <b>question</b> <b>answering.</b> Unlike pure vision models, they offer an intuitive way to access <b>visual</b> <b>content</b> <b>through</b> language <b>prompting.</b> The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced <b>visual</b> <b>biases</b> <b>through</b> <b>multimodal</b> fusion, or whether they simply inherit biases from pure vision models. One important <b>visual</b> <b>bias</b> <b>is</b> the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that <b>visual</b> <b>biases</b> <b>are</b> modulated to some extent through text in <b>multimodal</b> models. If text does indeed influence <b>visual</b> <b>biases,</b> <b>this</b> suggests that we may be able to steer <b>visual</b> <b>biases</b> <b>not</b> just through <b>visual</b> <b>input</b> <b>but</b> also through language: a hypothesis that we confirm through extensive experiments. For instance, we are able to steer shape bias from as low as 49% to as high as 72% through <b>prompting</b> alone. For now, the strong human bias towards shape (96%) remains out of reach for all tested VLMs.

{{</citation>}}


### (18/91 | 18/254) Metadata-Driven Federated Learning of Connectional Brain Templates in Non-IID Multi-Domain Scenarios (Geng Chen et al., 2024)

{{<citation>}}

Geng Chen, Qingyue Wang, Islem Rekik. (2024)  
**Metadata-Driven Federated Learning of Connectional Brain Templates in Non-IID Multi-Domain Scenarios**
<br/>
<button class="copy-to-clipboard" title="Metadata-Driven Federated Learning of Connectional Brain Templates in Non-IID Multi-Domain Scenarios" index=18>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 53  
Keywords: Graph, Federated Learning, Supervised Learning, Unsupervised Learning, Unsupervised Learning, Stemming  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09139v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09139v1.pdf" filename="2403.09139v1.pdf">Download PDF</button>

---


**ABSTRACT**  
A connectional brain template (CBT) is a holistic representation of a population of multi-view brain connectivity <b>graphs,</b> encoding shared patterns and normalizing typical variations across individuals. The federation of CBT learning allows for an inclusive estimation of the representative center of multi-domain brain connectivity datasets in a fully data-preserving manner. However, existing methods overlook the non-independent and identically distributed (non-IDD) issue <b>stemming</b> from multidomain brain connectivity heterogeneity, in which data domains are drawn from different hospitals and imaging modalities. To overcome this limitation, we unprecedentedly propose a metadata-driven <b>federated</b> <b>learning</b> framework, called MetaFedCBT, for cross-domain CBT learning. Given the data drawn from a specific domain (i.e., hospital), our model aims to learn metadata in a fully <b>supervised</b> manner by introducing a local client-based regressor network. The generated meta-data is forced to meet the statistical attributes (e.g., mean) of other domains, while preserving their privacy. Our <b>supervised</b> meta-data generation approach boosts the <b>unsupervised</b> <b>learning</b> of a more centered, representative, and holistic CBT of a particular brain state across diverse domains. As the <b>federated</b> <b>learning</b> progresses over multiple rounds, the learned metadata and associated generated connectivities are continuously updated to better approximate the target domain information. MetaFedCBT overcomes the non-IID issue of existing methods by generating informative brain connectivities for privacy-preserving holistic CBT learning with guidance using metadata. Extensive experiments on multi-view morphological brain networks of normal and patient subjects demonstrate that our MetaFedCBT is a superior <b>federated</b> <b>CBT</b> learning model and significantly advances the state-of-the-art performance.

{{</citation>}}


### (19/91 | 19/254) SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with Spike Streams (Kang Chen et al., 2024)

{{<citation>}}

Kang Chen, Shiyan Chen, Jiyuan Zhang, Baoyue Zhang, Yajing Zheng, Tiejun Huang, Zhaofei Yu. (2024)  
**SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with Spike Streams**
<br/>
<button class="copy-to-clipboard" title="SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with Spike Streams" index=19>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 50  
Keywords: Knowledge Distillation, Knowledge Distillation, Self-supervised Learning, Supervised Learning, Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09486v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09486v1.pdf" filename="2403.09486v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Reconstructing a sequence of sharp images from the blurry input is crucial for enhancing our insights into the captured scene and poses a significant challenge due to the limited temporal features embedded in the image. Spike cameras, sampling at rates up to 40,000 Hz, have proven effective in capturing motion features and beneficial for solving this ill-posed problem. Nonetheless, existing methods fall into the <b>supervised</b> <b>learning</b> paradigm, which suffers from notable performance degradation when applied to real-world scenarios that diverge from the synthetic training data domain. Moreover, the quality of reconstructed images is capped by the generated images based on motion analysis interpolation, which inherently differs from the actual scene, affecting the generalization ability of these methods in real high-speed scenarios. To address these challenges, we propose the first <b>self-supervised</b> framework for the task of spike-guided motion deblurring. Our approach begins with the formulation of a spike-guided deblurring model that explores the theoretical relationships among spike streams, blurry images, and their corresponding sharp sequences. We subsequently develop a <b>self-supervised</b> cascaded framework to alleviate the issues of spike noise and spatial-resolution mismatching encountered in the deblurring model. With <b>knowledge</b> <b>distillation</b> and re-blurring loss, we further design a lightweight deblur network to generate high-quality sequences with brightness and texture consistency with the original input. Quantitative and qualitative experiments conducted on our real-world and synthetic datasets with spikes validate the superior generalization of the proposed framework. Our code, data and trained models will be available at \url{https://github.com/chenkang455/S-SDM}.

{{</citation>}}


### (20/91 | 20/254) Open-Vocabulary Object Detection with Meta Prompt Representation and Instance Contrastive Optimization (Zhao Wang et al., 2024)

{{<citation>}}

Zhao Wang, Aoxue Li, Fengwei Zhou, Zhenguo Li, Qi Dou. (2024)  
**Open-Vocabulary Object Detection with Meta Prompt Representation and Instance Contrastive Optimization**
<br/>
<button class="copy-to-clipboard" title="Open-Vocabulary Object Detection with Meta Prompt Representation and Instance Contrastive Optimization" index=20>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 50  
Keywords: Object Detection, Contrastive Learning, Knowledge Distillation, Knowledge Distillation, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09433v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09433v1.pdf" filename="2403.09433v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Classical <b>object</b> <b>detectors</b> are incapable of detecting novel class <b>objects</b> <b>that</b> are not encountered before. Regarding this issue, Open-Vocabulary <b>Object</b> <b>Detection</b> (OVOD) is proposed, which aims to detect the <b>objects</b> <b>in</b> the candidate class list. However, current OVOD models are suffering from overfitting on the base classes, heavily relying on the large-scale extra data, and complex training process. To overcome these issues, we propose a novel framework with Meta <b>prompt</b> and Instance <b>Contrastive</b> <b>learning</b> (MIC) schemes. Firstly, we simulate a novel-class-emerging scenario to help the <b>prompt</b> learner that learns class and background <b>prompts</b> generalize to novel classes. Secondly, we design an instance-level <b>contrastive</b> <b>strategy</b> to promote intra-class compactness and inter-class separation, which benefits generalization of the detector to novel class <b>objects.</b> <b>Without</b> using <b>knowledge</b> <b>distillation,</b> ensemble model or extra training data during detector training, our proposed MIC outperforms previous SOTA methods trained with these complex techniques on LVIS. Most importantly, MIC shows great generalization ability on novel classes, e.g., with $+4.3\%$ and $+1.9\% \ \mathrm{AP}$ improvement compared with previous SOTA on COCO and Objects365, respectively.

{{</citation>}}


### (21/91 | 21/254) XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization (Yequan Bie et al., 2024)

{{<citation>}}

Yequan Bie, Luyang Luo, Zhixuan Chen, Hao Chen. (2024)  
**XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization**
<br/>
<button class="copy-to-clipboard" title="XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization" index=21>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 50  
Keywords: Foundation Model, Large Language Model, Prompt, Prompt Learning, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09410v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09410v1.pdf" filename="2403.09410v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Utilizing potent representations of the <b>large</b> <b>vision-language</b> <b>models</b> (VLMs) to accomplish various downstream tasks has attracted increasing attention. Within this research field, soft <b>prompt</b> <b>learning</b> has become a representative approach for efficiently adapting VLMs such as CLIP, to tasks like image classification. However, most existing <b>prompt</b> <b>learning</b> methods learn text tokens that are unexplainable, which cannot satisfy the stringent interpretability requirements of Explainable Artificial Intelligence (XAI) in high-stakes scenarios like healthcare. To address this issue, we propose a novel explainable <b>prompt</b> <b>learning</b> framework that leverages medical knowledge by aligning the semantics of images, learnable <b>prompts,</b> <b>and</b> clinical concept-driven <b>prompts</b> <b>at</b> multiple granularities. Moreover, our framework addresses the lack of valuable concept annotations by eliciting knowledge from <b>large</b> <b>language</b> <b>models</b> and offers both visual and textual explanations for the <b>prompts.</b> <b>Extensive</b> experiments and explainability analyses conducted on various datasets, with and without concept labels, demonstrate that our method simultaneously achieves superior diagnostic performance, flexibility, and interpretability, shedding light on the effectiveness of <b>foundation</b> <b>models</b> in facilitating XAI. The code will be made publically available.

{{</citation>}}


### (22/91 | 22/254) Video Editing via Factorized Diffusion Distillation (Uriel Singer et al., 2024)

{{<citation>}}

Uriel Singer, Amit Zohar, Yuval Kirstain, Shelly Sheynin, Adam Polyak, Devi Parikh, Yaniv Taigman. (2024)  
**Video Editing via Factorized Diffusion Distillation**
<br/>
<button class="copy-to-clipboard" title="Video Editing via Factorized Diffusion Distillation" index=22>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 50  
Keywords: Knowledge Distillation, Knowledge Distillation, Supervised Learning, Unsupervised Learning, Text2image  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09334v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09334v1.pdf" filename="2403.09334v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We introduce Emu Video Edit (EVE), a model that establishes a new state-of-the art in video editing without relying on any <b>supervised</b> video editing data. To develop EVE we separately train an image editing adapter and a video generation adapter, and attach both to the same <b>text-to-image</b> model. Then, to align the adapters towards video editing we introduce a new <b>unsupervised</b> <b>distillation</b> procedure, Factorized Diffusion <b>Distillation.</b> This procedure <b>distills</b> knowledge from one or more teachers simultaneously, without any <b>supervised</b> data. We utilize this procedure to teach EVE to edit videos by jointly <b>distilling</b> knowledge to (i) precisely edit each individual frame from the image editing adapter, and (ii) ensure temporal consistency among the edited frames using the video generation adapter. Finally, to demonstrate the potential of our approach in unlocking other capabilities, we align additional combinations of adapters

{{</citation>}}


### (23/91 | 23/254) Explore In-Context Segmentation via Latent Diffusion Models (Chaoyang Wang et al., 2024)

{{<citation>}}

Chaoyang Wang, Xiangtai Li, Henghui Ding, Lu Qi, Jiangning Zhang, Yunhai Tong, Chen Change Loy, Shuicheng Yan. (2024)  
**Explore In-Context Segmentation via Latent Diffusion Models**
<br/>
<button class="copy-to-clipboard" title="Explore In-Context Segmentation via Latent Diffusion Models" index=23>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 43  
Keywords: Diffusion Model, Benchmarking, Foundation Model, In-context Learning, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09616v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09616v1.pdf" filename="2403.09616v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>In-context</b> segmentation has drawn more attention with the introduction of vision <b>foundation</b> <b>models.</b> Most existing approaches adopt metric learning or masked image modeling to build the correlation between visual <b>prompts</b> and input image queries. In this work, we explore this problem from a new perspective, using one representative generation model, the latent <b>diffusion</b> <b>model</b> (LDM). We observe a task gap between generation and segmentation in <b>diffusion</b> <b>models,</b> but LDM is still an effective minimalist for <b>in-context</b> segmentation. In particular, we propose two meta-architectures and correspondingly design several output alignment and optimization strategies. We have conducted comprehensive ablation studies and empirically found that the segmentation quality counts on output alignment and <b>in-context</b> instructions. Moreover, we build a new and fair <b>in-context</b> segmentation <b>benchmark</b> that includes both image and video datasets. Experiments validate the efficiency of our approach, demonstrating comparable or even stronger results than previous specialist models or visual <b>foundation</b> <b>models.</b> Our study shows that LDMs can also achieve good enough results for challenging <b>in-context</b> segmentation tasks.

{{</citation>}}


### (24/91 | 24/254) Semi- and Weakly-Supervised Learning for Mammogram Mass Segmentation with Limited Annotations (Xinyu Xiong et al., 2024)

{{<citation>}}

Xinyu Xiong, Churan Wang, Wenxue Li, Guanbin Li. (2024)  
**Semi- and Weakly-Supervised Learning for Mammogram Mass Segmentation with Limited Annotations**
<br/>
<button class="copy-to-clipboard" title="Semi- and Weakly-Supervised Learning for Mammogram Mass Segmentation with Limited Annotations" index=24>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Weakly-supervised Learning, Weakly-supervised Learning, Prompt, Weakly Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09315v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09315v1.pdf" filename="2403.09315v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Accurate identification of breast masses is crucial in diagnosing breast cancer; however, it can be challenging due to their small size and being camouflaged in surrounding normal glands. Worse still, it is also expensive in clinical practice to obtain adequate pixel-wise annotations for training deep neural networks. To overcome these two difficulties with one stone, we propose a semi- and <b>weakly-supervised</b> <b>learning</b> <b>framework</b> for mass segmentation that utilizes limited strongly-labeled samples and sufficient <b>weakly-labeled</b> <b>samples</b> <b>to</b> achieve satisfactory performance. The framework consists of an auxiliary branch to exclude lesion-irrelevant background areas, a segmentation branch for final prediction, and a spatial <b>prompting</b> module to integrate the complementary information of the two branches. We further disentangle encoded obscure features into lesion-related and others to boost performance. Experiments on CBIS-DDSM and INbreast datasets demonstrate the effectiveness of our method.

{{</citation>}}


### (25/91 | 25/254) Knowledge Distillation in YOLOX-ViT for Side-Scan Sonar Object Detection (Martin Aubard et al., 2024)

{{<citation>}}

Martin Aubard, László Antal, Ana Madureira, Erika Ábrahám. (2024)  
**Knowledge Distillation in YOLOX-ViT for Side-Scan Sonar Object Detection**
<br/>
<button class="copy-to-clipboard" title="Knowledge Distillation in YOLOX-ViT for Side-Scan Sonar Object Detection" index=25>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Object Detection, Knowledge Distillation, Knowledge Distillation, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09313v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09313v1.pdf" filename="2403.09313v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper we present YOLOX-ViT, a novel <b>object</b> <b>detection</b> model, and investigate the efficacy of <b>knowledge</b> <b>distillation</b> for model size reduction without sacrificing performance. Focused on underwater robotics, our research addresses key questions about the viability of smaller models and the impact of the visual <b>transformer</b> layer in YOLOX. Furthermore, we introduce a new side-scan sonar image dataset, and use it to evaluate our <b>object</b> <b>detector's</b> performance. Results show that <b>knowledge</b> <b>distillation</b> effectively reduces false positives in wall detection. Additionally, the introduced visual <b>transformer</b> layer significantly improves <b>object</b> <b>detection</b> accuracy in the underwater environment. The source code of the <b>knowledge</b> <b>distillation</b> in the YOLOX-ViT is at https://github.com/remaro-network/KD-YOLOX-ViT.

{{</citation>}}


### (26/91 | 26/254) Annotation Free Semantic Segmentation with Vision Foundation Models (Soroush Seifi et al., 2024)

{{<citation>}}

Soroush Seifi, Daniel Olmeda Reino, Fabien Despinoy, Rahaf Aljundi. (2024)  
**Annotation Free Semantic Segmentation with Vision Foundation Models**
<br/>
<button class="copy-to-clipboard" title="Annotation Free Semantic Segmentation with Vision Foundation Models" index=26>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Foundation Model, Self-supervised Learning, Zero-shot, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09307v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09307v1.pdf" filename="2403.09307v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Semantic Segmentation is one of the most challenging vision tasks, usually requiring large amounts of training data with expensive pixel-level annotations. With the success of <b>foundation</b> <b>models</b> and especially <b>vision-language</b> models, recent works attempt to achieve <b>zero-shot</b> semantic segmentation while requiring either large scale training or additional image/pixel-level annotations. In this work, we build a lightweight module on top of a <b>self-supervised</b> pretrained vision encoder to align patch features with a pre-trained text encoder. Importantly, we generate free annotations for any semantic segmentation dataset using existing <b>foundation</b> <b>models</b> and train our alignment module cost free. We use CLIP to detect objects and SAM to generate high quality object masks. Our approach can bring language-based semantics to any pre-trained vision encoder with minimal training. Our module is lightweight, uses <b>foundation</b> <b>models</b> as a sole source of supervision and shows impressive generalization capability from little training data with no annotation.

{{</citation>}}


### (27/91 | 27/254) Customizing Segmentation Foundation Model via Prompt Learning for Instance Segmentation (Hyung-Il Kim et al., 2024)

{{<citation>}}

Hyung-Il Kim, Kimin Yun, Jun-Seok Yun, Yuseok Bae. (2024)  
**Customizing Segmentation Foundation Model via Prompt Learning for Instance Segmentation**
<br/>
<button class="copy-to-clipboard" title="Customizing Segmentation Foundation Model via Prompt Learning for Instance Segmentation" index=27>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Foundation Model, Pre-trained Language Model, Prompt, Prompt Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09199v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09199v1.pdf" filename="2403.09199v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recently, <b>foundation</b> <b>models</b> trained on massive datasets to adapt to a wide range of domains have attracted considerable attention and are actively being explored within the computer vision community. Among these, the Segment Anything Model (SAM) stands out for its remarkable progress in generalizability and flexibility for image segmentation tasks, achieved through <b>prompt-based</b> <b>object</b> mask generation. However, despite its strength, SAM faces two key limitations when applied to customized instance segmentation that segments specific objects or those in unique environments not typically present in the training data: 1) the ambiguity inherent in input <b>prompts</b> <b>and</b> 2) the necessity for extensive additional training to achieve optimal segmentation. To address these challenges, we propose a novel method, customized instance segmentation via <b>prompt</b> <b>learning</b> tailored to SAM. Our method involves a <b>prompt</b> <b>learning</b> module <b>(PLM),</b> which adjusts input <b>prompts</b> <b>into</b> the embedding space to better align with user intentions, thereby enabling more efficient training. Furthermore, we introduce a point matching module (PMM) to enhance the feature representation for finer segmentation by ensuring detailed alignment with ground truth boundaries. Experimental results on various customized instance segmentation scenarios demonstrate the effectiveness of the proposed method.

{{</citation>}}


### (28/91 | 28/254) SAM-Lightening: A Lightweight Segment Anything Model with Dilated Flash Attention to Achieve 30 times Acceleration (Yanfei Songa et al., 2024)

{{<citation>}}

Yanfei Songa, Bangzheng Pua, Peng Wanga, Hongxu Jiang, Dong Donga, Yiqing Shen. (2024)  
**SAM-Lightening: A Lightweight Segment Anything Model with Dilated Flash Attention to Achieve 30 times Acceleration**
<br/>
<button class="copy-to-clipboard" title="SAM-Lightening: A Lightweight Segment Anything Model with Dilated Flash Attention to Achieve 30 times Acceleration" index=28>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Transfer, Zero-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09195v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09195v1.pdf" filename="2403.09195v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Segment Anything Model (SAM) has garnered significant attention in segmentation tasks due to their <b>zero-shot</b> generalization ability. However, a broader application of SAMs to real-world practice has been restricted by their low inference speed and high computational memory demands, which mainly stem from the attention mechanism. Existing work concentrated on optimizing the encoder, yet has not adequately addressed the inefficiency of the attention mechanism itself, even when <b>distilled</b> to a smaller model, which thus leaves space for further improvement. In response, we introduce SAM-Lightening, a variant of SAM, that features a re-engineered attention mechanism, termed Dilated Flash Attention. It not only facilitates higher parallelism, enhancing processing efficiency but also retains compatibility with the existing FlashAttention. Correspondingly, we propose a progressive <b>distillation</b> to enable an efficient <b>knowledge</b> <b>transfer</b> from the vanilla SAM without costly training from scratch. Experiments on COCO and LVIS reveal that SAM-Lightening significantly outperforms the state-of-the-art methods in both run-time efficiency and segmentation accuracy. Specifically, it can achieve an inference speed of 7 milliseconds (ms) per image, for images of size 1024*1024 pixels, which is 30.1 times faster than the vanilla SAM and 2.1 times than the state-of-the-art. Moreover, it takes only 244MB memory, which is 3.5\% of the vanilla SAM. The code and weights are available at https://anonymous.4open.science/r/SAM-LIGHTENING-BC25/.

{{</citation>}}


### (29/91 | 29/254) PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation (Yizhe Xiong et al., 2024)

{{<citation>}}

Yizhe Xiong, Hui Chen, Tianxiang Hao, Zijia Lin, Jungong Han, Yuesong Zhang, Guoxin Wang, Yongjun Bao, Guiguang Ding. (2024)  
**PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation**
<br/>
<button class="copy-to-clipboard" title="PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation" index=29>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Fine-tuning, Foundation Model, Model Compression, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09192v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09192v1.pdf" filename="2403.09192v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recently, the scale of <b>transformers</b> has grown rapidly, which introduces considerable challenges in terms of training overhead and inference efficiency in the scope of task adaptation. Existing works, namely Parameter-Efficient <b>Fine-Tuning</b> (PEFT) and <b>model</b> <b>compression,</b> have separately investigated the challenges. However, PEFT cannot guarantee the inference efficiency of the original backbone, especially for large-scale <b>models.</b> <b>Model</b> <b>compression</b> requires significant training costs for structure searching and re-training. Consequently, a simple combination of them cannot guarantee accomplishing both training efficiency and inference efficiency with minimal costs. In this paper, we propose a novel Parallel Yielding Re-Activation (PYRA) method for such a challenge of training-inference efficient task adaptation. PYRA first utilizes parallel yielding adaptive weights to comprehensively perceive the data distribution in downstream tasks. A re-activation strategy for token modulation is then applied for tokens to be merged, leading to calibrated token features. Extensive experiments demonstrate that PYRA outperforms all competing methods under both low compression rate and high compression rate, demonstrating its effectiveness and superiority in maintaining both training efficiency and inference efficiency for large-scale <b>foundation</b> <b>models.</b> <b>Our</b> code will be released to the public.

{{</citation>}}


### (30/91 | 30/254) CardioCaps: Attention-based Capsule Network for Class-Imbalanced Echocardiogram Classification (Hyunkyung Han et al., 2024)

{{<citation>}}

Hyunkyung Han, Jihyeon Seong, Jaesik Choi. (2024)  
**CardioCaps: Attention-based Capsule Network for Class-Imbalanced Echocardiogram Classification**
<br/>
<button class="copy-to-clipboard" title="CardioCaps: Attention-based Capsule Network for Class-Imbalanced Echocardiogram Classification" index=30>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Logistic Regression  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09108v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09108v1.pdf" filename="2403.09108v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Capsule Neural Networks (CapsNets) is a novel architecture that utilizes vector-wise representations formed by multiple neurons. Specifically, the Dynamic Routing CapsNets (DR-CapsNets) employ an affine matrix and dynamic routing mechanism to train capsules and acquire translation-equivariance properties, enhancing its robustness compared to traditional <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs).</b> Echocardiograms, which capture moving images of the heart, present unique challenges for traditional image classification methods. In this paper, we explore the potential of DR-CapsNets and propose CardioCaps, a novel attention-based DR-CapsNet architecture for class-imbalanced echocardiogram classification. CardioCaps comprises two key components: a weighted margin loss incorporating a regression auxiliary loss and an attention mechanism. First, the weighted margin loss prioritizes positive cases, supplemented by an auxiliary loss function based on the Ejection Fraction (EF) regression task, a crucial measure of cardiac function. This approach enhances the model's resilience in the face of class imbalance. Second, recognizing the quadratic complexity of dynamic routing leading to training inefficiencies, we adopt the attention mechanism as a more computationally efficient alternative. Our results demonstrate that CardioCaps surpasses traditional machine learning baseline methods, including <b>Logistic</b> <b>Regression,</b> Random Forest, and XGBoost with sampling methods and a class weight matrix. Furthermore, CardioCaps outperforms other deep learning baseline methods such as <b>CNNs,</b> ResNets, U-Nets, and ViTs, as well as advanced CapsNets methods such as EM-CapsNets and Efficient-CapsNets. Notably, our model demonstrates robustness to class imbalance, achieving high precision even in datasets with a substantial proportion of negative cases.

{{</citation>}}


### (31/91 | 31/254) OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale Outdoor Environments (Yinan Deng et al., 2024)

{{<citation>}}

Yinan Deng, Jiahui Wang, Jingyu Zhao, Xinyu Tian, Guangyan Chen, Yi Yang, Yufeng Yue. (2024)  
**OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale Outdoor Environments**
<br/>
<button class="copy-to-clipboard" title="OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale Outdoor Environments" index=31>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-RO, cs.CV  
Keyword Score: 39  
Keywords: Graph, Fine-tuning, Foundation Model, Multi-modal, Multi-modal, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09412v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09412v1.pdf" filename="2403.09412v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Environment maps endowed with sophisticated semantics are pivotal for facilitating seamless interaction between robots and humans, enabling them to effectively carry out various tasks. Open-vocabulary maps, powered by Visual-Language models (VLMs), possess inherent advantages, including <b>multimodal</b> retrieval and open-set classes. However, existing open-vocabulary maps are constrained to closed indoor scenarios and VLM features, thereby diminishing their usability and inference capabilities. Moreover, the absence of topological relationships further complicates the accurate querying of specific instances. In this work, we propose OpenGraph, a representation of open-vocabulary hierarchical <b>graph</b> structure designed for large-scale outdoor environments. OpenGraph initially extracts instances and their captions from visual images using 2D <b>foundation</b> <b>models,</b> encoding the captions with features to enhance textual <b>reasoning.</b> Subsequently, 3D incremental panoramic mapping with feature embedding is achieved by projecting images onto LiDAR point clouds. Finally, the environment is segmented based on lane <b>graph</b> connectivity to construct a hierarchical <b>graph.</b> Validation results from real public dataset SemanticKITTI demonstrate that, even without <b>fine-tuning</b> the models, OpenGraph exhibits the ability to generalize to novel semantic classes and achieve the highest segmentation and query accuracy. The source code of OpenGraph is publicly available at https://github.com/BIT-DYN/OpenGraph.

{{</citation>}}


### (32/91 | 32/254) AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions (Hao Zhang et al., 2024)

{{<citation>}}

Hao Zhang, Wenqi Shao, Hong Liu, Yongqiang Ma, Ping Luo, Yu Qiao, Kaipeng Zhang. (2024)  
**AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions**
<br/>
<button class="copy-to-clipboard" title="AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions" index=32>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 39  
Keywords: Benchmarking, Fairness, Multi-modal, Multi-modal, GPT, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09346v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09346v1.pdf" filename="2403.09346v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Large <b>Vision-Language</b> Models (LVLMs) have shown significant progress in well responding to visual-instructions from users. However, these instructions, encompassing images and text, are susceptible to both intentional and inadvertent attacks. Despite the critical importance of LVLMs' robustness against such threats, current research in this area remains limited. To bridge this gap, we introduce AVIBench, a framework designed to analyze the robustness of LVLMs when facing various adversarial visual-instructions (AVIs), including four types of image-based AVIs, ten types of text-based AVIs, and nine types of content bias AVIs (such as gender, violence, cultural, and racial biases, among others). We generate 260K AVIs encompassing five categories of <b>multimodal</b> capabilities (nine tasks) and content bias. We then conduct a comprehensive evaluation involving 14 open-source LVLMs to assess their performance. AVIBench also serves as a convenient tool for practitioners to evaluate the robustness of LVLMs against AVIs. Our findings and extensive experimental results shed light on the vulnerabilities of LVLMs, and highlight that inherent biases exist even in advanced closed-source LVLMs like GeminiProVision and <b>GPT-4V.</b> This underscores the importance of enhancing the robustness, security, and <b>fairness</b> of LVLMs. The source code and <b>benchmark</b> will be made publicly available.

{{</citation>}}


### (33/91 | 33/254) Unsupervised Modality-Transferable Video Highlight Detection with Representation Activation Sequence Learning (Tingtian Li et al., 2024)

{{<citation>}}

Tingtian Li, Zixun Sun, Xinyu Xiao. (2024)  
**Unsupervised Modality-Transferable Video Highlight Detection with Representation Activation Sequence Learning**
<br/>
<button class="copy-to-clipboard" title="Unsupervised Modality-Transferable Video Highlight Detection with Representation Activation Sequence Learning" index=33>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 36  
Keywords: Contrastive Learning, Multi-modal, Multi-modal, Supervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09401v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09401v1.pdf" filename="2403.09401v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Identifying highlight moments of raw video materials is crucial for improving the efficiency of editing videos that are pervasive on internet platforms. However, the extensive work of manually labeling footage has created obstacles to applying <b>supervised</b> methods to videos of unseen categories. The absence of an audio modality that contains valuable cues for highlight detection in many videos also makes it difficult to use <b>multimodal</b> strategies. In this paper, we propose a novel model with cross-modal perception for <b>unsupervised</b> highlight detection. The proposed model learns representations with visual-audio level semantics from image-audio pair data via a self-reconstruction task. To achieve <b>unsupervised</b> highlight detection, we investigate the latent representations of the network and propose the representation activation sequence learning (RASL) module with k-point <b>contrastive</b> <b>learning</b> to learn significant representation activations. To connect the visual modality with the audio modality, we use the symmetric <b>contrastive</b> <b>learning</b> (SCL) module to learn the paired visual and audio representations. Furthermore, an auxiliary task of masked feature vector sequence (FVS) reconstruction is simultaneously conducted during pretraining for representation enhancement. During inference, the cross-modal pretrained model can generate representations with paired visual-audio semantics given only the visual modality. The RASL module is used to output the highlight scores. The experimental results show that the proposed framework achieves superior performance compared to other state-of-the-art approaches.

{{</citation>}}


### (34/91 | 34/254) Introducing Routing Functions to Vision-Language Parameter-Efficient Fine-Tuning with Low-Rank Bottlenecks (Tingyu Qu et al., 2024)

{{<citation>}}

Tingyu Qu, Tinne Tuytelaars, Marie-Francine Moens. (2024)  
**Introducing Routing Functions to Vision-Language Parameter-Efficient Fine-Tuning with Low-Rank Bottlenecks**
<br/>
<button class="copy-to-clipboard" title="Introducing Routing Functions to Vision-Language Parameter-Efficient Fine-Tuning with Low-Rank Bottlenecks" index=34>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 36  
Keywords: Fine-tuning, Multi-modal, Multi-modal, GPT-2, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09377v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09377v1.pdf" filename="2403.09377v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Mainstream parameter-efficient <b>fine-tuning</b> (PEFT) methods, such as LoRA or Adapter, project a model's hidden states to a lower dimension, allowing pre-trained models to adapt to new data through this low-rank bottleneck. However, PEFT tasks involving multiple modalities, like <b>vision-language</b> (VL) tasks, require not only adaptation to new data but also learning the relationship between different modalities. Targeting at VL PEFT tasks, we propose a family of operations, called routing functions, to enhance VL alignment in the low-rank bottlenecks. The routing functions adopt linear operations and do not introduce new trainable parameters. In-depth analyses are conducted to study their behavior. In various VL PEFT settings, the routing functions significantly improve performance of the original PEFT methods, achieving over 20% improvement on VQAv2 ($\text{RoBERTa}_{\text{large}}$+ViT-L/16) and 30% on COCO Captioning <b>(GPT2-medium+ViT-L/16).</b> Also when <b>fine-tuning</b> a pre-trained <b>multimodal</b> model such as CLIP-BART, we observe smaller but consistent improvements across a range of VL PEFT tasks.

{{</citation>}}


### (35/91 | 35/254) GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding (Chengyao Wang et al., 2024)

{{<citation>}}

Chengyao Wang, Li Jiang, Xiaoyang Wu, Zhuotao Tian, Bohao Peng, Hengshuang Zhao, Jiaya Jia. (2024)  
**GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding**
<br/>
<button class="copy-to-clipboard" title="GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding" index=35>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 35  
Keywords: Contrastive Learning, Representation Learning, Self-supervised Learning, Transfer Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09639v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09639v1.pdf" filename="2403.09639v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Self-supervised</b> 3D <b>representation</b> <b>learning</b> aims to learn effective <b>representations</b> <b>from</b> large-scale unlabeled point clouds. Most existing approaches adopt point discrimination as the pretext task, which assigns matched points in two distinct views as positive pairs and unmatched points as negative pairs. However, this approach often results in semantically identical points having dissimilar <b>representations,</b> <b>leading</b> to a high number of false negatives and introducing a "semantic conflict" problem. To address this issue, we propose GroupContrast, a novel approach that combines segment grouping and semantic-aware <b>contrastive</b> <b>learning.</b> Segment grouping partitions points into semantically meaningful regions, which enhances semantic coherence and provides semantic guidance for the subsequent <b>contrastive</b> <b>representation</b> <b>learning.</b> Semantic-aware <b>contrastive</b> <b>learning</b> augments the semantic information extracted from segment grouping and helps to alleviate the issue of "semantic conflict". We conducted extensive experiments on multiple 3D scene understanding tasks. The results demonstrate that GroupContrast learns semantically meaningful <b>representations</b> <b>and</b> achieves promising <b>transfer</b> <b>learning</b> performance.

{{</citation>}}


### (36/91 | 36/254) OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning (Lingyi Hong et al., 2024)

{{<citation>}}

Lingyi Hong, Shilin Yan, Renrui Zhang, Wanyun Li, Xinyu Zhou, Pinxue Guo, Kaixun Jiang, Yiting Chen, Jinglun Li, Zhaoyu Chen, Wenqiang Zhang. (2024)  
**OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning**
<br/>
<button class="copy-to-clipboard" title="OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning" index=36>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 33  
Keywords: Benchmarking, Fine-tuning, Foundation Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09634v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09634v1.pdf" filename="2403.09634v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Visual object tracking aims to localize the target object of each frame based on its initial appearance in the first frame. Depending on the input modility, tracking tasks can be divided into RGB tracking and RGB+X (e.g. RGB+N, and RGB+D) tracking. Despite the different input modalities, the core aspect of tracking is the temporal matching. Based on this common ground, we present a general framework to unify various tracking tasks, termed as OneTracker. OneTracker first performs a large-scale pre-training on a RGB tracker called <b>Foundation</b> <b>Tracker.</b> This pretraining phase equips the <b>Foundation</b> <b>Tracker</b> with a stable ability to estimate the location of the target object. Then we regard other modality information as <b>prompt</b> and build <b>Prompt</b> Tracker upon <b>Foundation</b> <b>Tracker.</b> Through freezing the <b>Foundation</b> <b>Tracker</b> and only adjusting some additional trainable parameters, <b>Prompt</b> Tracker inhibits the strong localization ability from <b>Foundation</b> <b>Tracker</b> and achieves parameter-efficient <b>finetuning</b> on downstream RGB+X tracking tasks. To evaluate the effectiveness of our general framework OneTracker, which is consisted of <b>Foundation</b> <b>Tracker</b> and <b>Prompt</b> Tracker, we conduct extensive experiments on 6 popular tracking tasks across 11 <b>benchmarks</b> and our OneTracker outperforms other models and achieves state-of-the-art performance.

{{</citation>}}


### (37/91 | 37/254) Generalized Predictive Model for Autonomous Driving (Jiazhi Yang et al., 2024)

{{<citation>}}

Jiazhi Yang, Shenyuan Gao, Yihang Qiu, Li Chen, Tianyu Li, Bo Dai, Kashyap Chitta, Penghao Wu, Jia Zeng, Ping Luo, Jun Zhang, Andreas Geiger, Yu Qiao, Hongyang Li. (2024)  
**Generalized Predictive Model for Autonomous Driving**
<br/>
<button class="copy-to-clipboard" title="Generalized Predictive Model for Autonomous Driving" index=37>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Diffusion Model, Zero-shot, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09630v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09630v1.pdf" filename="2403.09630v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we introduce the first large-scale video prediction model in the autonomous driving discipline. To eliminate the restriction of high-cost data collection and empower the generalization ability of our model, we acquire massive data from the web and pair it with diverse and high-quality text descriptions. The resultant dataset accumulates over 2000 hours of driving videos, spanning areas all over the world with diverse weather conditions and traffic scenarios. Inheriting the merits from recent latent <b>diffusion</b> <b>models,</b> our model, dubbed GenAD, handles the challenging dynamics in driving scenes with novel temporal <b>reasoning</b> blocks. We showcase that it can generalize to various unseen driving datasets in a <b>zero-shot</b> manner, surpassing general or driving-specific video prediction counterparts. Furthermore, GenAD can be adapted into an action-conditioned prediction model or a motion planner, holding great potential for real-world driving applications.

{{</citation>}}


### (38/91 | 38/254) Video Mamba Suite: State Space Model as a Versatile Alternative for Video Understanding (Guo Chen et al., 2024)

{{<citation>}}

Guo Chen, Yifei Huang, Jilan Xu, Baoqi Pei, Zhe Chen, Zhiqi Li, Jiahao Wang, Kunchang Li, Tong Lu, Limin Wang. (2024)  
**Video Mamba Suite: State Space Model as a Versatile Alternative for Video Understanding**
<br/>
<button class="copy-to-clipboard" title="Video Mamba Suite: State Space Model as a Versatile Alternative for Video Understanding" index=38>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Convolutional Neural Network, Recurrent Neural Network, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09626v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09626v1.pdf" filename="2403.09626v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Understanding videos is one of the fundamental directions in computer vision research, with extensive efforts dedicated to exploring various architectures such as <b>RNN,</b> 3D <b>CNN,</b> and <b>Transformers.</b> The newly proposed architecture of state space model, e.g., Mamba, shows promising traits to extend its success in long sequence modeling to video modeling. To assess whether Mamba can be a viable alternative to <b>Transformers</b> in the video understanding domain, in this work, we conduct a comprehensive set of studies, probing different roles Mamba can play in modeling videos, while investigating diverse tasks where Mamba could exhibit superiority. We categorize Mamba into four roles for modeling videos, deriving a Video Mamba Suite composed of 14 models/modules, and evaluating them on 12 video understanding tasks. Our extensive experiments reveal the strong potential of Mamba on both video-only and video-language tasks while showing promising efficiency-performance trade-offs. We hope this work could provide valuable data points and insights for future research on video understanding. Code is public: https://github.com/OpenGVLab/video-mamba-suite.

{{</citation>}}


### (39/91 | 39/254) Counterfactual contrastive learning: robust representations via causal image synthesis (Melanie Roschewitz et al., 2024)

{{<citation>}}

Melanie Roschewitz, Fabio De Sousa Ribeiro, Tian Xia, Galvin Khara, Ben Glocker. (2024)  
**Counterfactual contrastive learning: robust representations via causal image synthesis**
<br/>
<button class="copy-to-clipboard" title="Counterfactual contrastive learning: robust representations via causal image synthesis" index=39>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Contrastive Learning, Counter-factual, Out-of-distribution  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09605v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09605v1.pdf" filename="2403.09605v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Contrastive</b> <b>pretraining</b> is well-known to improve downstream task performance and model generalisation, especially in limited label settings. However, it is sensitive to the choice of augmentation pipeline. Positive pairs should preserve semantic information while destroying domain-specific information. Standard augmentation pipelines emulate domain-specific changes with pre-defined photometric transformations, but what if we could simulate realistic domain changes instead? In this work, we show how to utilise recent progress in <b>counterfactual</b> image generation to this effect. We propose CF-SimCLR, a <b>counterfactual</b> <b>contrastive</b> <b>learning</b> approach which leverages approximate <b>counterfactual</b> inference for positive pair creation. Comprehensive evaluation across five datasets, on chest radiography and mammography, demonstrates that CF-SimCLR substantially improves robustness to acquisition shift with higher downstream performance on both in- and <b>out-of-distribution</b> data, particularly for domains which are under-represented during training.

{{</citation>}}


### (40/91 | 40/254) Faceptor: A Generalist Model for Face Perception (Lixiong Qin et al., 2024)

{{<citation>}}

Lixiong Qin, Mei Wang, Xuannan Liu, Yuhang Zhang, Wei Deng, Xiaoshuai Song, Weiran Xu, Weihong Deng. (2024)  
**Faceptor: A Generalist Model for Face Perception**
<br/>
<button class="copy-to-clipboard" title="Faceptor: A Generalist Model for Face Perception" index=40>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Face Recognition, Supervised Learning, Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09500v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09500v1.pdf" filename="2403.09500v1.pdf">Download PDF</button>

---


**ABSTRACT**  
With the comprehensive research conducted on various <b>face</b> <b>analysis</b> tasks, there is a growing interest among researchers to develop a unified approach to <b>face</b> <b>perception.</b> Existing methods mainly discuss unified representation and training, which lack task extensibility and application efficiency. To tackle this issue, we focus on the unified model structure, exploring a <b>face</b> <b>generalist</b> model. As an intuitive design, Naive Faceptor enables tasks with the same output shape and granularity to share the structural design of the standardized output head, achieving improved task extensibility. Furthermore, Faceptor is proposed to adopt a well-designed single-encoder dual-decoder architecture, allowing task-specific queries to represent new-coming semantics. This design enhances the unification of model structure while improving application efficiency in terms of storage overhead. Additionally, we introduce Layer-Attention into Faceptor, enabling the model to adaptively select features from optimal layers to perform the desired tasks. Through joint training on 13 <b>face</b> <b>perception</b> datasets, Faceptor achieves exceptional performance in facial landmark localization, <b>face</b> <b>parsing,</b> age estimation, expression recognition, binary attribute classification, and <b>face</b> <b>recognition,</b> achieving or surpassing specialized methods in most tasks. Our training framework can also be applied to auxiliary <b>supervised</b> <b>learning,</b> significantly improving performance in data-sparse tasks such as age estimation and expression recognition. The code and models will be made publicly available at https://github.com/lxq1000/Faceptor.

{{</citation>}}


### (41/91 | 41/254) ConDiSR: Contrastive Disentanglement and Style Regularization for Single Domain Generalization (Aleksandr Matsun et al., 2024)

{{<citation>}}

Aleksandr Matsun, Numan Saeed, Fadillah Adamsyah Maani, Mohammad Yaqub. (2024)  
**ConDiSR: Contrastive Disentanglement and Style Regularization for Single Domain Generalization**
<br/>
<button class="copy-to-clipboard" title="ConDiSR: Contrastive Disentanglement and Style Regularization for Single Domain Generalization" index=41>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Distribution Shift, Distribution Shift, Supervised Learning, Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09400v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09400v1.pdf" filename="2403.09400v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Medical data often exhibits <b>distribution</b> <b>shifts,</b> which cause test-time performance degradation for deep learning models trained using standard <b>supervised</b> <b>learning</b> pipelines. This challenge is addressed in the field of Domain Generalization (DG) with the sub-field of Single Domain Generalization (SDG) being specifically interesting due to the privacy- or logistics-related issues often associated with medical data. Existing disentanglement-based SDG methods heavily rely on structural information embedded in segmentation masks, however classification labels do not provide such dense information. This work introduces a novel SDG method aimed at medical image classification that leverages channel-wise contrastive disentanglement. It is further enhanced with reconstruction-based style regularization to ensure extraction of distinct style and structure feature representations. We evaluate our method on the complex task of multicenter histopathology image classification, comparing it against state-of-the-art (SOTA) SDG baselines. Results demonstrate that our method surpasses the SOTA by a margin of 1% in average accuracy while also showing more stable performance. This study highlights the importance and challenges of exploring SDG frameworks in the context of the classification task. The code is publicly available at https://github.com/BioMedIA-MBZUAI/ConDiSR

{{</citation>}}


### (42/91 | 42/254) StreamMultiDiffusion: Real-Time Interactive Generation with Region-Based Semantic Control (Jaerin Lee et al., 2024)

{{<citation>}}

Jaerin Lee, Daniel Sungho Jung, Kanggeon Lee, Kyoung Mu Lee. (2024)  
**StreamMultiDiffusion: Real-Time Interactive Generation with Region-Based Semantic Control**
<br/>
<button class="copy-to-clipboard" title="StreamMultiDiffusion: Real-Time Interactive Generation with Region-Based Semantic Control" index=42>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Diffusion Model, Text2image, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09055v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09055v1.pdf" filename="2403.09055v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The enormous success of <b>diffusion</b> <b>models</b> in <b>text-to-image</b> synthesis has made them promising candidates for the next generation of end-user applications for image generation and editing. Previous works have focused on improving the usability of <b>diffusion</b> <b>models</b> by reducing the inference time or increasing user interactivity by allowing new, fine-grained controls such as region-based text <b>prompts.</b> However, we empirically find that integrating both branches of works is nontrivial, limiting the potential of <b>diffusion</b> <b>models.</b> To solve this incompatibility, we present StreamMultiDiffusion, the first real-time region-based <b>text-to-image</b> generation framework. By stabilizing fast inference techniques and restructuring the model into a newly proposed multi-prompt stream batch architecture, we achieve $\times 10$ faster panorama generation than existing solutions, and the generation speed of 1.57 FPS in region-based <b>text-to-image</b> synthesis on a single RTX 2080 Ti GPU. Our solution opens up a new paradigm for interactive image generation named semantic palette, where high-quality images are generated in real-time from given multiple hand-drawn regions, encoding prescribed semantic meanings (e.g., eagle, girl). Our code and demo application are available at https://github.com/ironjr/StreamMultiDiffusion.

{{</citation>}}


### (43/91 | 43/254) SELECTOR: Heterogeneous graph network with convolutional masked autoencoder for multimodal robust prediction of cancer survival (Liangrui Pan et al., 2024)

{{<citation>}}

Liangrui Pan, Yijun Peng, Yan Li, Xiang Wang, Wenjuan Liu, Liwen Xu, Qingchun Liang, Shaoliang Peng. (2024)  
**SELECTOR: Heterogeneous graph network with convolutional masked autoencoder for multimodal robust prediction of cancer survival**
<br/>
<button class="copy-to-clipboard" title="SELECTOR: Heterogeneous graph network with convolutional masked autoencoder for multimodal robust prediction of cancer survival" index=43>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs.CV  
Keyword Score: 29  
Keywords: Graph, Autoencoder, Convolution, Multi-modal, Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09290v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09290v1.pdf" filename="2403.09290v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Accurately predicting the survival rate of cancer patients is crucial for aiding clinicians in planning appropriate treatment, reducing cancer-related medical expenses, and significantly enhancing patients' quality of life. <b>Multimodal</b> prediction of cancer patient survival offers a more comprehensive and precise approach. However, existing methods still grapple with challenges related to missing <b>multimodal</b> data and information interaction within modalities. This paper introduces SELECTOR, a heterogeneous <b>graph-aware</b> network based on <b>convolutional</b> mask encoders for robust <b>multimodal</b> prediction of cancer patient survival. SELECTOR comprises feature edge reconstruction, <b>convolutional</b> mask encoder, feature cross-fusion, and <b>multimodal</b> survival prediction modules. Initially, we construct a <b>multimodal</b> heterogeneous <b>graph</b> and employ the meta-path method for feature edge reconstruction, ensuring comprehensive incorporation of feature information from <b>graph</b> edges and effective embedding of nodes. To mitigate the impact of missing features within the modality on prediction accuracy, we devised a <b>convolutional</b> masked <b>autoencoder</b> (CMAE) to process the heterogeneous <b>graph</b> post-feature reconstruction. Subsequently, the feature cross-fusion module facilitates communication between modalities, ensuring that output features encompass all features of the modality and relevant information from other modalities. Extensive experiments and analysis on six cancer datasets from TCGA demonstrate that our method significantly outperforms state-of-the-art methods in both modality-missing and intra-modality information-confirmed cases. Our codes are made available at https://github.com/panliangrui/Selector.

{{</citation>}}


### (44/91 | 44/254) Anatomical Structure-Guided Medical Vision-Language Pre-training (Qingqiu Li et al., 2024)

{{<citation>}}

Qingqiu Li, Xiaohan Yan, Jilan Xu, Runtian Yuan, Yuejie Zhang, Rui Feng, Quanli Shen, Xiaobo Zhang, Shujun Wang. (2024)  
**Anatomical Structure-Guided Medical Vision-Language Pre-training**
<br/>
<button class="copy-to-clipboard" title="Anatomical Structure-Guided Medical Vision-Language Pre-training" index=44>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CL, cs-CV, cs.CV  
Keyword Score: 28  
Keywords: Benchmarking, Contrastive Learning, Representation Learning, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09294v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09294v1.pdf" filename="2403.09294v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Learning medical visual <b>representations</b> <b>through</b> <b>vision-language</b> pre-training has reached remarkable progress. Despite the promising performance, it still faces challenges, i.e., local alignment lacks interpretability and clinical relevance, and the insufficient internal and external <b>representation</b> <b>learning</b> of image-report pairs. To address these issues, we propose an Anatomical Structure-Guided (ASG) framework. Specifically, we parse raw reports into triplets <anatomical region, finding, existence>, and fully utilize each element as supervision to enhance <b>representation</b> <b>learning.</b> For anatomical region, we design an automatic anatomical region-sentence alignment paradigm in collaboration with radiologists, considering them as the minimum semantic units to explore fine-grained local alignment. For finding and existence, we regard them as image tags, applying an image-tag recognition decoder to associate image features with their respective tags within each sample and constructing soft labels for <b>contrastive</b> <b>learning</b> to improve the semantic association of different image-report pairs. We evaluate the proposed ASG framework on two downstream tasks, including five public <b>benchmarks.</b> Experimental results demonstrate that our method outperforms the state-of-the-art methods.

{{</citation>}}


### (45/91 | 45/254) SHAN: Object-Level Privacy Detection via Inference on Scene Heterogeneous Graph (Zhuohang Jiang et al., 2024)

{{<citation>}}

Zhuohang Jiang, Bingkui Tong, Xia Du, Ahmed Alhammadi, Jizhe Zhou. (2024)  
**SHAN: Object-Level Privacy Detection via Inference on Scene Heterogeneous Graph**
<br/>
<button class="copy-to-clipboard" title="SHAN: Object-Level Privacy Detection via Inference on Scene Heterogeneous Graph" index=45>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 26  
Keywords: Object Detection, Graph, Benchmarking, Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09172v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09172v1.pdf" filename="2403.09172v1.pdf">Download PDF</button>

---


**ABSTRACT**  
With the rise of social platforms, protecting privacy has become an important issue. Privacy <b>object</b> <b>detection</b> aims to accurately locate private <b>objects</b> <b>in</b> images. It is the foundation of safeguarding individuals' privacy rights and ensuring responsible data handling practices in the digital age. Since privacy of <b>object</b> <b>is</b> not shift-invariant, the essence of the privacy <b>object</b> <b>detection</b> task is inferring <b>object</b> <b>privacy</b> based on scene information. However, privacy <b>object</b> <b>detection</b> has long been studied as a subproblem of common <b>object</b> <b>detection</b> tasks. Therefore, existing methods suffer from serious deficiencies in accuracy, generalization, and interpretability. Moreover, creating large-scale privacy datasets is difficult due to legal constraints and existing privacy datasets lack label granularity. The granularity of existing privacy detection methods remains limited to the image level. To address the above two issues, we introduce two <b>benchmark</b> datasets for <b>object-level</b> <b>privacy</b> detection and propose SHAN, Scene Heterogeneous <b>graph</b> Attention Network, a model constructs a scene heterogeneous <b>graph</b> from an image and utilizes <b>self-attention</b> mechanisms for scene inference to obtain <b>object</b> <b>privacy.</b> Through experiments, we demonstrated that SHAN performs excellently in privacy <b>object</b> <b>detection</b> tasks, with all metrics surpassing those of the baseline model.

{{</citation>}}


### (46/91 | 46/254) Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians (Licheng Zhong et al., 2024)

{{<citation>}}

Licheng Zhong, Hong-Xing Yu, Jiajun Wu, Yunzhu Li. (2024)  
**Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians**
<br/>
<button class="copy-to-clipboard" title="Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians" index=46>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 25  
Keywords: Geometry, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09434v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09434v1.pdf" filename="2403.09434v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Reconstructing and simulating elastic objects from visual observations is crucial for applications in computer vision and robotics. Existing methods, such as 3D Gaussians, provide modeling for 3D appearance and <b>geometry</b> but lack the ability to simulate physical properties or optimize parameters for heterogeneous objects. We propose Spring-Gaus, a novel framework that integrates 3D Gaussians with physics-based <b>simulation</b> for reconstructing and simulating elastic objects from multi-view videos. Our method utilizes a 3D Spring-Mass model, enabling the optimization of physical parameters at the individual point level while decoupling the learning of physics and appearance. This approach achieves great sample efficiency, enhances generalization, and reduces sensitivity to the distribution of <b>simulation</b> particles. We evaluate Spring-Gaus on both synthetic and real-world datasets, demonstrating accurate reconstruction and <b>simulation</b> of elastic objects. This includes future prediction and <b>simulation</b> under varying initial states and environmental parameters. Project page: https://zlicheng.com/spring_gaus.

{{</citation>}}


### (47/91 | 47/254) Sentinel-Guided Zero-Shot Learning: A Collaborative Paradigm without Real Data Exposure (Fan Wan et al., 2024)

{{<citation>}}

Fan Wan, Xingyu Miao, Haoran Duan, Jingjing Deng, Rui Gao, Yang Long. (2024)  
**Sentinel-Guided Zero-Shot Learning: A Collaborative Paradigm without Real Data Exposure**
<br/>
<button class="copy-to-clipboard" title="Sentinel-Guided Zero-Shot Learning: A Collaborative Paradigm without Real Data Exposure" index=47>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 25  
Keywords: Black Box, Zero-shot, Zero-shot Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09363v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09363v1.pdf" filename="2403.09363v1.pdf">Download PDF</button>

---


**ABSTRACT**  
With increasing concerns over data privacy and model copyrights, especially in the context of collaborations between AI service providers and data owners, an innovative SG-ZSL paradigm is proposed in this work. SG-ZSL is designed to foster efficient collaboration without the need to exchange models or sensitive data. It consists of a teacher model, a student model and a generator that links both model entities. The teacher model serves as a sentinel on behalf of the data owner, replacing real data, to guide the student model at the AI service provider's end during training. Considering the disparity of knowledge space between the teacher and student, we introduce two variants of the teacher model: the omniscient and the quasi-omniscient teachers. Under these teachers' guidance, the student model seeks to match the teacher model's performance and explores domains that the teacher has not covered. To trade off between privacy and performance, we further introduce two distinct security-level training protocols: white-box and <b>black-box,</b> <b>enhancing</b> the paradigm's adaptability. Despite the inherent challenges of real data absence in the SG-ZSL paradigm, it consistently outperforms in ZSL and GZSL tasks, notably in the white-box protocol. Our comprehensive evaluation further attests to its robustness and efficiency across various setups, including stringent <b>black-box</b> <b>training</b> protocol.

{{</citation>}}


### (48/91 | 48/254) Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering (Zeyu Liu et al., 2024)

{{<citation>}}

Zeyu Liu, Weicong Liang, Zhanhao Liang, Chong Luo, Ji Li, Gao Huang, Yuhui Yuan. (2024)  
**Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering**
<br/>
<button class="copy-to-clipboard" title="Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering" index=48>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Benchmarking, Fine-tuning, Text2image  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09622v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09622v1.pdf" filename="2403.09622v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Visual text rendering poses a fundamental challenge for contemporary <b>text-to-image</b> generation models, with the core problem lying in text encoder deficiencies. To achieve accurate text rendering, we identify two crucial requirements for text encoders: character awareness and alignment with glyphs. Our solution involves crafting a series of customized text encoder, Glyph-ByT5, by <b>fine-tuning</b> the character-aware ByT5 encoder using a meticulously curated paired glyph-text dataset. We present an effective method for integrating Glyph-ByT5 with SDXL, resulting in the creation of the Glyph-SDXL model for design image generation. This significantly enhances text rendering accuracy, improving it from less than $20\%$ to nearly $90\%$ on our design image <b>benchmark.</b> Noteworthy is Glyph-SDXL's newfound ability for text paragraph rendering, achieving high spelling accuracy for tens to hundreds of characters with automated multi-line layouts. Finally, through <b>fine-tuning</b> Glyph-SDXL with a small set of high-quality, photorealistic images featuring visual text, we showcase a substantial improvement in scene text rendering capabilities in open-domain real images. These compelling outcomes aim to encourage further exploration in designing customized text encoders for diverse and challenging tasks.

{{</citation>}}


### (49/91 | 49/254) Renovating Names in Open-Vocabulary Segmentation Benchmarks (Haiwen Huang et al., 2024)

{{<citation>}}

Haiwen Huang, Songyou Peng, Dan Zhang, Andreas Geiger. (2024)  
**Renovating Names in Open-Vocabulary Segmentation Benchmarks**
<br/>
<button class="copy-to-clipboard" title="Renovating Names in Open-Vocabulary Segmentation Benchmarks" index=49>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Benchmarking, Prompt, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09593v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09593v1.pdf" filename="2403.09593v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Names are essential to both human cognition and <b>vision-language</b> models. Open-vocabulary models utilize class names as text <b>prompts</b> to generalize to categories unseen during training. However, name qualities are often overlooked and lack sufficient precision in existing datasets. In this paper, we address this underexplored problem by presenting a framework for "renovating" names in open-vocabulary segmentation <b>benchmarks</b> (RENOVATE). Through human study, we demonstrate that the names generated by our model are more precise descriptions of the visual segments and hence enhance the quality of existing datasets by means of simple renaming. We further demonstrate that using our renovated names enables training of stronger open-vocabulary segmentation models. Using open-vocabulary segmentation for name quality evaluation, we show that our renovated names lead to up to 16% relative improvement from the original names on various <b>benchmarks</b> across various state-of-the-art models. We provide our code and relabelings for several popular segmentation datasets (ADE20K, Cityscapes, PASCAL Context) to the research community.

{{</citation>}}


### (50/91 | 50/254) WeakSurg: Weakly supervised surgical instrument segmentation using temporal equivariance and semantic continuity (Qiyuan Wang et al., 2024)

{{<citation>}}

Qiyuan Wang, Yanzhe Liu, Shang Zhao, Rong Liu, S. Kevin Zhou. (2024)  
**WeakSurg: Weakly supervised surgical instrument segmentation using temporal equivariance and semantic continuity**
<br/>
<button class="copy-to-clipboard" title="WeakSurg: Weakly supervised surgical instrument segmentation using temporal equivariance and semantic continuity" index=50>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Benchmarking, Supervised Learning, Weakly-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09551v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09551v1.pdf" filename="2403.09551v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Weakly <b>supervised</b> surgical instrument segmentation with only instrument presence labels has been rarely explored in surgical domain. To mitigate the highly under-constrained challenges, we extend a two-stage weakly <b>supervised</b> segmentation paradigm with temporal attributes from two perspectives. From a temporal equivariance perspective, we propose a prototype-based temporal equivariance regulation loss to enhance pixel-wise consistency between adjacent features. From a semantic continuity perspective, we propose a class-aware temporal semantic continuity loss to constrain the semantic consistency between a global view of target frame and local non-discriminative regions of adjacent reference frame. To the best of our knowledge, WeakSurg is the first instrument-presence-only weakly <b>supervised</b> segmentation architecture to take temporal information into account for surgical scenarios. Extensive experiments are validated on Cholec80, an open <b>benchmark</b> for phase and instrument recognition. We annotate instance-wise instrument labels with fixed time-steps which are double checked by a clinician with 3-years experience. Our results show that WeakSurg compares favorably with state-of-the-art methods not only on semantic segmentation metrics but also on instance segmentation metrics.

{{</citation>}}


### (51/91 | 51/254) Eta Inversion: Designing an Optimal Eta Function for Diffusion-based Real Image Editing (Wonjun Kang et al., 2024)

{{<citation>}}

Wonjun Kang, Kevin Galim, Hyung Il Koo. (2024)  
**Eta Inversion: Designing an Optimal Eta Function for Diffusion-based Real Image Editing**
<br/>
<button class="copy-to-clipboard" title="Eta Inversion: Designing an Optimal Eta Function for Diffusion-based Real Image Editing" index=51>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Diffusion Model, Benchmarking, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09468v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09468v1.pdf" filename="2403.09468v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Diffusion</b> <b>models</b> have achieved remarkable success in the domain of text-guided image generation and, more recently, in text-guided image editing. A commonly adopted strategy for editing real images involves inverting the <b>diffusion</b> <b>process</b> to obtain a noisy representation of the original image, which is then denoised to achieve the desired edits. However, current methods for <b>diffusion</b> <b>inversion</b> often struggle to produce edits that are both faithful to the specified text <b>prompt</b> and closely resemble the source image. To overcome these limitations, we introduce a novel and adaptable <b>diffusion</b> <b>inversion</b> technique for real image editing, which is grounded in a theoretical analysis of the role of $\eta$ in the DDIM sampling equation for enhanced editability. By designing a universal <b>diffusion</b> <b>inversion</b> method with a time- and region-dependent $\eta$ function, we enable flexible control over the editing extent. Through a comprehensive series of quantitative and qualitative assessments, involving a comparison with a broad array of recent methods, we demonstrate the superiority of our approach. Our method not only sets a new <b>benchmark</b> in the field but also significantly outperforms existing strategies. Our code is available at https://github.com/furiosa-ai/eta-inversion

{{</citation>}}


### (52/91 | 52/254) Distribution and Depth-Aware Transformers for 3D Human Mesh Recovery (Jerrin Bright et al., 2024)

{{<citation>}}

Jerrin Bright, Bavesh Balaji, Harish Prakash, Yuhao Chen, David A Clausi, John Zelek. (2024)  
**Distribution and Depth-Aware Transformers for 3D Human Mesh Recovery**
<br/>
<button class="copy-to-clipboard" title="Distribution and Depth-Aware Transformers for 3D Human Mesh Recovery" index=52>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Multi-modal, Out-of-distribution, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09063v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09063v1.pdf" filename="2403.09063v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Precise Human Mesh Recovery (HMR) with in-the-wild data is a formidable challenge and is often hindered by depth ambiguities and reduced precision. Existing works resort to either pose priors or <b>multi-modal</b> data such as multi-view or point cloud information, though their methods often overlook the valuable scene-depth information inherently present in a single image. Moreover, achieving robust HMR for <b>out-of-distribution</b> (OOD) data is exceedingly challenging due to inherent variations in pose, shape and depth. Consequently, understanding the underlying distribution becomes a vital subproblem in modeling human forms. Motivated by the need for unambiguous and robust human modeling, we introduce Distribution and depth-aware human mesh recovery (D2A-HMR), an end-to-end <b>transformer</b> architecture meticulously designed to minimize the disparity between distributions and incorporate scene-depth leveraging prior depth information. Our approach demonstrates superior performance in handling OOD data in certain scenarios while consistently achieving competitive results against state-of-the-art HMR methods on controlled datasets.

{{</citation>}}


### (53/91 | 53/254) The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models? (Qinyu Zhao et al., 2024)

{{<citation>}}

Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, Stephen Gould. (2024)  
**The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?**
<br/>
<button class="copy-to-clipboard" title="The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?" index=53>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CL, cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Fine-tuning, Multi-modal, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09037v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09037v1.pdf" filename="2403.09037v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Large <b>vision-language</b> models (LVLMs), designed to interpret and respond to human instructions, occasionally generate hallucinated or harmful content due to inappropriate instructions. This study uses linear probing to shed light on the hidden knowledge at the output layer of LVLMs. We demonstrate that the logit distributions of the first tokens contain sufficient information to determine whether to respond to the instructions, including recognizing unanswerable visual questions, defending against <b>multi-modal</b> jailbreaking attack, and identifying deceptive questions. Such hidden knowledge is gradually lost in logits of subsequent tokens during response generation. Then, we illustrate a simple decoding strategy at the generation of the first token, effectively improving the generated content. In experiments, we find a few interesting insights: First, the CLIP model already contains a strong signal for solving these tasks, indicating potential bias in the existing datasets. Second, we observe performance improvement by utilizing the first logit distributions on three additional tasks, including indicting uncertainty in math solving, mitigating hallucination, and image classification. Last, with the same training data, simply <b>finetuning</b> LVLMs improve models' performance but is still inferior to linear probing on these tasks.

{{</citation>}}


### (54/91 | 54/254) Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation (Fangfu Liu et al., 2024)

{{<citation>}}

Fangfu Liu, Hanyang Wang, Weiliang Chen, Haowen Sun, Yueqi Duan. (2024)  
**Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation**
<br/>
<button class="copy-to-clipboard" title="Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation" index=54>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 20  
Keywords: Diffusion Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09625v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09625v1.pdf" filename="2403.09625v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent years have witnessed the strong power of 3D generation models, which offer a new level of creative flexibility by allowing users to guide the 3D content generation process through a single image or natural language. However, it remains challenging for existing 3D generation methods to create subject-driven 3D content across diverse <b>prompts.</b> In this paper, we introduce a novel 3D customization method, dubbed Make-Your-3D that can personalize high-fidelity and consistent 3D content from only a single image of a subject with text description within 5 minutes. Our key insight is to harmonize the distributions of a multi-view <b>diffusion</b> <b>model</b> and an identity-specific 2D generative model, aligning them with the distribution of the desired 3D subject. Specifically, we design a co-evolution framework to reduce the variance of distributions, where each model undergoes a process of learning from the other through identity-aware optimization and subject-prior optimization, respectively. Extensive experiments demonstrate that our method can produce high-quality, consistent, and subject-specific 3D content with text-driven modifications that are unseen in subject image.

{{</citation>}}


### (55/91 | 55/254) PosSAM: Panoptic Open-vocabulary Segment Anything (Vibashan VS et al., 2024)

{{<citation>}}

Vibashan VS, Shubhankar Borse, Hyojin Park, Debasmit Das, Vishal Patel, Munawar Hayat, Fatih Porikli. (2024)  
**PosSAM: Panoptic Open-vocabulary Segment Anything**
<br/>
<button class="copy-to-clipboard" title="PosSAM: Panoptic Open-vocabulary Segment Anything" index=55>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Prompt, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09620v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09620v1.pdf" filename="2403.09620v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we introduce an open-vocabulary panoptic segmentation model that effectively unifies the strengths of the Segment Anything Model (SAM) with the <b>vision-language</b> CLIP model in an end-to-end framework. While SAM excels in generating spatially-aware masks, it's decoder falls short in recognizing object class information and tends to oversegment without additional guidance. Existing approaches address this limitation by using multi-stage techniques and employing separate models to generate class-aware <b>prompts,</b> such as bounding boxes or segmentation masks. Our proposed method, PosSAM is an end-to-end model which leverages SAM's spatially rich features to produce instance-aware masks and harnesses CLIP's semantically discriminative features for effective instance classification. Specifically, we address the limitations of SAM and propose a novel Local Discriminative Pooling (LDP) module leveraging class-agnostic SAM and class-aware CLIP features for unbiased open-vocabulary classification. Furthermore, we introduce a Mask-Aware Selective Ensembling (MASE) algorithm that adaptively enhances the quality of generated masks and boosts the performance of open-vocabulary classification during inference for each image. We conducted extensive experiments to demonstrate our methods strong generalization properties across multiple datasets, achieving state-of-the-art performance with substantial improvements over SOTA open-vocabulary panoptic segmentation methods. In both COCO to ADE20K and ADE20K to COCO settings, PosSAM outperforms the previous state-of-the-art methods by a large margin, 2.4 PQ and 4.6 PQ, respectively. Project Website: https://vibashan.github.io/possam-web/.

{{</citation>}}


### (56/91 | 56/254) Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition (Yitian Zhang et al., 2024)

{{<citation>}}

Yitian Zhang, Yue Bai, Huan Wang, Yizhou Wang, Yun Fu. (2024)  
**Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition**
<br/>
<button class="copy-to-clipboard" title="Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition" index=56>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs.CV  
Keyword Score: 20  
Keywords: Data Augmentation, Distribution Shift, Distribution Shift  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09506v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09506v1.pdf" filename="2403.09506v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Current training pipelines in object recognition neglect Hue Jittering when doing <b>data</b> <b>augmentation</b> as it not only brings appearance changes that are detrimental to classification, but also the implementation is inefficient in practice. In this study, we investigate the effect of hue variance in the context of video recognition and find this variance to be beneficial since static appearances are less important in videos that contain motion information. Based on this observation, we propose a <b>data</b> <b>augmentation</b> method for video recognition, named Motion Coherent Augmentation (MCA), that introduces appearance variation in videos and implicitly encourages the model to prioritize motion patterns, rather than static appearances. Concretely, we propose an operation SwapMix to efficiently modify the appearance of video samples, and introduce Variation Alignment (VA) to resolve the <b>distribution</b> <b>shift</b> caused by SwapMix, enforcing the model to learn appearance invariant representations. Comprehensive empirical evaluation across various architectures and different datasets solidly validates the effectiveness and generalization ability of MCA, and the application of VA in other augmentation methods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.

{{</citation>}}


### (57/91 | 57/254) Mitigating attribute amplification in counterfactual image generation (Tian Xia et al., 2024)

{{<citation>}}

Tian Xia, Mélanie Roschewitz, Fabio De Sousa Ribeiro, Charles Jones, Ben Glocker. (2024)  
**Mitigating attribute amplification in counterfactual image generation**
<br/>
<button class="copy-to-clipboard" title="Mitigating attribute amplification in counterfactual image generation" index=57>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Counter-factual, Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09422v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09422v1.pdf" filename="2403.09422v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Causal generative modelling is gaining interest in medical imaging due to its ability to answer interventional and <b>counterfactual</b> queries. Most work focuses on generating <b>counterfactual</b> images that look plausible, using auxiliary classifiers to enforce effectiveness of simulated interventions. We investigate pitfalls in this approach, discovering the issue of attribute amplification, where unrelated attributes are spuriously affected during interventions, leading to biases across protected characteristics and disease status. We show that attribute amplification is caused by the use of hard labels in the <b>counterfactual</b> training process and propose soft <b>counterfactual</b> <b>fine-tuning</b> to mitigate this issue. Our method substantially reduces the amplification effect while maintaining effectiveness of generated images, demonstrated on a large chest X-ray dataset. Our work makes an important advancement towards more faithful and unbiased causal modelling in medical imaging.

{{</citation>}}


### (58/91 | 58/254) D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap for Domain-Adaptive Object Detection (Dinh Phat Do et al., 2024)

{{<citation>}}

Dinh Phat Do, Taehoon Kim, Jaemin Na, Jiwon Kim, Keonho Lee, Kyunghwan Cho, Wonjun Hwang. (2024)  
**D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap for Domain-Adaptive Object Detection**
<br/>
<button class="copy-to-clipboard" title="D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap for Domain-Adaptive Object Detection" index=58>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Object Detection, Domain Adaptation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09359v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09359v1.pdf" filename="2403.09359v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Domain</b> <b>adaptation</b> for <b>object</b> <b>detection</b> typically entails transferring knowledge from one visible <b>domain</b> <b>to</b> another visible <b>domain.</b> <b>However,</b> there are limited studies on adapting from the visible to the thermal <b>domain,</b> <b>because</b> the <b>domain</b> <b>gap</b> between the visible and thermal <b>domains</b> <b>is</b> much larger than expected, and traditional <b>domain</b> <b>adaptation</b> can not successfully facilitate learning in this situation. To overcome this challenge, we propose a Distinctive Dual-Domain Teacher (D3T) framework that employs distinct training paradigms for each <b>domain.</b> <b>Specifically,</b> we segregate the source and target training sets for building dual-teachers and successively deploy exponential moving average to the student model to individual teachers of each <b>domain.</b> <b>The</b> framework further incorporates a zigzag learning method between dual teachers, facilitating a gradual transition from the visible to thermal <b>domains</b> <b>during</b> training. We validate the superiority of our method through newly designed experimental protocols with well-known thermal datasets, i.e., FLIR and KAIST. Source code is available at https://github.com/EdwardDo69/D3T .

{{</citation>}}


### (59/91 | 59/254) Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening (Andrew Wang et al., 2024)

{{<citation>}}

Andrew Wang, Mike Davies. (2024)  
**Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening**
<br/>
<button class="copy-to-clipboard" title="Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening" index=59>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV, eess-IV  
Keyword Score: 20  
Keywords: Unsupervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09327v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09327v1.pdf" filename="2403.09327v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Ill-posed image reconstruction problems appear in many scenarios such as remote sensing, where obtaining high quality images is crucial for environmental monitoring, disaster management and urban planning. Deep learning has seen great success in overcoming the limitations of traditional methods. However, these inverse problems rarely come with ground truth data, highlighting the importance of <b>unsupervised</b> <b>learning</b> from partial and noisy measurements alone. We propose perspective-equivariant imaging (EI), a framework that leverages perspective variability in optical camera-based imaging systems, such as satellites or handheld cameras, to recover information lost in ill-posed optical camera imaging problems. This extends previous EI work to include a much richer non-linear class of group transforms and is shown to be an excellent prior for satellite and urban image data, where perspective-EI achieves state-of-the-art results in multispectral pansharpening, outperforming other <b>unsupervised</b> <b>methods</b> in the literature. Code at https://andrewwango.github.io/perspective-equivariant-imaging

{{</citation>}}


### (60/91 | 60/254) CLIP-EBC: CLIP Can Count Accurately through Enhanced Blockwise Classification (Yiming Ma et al., 2024)

{{<citation>}}

Yiming Ma, Victor Sanchez, Tanaya Guha. (2024)  
**CLIP-EBC: CLIP Can Count Accurately through Enhanced Blockwise Classification**
<br/>
<button class="copy-to-clipboard" title="CLIP-EBC: CLIP Can Count Accurately through Enhanced Blockwise Classification" index=60>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Object Detection, Zero-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09281v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09281v1.pdf" filename="2403.09281v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The CLIP (Contrastive Language-Image Pretraining) model has exhibited outstanding performance in recognition problems, such as <b>zero-shot</b> image classification and <b>object</b> <b>detection.</b> However, its ability to count remains understudied due to the inherent challenges of transforming counting--a regression task--into a recognition task. In this paper, we investigate CLIP's potential in counting, focusing specifically on estimating crowd sizes. Existing classification-based crowd-counting methods have encountered issues, including inappropriate discretization strategies, which impede the application of CLIP and result in suboptimal performance. To address these challenges, we propose the Enhanced Blockwise Classification (EBC) framework. In contrast to previous methods, EBC relies on integer-valued bins that facilitate the learning of robust decision boundaries. Within our model-agnostic EBC framework, we introduce CLIP-EBC, the first fully CLIP-based crowd-counting model capable of generating density maps. Comprehensive evaluations across diverse crowd-counting datasets demonstrate the state-of-the-art performance of our methods. Particularly, EBC can improve existing models by up to 76.9%. Moreover, our CLIP-EBC model surpasses current crowd-counting methods, achieving mean absolute errors of 55.0 and 6.3 on ShanghaiTech part A and part B datasets, respectively. The code will be made publicly available.

{{</citation>}}


### (61/91 | 61/254) WSI-SAM: Multi-resolution Segment Anything Model (SAM) for histopathology whole-slide images (Hong Liu et al., 2024)

{{<citation>}}

Hong Liu, Haosen Yang, Paul J. van Diest, Josien P. W. Pluim, Mitko Veta. (2024)  
**WSI-SAM: Multi-resolution Segment Anything Model (SAM) for histopathology whole-slide images**
<br/>
<button class="copy-to-clipboard" title="WSI-SAM: Multi-resolution Segment Anything Model (SAM) for histopathology whole-slide images" index=61>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Zero-shot, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09257v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09257v1.pdf" filename="2403.09257v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The Segment Anything Model (SAM) marks a significant advancement in segmentation models, offering powerful <b>zero-shot</b> capabilities and dynamic <b>prompting.</b> However, existing medical SAMs are not suitable for the multi-scale nature of whole-slide images (WSIs), restricting their effectiveness. To resolve this drawback, we present WSI-SAM, enhancing SAM with precise object segmentation capabilities for histopathology images using multi-resolution patches, while preserving its original <b>prompt-driven</b> design, efficiency, and <b>zero-shot</b> adaptability. To fully exploit pretrained knowledge while minimizing training overhead, we keep SAM frozen, only introducing minimal additional parameters and computation. In particular, we introduce High-Resolution (HR) token, Low-Resolution (LR) token and dual mask decoder. This decoder integrates the original SAM mask decoder with a lightweight fusion module that integrates features at multiple scales. Instead of predicting a mask independently, we integrate HR and LR token at intermediate layer to jointly learn features of the same object across multiple resolutions. Experiments show that our WSI-SAM outperforms state-of-the-art SAM and its variants. In particular, our model outperforms SAM by 4.1 and 2.5 percent points on a ductal carcinoma in situ (DCIS) segmentation tasks and breast cancer metastasis segmentation task (CAMELYON16 dataset). The code will be available at https://github.com/HongLiuuuuu/WSI-SAM.

{{</citation>}}


### (62/91 | 62/254) Generalized Relevance Learning Grassmann Quantization (M. Mohammadi et al., 2024)

{{<citation>}}

M. Mohammadi, M. Babai, M. H. F. Wilkinson. (2024)  
**Generalized Relevance Learning Grassmann Quantization**
<br/>
<button class="copy-to-clipboard" title="Generalized Relevance Learning Grassmann Quantization" index=62>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 20  
Keywords: Face Recognition, Quantization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09183v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09183v1.pdf" filename="2403.09183v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Due to advancements in digital cameras, it is easy to gather multiple images (or videos) from an object under different conditions. Therefore, image-set classification has attracted more attention, and different solutions were proposed to model them. A popular way to model image sets is subspaces, which form a manifold called the Grassmann manifold. In this contribution, we extend the application of Generalized Relevance Learning Vector <b>Quantization</b> to deal with Grassmann manifold. The proposed model returns a set of prototype subspaces and a relevance vector. While prototypes model typical behaviours within classes, the relevance factors specify the most discriminative principal vectors (or images) for the classification task. They both provide insights into the model's decisions by highlighting influential images and pixels for predictions. Moreover, due to learning prototypes, the model complexity of the new method during inference is independent of dataset size, unlike previous works. We applied it to several recognition tasks including handwritten digit recognition, <b>face</b> <b>recognition,</b> activity recognition, and object recognition. Experiments demonstrate that it outperforms previous works with lower complexity and can successfully model the variation, such as handwritten style or lighting conditions. Moreover, the presence of relevances makes the model robust to the selection of subspaces' dimensionality.

{{</citation>}}


### (63/91 | 63/254) Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts (Byeongjun Park et al., 2024)

{{<citation>}}

Byeongjun Park, Hyojun Go, Jin-Young Kim, Sangmin Woo, Seokil Ham, Changick Kim. (2024)  
**Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts**
<br/>
<button class="copy-to-clipboard" title="Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts" index=63>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Diffusion Model, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09176v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09176v1.pdf" filename="2403.09176v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Diffusion</b> <b>models</b> have achieved remarkable success across a range of generative tasks. Recent efforts to enhance <b>diffusion</b> <b>model</b> architectures have reimagined them as a form of multi-task learning, where each task corresponds to a denoising task at a specific noise level. While these efforts have focused on parameter isolation and task routing, they fall short of capturing detailed inter-task relationships and risk losing semantic information, respectively. In response, we introduce Switch <b>Diffusion</b> <b>Transformer</b> (Switch-DiT), which establishes inter-task relationships between conflicting tasks without compromising semantic information. To achieve this, we employ a sparse mixture-of-experts within each <b>transformer</b> block to utilize semantic information and facilitate handling conflicts in tasks through parameter isolation. Additionally, we propose a <b>diffusion</b> <b>prior</b> loss, encouraging similar tasks to share their denoising paths while isolating conflicting ones. Through these, each <b>transformer</b> block contains a shared expert across all tasks, where the common and task-specific denoising paths enable the <b>diffusion</b> <b>model</b> to construct its beneficial way of synergizing denoising tasks. Extensive experiments validate the effectiveness of our approach in improving both image quality and convergence rate, and further analysis demonstrates that Switch-DiT constructs tailored denoising paths across various generation scenarios.

{{</citation>}}


### (64/91 | 64/254) Dyadic Interaction Modeling for Social Behavior Generation (Minh Tran et al., 2024)

{{<citation>}}

Minh Tran, Di Chang, Maksim Siniukov, Mohammad Soleymani. (2024)  
**Dyadic Interaction Modeling for Social Behavior Generation**
<br/>
<button class="copy-to-clipboard" title="Dyadic Interaction Modeling for Social Behavior Generation" index=64>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Contrastive Learning, Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09069v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09069v1.pdf" filename="2403.09069v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Human-human communication is like a delicate dance where listeners and speakers concurrently interact to maintain conversational dynamics. Hence, an effective model for generating listener nonverbal behaviors requires understanding the dyadic context and interaction. In this paper, we present an effective framework for creating 3D facial motions in dyadic interactions. Existing work consider a listener as a reactive agent with reflexive behaviors to the speaker's voice and facial motions. The heart of our framework is Dyadic Interaction Modeling (DIM), a pre-training approach that jointly models speakers' and listeners' motions through masking and <b>contrastive</b> <b>learning</b> to learn representations that capture the dyadic context. To enable the generation of non-deterministic behaviors, we encode both listener and speaker motions into discrete latent representations, through VQ-VAE. The pre-trained model is further <b>fine-tuned</b> for motion generation. Extensive experiments demonstrate the superiority of our framework in generating listener motions, establishing a new state-of-the-art according to the quantitative measures capturing the diversity and realism of generated motions. Qualitative results demonstrate the superior capabilities of the proposed approach in generating diverse and realistic expressions, eye blinks and head gestures.

{{</citation>}}


### (65/91 | 65/254) Leveraging Foundation Model Automatic Data Augmentation Strategies and Skeletal Points for Hands Action Recognition in Industrial Assembly Lines (Liang Wu et al., 2024)

{{<citation>}}

Liang Wu, X. -G. Ma. (2024)  
**Leveraging Foundation Model Automatic Data Augmentation Strategies and Skeletal Points for Hands Action Recognition in Industrial Assembly Lines**
<br/>
<button class="copy-to-clipboard" title="Leveraging Foundation Model Automatic Data Augmentation Strategies and Skeletal Points for Hands Action Recognition in Industrial Assembly Lines" index=65>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Data Augmentation, Foundation Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09056v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09056v1.pdf" filename="2403.09056v1.pdf">Download PDF</button>

---


**ABSTRACT**  
On modern industrial assembly lines, many intelligent algorithms have been developed to replace or supervise workers. However, we found that there were bottlenecks in both training datasets and real-time performance when deploying algorithms on actual assembly line. Therefore, we developed a promising strategy for expanding industrial datasets, which utilized large models with strong generalization abilities to achieve efficient, high-quality, and large-scale dataset expansion, solving the problem of insufficient and low-quality industrial datasets. We also applied this strategy to video action recognition. We proposed a method of converting hand action recognition problems into hand skeletal trajectory classification problems, which solved the real-time performance problem of industrial algorithms. In the "hand movements during wire insertion" scenarios on the actual assembly line, the accuracy of hand action recognition reached 98.8\%. We conducted detailed experimental analysis to demonstrate the effectiveness and superiority of the method, and deployed the entire process on Midea's actual assembly line.

{{</citation>}}


### (66/91 | 66/254) MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models (Zunnan Xu et al., 2024)

{{<citation>}}

Zunnan Xu, Yukang Lin, Haonan Han, Sicheng Yang, Ronghui Li, Yachao Zhang, Xiu Li. (2024)  
**MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models**
<br/>
<button class="copy-to-clipboard" title="MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models" index=66>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-HC, cs.CV  
Keyword Score: 16  
Keywords: Diffusion Model, Multi-modal, Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09471v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09471v1.pdf" filename="2403.09471v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Gesture synthesis is a vital realm of human-computer interaction, with wide-ranging applications across various fields like film, robotics, and virtual reality. Recent advancements have utilized the <b>diffusion</b> <b>model</b> and attention mechanisms to improve gesture synthesis. However, due to the high computational complexity of these techniques, generating long and diverse sequences with low latency remains a challenge. We explore the potential of state space models (SSMs) to address the challenge, implementing a two-stage modeling strategy with discrete motion priors to enhance the quality of gestures. Leveraging the foundational Mamba block, we introduce MambaTalk, enhancing gesture diversity and rhythm through <b>multimodal</b> integration. Extensive experiments demonstrate that our method matches or exceeds the performance of state-of-the-art models.

{{</citation>}}


### (67/91 | 67/254) EfficientMFD: Towards More Efficient Multimodal Synchronous Fusion Detection (Jiaqing Zhang et al., 2024)

{{<citation>}}

Jiaqing Zhang, Mingxiang Cao, Xue Yang, Weiying Xie, Jie Lei, Daixun Li, Geng Yang, Wenbo Huang, Yunsong Li. (2024)  
**EfficientMFD: Towards More Efficient Multimodal Synchronous Fusion Detection**
<br/>
<button class="copy-to-clipboard" title="EfficientMFD: Towards More Efficient Multimodal Synchronous Fusion Detection" index=67>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 16  
Keywords: Object Detection, Multi-modal, Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09323v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09323v1.pdf" filename="2403.09323v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Multimodal</b> image fusion and <b>object</b> <b>detection</b> play a vital role in autonomous driving. Current joint learning methods have made significant progress in the <b>multimodal</b> fusion detection task combining the texture detail and objective semantic information. However, the tedious training steps have limited its applications to wider real-world industrial deployment. To address this limitation, we propose a novel end-to-end <b>multimodal</b> fusion detection algorithm, named EfficientMFD, to simplify models that exhibit decent performance with only one training step. Synchronous joint optimization is utilized in an end-to-end manner between two components, thus not being affected by the local optimal solution of the individual task. Besides, a comprehensive optimization is established in the gradient matrix between the shared parameters for both tasks. It can converge to an optimal point with fusion detection weights. We extensively test it on several public datasets, demonstrating superior performance on not only visually appealing fusion but also favorable detection performance (e.g., 6.6% mAP50:95) over other state-of-the-art approaches.

{{</citation>}}


### (68/91 | 68/254) PoIFusion: Multi-Modal 3D Object Detection via Fusion at Points of Interest (Jiajun Deng et al., 2024)

{{<citation>}}

Jiajun Deng, Sha Zhang, Feras Dayoub, Wanli Ouyang, Yanyong Zhang, Ian Reid. (2024)  
**PoIFusion: Multi-Modal 3D Object Detection via Fusion at Points of Interest**
<br/>
<button class="copy-to-clipboard" title="PoIFusion: Multi-Modal 3D Object Detection via Fusion at Points of Interest" index=68>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 16  
Keywords: Object Detection, Benchmarking, Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09212v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09212v1.pdf" filename="2403.09212v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this work, we present PoIFusion, a simple yet effective <b>multi-modal</b> 3D <b>object</b> <b>detection</b> framework to fuse the information of RGB images and LiDAR point clouds at the point of interest (abbreviated as PoI). Technically, our PoIFusion follows the paradigm of query-based <b>object</b> <b>detection,</b> formulating <b>object</b> <b>queries</b> as dynamic 3D boxes. The PoIs are adaptively generated from each query box on the fly, serving as the keypoints to represent a 3D <b>object</b> <b>and</b> play the role of basic units in <b>multi-modal</b> fusion. Specifically, we project PoIs into the view of each modality to sample the corresponding feature and integrate the <b>multi-modal</b> features at each PoI through a dynamic fusion block. Furthermore, the features of PoIs derived from the same query box are aggregated together to update the query feature. Our approach prevents information loss caused by view transformation and eliminates the computation-intensive global attention, making the <b>multi-modal</b> 3D <b>object</b> <b>detector</b> more applicable. We conducted extensive experiments on the nuScenes dataset to evaluate our approach. Remarkably, our PoIFusion achieves 74.9\% NDS and 73.4\% mAP, setting a state-of-the-art record on the <b>multi-modal</b> 3D <b>object</b> <b>detection</b> <b>benchmark.</b> Codes will be made available via \url{https://djiajunustc.github.io/projects/poifusion}.

{{</citation>}}


### (69/91 | 69/254) Holo-Relighting: Controllable Volumetric Portrait Relighting from a Single Image (Yiqun Mei et al., 2024)

{{<citation>}}

Yiqun Mei, Yu Zeng, He Zhang, Zhixin Shu, Xuaner Zhang, Sai Bi, Jianming Zhang, HyunJoon Jung, Vishal M. Patel. (2024)  
**Holo-Relighting: Controllable Volumetric Portrait Relighting from a Single Image**
<br/>
<button class="copy-to-clipboard" title="Holo-Relighting: Controllable Volumetric Portrait Relighting from a Single Image" index=69>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 15  
Keywords: Generative Adversarial Network, Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09632v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09632v1.pdf" filename="2403.09632v1.pdf">Download PDF</button>

---


**ABSTRACT**  
At the core of portrait photography is the search for ideal lighting and viewpoint. The process often requires advanced knowledge in photography and an elaborate studio setup. In this work, we propose Holo-Relighting, a volumetric relighting method that is capable of synthesizing novel viewpoints, and novel lighting from a single image. Holo-Relighting leverages the pretrained 3D <b>GAN</b> (EG3D) to reconstruct <b>geometry</b> and appearance from an input portrait as a set of 3D-aware features. We design a relighting module conditioned on a given lighting to process these features, and predict a relit 3D representation in the form of a tri-plane, which can render to an arbitrary viewpoint through volume rendering. Besides viewpoint and lighting control, Holo-Relighting also takes the head pose as a condition to enable head-pose-dependent lighting effects. With these novel designs, Holo-Relighting can generate complex non-Lambertian lighting effects (e.g., specular highlights and cast shadows) without using any explicit physical lighting priors. We train Holo-Relighting with data captured with a light stage, and propose two data-rendering techniques to improve the data quality for training the volumetric relighting system. Through quantitative and qualitative experiments, we demonstrate Holo-Relighting can achieve state-of-the-arts relighting quality with better photorealism, 3D consistency and controllability.

{{</citation>}}


### (70/91 | 70/254) 3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation (Frank Zhang et al., 2024)

{{<citation>}}

Frank Zhang, Yibo Zhang, Quan Zheng, Rui Ma, Wei Hua, Hujun Bao, Weiwei Xu, Changqing Zou. (2024)  
**3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation**
<br/>
<button class="copy-to-clipboard" title="3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation" index=70>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 15  
Keywords: Diffusion Model, Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09439v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09439v1.pdf" filename="2403.09439v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Text-driven 3D scene generation techniques have made rapid progress in recent years. Their success is mainly attributed to using existing generative models to iteratively perform image warping and inpainting to generate 3D scenes. However, these methods heavily rely on the outputs of existing models, leading to error accumulation in <b>geometry</b> and appearance that prevent the models from being used in various scenarios (e.g., outdoor and unreal scenarios). To address this limitation, we generatively refine the newly generated local views by querying and aggregating global 3D information, and then progressively generate the 3D scene. Specifically, we employ a tri-plane features-based NeRF as a unified representation of the 3D scene to constrain global 3D consistency, and propose a generative refinement network to synthesize new contents with higher quality by exploiting the natural image prior from 2D <b>diffusion</b> <b>model</b> as well as the global 3D information of the current scene. Our extensive experiments demonstrate that, in comparison to previous methods, our approach supports wide variety of scene generation and arbitrary camera trajectories with improved visual quality and 3D consistency.

{{</citation>}}


### (71/91 | 71/254) SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios (Ding-Tao Huang et al., 2024)

{{<citation>}}

Ding-Tao Huang, En-Te Lin, Lipeng Chen, Li-Fu Liu, Long Zeng. (2024)  
**SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios**
<br/>
<button class="copy-to-clipboard" title="SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios" index=71>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 15  
Keywords: Geometry, Domain Adaptation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09317v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09317v1.pdf" filename="2403.09317v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Despite the success in 6D pose estimation in bin-picking scenarios, existing methods still struggle to produce accurate prediction results for symmetry objects and real world scenarios. The primary bottlenecks include 1) the ambiguity keypoints caused by object symmetries; 2) the <b>domain</b> <b>gap</b> between real and synthetic data. To circumvent these problem, we propose a new 6D pose estimation network with symmetric-aware keypoint prediction and self-training <b>domain</b> <b>adaptation</b> (SD-Net). SD-Net builds on pointwise keypoint regression and deep hough voting to perform reliable detection keypoint under clutter and occlusion. Specifically, at the keypoint prediction stage, we designe a robust 3D keypoints selection strategy considering the symmetry class of objects and equivalent keypoints, which facilitate locating 3D keypoints even in highly occluded scenes. Additionally, we build an effective filtering algorithm on predicted keypoint to dynamically eliminate multiple ambiguity and outlier keypoint candidates. At the <b>domain</b> <b>adaptation</b> stage, we propose the self-training framework using a student-teacher training scheme. To carefully distinguish reliable predictions, we harnesses a tailored heuristics for 3D <b>geometry</b> pseudo labelling based on semi-chamfer distance. On public Sil'eane dataset, SD-Net achieves state-of-the-art results, obtaining an average precision of 96%. Testing learning and generalization abilities on public Parametric datasets, SD-Net is 8% higher than the state-of-the-art method. The code is available at https://github.com/dingthuang/SD-Net.

{{</citation>}}


### (72/91 | 72/254) Sculpt3D: Multi-View Consistent Text-to-3D Generation with Sparse 3D Prior (Cheng Chen et al., 2024)

{{<citation>}}

Cheng Chen, Xiaofeng Yang, Fan Yang, Chengzeng Feng, Zhoujie Fu, Chuan-Sheng Foo, Guosheng Lin, Fayao Liu. (2024)  
**Sculpt3D: Multi-View Consistent Text-to-3D Generation with Sparse 3D Prior**
<br/>
<button class="copy-to-clipboard" title="Sculpt3D: Multi-View Consistent Text-to-3D Generation with Sparse 3D Prior" index=72>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 15  
Keywords: Diffusion Model, Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09140v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09140v1.pdf" filename="2403.09140v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent works on text-to-3d generation show that using only 2D <b>diffusion</b> <b>supervision</b> for 3D generation tends to produce results with inconsistent appearances (e.g., faces on the back view) and inaccurate shapes (e.g., animals with extra legs). Existing methods mainly address this issue by retraining <b>diffusion</b> <b>models</b> with images rendered from 3D data to ensure multi-view consistency while struggling to balance 2D generation quality with 3D consistency. In this paper, we present a new framework Sculpt3D that equips the current pipeline with explicit injection of 3D priors from retrieved reference objects without re-training the 2D <b>diffusion</b> <b>model.</b> Specifically, we demonstrate that high-quality and diverse 3D <b>geometry</b> can be guaranteed by keypoints supervision through a sparse ray sampling approach. Moreover, to ensure accurate appearances of different views, we further modulate the output of the 2D <b>diffusion</b> <b>model</b> to the correct patterns of the template views without altering the generated object's style. These two decoupled designs effectively harness 3D information from reference objects to generate 3D objects while preserving the generation quality of the 2D <b>diffusion</b> <b>model.</b> Extensive experiments show our method can largely improve the multi-view consistency while retaining fidelity and diversity. Our project page is available at: https://stellarcheng.github.io/Sculpt3D/.

{{</citation>}}


### (73/91 | 73/254) Score-Guided Diffusion for 3D Human Recovery (Anastasis Stathopoulos et al., 2024)

{{<citation>}}

Anastasis Stathopoulos, Ligong Han, Dimitris Metaxas. (2024)  
**Score-Guided Diffusion for 3D Human Recovery**
<br/>
<button class="copy-to-clipboard" title="Score-Guided Diffusion for 3D Human Recovery" index=73>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 13  
Keywords: Diffusion Model, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09623v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09623v1.pdf" filename="2403.09623v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present Score-Guided Human Mesh Recovery (ScoreHMR), an approach for solving inverse problems for 3D human pose and shape reconstruction. These inverse problems involve fitting a human body model to image observations, traditionally solved through optimization techniques. ScoreHMR mimics model fitting approaches, but alignment with the image observation is achieved through score guidance in the latent space of a <b>diffusion</b> <b>model.</b> The <b>diffusion</b> <b>model</b> is trained to capture the conditional distribution of the human model parameters given an input image. By guiding its denoising process with a task-specific score, ScoreHMR effectively solves inverse problems for various applications without the need for retraining the task-agnostic <b>diffusion</b> <b>model.</b> We evaluate our approach on three settings/applications. These are: (i) single-frame model fitting; (ii) reconstruction from multiple uncalibrated views; (iii) reconstructing humans in video sequences. ScoreHMR consistently outperforms all optimization baselines on popular <b>benchmarks</b> across all settings. We make our code and models available at the https://statho.github.io/ScoreHMR.

{{</citation>}}


### (74/91 | 74/254) Efficient Transferability Assessment for Selection of Pre-trained Detectors (Zhao Wang et al., 2024)

{{<citation>}}

Zhao Wang, Aoxue Li, Zhenguo Li, Qi Dou. (2024)  
**Efficient Transferability Assessment for Selection of Pre-trained Detectors**
<br/>
<button class="copy-to-clipboard" title="Efficient Transferability Assessment for Selection of Pre-trained Detectors" index=74>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 13  
Keywords: Benchmarking, Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09432v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09432v1.pdf" filename="2403.09432v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Large-scale pre-training followed by downstream <b>fine-tuning</b> is an effective solution for transferring deep-learning-based models. Since <b>finetuning</b> all possible pre-trained models is computational costly, we aim to predict the transferability performance of these pre-trained models in a computational efficient manner. Different from previous work that seek out suitable models for downstream classification and segmentation tasks, this paper studies the efficient transferability assessment of pre-trained object detectors. To this end, we build up a detector transferability <b>benchmark</b> which contains a large and diverse zoo of pre-trained detectors with various architectures, source datasets and training schemes. Given this zoo, we adopt 7 target datasets from 5 diverse domains as the downstream target tasks for evaluation. Further, we propose to assess classification and regression sub-tasks simultaneously in a unified framework. Additionally, we design a complementary metric for evaluating tasks with varying objects. Experimental results demonstrate that our method outperforms other state-of-the-art approaches in assessing transferability under different target domains while efficiently reducing wall-clock time 32$\times$ and requires a mere 5.2\% memory footprint compared to brute-force <b>fine-tuning</b> of all pre-trained detectors.

{{</citation>}}


### (75/91 | 75/254) Gradient-Aware Logit Adjustment Loss for Long-tailed Classifier (Fan Zhang et al., 2024)

{{<citation>}}

Fan Zhang, Wei Qin, Weijieying Ren, Lei Wang, Zetong Chen, Richang Hong. (2024)  
**Gradient-Aware Logit Adjustment Loss for Long-tailed Classifier**
<br/>
<button class="copy-to-clipboard" title="Gradient-Aware Logit Adjustment Loss for Long-tailed Classifier" index=75>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 13  
Keywords: Graph Contrastive Learning, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09036v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09036v1.pdf" filename="2403.09036v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In the real-world setting, data often follows a long-tailed distribution, where head classes contain significantly more training samples than tail classes. Consequently, models trained on such data tend to be biased toward head classes. The medium of this bias is imbalanced gradients, which include not only the ratio of scale between positive and negative gradients but also imbalanced gradients from different negative classes. Therefore, we propose the Gradient-Aware Logit Adjustment (GALA) loss, which adjusts the logits based on accumulated gradients to balance the optimization process. Additionally, We find that most of the solutions to long-tailed problems are still biased towards head classes in the end, and we propose a simple and post hoc prediction re-balancing strategy to further mitigate the basis toward head class. Extensive experiments are conducted on multiple popular long-tailed recognition <b>benchmark</b> datasets to evaluate the effectiveness of these two designs. Our approach achieves top-1 accuracy of 48.5\%, 41.4\%, and 73.3\% on CIFAR100-LT, Places-LT, and iNaturalist, outperforming the state-of-the-art method <b>GCL</b> by a significant margin of 3.62\%, 0.76\% and 1.2\%, respectively. Code is available at https://github.com/lt-project-repository/lt-project.

{{</citation>}}


### (76/91 | 76/254) Explorations in Texture Learning (Blaine Hoak et al., 2024)

{{<citation>}}

Blaine Hoak, Patrick McDaniel. (2024)  
**Explorations in Texture Learning**
<br/>
<button class="copy-to-clipboard" title="Explorations in Texture Learning" index=76>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 10  
Keywords: Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09543v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09543v1.pdf" filename="2403.09543v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this work, we investigate \textit{texture learning}: the identification of textures learned by object classification models, and the extent to which they rely on these textures. We build texture-object associations that uncover new insights about the relationships between texture and object classes in <b>CNNs</b> and find three classes of results: associations that are strong and expected, strong and not expected, and expected but not present. Our analysis demonstrates that investigations in texture learning enable new methods for interpretability and have the potential to uncover unexpected biases.

{{</citation>}}


### (77/91 | 77/254) What Sketch Explainability Really Means for Downstream Tasks (Hmrishav Bandyopadhyay et al., 2024)

{{<citation>}}

Hmrishav Bandyopadhyay, Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Tao Xiang, Yi-Zhe Song. (2024)  
**What Sketch Explainability Really Means for Downstream Tasks**
<br/>
<button class="copy-to-clipboard" title="What Sketch Explainability Really Means for Downstream Tasks" index=77>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Adversarial Attack  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09480v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09480v1.pdf" filename="2403.09480v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we explore the unique modality of sketch for explainability, emphasising the profound impact of human strokes compared to conventional pixel-oriented studies. Beyond explanations of network behavior, we discern the genuine implications of explainability across diverse downstream sketch-related tasks. We propose a lightweight and portable explainability solution -- a seamless plugin that integrates effortlessly with any pre-trained model, eliminating the need for re-training. Demonstrating its adaptability, we present four applications: highly studied retrieval and generation, and completely novel assisted drawing and sketch <b>adversarial</b> <b>attacks.</b> The centrepiece to our solution is a stroke-level attribution map that takes different forms when linked with downstream tasks. By addressing the inherent non-differentiability of rasterisation, we enable explanations at both coarse stroke level (SLA) and partial stroke level (P-SLA), each with its advantages for specific downstream tasks.

{{</citation>}}


### (78/91 | 78/254) Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting (Pawel Knap et al., 2024)

{{<citation>}}

Pawel Knap, Peter Hardy, Alberto Tamajo, Hwasup Lim, Hansung Kim. (2024)  
**Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting**
<br/>
<button class="copy-to-clipboard" title="Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting" index=78>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09437v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09437v1.pdf" filename="2403.09437v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Current human pose estimation systems focus on retrieving an accurate 3D global estimate of a single person. Therefore, this paper presents one of the first 3D multi-person human pose estimation systems that is able to work in real-time and is also able to handle basic forms of occlusion. First, we adjust an off-the-shelf 2D detector and an <b>unsupervised</b> 2D-3D lifting model for use with a 360$^\circ$ panoramic camera and mmWave radar sensors. We then introduce several contributions, including camera and radar calibrations, and the improved matching of people within the image and radar space. The system addresses both the depth and scale ambiguity problems by employing a lightweight 2D-3D pose lifting algorithm that is able to work in real-time while exhibiting accurate performance in both indoor and outdoor environments which offers both an affordable and scalable solution. Notably, our system's time complexity remains nearly constant irrespective of the number of detected individuals, achieving a frame rate of approximately 7-8 fps on a laptop with a commercial-grade GPU.

{{</citation>}}


### (79/91 | 79/254) EventRPG: Event Data Augmentation with Relevance Propagation Guidance (Mingyuan Sun et al., 2024)

{{<citation>}}

Mingyuan Sun, Donghao Zhang, Zongyuan Ge, Jiaxu Wang, Jia Li, Zheng Fang, Renjing Xu. (2024)  
**EventRPG: Event Data Augmentation with Relevance Propagation Guidance**
<br/>
<button class="copy-to-clipboard" title="EventRPG: Event Data Augmentation with Relevance Propagation Guidance" index=79>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Data Augmentation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09274v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09274v1.pdf" filename="2403.09274v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Event camera, a novel bio-inspired vision sensor, has drawn a lot of attention for its low latency, low power consumption, and high dynamic range. Currently, overfitting remains a critical problem in event-based classification tasks for Spiking Neural Network (SNN) due to its relatively weak spatial representation capability. <b>Data</b> <b>augmentation</b> is a simple but efficient method to alleviate overfitting and improve the generalization ability of neural networks, and saliency-based augmentation methods are proven to be effective in the image processing field. However, there is no approach available for extracting saliency maps from SNNs. Therefore, for the first time, we present Spiking Layer-Time-wise Relevance Propagation rule (SLTRP) and Spiking Layer-wise Relevance Propagation rule (SLRP) in order for SNN to generate stable and accurate CAMs and saliency maps. Based on this, we propose EventRPG, which leverages relevance propagation on the spiking neural network for more efficient augmentation. Our proposed method has been evaluated on several SNN structures, achieving state-of-the-art performance in object recognition tasks including N-Caltech101, CIFAR10-DVS, with accuracies of 85.62% and 85.55%, as well as action recognition task SL-Animals with an accuracy of 91.59%. Our code is available at https://github.com/myuansun/EventRPG.

{{</citation>}}


### (80/91 | 80/254) D-YOLO a robust framework for object detection in adverse weather conditions (Zihan Chu, 2024)

{{<citation>}}

Zihan Chu. (2024)  
**D-YOLO a robust framework for object detection in adverse weather conditions**
<br/>
<button class="copy-to-clipboard" title="D-YOLO a robust framework for object detection in adverse weather conditions" index=80>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV, eess-IV  
Keyword Score: 10  
Keywords: Object Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09233v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09233v1.pdf" filename="2403.09233v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Adverse weather conditions including haze, snow and rain lead to decline in image qualities, which often causes a decline in performance for deep-learning based detection networks. Most existing approaches attempts to rectify hazy images before performing <b>object</b> <b>detection,</b> which increases the complexity of the network and may result in the loss in latent information. To better integrate image restoration and <b>object</b> <b>detection</b> tasks, we designed a double-route network with an attention feature fusion module, taking both hazy and dehazed features into consideration. We also proposed a subnetwork to provide haze-free features to the detection network. Specifically, our D-YOLO improves the performance of the detection network by minimizing the distance between the clear feature extraction subnetwork and detection network. Experiments on RTTS and FoggyCityscapes datasets show that D-YOLO demonstrates better performance compared to the state-of-the-art methods. It is a robust detection framework for bridging the gap between low-level dehazing and high-level detection.

{{</citation>}}


### (81/91 | 81/254) Improving Distant 3D Object Detection Using 2D Box Supervision (Zetong Yang et al., 2024)

{{<citation>}}

Zetong Yang, Zhiding Yu, Chris Choy, Renhao Wang, Anima Anandkumar, Jose M. Alvarez. (2024)  
**Improving Distant 3D Object Detection Using 2D Box Supervision**
<br/>
<button class="copy-to-clipboard" title="Improving Distant 3D Object Detection Using 2D Box Supervision" index=81>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Object Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09230v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09230v1.pdf" filename="2403.09230v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Improving the detection of distant 3d <b>objects</b> <b>is</b> an important yet challenging task. For camera-based 3D perception, the annotation of 3d bounding relies heavily on LiDAR for accurate depth information. As such, the distance of annotation is often limited due to the sparsity of LiDAR points on distant <b>objects,</b> <b>which</b> hampers the capability of existing detectors for long-range scenarios. We address this challenge by considering only 2D box supervision for distant <b>objects</b> <b>since</b> they are easy to annotate. We propose LR3D, a framework that learns to recover the missing depth of distant <b>objects.</b> <b>LR3D</b> adopts an implicit projection head to learn the generation of mapping between 2D boxes and depth using the 3D supervision on close <b>objects.</b> <b>This</b> mapping allows the depth estimation of distant <b>objects</b> <b>conditioned</b> on their 2D boxes, making long-range 3D detection with 2D supervision feasible. Experiments show that without distant 3D annotations, LR3D allows camera-based methods to detect distant <b>objects</b> <b>(over</b> 200m) with comparable accuracy to full 3D supervision. Our framework is general, and could widely benefit 3D detection methods to a large extent.

{{</citation>}}


### (82/91 | 82/254) Noise Dimension of GAN: An Image Compression Perspective (Ziran Zhu et al., 2024)

{{<citation>}}

Ziran Zhu, Tongda Xu, Ling Li, Yan Wang. (2024)  
**Noise Dimension of GAN: An Image Compression Perspective**
<br/>
<button class="copy-to-clipboard" title="Noise Dimension of GAN: An Image Compression Perspective" index=82>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Generative Adversarial Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09196v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09196v1.pdf" filename="2403.09196v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Generative adversial network <b>(GAN)</b> is a type of generative model that maps a high-dimensional noise to samples in target distribution. However, the dimension of noise required in <b>GAN</b> is not well understood. Previous approaches view <b>GAN</b> as a mapping from a continuous distribution to another continous distribution. In this paper, we propose to view <b>GAN</b> as a discrete sampler instead. From this perspective, we build a connection between the minimum noise required and the bits to losslessly compress the images. Furthermore, to understand the behaviour of <b>GAN</b> when noise dimension is limited, we propose divergence-entropy trade-off. This trade-off depicts the best divergence we can achieve when noise is limited. And as rate distortion trade-off, it can be numerically solved when source distribution is known. Finally, we verifies our theory with experiments on image generation.

{{</citation>}}


### (83/91 | 83/254) Intention-driven Ego-to-Exo Video Generation (Hongchen Luo et al., 2024)

{{<citation>}}

Hongchen Luo, Kai Zhu, Wei Zhai, Yang Cao. (2024)  
**Intention-driven Ego-to-Exo Video Generation**
<br/>
<button class="copy-to-clipboard" title="Intention-driven Ego-to-Exo Video Generation" index=83>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Diffusion Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09194v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09194v1.pdf" filename="2403.09194v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Ego-to-exo video generation refers to generating the corresponding exocentric video according to the egocentric video, providing valuable applications in AR/VR and embodied AI. Benefiting from advancements in <b>diffusion</b> <b>model</b> techniques, notable progress has been achieved in video generation. However, existing methods build upon the spatiotemporal consistency assumptions between adjacent frames, which cannot be satisfied in the ego-to-exo scenarios due to drastic changes in views. To this end, this paper proposes an Intention-Driven Ego-to-exo video generation framework (IDE) that leverages action intention consisting of human movement and action description as view-independent representation to guide video generation, preserving the consistency of content and motion. Specifically, the egocentric head trajectory is first estimated through multi-view stereo matching. Then, cross-view feature perception module is introduced to establish correspondences between exo- and ego- views, guiding the trajectory transformation module to infer human full-body movement from the head trajectory. Meanwhile, we present an action description unit that maps the action semantics into the feature space consistent with the exocentric image. Finally, the inferred human movement and high-level action descriptions jointly guide the generation of exocentric motion and interaction content (i.e., corresponding optical flow and occlusion maps) in the backward process of the <b>diffusion</b> <b>model,</b> ultimately warping them into the corresponding exocentric video. We conduct extensive experiments on the relevant dataset with diverse exo-ego video pairs, and our IDE outperforms state-of-the-art models in both subjective and objective assessments, demonstrating its efficacy in ego-to-exo video generation.

{{</citation>}}


### (84/91 | 84/254) Intention-aware Denoising Diffusion Model for Trajectory Prediction (Chen Liu et al., 2024)

{{<citation>}}

Chen Liu, Shibo He, Haoyu Liu, Jiming Chen. (2024)  
**Intention-aware Denoising Diffusion Model for Trajectory Prediction**
<br/>
<button class="copy-to-clipboard" title="Intention-aware Denoising Diffusion Model for Trajectory Prediction" index=84>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Diffusion Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09190v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09190v1.pdf" filename="2403.09190v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Trajectory prediction is an essential component in autonomous driving, particularly for collision avoidance systems. Considering the inherent uncertainty of the task, numerous studies have utilized generative models to produce multiple plausible future trajectories for each agent. However, most of them suffer from restricted representation ability or unstable training issues. To overcome these limitations, we propose utilizing the <b>diffusion</b> <b>model</b> to generate the distribution of future trajectories. Two cruxes are to be settled to realize such an idea. First, the diversity of intention is intertwined with the uncertain surroundings, making the true distribution hard to parameterize. Second, the <b>diffusion</b> <b>process</b> is time-consuming during the inference phase, rendering it unrealistic to implement in a real-time driving system. We propose an Intention-aware denoising <b>Diffusion</b> <b>Model</b> (IDM), which tackles the above two problems. We decouple the original uncertainty into intention uncertainty and action uncertainty and model them with two dependent <b>diffusion</b> <b>processes.</b> To decrease the inference time, we reduce the variable dimensions in the intention-aware <b>diffusion</b> <b>process</b> and restrict the initial distribution of the action-aware <b>diffusion</b> <b>process,</b> which leads to fewer <b>diffusion</b> <b>steps.</b> To validate our approach, we conduct experiments on the Stanford Drone Dataset (SDD) and ETH/UCY dataset. Our methods achieve state-of-the-art results, with an FDE of 13.83 pixels on the SDD dataset and 0.36 meters on the ETH/UCY dataset. Compared with the original <b>diffusion</b> <b>model,</b> IDM reduces inference time by two-thirds. Interestingly, our experiments further reveal that introducing intention information is beneficial in modeling the <b>diffusion</b> <b>process</b> of fewer steps.

{{</citation>}}


### (85/91 | 85/254) Rethinking Referring Object Removal (Xiangtian Xue et al., 2024)

{{<citation>}}

Xiangtian Xue, Jiasong Wu, Youyong Kong, Lotfi Senhadji, Huazhong Shu. (2024)  
**Rethinking Referring Object Removal**
<br/>
<button class="copy-to-clipboard" title="Rethinking Referring Object Removal" index=85>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Diffusion Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09128v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09128v1.pdf" filename="2403.09128v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Referring object removal refers to removing the specific object in an image referred by natural language expressions and filling the missing region with reasonable semantics. To address this task, we construct the ComCOCO, a synthetic dataset consisting of 136,495 referring expressions for 34,615 objects in 23,951 image pairs. Each pair contains an image with referring expressions and the ground truth after elimination. We further propose an end-to-end syntax-aware hybrid mapping network with an encoding-decoding structure. Linguistic features are hierarchically extracted at the syntactic level and fused in the downsampling process of visual features with multi-head attention. The feature-aligned pyramid network is leveraged to generate segmentation masks and replace internal pixels with region affinity learned from external semantics in high-level feature maps. Extensive experiments demonstrate that our model outperforms <b>diffusion</b> <b>models</b> and two-stage methods which process the segmentation and inpainting task separately by a significant margin.

{{</citation>}}


### (86/91 | 86/254) Desigen: A Pipeline for Controllable Design Template Generation (Haohan Weng et al., 2024)

{{<citation>}}

Haohan Weng, Danqing Huang, Yu Qiao, Zheng Hu, Chin-Yew Lin, Tong Zhang, C. L. Philip Chen. (2024)  
**Desigen: A Pipeline for Controllable Design Template Generation**
<br/>
<button class="copy-to-clipboard" title="Desigen: A Pipeline for Controllable Design Template Generation" index=86>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09093v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09093v1.pdf" filename="2403.09093v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Templates serve as a good starting point to implement a design (e.g., banner, slide) but it takes great effort from designers to manually create. In this paper, we present Desigen, an automatic template creation pipeline which generates background images as well as harmonious layout elements over the background. Different from natural images, a background image should preserve enough non-salient space for the overlaying layout elements. To equip existing advanced diffusion-based models with stronger spatial control, we propose two simple but effective techniques to constrain the saliency distribution and reduce the attention weight in desired regions during the background generation process. Then conditioned on the background, we synthesize the layout with a <b>Transformer-based</b> autoregressive generator. To achieve a more harmonious composition, we propose an iterative inference strategy to adjust the synthesized background and layout in multiple rounds. We constructed a design dataset with more than 40k advertisement banners to verify our approach. Extensive experiments demonstrate that the proposed pipeline generates high-quality templates comparable to human designers. More than a single-page design, we further show an application of presentation generation that outputs a set of theme-consistent slides. The data and code are available at https://whaohan.github.io/desigen.

{{</citation>}}


### (87/91 | 87/254) CLOAF: CoLlisiOn-Aware Human Flow (Andrey Davydov et al., 2024)

{{<citation>}}

Andrey Davydov, Martin Engilberge, Mathieu Salzmann, Pascal Fua. (2024)  
**CLOAF: CoLlisiOn-Aware Human Flow**
<br/>
<button class="copy-to-clipboard" title="CLOAF: CoLlisiOn-Aware Human Flow" index=87>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09050v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09050v1.pdf" filename="2403.09050v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Even the best current algorithms for estimating body 3D shape and pose yield results that include body self-intersections. In this paper, we present CLOAF, which exploits the diffeomorphic nature of Ordinary Differential Equations to eliminate such self-intersections while still imposing body shape constraints. We show that, unlike earlier approaches to addressing this issue, ours completely eliminates the self-intersections without compromising the accuracy of the reconstructions. Being differentiable, CLOAF can be used to <b>fine-tune</b> pose and shape estimation baselines to improve their overall performance and eliminate self-intersections in their predictions. Furthermore, we demonstrate how our CLOAF strategy can be applied to practically any motion field induced by the user. CLOAF also makes it possible to edit motion to interact with the environment without worrying about potential collision or loss of body-shape prior.

{{</citation>}}


### (88/91 | 88/254) The NeRFect Match: Exploring NeRF Features for Visual Localization (Qunjie Zhou et al., 2024)

{{<citation>}}

Qunjie Zhou, Maxim Maximov, Or Litany, Laura Leal-Taixé. (2024)  
**The NeRFect Match: Exploring NeRF Features for Visual Localization**
<br/>
<button class="copy-to-clipboard" title="The NeRFect Match: Exploring NeRF Features for Visual Localization" index=88>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 8  
Keywords: Benchmarking, Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09577v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09577v1.pdf" filename="2403.09577v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene representation for visual localization. Recently, NeRF has been employed to enhance pose regression and scene coordinate regression models by augmenting the training database, providing auxiliary supervision through rendered images, or serving as an iterative refinement module. We extend its recognized advantages -- its ability to provide a compact scene representation with realistic appearances and accurate <b>geometry</b> -- by exploring the potential of NeRF's internal features in establishing precise 2D-3D matches for localization. To this end, we conduct a comprehensive examination of NeRF's implicit knowledge, acquired through view synthesis, for matching under various conditions. This includes exploring different matching network architectures, extracting encoder features at multiple layers, and varying training configurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D matching function that capitalizes on the internal knowledge of NeRF learned via view synthesis. Our evaluation of NeRFMatch on standard localization <b>benchmarks,</b> within a structure-based pipeline, sets a new state-of-the-art for localization performance on Cambridge Landmarks.

{{</citation>}}


### (89/91 | 89/254) M&M: Multimodal-Multitask Model Integrating Audiovisual Cues in Cognitive Load Assessment (Long Nguyen-Phuoc et al., 2024)

{{<citation>}}

Long Nguyen-Phuoc, Renald Gaboriau, Dimitri Delacroix, Laurent Navarro. (2024)  
**M&M: Multimodal-Multitask Model Integrating Audiovisual Cues in Cognitive Load Assessment**
<br/>
<button class="copy-to-clipboard" title="M&M: Multimodal-Multitask Model Integrating Audiovisual Cues in Cognitive Load Assessment" index=89>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-MM, cs-SD, cs.CV, eess-AS  
Keyword Score: 6  
Keywords: Multi-modal, Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09451v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09451v1.pdf" filename="2403.09451v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper introduces the M&M model, a novel <b>multimodal-multitask</b> learning framework, applied to the AVCAffe dataset for cognitive load assessment (CLA). M&M uniquely integrates audiovisual cues through a dual-pathway architecture, featuring specialized streams for audio and video inputs. A key innovation lies in its cross-modality multihead attention mechanism, fusing the different modalities for synchronized multitasking. Another notable feature is the model's three specialized branches, each tailored to a specific cognitive load label, enabling nuanced, task-specific analysis. While it shows modest performance compared to the AVCAffe's single-task baseline, M\&M demonstrates a promising framework for integrated <b>multimodal</b> processing. This work paves the way for future enhancements in <b>multimodal-multitask</b> learning systems, emphasizing the fusion of diverse data types for complex task handling.

{{</citation>}}


### (90/91 | 90/254) Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph (Donglin Di et al., 2024)

{{<citation>}}

Donglin Di, Jiahui Yang, Chaofan Luo, Zhou Xue, Wei Chen, Xun Yang, Yue Gao. (2024)  
**Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph**
<br/>
<button class="copy-to-clipboard" title="Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph" index=90>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 5  
Keywords: Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09236v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09236v1.pdf" filename="2403.09236v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Text-to-3D generation represents an exciting field that has seen rapid advancements, facilitating the transformation of textual descriptions into detailed 3D models. However, current progress often neglects the intricate high-order correlation of <b>geometry</b> and texture within 3D objects, leading to challenges such as over-smoothness, over-saturation and the Janus problem. In this work, we propose a method named ``3D Gaussian Generation via Hypergraph (Hyper-3DG)'', designed to capture the sophisticated high-order correlations present within 3D objects. Our framework is anchored by a well-established mainflow and an essential module, named ``Geometry and Texture Hypergraph Refiner (HGRefiner)''. This module not only refines the representation of 3D Gaussians but also accelerates the update process of these 3D Gaussians by conducting the Patch-3DGS Hypergraph Learning on both explicit attributes and latent visual features. Our framework allows for the production of finely generated 3D objects within a cohesive optimization, effectively circumventing degradation. Extensive experimentation has shown that our proposed method significantly enhances the quality of 3D generation while incurring no additional computational overhead for the underlying framework. (Project code: https://github.com/yjhboy/Hyper3DG)

{{</citation>}}


### (91/91 | 91/254) rFaceNet: An End-to-End Network for Enhanced Physiological Signal Extraction through Identity-Specific Facial Contours (Dali Zhu et al., 2024)

{{<citation>}}

Dali Zhu, Wenli Zhang, Hualin Zeng, Xiaohao Liu, Long Yang, Jiaqi Zheng. (2024)  
**rFaceNet: An End-to-End Network for Enhanced Physiological Signal Extraction through Identity-Specific Facial Contours**
<br/>
<button class="copy-to-clipboard" title="rFaceNet: An End-to-End Network for Enhanced Physiological Signal Extraction through Identity-Specific Facial Contours" index=91>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09034v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09034v1.pdf" filename="2403.09034v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Remote photoplethysmography (rPPG) technique extracts blood volume pulse (BVP) signals from subtle pixel changes in video frames. This study introduces rFaceNet, an advanced rPPG method that enhances the extraction of facial BVP signals with a focus on facial contours. rFaceNet integrates identity-specific facial contour information and eliminates redundant data. It efficiently extracts facial contours from temporally normalized frame inputs through a Temporal Compressor Unit (TCU) and steers the model focus to relevant facial regions by using the Cross-Task Feature Combiner (CTFC). Through elaborate training, the quality and interpretability of facial physiological signals extracted by rFaceNet are greatly improved compared to previous methods. Moreover, our novel approach demonstrates superior performance than SOTA methods in various heart rate estimation <b>benchmarks.</b>

{{</citation>}}


## cs.LG (31)



### (1/31 | 92/254) Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly Detection in Dynamic Graphs (Jie Liu et al., 2024)

{{<citation>}}

Jie Liu, Xuequn Shang, Xiaolin Han, Wentao Zhang, Hongzhi Yin. (2024)  
**Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly Detection in Dynamic Graphs**
<br/>
<button class="copy-to-clipboard" title="Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly Detection in Dynamic Graphs" index=92>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 93  
Keywords: Graph Attention Networks, Graph, Graph Embedding, Graph Neural Network, Graph Neural Network, Anomaly Detection, Autoencoder, Convolution, Unsupervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09039v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09039v1.pdf" filename="2403.09039v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Anomaly</b> <b>detection</b> in dynamic <b>graphs</b> <b>presents</b> <b>a</b> significant challenge due to the temporal evolution of <b>graph</b> <b>structures</b> <b>and</b> attributes. The conventional approaches that tackle this problem typically employ an <b>unsupervised</b> <b>learning</b> framework, capturing normality patterns with exclusive normal data during training and identifying deviations as anomalies during testing. However, these methods face critical drawbacks: they either only depend on proxy tasks for general representation without directly pinpointing normal patterns, or they neglect to differentiate between spatial and temporal normality patterns, leading to diminished efficacy in <b>anomaly</b> <b>detection.</b> To address these challenges, we introduce a novel Spatial-Temporal memories-enhanced <b>graph</b> <b>autoencoder</b> <b>(STRIPE).</b> Initially, STRIPE employs <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> and <b>gated</b> temporal <b>convolution</b> layers to extract spatial features and temporal features, respectively. Then STRIPE incorporates separate spatial and temporal memory networks, which capture and store prototypes of normal patterns, thereby preserving the uniqueness of spatial and temporal normality. After that, through a mutual attention mechanism, these stored patterns are then retrieved and integrated with encoded <b>graph</b> <b>embeddings.</b> <b>Finally,</b> the integrated features are fed into the decoder to reconstruct the <b>graph</b> <b>streams</b> <b>which</b> serve as the proxy task for <b>anomaly</b> <b>detection.</b> This comprehensive approach not only minimizes reconstruction errors but also refines the model by emphasizing the compactness and distinctiveness of the embeddings in relation to the nearest memory prototypes. Through extensive testing, STRIPE has demonstrated a superior capability to discern anomalies by effectively leveraging the distinct spatial and temporal dynamics of dynamic <b>graphs,</b> <b>significantly</b> <b>outperforming</b> existing methodologies, with an average improvement of 15.39% on AUC values.

{{</citation>}}


### (2/31 | 93/254) Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference (Muhammad Adnan et al., 2024)

{{<citation>}}

Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J. Nair, Ilya Soloveychik, Purushotham Kamath. (2024)  
**Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference**
<br/>
<button class="copy-to-clipboard" title="Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference" index=93>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: 68U35, I-2-7; C-0, cs-AI, cs-AR, cs-CL, cs-LG, cs.LG  
Keyword Score: 70  
Keywords: GPT, Transformer, Text Generation, Large Language Model, Large Language Model, Prompt, Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09054v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09054v1.pdf" filename="2403.09054v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Transformers</b> have emerged as the underpinning architecture for <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> In generative language models, the inference process involves two primary phases: <b>prompt</b> processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive <b>text</b> <b>generation,</b> both of which are increasingly crucial for <b>LLMs.</b> This paper introduces "Keyformer", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90% of the attention weight in generative inference focuses on a specific subset of tokens, referred to as "key" tokens. Keyformer retains only the key tokens in the KV cache by identifying these crucial tokens using a novel score function. This approach effectively reduces both the KV cache size and memory bandwidth usage without compromising model accuracy. We evaluate Keyformer's performance across three foundational models: <b>GPT-J,</b> Cerebras-GPT, and MPT, which employ various positional embedding algorithms. Our assessment encompasses a variety of tasks, with a particular emphasis on <b>summarization</b> and conversation tasks involving extended contexts. Keyformer's reduction of KV cache reduces inference latency by 2.1x and improves token generation throughput by 2.4x, while preserving the model's accuracy.

{{</citation>}}


### (3/31 | 94/254) Adversarial Fine-tuning of Compressed Neural Networks for Joint Improvement of Robustness and Efficiency (Hallgrimur Thorsteinsson et al., 2024)

{{<citation>}}

Hallgrimur Thorsteinsson, Valdemar J Henriksen, Tong Chen, Raghavendra Selvan. (2024)  
**Adversarial Fine-tuning of Compressed Neural Networks for Joint Improvement of Robustness and Efficiency**
<br/>
<button class="copy-to-clipboard" title="Adversarial Fine-tuning of Compressed Neural Networks for Joint Improvement of Robustness and Efficiency" index=94>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 63  
Keywords: Adversarial Learning, Benchmarking, Fine-tuning, Model Compression, Pruning, Quantization, Adversarial Attack  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09441v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09441v1.pdf" filename="2403.09441v1.pdf">Download PDF</button>

---


**ABSTRACT**  
As deep learning (DL) <b>models</b> <b>are</b> increasingly being integrated into our everyday lives, ensuring their safety by making them robust against <b>adversarial</b> <b>attacks</b> has become increasingly critical. DL <b>models</b> <b>have</b> been found to be susceptible to <b>adversarial</b> <b>attacks</b> which can be achieved by introducing small, targeted perturbations to disrupt the input data. <b>Adversarial</b> <b>training</b> has been presented as a mitigation strategy which can result in more robust <b>models.</b> <b>This</b> <b>adversarial</b> <b>robustness</b> comes with additional computational costs required to design <b>adversarial</b> <b>attacks</b> during training. The two objectives -- <b>adversarial</b> <b>robustness</b> and computational efficiency -- then appear to be in conflict of each other. In this work, we explore the effects of two different <b>model</b> <b>compression</b> methods -- structured weight <b>pruning</b> and <b>quantization</b> -- on <b>adversarial</b> <b>robustness.</b> We specifically explore the effects of <b>fine-tuning</b> on compressed <b>models,</b> <b>and</b> present the trade-off between standard <b>fine-tuning</b> and <b>adversarial</b> <b>fine-tuning.</b> Our results show that compression does not inherently lead to loss in <b>model</b> <b>robustness</b> and <b>adversarial</b> <b>fine-tuning</b> of a compressed <b>model</b> <b>can</b> yield large improvement to the robustness performance of <b>models.</b> <b>We</b> present experiments on two <b>benchmark</b> datasets showing that <b>adversarial</b> <b>fine-tuning</b> of compressed <b>models</b> <b>can</b> achieve robustness performance comparable to adversarially trained <b>models,</b> <b>while</b> also improving computational efficiency.

{{</citation>}}


### (4/31 | 95/254) ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks (Zhaoliang Chen et al., 2024)

{{<citation>}}

Zhaoliang Chen, Zhihao Wu, Ylli Sadikaj, Claudia Plant, Hong-Ning Dai, Shiping Wang, Wenzhong Guo. (2024)  
**ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks**
<br/>
<button class="copy-to-clipboard" title="ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks" index=95>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 56  
Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network, Adversarial Learning, Benchmarking, Stochastic Gradient Descent  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09171v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09171v1.pdf" filename="2403.09171v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Although <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have exhibited the powerful ability to gather <b>graph-structured</b> <b>information</b> <b>from</b> neighborhood nodes via various <b>message-passing</b> mechanisms, the performance of <b>GNNs</b> is limited by poor generalization and fragile robustness caused by noisy and redundant <b>graph</b> <b>data.</b> <b>As</b> a prominent solution, <b>Graph</b> <b>Augmentation</b> <b>Learning</b> (GAL) has recently received increasing attention. Among prior GAL approaches, edge-dropping methods that randomly remove edges from a <b>graph</b> <b>during</b> <b>training</b> are effective techniques to improve the robustness of <b>GNNs.</b> However, randomly dropping edges often results in bypassing critical edges, consequently weakening the effectiveness of message passing. In this paper, we propose a novel <b>adversarial</b> <b>edge-dropping</b> method (ADEdgeDrop) that leverages an <b>adversarial</b> <b>edge</b> predictor guiding the removal of edges, which can be flexibly incorporated into diverse <b>GNN</b> backbones. Employing an <b>adversarial</b> <b>training</b> framework, the edge predictor utilizes the line <b>graph</b> <b>transformed</b> <b>from</b> the original <b>graph</b> <b>to</b> <b>estimate</b> the edges to be dropped, which improves the interpretability of the edge-dropping method. The proposed ADEdgeDrop is optimized alternately by <b>stochastic</b> <b>gradient</b> <b>descent</b> and projected gradient descent. Comprehensive experiments on six <b>graph</b> <b>benchmark</b> <b>datasets</b> demonstrate that the proposed ADEdgeDrop outperforms state-of-the-art baselines across various <b>GNN</b> backbones, demonstrating improved generalization and robustness.

{{</citation>}}


### (5/31 | 96/254) Learning from straggler clients in federated learning (Andrew Hard et al., 2024)

{{<citation>}}

Andrew Hard, Antonious M. Girgis, Ehsan Amid, Sean Augenstein, Lara McConnaughey, Rajiv Mathews, Rohan Anil. (2024)  
**Learning from straggler clients in federated learning**
<br/>
<button class="copy-to-clipboard" title="Learning from straggler clients in federated learning" index=96>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 43  
Keywords: Benchmarking, Federated Learning, Knowledge Distillation, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09086v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09086v1.pdf" filename="2403.09086v1.pdf">Download PDF</button>

---


**ABSTRACT**  
How well do existing <b>federated</b> <b>learning</b> algorithms learn from client devices that return model updates with a significant time delay? Is it even possible to learn effectively from clients that report back minutes, hours, or days after being scheduled? We answer these questions by developing Monte Carlo <b>simulations</b> of client latency that are guided by real-world applications. We study synchronous optimization algorithms like FedAvg and FedAdam as well as the asynchronous FedBuff algorithm, and observe that all these existing approaches struggle to learn from severely delayed clients. To improve upon this situation, we experiment with modifications, including <b>distillation</b> regularization and exponential moving averages of model weights. Finally, we introduce two new algorithms, FARe-DUST and FeAST-on-MSG, based on <b>distillation</b> and averaging, respectively. Experiments with the EMNIST, CIFAR-100, and StackOverflow <b>benchmark</b> <b>federated</b> <b>learning</b> tasks demonstrate that our new algorithms outperform existing ones in terms of accuracy for straggler clients, while also providing better trade-offs between training time and total accuracy.

{{</citation>}}


### (6/31 | 97/254) Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity (Zhuo Zhi et al., 2024)

{{<citation>}}

Zhuo Zhi, Ziquan Liu, Moe Elbadawi, Adam Daneshmend, Mine Orlu, Abdul Basit, Andreas Demosthenous, Miguel Rodrigues. (2024)  
**Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity**
<br/>
<button class="copy-to-clipboard" title="Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity" index=97>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 39  
Keywords: Multi-modal, Multi-modal, Sample Size, Transformer, In-context Learning, In-context Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09428v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09428v1.pdf" filename="2403.09428v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Multimodal</b> machine learning with missing modalities is an increasingly relevant challenge arising in various applications such as healthcare. This paper extends the current research into missing modalities to the low-data regime, i.e., a downstream task has both missing modalities and limited <b>sample</b> <b>size</b> issues. This problem setting is particularly challenging and also practical as it is often expensive to get full-modality data and sufficient annotated training <b>samples.</b> <b>We</b> propose to use retrieval-augmented <b>in-context</b> <b>learning</b> to address these two crucial issues by unleashing the potential of a <b>transformer's</b> <b>in-context</b> <b>learning</b> ability. Diverging from existing methods, which primarily belong to the parametric paradigm and often require sufficient training <b>samples,</b> <b>our</b> work exploits the value of the available full-modality data, offering a novel perspective on resolving the challenge. The proposed data-dependent framework exhibits a higher degree of <b>sample</b> <b>efficiency</b> and is empirically demonstrated to enhance the classification model's performance on both full- and missing-modality data in the low-data regime across various <b>multimodal</b> learning tasks. When only 1% of the training data are available, our proposed method demonstrates an average improvement of 6.1% over a recent strong baseline across various datasets and missing states. Notably, our method also reduces the performance gap between full-modality and missing-modality data compared with the baseline.

{{</citation>}}


### (7/31 | 98/254) EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning (Jongsuk Kim et al., 2024)

{{<citation>}}

Jongsuk Kim, Hyeongkeun Lee, Kyeongha Rho, Junmo Kim, Joon Son Chung. (2024)  
**EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning**
<br/>
<button class="copy-to-clipboard" title="EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning" index=98>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs-MM, cs.LG  
Keyword Score: 38  
Keywords: Benchmarking, Contrastive Learning, Data Augmentation, Representation Learning, Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09502v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09502v1.pdf" filename="2403.09502v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advancements in <b>self-supervised</b> audio-visual <b>representation</b> <b>learning</b> have demonstrated its potential to capture rich and comprehensive <b>representations.</b> <b>However,</b> despite the advantages of <b>data</b> <b>augmentation</b> verified in many learning methods, audio-visual learning has struggled to fully harness these benefits, as augmentations can easily disrupt the correspondence between input pairs. To address this limitation, we introduce EquiAV, a novel framework that leverages equivariance for audio-visual <b>contrastive</b> <b>learning.</b> Our approach begins with extending equivariance to audio-visual learning, facilitated by a shared attention-based transformation predictor. It enables the aggregation of features from diverse augmentations into a representative embedding, providing robust supervision. Notably, this is achieved with minimal computational overhead. Extensive ablation studies and qualitative results verify the effectiveness of our method. EquiAV outperforms previous works across various audio-visual <b>benchmarks.</b>

{{</citation>}}


### (8/31 | 99/254) Towards a theory of model distillation (Enric Boix-Adsera, 2024)

{{<citation>}}

Enric Boix-Adsera. (2024)  
**Towards a theory of model distillation**
<br/>
<button class="copy-to-clipboard" title="Towards a theory of model distillation" index=99>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs-NE, cs.LG  
Keyword Score: 30  
Keywords: Knowledge Distillation, Knowledge Distillation, Model Distillation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09053v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09053v1.pdf" filename="2403.09053v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Distillation</b> is the task of replacing a complicated machine learning <b>model</b> <b>with</b> a simpler <b>model</b> <b>that</b> approximates the original [BCNM06,HVD15]. Despite many practical applications, basic questions about the extent to which <b>models</b> <b>can</b> be <b>distilled,</b> and the runtime and amount of data needed to <b>distill,</b> remain largely open. To study these questions, we initiate a general theory of <b>distillation,</b> defining PAC-distillation in an analogous way to PAC-learning [Val84]. As applications of this theory: (1) we propose new algorithms to extract the knowledge stored in the trained weights of neural networks -- we show how to efficiently <b>distill</b> neural networks into succinct, explicit decision tree representations when possible by using the ``linear representation hypothesis''; and (2) we prove that <b>distillation</b> can be much cheaper than learning from scratch, and make progress on characterizing its complexity.

{{</citation>}}


### (9/31 | 100/254) Soften to Defend: Towards Adversarial Robustness via Self-Guided Label Refinement (Daiwei Yu et al., 2024)

{{<citation>}}

Daiwei Yu, Zhuorong Li, Lina Wei, Canghong Jin, Yun Zhang, Sixian Chan. (2024)  
**Soften to Defend: Towards Adversarial Robustness via Self-Guided Label Refinement**
<br/>
<button class="copy-to-clipboard" title="Soften to Defend: Towards Adversarial Robustness via Self-Guided Label Refinement" index=100>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CR, cs-CV, cs-LG, cs.LG  
Keyword Score: 23  
Keywords: Adversarial Learning, Benchmarking, Adversarial Attack  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09101v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09101v1.pdf" filename="2403.09101v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Adversarial</b> <b>training</b> (AT) is currently one of the most effective ways to obtain the robustness of deep neural networks against <b>adversarial</b> <b>attacks.</b> However, most AT methods suffer from robust overfitting, i.e., a significant generalization gap in <b>adversarial</b> <b>robustness</b> between the training and testing curves. In this paper, we first identify a connection between robust overfitting and the excessive memorization of noisy labels in AT from a view of gradient norm. As such label noise is mainly caused by a distribution mismatch and improper label assignments, we are motivated to propose a label refinement approach for AT. Specifically, our Self-Guided Label Refinement first self-refines a more accurate and informative label distribution from over-confident hard labels, and then it calibrates the training by dynamically incorporating knowledge from self-distilled models into the current model and thus requiring no external teachers. Empirical results demonstrate that our method can simultaneously boost the standard accuracy and robust performance across multiple <b>benchmark</b> datasets, attack types, and architectures. In addition, we also provide a set of analyses from the perspectives of information theory to dive into our method and suggest the importance of soft labels for robust generalization.

{{</citation>}}


### (10/31 | 101/254) Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning (Zhishuai Liu et al., 2024)

{{<citation>}}

Zhishuai Liu, Pan Xu. (2024)  
**Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning" index=101>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG, stat-ML  
Keyword Score: 20  
Keywords: Offline Reinforcement Learning, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09621v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09621v1.pdf" filename="2403.09621v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Distributionally robust <b>offline</b> <b>reinforcement</b> <b>learning</b> (RL), which seeks robust policy training against environment perturbation by modeling dynamics uncertainty, calls for function approximations when facing large state-action spaces. However, the consideration of dynamics uncertainty introduces essential nonlinearity and computational burden, posing unique challenges for analyzing and practically employing function approximation. Focusing on a basic setting where the nominal model and perturbed models are linearly parameterized, we propose minimax optimal and computationally efficient algorithms realizing function approximation and initiate the study on instance-dependent suboptimality analysis in the context of robust <b>offline</b> <b>RL.</b> <b>Our</b> results uncover that function approximation in robust <b>offline</b> <b>RL</b> <b>is</b> essentially distinct from and probably harder than that in standard <b>offline</b> <b>RL.</b> <b>Our</b> algorithms and theoretical results crucially depend on a variety of new techniques, involving a novel function approximation mechanism incorporating variance information, a new procedure of suboptimality and estimation uncertainty decomposition, a quantification of the robust value function shrinkage, and a meticulously designed family of hard instances, which might be of independent interest.

{{</citation>}}


### (11/31 | 102/254) Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training (Yanlai Yang et al., 2024)

{{<citation>}}

Yanlai Yang, Matt Jones, Michael C. Mozer, Mengye Ren. (2024)  
**Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training**
<br/>
<button class="copy-to-clipboard" title="Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training" index=102>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CL, cs-LG, cs.LG  
Keyword Score: 20  
Keywords: Fine-tuning, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09613v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09613v1.pdf" filename="2403.09613v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We explore the training dynamics of neural networks in a structured non-IID setting where documents are presented cyclically in a fixed, repeated sequence. Typically, networks suffer from catastrophic interference when training on a sequence of documents; however, we discover a curious and remarkable property of <b>LLMs</b> <b>fine-tuned</b> sequentially in this setting: they exhibit anticipatory behavior, recovering from the forgetting on documents before encountering them again. The behavior emerges and becomes more robust as the architecture scales up its number of parameters. Through comprehensive experiments and visualizations, we uncover new insights into training over-parameterized networks in structured environments.

{{</citation>}}


### (12/31 | 103/254) Self-Consistency Training for Hamiltonian Prediction (He Zhang et al., 2024)

{{<citation>}}

He Zhang, Chang Liu, Zun Wang, Xinran Wei, Siyuan Liu, Nanning Zheng, Bin Shao, Tie-Yan Liu. (2024)  
**Self-Consistency Training for Hamiltonian Prediction**
<br/>
<button class="copy-to-clipboard" title="Self-Consistency Training for Hamiltonian Prediction" index=103>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, physics-chem-ph, q-bio-BM  
Keyword Score: 20  
Keywords: Out-of-distribution, Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09560v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09560v1.pdf" filename="2403.09560v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Hamiltonian prediction is a versatile formulation to leverage machine learning for solving molecular science problems. Yet, its applicability is limited by insufficient labeled data for training. In this work, we highlight that Hamiltonian prediction possesses a self-consistency principle, based on which we propose an exact training method that does not require labeled data. This merit addresses the data scarcity difficulty, and distinguishes the task from other property prediction formulations with unique benefits: (1) self-consistency training enables the model to be trained on a large amount of unlabeled data, hence substantially enhances generalization; (2) self-consistency training is more efficient than labeling data with DFT for <b>supervised</b> training, since it is an amortization of DFT calculation over a set of molecular structures. We empirically demonstrate the better generalization in data-scarce and <b>out-of-distribution</b> scenarios, and the better efficiency from the amortization. These benefits push forward the applicability of Hamiltonian prediction to an ever larger scale.

{{</citation>}}


### (13/31 | 104/254) On using Machine Learning Algorithms for Motorcycle Collision Detection (Philipp Rodegast et al., 2024)

{{<citation>}}

Philipp Rodegast, Steffen Maier, Jonas Kneifl, Jörg Fehr. (2024)  
**On using Machine Learning Algorithms for Motorcycle Collision Detection**
<br/>
<button class="copy-to-clipboard" title="On using Machine Learning Algorithms for Motorcycle Collision Detection" index=104>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, math-DS  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09491v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09491v1.pdf" filename="2403.09491v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Globally, motorcycles attract vast and varied users. However, since the rate of severe injury and fatality in motorcycle accidents far exceeds passenger car accidents, efforts have been directed toward increasing passive safety systems. Impact <b>simulations</b> show that the risk of severe injury or death in the event of a motorcycle-to-car impact can be greatly reduced if the motorcycle is equipped with passive safety measures such as airbags and seat belts. For the passive safety systems to be activated, a collision must be detected within milliseconds for a wide variety of impact configurations, but under no circumstances may it be falsely triggered. For the challenge of reliably detecting impending collisions, this paper presents an investigation towards the applicability of machine learning algorithms. First, a series of <b>simulations</b> of accidents and driving operation is introduced to collect data to train machine learning classification models. Their performance is henceforth assessed and compared via multiple representative and application-oriented criteria.

{{</citation>}}


### (14/31 | 105/254) Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks (Yuncheng Huang et al., 2024)

{{<citation>}}

Yuncheng Huang, Qianyu He, Yipei Xu, Jiaqing Liang, Yanghua Xiao. (2024)  
**Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks**
<br/>
<button class="copy-to-clipboard" title="Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks" index=105>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CL, cs-LG, cs.LG  
Keyword Score: 20  
Keywords: Curriculum Learning, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09479v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09479v1.pdf" filename="2403.09479v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Current language models have demonstrated their capability to develop basic <b>reasoning,</b> but struggle in more complicated <b>reasoning</b> tasks that require a combination of atomic skills, such as math word problem requiring skills like arithmetic and unit conversion. Previous methods either do not improve the inherent atomic skills of models or not attempt to generalize the atomic skills to complex <b>reasoning</b> tasks. In this paper, we first propose a probing framework to investigate whether the atomic skill can spontaneously generalize to complex <b>reasoning</b> tasks. Then, we introduce a hierarchical <b>curriculum</b> <b>learning</b> training strategy to achieve better skill generalization. In our experiments, we find that atomic skills can not spontaneously generalize to compositional tasks. By leveraging hierarchical <b>curriculum</b> <b>learning,</b> we successfully induce generalization, significantly improve the performance of open-source LMs on complex <b>reasoning</b> tasks. Promisingly, the skill generalization exhibit effective in cross-dataset and cross-domain scenarios. Complex <b>reasoning</b> can also help enhance atomic skills. Our findings offer valuable guidance for designing better training strategies for complex <b>reasoning</b> tasks.

{{</citation>}}


### (15/31 | 106/254) Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision (Zhiqing Sun et al., 2024)

{{<citation>}}

Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, Chuang Gan. (2024)  
**Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision**
<br/>
<button class="copy-to-clipboard" title="Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision" index=106>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CL, cs-LG, cs.LG  
Keyword Score: 20  
Keywords: Reinforcement Learning, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09472v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09472v1.pdf" filename="2403.09472v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard <b>reasoning</b> tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as \textit{easy-to-hard generalization}. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervised reward models on easy problems (e.g., level 1-3), and then uses them to evaluate the performance of policy models on hard problems. We show that such \textit{easy-to-hard generalization from evaluators} can enable \textit{easy-to-hard generalizations in generators} either through re-ranking or <b>reinforcement</b> <b>learning</b> (RL). Notably, our process-supervised 7b RL model achieves an accuracy of 34.0\% on MATH500, despite only using human supervision on easy problems. Our approach suggests a promising path toward AI systems that advance beyond the frontier of human supervision.

{{</citation>}}


### (16/31 | 107/254) Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk (Zhangheng Li et al., 2024)

{{<citation>}}

Zhangheng Li, Junyuan Hong, Bo Li, Zhangyang Wang. (2024)  
**Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk**
<br/>
<button class="copy-to-clipboard" title="Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk" index=107>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CR, cs-LG, cs.LG  
Keyword Score: 20  
Keywords: Diffusion Model, Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09450v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09450v1.pdf" filename="2403.09450v1.pdf">Download PDF</button>

---


**ABSTRACT**  
While <b>diffusion</b> <b>models</b> have recently demonstrated remarkable progress in generating realistic images, privacy risks also arise: published models or APIs could generate training images and thus leak privacy-sensitive training information. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that <b>fine-tuning</b> the pre-trained models with manipulated data can amplify the existing privacy risks. We demonstrate that S2L could occur in various standard <b>fine-tuning</b> strategies for <b>diffusion</b> <b>models,</b> including concept-injection methods (DreamBooth and Textual Inversion) and parameter-efficient methods (LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L can amplify the state-of-the-art membership inference attack (MIA) on <b>diffusion</b> <b>models</b> by $5.4\%$ (absolute difference) AUC and can increase extracted private samples from almost $0$ samples to $16.3$ samples on average per target domain. This discovery underscores that the privacy risk with <b>diffusion</b> <b>models</b> is even more severe than previously recognized. Codes are available at https://github.com/VITA-Group/Shake-to-Leak.

{{</citation>}}


### (17/31 | 108/254) Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective (Yu Cai et al., 2024)

{{<citation>}}

Yu Cai, Hao Chen, Kwang-Ting Cheng. (2024)  
**Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective**
<br/>
<button class="copy-to-clipboard" title="Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective" index=108>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CV, cs-LG, cs.LG  
Keyword Score: 20  
Keywords: Anomaly Detection, Autoencoder  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09303v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09303v1.pdf" filename="2403.09303v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Medical <b>anomaly</b> <b>detection</b> aims to identify abnormal findings using only normal training data, playing a crucial role in health screening and recognizing rare diseases. Reconstruction-based methods, particularly those utilizing <b>autoencoders</b> (AEs), are dominant in this field. They work under the assumption that AEs trained on only normal data cannot reconstruct unseen abnormal regions well, thereby enabling the <b>anomaly</b> <b>detection</b> based on reconstruction errors. However, this assumption does not always hold due to the mismatch between the reconstruction training objective and the <b>anomaly</b> <b>detection</b> task objective, rendering these methods theoretically unsound. This study focuses on providing a theoretical foundation for AE-based reconstruction methods in <b>anomaly</b> <b>detection.</b> By leveraging information theory, we elucidate the principles of these methods and reveal that the key to improving AE in <b>anomaly</b> <b>detection</b> lies in minimizing the information entropy of latent vectors. Experiments on four datasets with two image modalities validate the effectiveness of our theory. To the best of our knowledge, this is the first effort to theoretically clarify the principles and design philosophy of AE for <b>anomaly</b> <b>detection.</b> Code will be available upon acceptance.

{{</citation>}}


### (18/31 | 109/254) SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning (Nicholas Zolman et al., 2024)

{{<citation>}}

Nicholas Zolman, Urban Fasel, J. Nathan Kutz, Steven L. Brunton. (2024)  
**SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning" index=109>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs-SY, cs.LG, eess-SY, math-DS, math-OC  
Keyword Score: 18  
Keywords: Benchmarking, Black Box, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09110v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09110v1.pdf" filename="2403.09110v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Deep <b>reinforcement</b> <b>learning</b> (DRL) has shown significant promise for uncovering sophisticated control policies that interact in environments with complicated dynamics, such as stabilizing the magnetohydrodynamics of a tokamak fusion reactor or minimizing the drag force exerted on an object in a fluid flow. However, these algorithms require an abundance of training examples and may become prohibitively expensive for many applications. In addition, the reliance on deep neural networks often results in an uninterpretable, <b>black-box</b> <b>policy</b> that may be too computationally expensive to use with certain embedded systems. Recent advances in sparse dictionary learning, such as the sparse identification of nonlinear dynamics (SINDy), have shown promise for creating efficient and interpretable data-driven models in the low-data regime. In this work we introduce SINDy-RL, a unifying framework for combining SINDy and DRL to create efficient, interpretable, and trustworthy representations of the dynamics model, reward function, and control policy. We demonstrate the effectiveness of our approaches on <b>benchmark</b> control environments and challenging fluids problems. SINDy-RL achieves comparable performance to state-of-the-art DRL algorithms using significantly fewer interactions in the environment and results in an interpretable control policy orders of magnitude smaller than a deep neural network policy.

{{</citation>}}


### (19/31 | 110/254) Hyperparameters in Continual Learning: a Reality Check (Sungmin Cha et al., 2024)

{{<citation>}}

Sungmin Cha, Kyunghyun Cho. (2024)  
**Hyperparameters in Continual Learning: a Reality Check**
<br/>
<button class="copy-to-clipboard" title="Hyperparameters in Continual Learning: a Reality Check" index=110>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CV, cs-LG, cs.LG  
Keyword Score: 13  
Keywords: Benchmarking, Continual Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09066v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09066v1.pdf" filename="2403.09066v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Various algorithms for <b>continual</b> <b>learning</b> (CL) have been designed with the goal of effectively alleviating the trade-off between stability and plasticity during the CL process. To achieve this goal, tuning appropriate hyperparameters for each algorithm is essential. As an evaluation protocol, it has been common practice to train a CL algorithm using diverse hyperparameter values on a CL scenario constructed with a <b>benchmark</b> dataset. Subsequently, the best performance attained with the optimal hyperparameter value serves as the criterion for evaluating the CL algorithm. In this paper, we contend that this evaluation protocol is not only impractical but also incapable of effectively assessing the CL capability of a CL algorithm. Returning to the fundamental principles of model evaluation in machine learning, we propose an evaluation protocol that involves Hyperparameter Tuning and Evaluation phases. Those phases consist of different datasets but share the same CL scenario. In the Hyperparameter Tuning phase, each algorithm is iteratively trained with different hyperparameter values to find the optimal hyperparameter values. Subsequently, in the Evaluation phase, the optimal hyperparameter values is directly applied for training each algorithm, and their performance in the Evaluation phase serves as the criterion for evaluating them. Through experiments on CIFAR-100 and ImageNet-100 based on the proposed protocol in class-incremental learning, we not only observed that the existing evaluation method fail to properly assess the CL capability of each algorithm but also observe that some recently proposed state-of-the-art algorithms, which reported superior performance, actually exhibit inferior performance compared to the previous algorithm.

{{</citation>}}


### (20/31 | 111/254) Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains (Lei Wang et al., 2024)

{{<citation>}}

Lei Wang, Jieming Bian, Letian Zhang, Chen Chen, Jie Xu. (2024)  
**Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains**
<br/>
<button class="copy-to-clipboard" title="Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains" index=111>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CV, cs-LG, cs.LG  
Keyword Score: 13  
Keywords: Clustering, Federated Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09048v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09048v1.pdf" filename="2403.09048v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Federated</b> <b>learning</b> (FL) allows collaborative machine learning training without sharing private data. While most FL methods assume identical data domains across clients, real-world scenarios often involve heterogeneous data domains. <b>Federated</b> <b>Prototype</b> Learning (FedPL) addresses this issue, using mean feature vectors as prototypes to enhance model generalization. However, existing FedPL methods create the same number of prototypes for each client, leading to cross-domain performance gaps and disparities for clients with varied data distributions. To mitigate cross-domain feature representation variance, we introduce FedPLVM, which establishes variance-aware dual-level prototypes <b>clustering</b> and employs a novel $\alpha$-sparsity prototype loss. The dual-level prototypes <b>clustering</b> strategy creates local clustered prototypes based on private data features, then performs global prototypes <b>clustering</b> to reduce communication complexity and preserve local data privacy. The $\alpha$-sparsity prototype loss aligns samples from underrepresented domains, enhancing intra-class similarity and reducing inter-class similarity. Evaluations on Digit-5, Office-10, and DomainNet datasets demonstrate our method's superiority over existing approaches.

{{</citation>}}


### (21/31 | 112/254) Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields (Yi-Lun Liao et al., 2024)

{{<citation>}}

Yi-Lun Liao, Tess Smidt, Abhishek Das. (2024)  
**Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields**
<br/>
<button class="copy-to-clipboard" title="Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields" index=112>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG, physics-chem-ph, physics-comp-ph  
Keyword Score: 10  
Keywords: Node Embedding  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09549v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09549v1.pdf" filename="2403.09549v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Understanding the interactions of atoms such as forces in 3D atomistic systems is fundamental to many applications like molecular dynamics and catalyst design. However, simulating these interactions requires compute-intensive ab initio calculations and thus results in limited data for training neural networks. In this paper, we propose to use denoising non-equilibrium structures (DeNS) as an auxiliary task to better leverage training data and improve performance. For training with DeNS, we first corrupt a 3D structure by adding noise to its 3D coordinates and then predict the noise. Different from previous works on denoising, which are limited to equilibrium structures, the proposed method generalizes denoising to a much larger set of non-equilibrium structures. The main difference is that a non-equilibrium structure does not correspond to local energy minima and has non-zero forces, and therefore it can have many possible atomic positions compared to an equilibrium structure. This makes denoising non-equilibrium structures an ill-posed problem since the target of denoising is not uniquely defined. Our key insight is to additionally encode the forces of the original non-equilibrium structure to specify which non-equilibrium structure we are denoising. Concretely, given a corrupted non-equilibrium structure and the forces of the original one, we predict the non-equilibrium structure satisfying the input forces instead of any arbitrary structures. Since DeNS requires encoding forces, DeNS favors equivariant networks, which can easily incorporate forces and other higher-order tensors in <b>node</b> <b>embeddings.</b> We study the effectiveness of training equivariant networks with DeNS on OC20, OC22 and MD17 datasets and demonstrate that DeNS can achieve new state-of-the-art results on OC20 and OC22 and significantly improve training efficiency on MD17.

{{</citation>}}


### (22/31 | 113/254) A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning (Nawazish Ali et al., 2024)

{{<citation>}}

Nawazish Ali, Abdul Wahid, Rachael Shaw, Karl Mason. (2024)  
**A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning**
<br/>
<button class="copy-to-clipboard" title="A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning" index=113>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09499v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09499v1.pdf" filename="2403.09499v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Dairy farming consumes a significant amount of energy, making it an energy-intensive sector within agriculture. Integrating renewable energy generation into dairy farming could help address this challenge. Effective battery management is important for integrating renewable energy generation. Managing battery charging and discharging poses significant challenges because of fluctuations in electrical consumption, the intermittent nature of renewable energy generation, and fluctuations in energy prices. Artificial Intelligence (AI) has the potential to significantly improve the use of renewable energy in dairy farming, however, there is limited research conducted in this particular domain. This research considers Ireland as a case study as it works towards attaining its 2030 energy strategy centered on the utilization of renewable sources. This study proposes a Q-learning-based algorithm for scheduling battery charging and discharging in a dairy farm setting. This research also explores the effect of the proposed algorithm by adding wind generation data and considering additional case studies. The proposed algorithm reduces the cost of imported electricity from the grid by 13.41\%, peak demand by 2\%, and 24.49\% when utilizing wind generation. These results underline how <b>reinforcement</b> <b>learning</b> is highly effective in managing batteries in the dairy farming sector.

{{</citation>}}


### (23/31 | 114/254) DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning (Xu Yang et al., 2024)

{{<citation>}}

Xu Yang, Jiyuan Feng, Songyue Guo, Ye Wang, Ye Ding, Binxing Fang, Qing Liao. (2024)  
**DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning**
<br/>
<button class="copy-to-clipboard" title="DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning" index=114>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-DC, cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Federated Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09284v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09284v1.pdf" filename="2403.09284v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Personalized <b>federated</b> <b>learning</b> becomes a hot research topic that can learn a personalized learning model for each client. Existing personalized <b>federated</b> <b>learning</b> models prefer to aggregate similar clients with similar data distribution to improve the performance of learning models. However, similaritybased personalized <b>federated</b> <b>learning</b> methods may exacerbate the class imbalanced problem. In this paper, we propose a novel Dynamic Affinity-based Personalized <b>Federated</b> <b>Learning</b> model (DA-PFL) to alleviate the class imbalanced problem during <b>federated</b> <b>learning.</b> Specifically, we build an affinity metric from a complementary perspective to guide which clients should be aggregated. Then we design a dynamic aggregation strategy to dynamically aggregate clients based on the affinity metric in each round to reduce the class imbalanced risk. Extensive experiments show that the proposed DA-PFL model can significantly improve the accuracy of each client in three real-world datasets with state-of-the-art comparison methods.

{{</citation>}}


### (24/31 | 115/254) Uncertainty Quantification for cross-subject Motor Imagery classification (Prithviraj Manivannan et al., 2024)

{{<citation>}}

Prithviraj Manivannan, Ivo Pascal de Jong, Matias Valdenegro-Toro, Andreea Ioana Sburlea. (2024)  
**Uncertainty Quantification for cross-subject Motor Imagery classification**
<br/>
<button class="copy-to-clipboard" title="Uncertainty Quantification for cross-subject Motor Imagery classification" index=115>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CV, cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09228v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09228v1.pdf" filename="2403.09228v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Uncertainty Quantification aims to determine when the prediction from a Machine Learning model is likely to be wrong. Computer Vision research has explored methods for determining epistemic uncertainty (also known as model uncertainty), which should correspond with generalisation error. These methods theoretically allow to predict misclassifications due to inter-subject variability. We applied a variety of Uncertainty Quantification methods to predict misclassifications for a Motor Imagery Brain Computer Interface. Deep Ensembles performed best, both in terms of classification performance and cross-subject Uncertainty Quantification performance. However, we found that standard <b>CNNs</b> with Softmax output performed better than some of the more advanced methods.

{{</citation>}}


### (25/31 | 116/254) MCformer: Multivariate Time Series Forecasting with Mixed-Channels Transformer (Wenyong Han et al., 2024)

{{<citation>}}

Wenyong Han, Tao Zhu Member, Liming Chen, Huansheng Ning, Yang Luo, Yaping Wan. (2024)  
**MCformer: Multivariate Time Series Forecasting with Mixed-Channels Transformer**
<br/>
<button class="copy-to-clipboard" title="MCformer: Multivariate Time Series Forecasting with Mixed-Channels Transformer" index=116>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, eess-SP  
Keyword Score: 10  
Keywords: Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09223v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09223v1.pdf" filename="2403.09223v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The massive generation of time-series data by largescale Internet of Things (IoT) devices necessitates the exploration of more effective models for multivariate time-series forecasting. In previous models, there was a predominant use of the Channel Dependence (CD) strategy (where each channel represents a univariate sequence). Current state-of-the-art (SOTA) models primarily rely on the Channel Independence (CI) strategy. The CI strategy treats all channels as a single channel, expanding the dataset to improve generalization performance and avoiding inter-channel correlation that disrupts long-term features. However, the CI strategy faces the challenge of interchannel correlation forgetting. To address this issue, we propose an innovative Mixed Channels strategy, combining the data expansion advantages of the CI strategy with the ability to counteract inter-channel correlation forgetting. Based on this strategy, we introduce MCformer, a multivariate time-series forecasting model with mixed channel features. The model blends a specific number of channels, leveraging an attention mechanism to effectively capture inter-channel correlation information when modeling long-term features. Experimental results demonstrate that the Mixed Channels strategy outperforms pure CI strategy in multivariate time-series forecasting tasks.

{{</citation>}}


### (26/31 | 117/254) On the Laplace Approximation as Model Selection Criterion for Gaussian Processes (Andreas Besginow et al., 2024)

{{<citation>}}

Andreas Besginow, Jan David Hüwel, Thomas Pawellek, Christian Beecks, Markus Lange-Hegermann. (2024)  
**On the Laplace Approximation as Model Selection Criterion for Gaussian Processes**
<br/>
<button class="copy-to-clipboard" title="On the Laplace Approximation as Model Selection Criterion for Gaussian Processes" index=117>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG, stat-ML  
Keyword Score: 10  
Keywords: Gaussian Process  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09215v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09215v1.pdf" filename="2403.09215v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Model selection aims to find the best model in terms of accuracy, interpretability or simplicity, preferably all at once. In this work, we focus on evaluating model performance of <b>Gaussian</b> <b>process</b> models, i.e. finding a metric that provides the best trade-off between all those criteria. While previous work considers metrics like the likelihood, AIC or dynamic nested sampling, they either lack performance or have significant runtime issues, which severely limits applicability. We address these challenges by introducing multiple metrics based on the Laplace approximation, where we overcome a severe inconsistency occuring during naive application of the Laplace approximation. Experiments show that our metrics are comparable in quality to the gold standard dynamic nested sampling without compromising for computational speed. Our model selection criteria allow significantly faster and high quality model selection of <b>Gaussian</b> <b>process</b> models.

{{</citation>}}


### (27/31 | 118/254) Design of an basis-projected layer for sparse datasets in deep learning training using gc-ms spectra as a case study (Yu Tang Chang et al., 2024)

{{<citation>}}

Yu Tang Chang, Shih Fang Chen. (2024)  
**Design of an basis-projected layer for sparse datasets in deep learning training using gc-ms spectra as a case study**
<br/>
<button class="copy-to-clipboard" title="Design of an basis-projected layer for sparse datasets in deep learning training using gc-ms spectra as a case study" index=118>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: 68-06, I-2-4; J-2, cs-LG, cs.LG, eess-SP  
Keyword Score: 10  
Keywords: Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09188v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09188v1.pdf" filename="2403.09188v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Deep learning (DL) models encompass millions or even billions of parameters and learn complex patterns from big data. However, not all data are initially stored in a suitable formation to effectively train a DL model, e.g., gas chromatography-mass spectrometry (GC-MS) spectra and DNA sequence. These datasets commonly contain many zero values, and the sparse data formation causes difficulties in optimizing DL models. A DL module called the basis-projected layer (BPL) was proposed to mitigate the issue by transforming the sparse data into a dense representation. The transformed data is expected to facilitate the gradient calculation and <b>finetuned</b> process in a DL training process. The dataset, example of a sparse dataset, contained 362 specialty coffee odorant spectra detected from GC-MS. The BPL layer was placed at the beginning of the DL model. The tunable parameters in the layer were learnable projected axes that were the bases of a new representation space. The layer rotated these bases when its parameters were updated. When the number of the bases was the same as the original dimension, the increasing percentage of the F1 scores was 8.56%. Furthermore, when the number was set as 768 (the original dimension was 490), the increasing percentage of the F1 score was 11.49%. The layer not only maintained the model performance and even constructed a better representation space in analyzing sparse datasets.

{{</citation>}}


### (28/31 | 119/254) DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers (Xiao Ma et al., 2024)

{{<citation>}}

Xiao Ma, Shengfeng He, Hezhe Qiao, Dong Ma. (2024)  
**DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers**
<br/>
<button class="copy-to-clipboard" title="DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers" index=119>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Emotion Recognition  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09035v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09035v1.pdf" filename="2403.09035v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Enabling efficient and accurate deep neural network (DNN) inference on microcontrollers is non-trivial due to the constrained on-chip resources. Current methodologies primarily focus on compressing larger models yet at the expense of model accuracy. In this paper, we rethink the problem from the inverse perspective by constructing small/weak models directly and improving their accuracy. Thus, we introduce DiTMoS, a novel DNN training and inference framework with a selector-classifiers architecture, where the selector routes each input sample to the appropriate classifier for classification. DiTMoS is grounded on a key insight: a composition of weak models can exhibit high diversity and the union of them can significantly boost the accuracy upper bound. To approach the upper bound, DiTMoS introduces three strategies including diverse training data splitting to increase the classifiers' diversity, adversarial selector-classifiers training to ensure synergistic interactions thereby maximizing their complementarity, and heterogeneous feature aggregation to improve the capacity of classifiers. We further propose a network slicing technique to alleviate the extra memory overhead incurred by feature aggregation. We deploy DiTMoS on the Neucleo STM32F767ZI board and evaluate it based on three time-series datasets for human activity recognition, keywords spotting, and <b>emotion</b> <b>recognition,</b> respectively. The experiment results manifest that: (a) DiTMoS achieves up to 13.4% accuracy improvement compared to the best baseline; (b) network slicing almost completely eliminates the memory overhead incurred by feature aggregation with a marginal increase of latency.

{{</citation>}}


### (29/31 | 120/254) S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering (Zhen Long et al., 2024)

{{<citation>}}

Zhen Long, Qiyuan Wang, Yazhou Ren, Yipeng Liu, Ce Zhu. (2024)  
**S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering**
<br/>
<button class="copy-to-clipboard" title="S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering" index=120>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CV, cs-LG, cs.LG  
Keyword Score: 6  
Keywords: Graph, Clustering  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09107v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09107v1.pdf" filename="2403.09107v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Anchor-based large-scale multi-view <b>clustering</b> has attracted considerable attention for its effectiveness in handling massive datasets. However, current methods mainly seek the consensus embedding feature for <b>clustering</b> by exploring global correlations between anchor <b>graphs</b> or projection matrices.In this paper, we propose a simple yet efficient scalable multi-view tensor <b>clustering</b> (S^2MVTC) approach, where our focus is on learning correlations of embedding features within and across views. Specifically, we first construct the embedding feature tensor by stacking the embedding features of different views into a tensor and rotating it. Additionally, we build a novel tensor low-frequency approximation (TLFA) operator, which incorporates <b>graph</b> similarity into embedding feature learning, efficiently achieving smooth representation of embedding features within different views. Furthermore, consensus constraints are applied to embedding features to ensure inter-view semantic consistency. Experimental results on six large-scale multi-view datasets demonstrate that S^2MVTC significantly outperforms state-of-the-art algorithms in terms of <b>clustering</b> performance and CPU execution time, especially when handling massive data. The code of S^2MVTC is publicly available at https://github.com/longzhen520/S2MVTC.

{{</citation>}}


### (30/31 | 121/254) Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search (Yunchuan Zhang et al., 2024)

{{<citation>}}

Yunchuan Zhang, Sangwoo Park, Osvaldo Simeone. (2024)  
**Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search**
<br/>
<button class="copy-to-clipboard" title="Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search" index=121>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-IT, cs-LG, cs.LG, eess-SP, math-IT  
Keyword Score: 5  
Keywords: Black Box  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09570v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09570v1.pdf" filename="2403.09570v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In many applications, ranging from logistics to engineering, a designer is faced with a sequence of optimization tasks for which the objectives are in the form of <b>black-box</b> <b>functions</b> that are costly to evaluate. For example, the designer may need to tune the hyperparameters of neural network models for different learning tasks over time. Rather than evaluating the objective function for each candidate solution, the designer may have access to approximations of the objective functions, for which higher-fidelity evaluations entail a larger cost. Existing multi-fidelity <b>black-box</b> <b>optimization</b> strategies select candidate solutions and fidelity levels with the goal of maximizing the information accrued about the optimal value or solution for the current task. Assuming that successive optimization tasks are related, this paper introduces a novel information-theoretic acquisition function that balances the need to acquire information about the current task with the goal of collecting information transferable to future tasks. The proposed method includes shared inter-task latent variables, which are transferred across tasks by implementing particle-based variational Bayesian updates. Experimental results across synthetic and real-world examples reveal that the proposed provident acquisition strategy that caters to future tasks can significantly improve the optimization efficiency as soon as a sufficient number of tasks is processed.

{{</citation>}}


### (31/31 | 122/254) Recursive Causal Discovery (Ehsan Mokhtarian et al., 2024)

{{<citation>}}

Ehsan Mokhtarian, Sepehr Elahi, Sina Akbari, Negar Kiyavash. (2024)  
**Recursive Causal Discovery**
<br/>
<button class="copy-to-clipboard" title="Recursive Causal Discovery" index=122>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, stat-ML  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09300v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09300v1.pdf" filename="2403.09300v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Causal discovery, i.e., learning the causal <b>graph</b> from data, is often the first step toward the identification and estimation of causal effects, a key requirement in numerous scientific domains. Causal discovery is hampered by two main challenges: limited data results in errors in statistical testing and the computational complexity of the learning task is daunting. This paper builds upon and extends four of our prior publications (Mokhtarian et al., 2021; Akbari et al., 2021; Mokhtarian et al., 2022, 2023a). These works introduced the concept of removable variables, which are the only variables that can be removed recursively for the purpose of causal discovery. Presence and identification of removable variables allow recursive approaches for causal discovery, a promising solution that helps to address the aforementioned challenges by reducing the problem size successively. This reduction not only minimizes conditioning sets in each conditional independence (CI) test, leading to fewer errors but also significantly decreases the number of required CI tests. The worst-case performances of these methods nearly match the lower bound. In this paper, we present a unified framework for the proposed algorithms, refined with additional details and enhancements for a coherent presentation. A comprehensive literature review is also included, comparing the computational complexity of our methods with existing approaches, showcasing their state-of-the-art efficiency. Another contribution of this paper is the release of RCD, a Python package that efficiently implements these algorithms. This package is designed for practitioners and researchers interested in applying these methods in practical scenarios. The package is available at github.com/ban-epfl/rcd, with comprehensive documentation provided at rcdpackage.com.

{{</citation>}}


## cs.CL (32)



### (1/32 | 123/254) MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation (Jiahuan Li et al., 2024)

{{<citation>}}

Jiahuan Li, Shanbo Cheng, Shujian Huang, Jiajun Chen. (2024)  
**MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation**
<br/>
<button class="copy-to-clipboard" title="MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation" index=123>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 83  
Keywords: Benchmarking, Fine-tuning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Neural Machine Translation, Neural Machine Translation, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09522v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09522v1.pdf" filename="2403.09522v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> have demonstrated their strong ability in the field of <b>machine</b> <b>translation</b> <b>(MT),</b> yet they suffer from high computational cost and latency. Therefore, transferring translation <b>knowledge</b> <b>from</b> giant <b>LLMs</b> to medium-sized <b>machine</b> <b>translation</b> models is a promising research direction. However, traditional <b>knowledge</b> <b>distillation</b> methods do not take the capability of student and teacher models into consideration, therefore repeatedly teaching student models on the <b>knowledge</b> <b>they</b> have learned, and failing to extend to novel contexts and <b>knowledge.</b> <b>In</b> this paper, we propose a framework called <b>MT-Patcher,</b> which transfers <b>knowledge</b> <b>from</b> <b>LLMs</b> to existing <b>MT</b> models in a selective, comprehensive and proactive manner. Considering the current translation ability of student <b>MT</b> models, we only identify and correct their translation errors, instead of <b>distilling</b> the whole translation from the teacher. Leveraging the strong language abilities of <b>LLMs,</b> we instruct <b>LLM</b> teachers to synthesize diverse contexts and anticipate more potential errors for the student. Experiment results on translating both specific language phenomena and general <b>MT</b> <b>benchmarks</b> demonstrate that <b>finetuning</b> the student <b>MT</b> model on about 10% examples can achieve comparable results to the traditional <b>knowledge</b> <b>distillation</b> method, and synthesized potential errors and diverse contexts further improve translation performances on unseen contexts and words.

{{</citation>}}


### (2/32 | 124/254) TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic Tasks (Viktor Moskvoretskii et al., 2024)

{{<citation>}}

Viktor Moskvoretskii, Ekaterina Neminova, Alina Lobanova, Alexander Panchenko, Irina Nikishina. (2024)  
**TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic Tasks**
<br/>
<button class="copy-to-clipboard" title="TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic Tasks" index=124>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 80  
Keywords: Few-shot, Few-shot Learning, Fine-tuning, Quantization, Zero-shot, LLaMA, Domain Adaptation, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09207v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09207v1.pdf" filename="2403.09207v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we explore the capabilities of <b>LLMs</b> in capturing lexical-semantic knowledge from WordNet on the example of the <b>LLaMA-2-7b</b> model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present TaxoLLaMA, the everything-in-one model, lightweight due to 4-bit <b>quantization</b> and LoRA. It achieves 11 SotA results, 4 top-2 results out of 16 tasks for the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates very strong <b>zero-shot</b> performance on Lexical Entailment and Taxonomy Construction with no <b>fine-tuning.</b> We also explore its hidden multilingual and <b>domain</b> <b>adaptation</b> capabilities with a little tuning or <b>few-shot</b> <b>learning.</b> All datasets, code, and model are available online at https://github.com/VityaVitalich/TaxoLLaMA

{{</citation>}}


### (3/32 | 125/254) Evaluating LLMs for Gender Disparities in Notable Persons (Lauren Rhue et al., 2024)

{{<citation>}}

Lauren Rhue, Sofie Goethals, Arun Sundararajan. (2024)  
**Evaluating LLMs for Gender Disparities in Notable Persons**
<br/>
<button class="copy-to-clipboard" title="Evaluating LLMs for Gender Disparities in Notable Persons" index=125>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-IR, cs.CL  
Keyword Score: 80  
Keywords: Fairness, GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09148v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09148v1.pdf" filename="2403.09148v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This study examines the use of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for retrieving factual information, addressing concerns over their propensity to produce factually incorrect "hallucinated" responses or to altogether decline to even answer <b>prompt</b> at all. Specifically, it investigates the presence of gender-based biases in <b>LLMs'</b> responses to factual inquiries. This paper takes a multi-pronged approach to evaluating <b>GPT</b> models by evaluating <b>fairness</b> across multiple dimensions of recall, hallucinations and declinations. Our findings reveal discernible gender disparities in the responses generated by <b>GPT-3.5.</b> While advancements in <b>GPT-4</b> have led to improvements in performance, they have not fully eradicated these gender disparities, notably in instances where responses are declined. The study further explores the origins of these disparities by examining the influence of gender associations in <b>prompts</b> and the homogeneity in the responses.

{{</citation>}}


### (4/32 | 126/254) ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning (Ahmed Masry et al., 2024)

{{<citation>}}

Ahmed Masry, Mehrad Shahmohammadi, Md Rizwan Parvez, Enamul Hoque, Shafiq Joty. (2024)  
**ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning**
<br/>
<button class="copy-to-clipboard" title="ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning" index=126>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 80  
Keywords: Fine-tuning, Instruction Following, Question Answering, Reasoning, Instruction Tuning, Large Language Model, Summarization, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09028v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09028v1.pdf" filename="2403.09028v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Charts provide visual representations of data and are widely used for analyzing information, addressing queries, and conveying insights to others. Various chart-related downstream tasks have emerged recently, such as <b>question-answering</b> <b>and</b> <b>summarization.</b> A common strategy to solve these tasks is to <b>fine-tune</b> various models originally trained on vision tasks language. However, such task-specific models are not capable of solving a wide range of chart-related tasks, constraining their real-world applicability. To overcome these challenges, we introduce ChartInstruct: a novel chart-specific <b>vision-language</b> <b>Instruction-following</b> <b>dataset</b> comprising 191K <b>instructions</b> <b>generated</b> with 71K charts. We then present two distinct systems for <b>instruction</b> <b>tuning</b> on such datasets: (1) an end-to-end model that connects a vision encoder for chart understanding with a <b>LLM;</b> and (2) a pipeline model that employs a two-step approach to extract chart data tables and input them into the <b>LLM.</b> In experiments on four downstream tasks, we first show the effectiveness of our model--achieving a new set of state-of-the-art results. Further evaluation shows that our <b>instruction-tuning</b> <b>approach</b> supports a wide array of real-world chart comprehension and <b>reasoning</b> scenarios, thereby expanding the scope and applicability of our models to new kinds of tasks.

{{</citation>}}


### (5/32 | 127/254) Komodo: A Linguistic Expedition into Indonesia's Regional Languages (Louis Owen et al., 2024)

{{<citation>}}

Louis Owen, Vishesh Tripathi, Abhay Kumar, Biddwan Ahmed. (2024)  
**Komodo: A Linguistic Expedition into Indonesia's Regional Languages**
<br/>
<button class="copy-to-clipboard" title="Komodo: A Linguistic Expedition into Indonesia's Regional Languages" index=127>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 73  
Keywords: Benchmarking, Cohere, GPT, GPT-3, GPT-3.5, LLaMA, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09362v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09362v1.pdf" filename="2403.09362v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The recent breakthroughs in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have mostly focused on languages with easily available and sufficient resources, such as English. However, there remains a significant gap for languages that lack sufficient linguistic resources in the public domain. Our work introduces Komodo-7B, 7-billion-parameter <b>Large</b> <b>Language</b> <b>Models</b> designed to address this gap by seamlessly operating across Indonesian, English, and 11 regional languages in Indonesia. Komodo-7B is a family of <b>LLMs</b> that consist of Komodo-7B-Base and Komodo-7B-Instruct. Komodo-7B-Instruct stands out by achieving state-of-the-art performance in various tasks and languages, outperforming the <b>benchmarks</b> set by OpenAI's <b>GPT-3.5,</b> <b>Cohere's</b> Aya-101, <b>Llama-2-Chat-13B,</b> Mixtral-8x7B-Instruct-v0.1, Gemma-7B-it , and many more. This model not only demonstrates superior performance in both language-specific and overall assessments but also highlights its capability to excel in linguistic diversity. Our commitment to advancing language models extends beyond well-resourced languages, aiming to bridge the gap for those with limited linguistic assets. Additionally, Komodo-7B-Instruct's better cross-language understanding contributes to addressing educational disparities in Indonesia, offering direct translations from English to 11 regional languages, a significant improvement compared to existing language translation services. Komodo-7B represents a crucial step towards inclusivity and effectiveness in language models, providing to the linguistic needs of diverse communities.

{{</citation>}}


### (6/32 | 128/254) Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records (Angelo Ziletti et al., 2024)

{{<citation>}}

Angelo Ziletti, Leonardo D'Ambrosi. (2024)  
**Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records**
<br/>
<button class="copy-to-clipboard" title="Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records" index=128>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 70  
Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Unsupervised Learning, Question Answering, Text2SQL, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09226v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09226v1.pdf" filename="2403.09226v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Electronic health records (EHR) and claims data are rich sources of real-world data that reflect patient health status and healthcare utilization. Querying these databases to answer epidemiological <b>questions</b> <b>is</b> challenging due to the intricacy of medical terminology and the need for complex SQL queries. Here, we introduce an end-to-end methodology that combines <b>text-to-SQL</b> generation with <b>retrieval</b> <b>augmented</b> <b>generation</b> <b>(RAG)</b> to answer epidemiological <b>questions</b> <b>using</b> EHR and claims data. We show that our approach, which integrates a medical coding step into the <b>text-to-SQL</b> process, significantly improves the performance over simple <b>prompting.</b> Our findings indicate that although current language models are not yet sufficiently accurate for <b>unsupervised</b> use, <b>RAG</b> offers a promising direction for improving their capabilities, as shown in a realistic industry setting.

{{</citation>}}


### (7/32 | 129/254) Exploring the Comprehension of ChatGPT in Traditional Chinese Medicine Knowledge (Li Yizhen et al., 2024)

{{<citation>}}

Li Yizhen, Huang Shaohan, Qi Jiaxing, Quan Lei, Han Dongran, Luan Zhongzhi. (2024)  
**Exploring the Comprehension of ChatGPT in Traditional Chinese Medicine Knowledge**
<br/>
<button class="copy-to-clipboard" title="Exploring the Comprehension of ChatGPT in Traditional Chinese Medicine Knowledge" index=129>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL, stat-AP  
Keyword Score: 70  
Keywords: Few-shot, Zero-shot, ChatGPT, Reasoning, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09164v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09164v1.pdf" filename="2403.09164v1.pdf">Download PDF</button>

---


**ABSTRACT**  
No previous work has studied the performance of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in the context of Traditional Chinese Medicine (TCM), an essential and distinct branch of medical knowledge with a rich history. To bridge this gap, we present a TCM question dataset named TCM-QA, which comprises three question types: single choice, multiple choice, and true or false, to examine the <b>LLM's</b> capacity for knowledge recall and comprehensive <b>reasoning</b> within the TCM domain. In our study, we evaluate two settings of the <b>LLM,</b> <b>zero-shot</b> and <b>few-shot</b> settings, while concurrently discussing the differences between English and Chinese <b>prompts.</b> Our results indicate that <b>ChatGPT</b> performs best in true or false questions, achieving the highest precision of 0.688 while scoring the lowest precision is 0.241 in multiple-choice questions. Furthermore, we observed that Chinese <b>prompts</b> outperformed English <b>prompts</b> in our evaluations. Additionally, we assess the quality of explanations generated by <b>ChatGPT</b> and their potential contribution to TCM knowledge comprehension. This paper offers valuable insights into the applicability of <b>LLMs</b> in specialized domains and paves the way for future research in leveraging these powerful models to advance TCM.

{{</citation>}}


### (8/32 | 130/254) ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text (Chang Zong et al., 2024)

{{<citation>}}

Chang Zong, Yuyan Chen, Weiming Lu, Jian Shao, Yueting Zhuang. (2024)  
**ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text**
<br/>
<button class="copy-to-clipboard" title="ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text" index=130>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: 68T50, I-2-7, cs-AI, cs-CL, cs.CL  
Keyword Score: 70  
Keywords: Fine-tuning, Text Generation, Text Summarization, Instruction Tuning, Large Language Model, Large Language Model, Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09131v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09131v1.pdf" filename="2403.09131v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated efficacy in various linguistic applications, including <b>text</b> <b>summarization</b> and controlled <b>text</b> <b>generation.</b> However, studies into their capacity of switching between styles via <b>fine-tuning</b> remain underexplored. This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided <b>instruction</b> <b>tuning.</b> ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; <b>instruction</b> <b>tuning</b> for optimizing language models with multiple levels of <b>instruction</b> <b>formats;</b> and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated <b>text.</b> <b>Comparative</b> analysis of ProSwitch against both general and specialized language models reveals that our approach outperforms baselines in switching between professional and non-professional <b>text</b> <b>generation.</b>

{{</citation>}}


### (9/32 | 131/254) Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking (Eric Zelikman et al., 2024)

{{<citation>}}

Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman. (2024)  
**Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking**
<br/>
<button class="copy-to-clipboard" title="Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking" index=131>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-LG, cs.CL  
Keyword Score: 60  
Keywords: Few-shot, Fine-tuning, Zero-shot, Question Answering, Reasoning, Perplexity  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09629v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09629v1.pdf" filename="2403.09629v1.pdf">Download PDF</button>

---


**ABSTRACT**  
When writing and talking, people sometimes pause to think. Although <b>reasoning-focused</b> works have often framed <b>reasoning</b> as a method of answering <b>questions</b> <b>or</b> completing agentic tasks, <b>reasoning</b> is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from <b>few-shot</b> examples in <b>question-answering</b> <b>and</b> learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought's start and end, and an extended teacher-forcing technique. Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM's ability to directly answer difficult <b>questions.</b> <b>In</b> particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find <b>zero-shot</b> improvements on GSM8K (5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and observe a <b>perplexity</b> improvement of difficult tokens in natural text. Crucially, these improvements require no <b>fine-tuning</b> on these tasks. Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way.

{{</citation>}}


### (10/32 | 132/254) Rectifying Demonstration Shortcut in In-Context Learning (Joonwon Jang et al., 2024)

{{<citation>}}

Joonwon Jang, Sanghwan Jang, Wonbin Kweon, Minjin Jeon, Hwanjo Yu. (2024)  
**Rectifying Demonstration Shortcut in In-Context Learning**
<br/>
<button class="copy-to-clipboard" title="Rectifying Demonstration Shortcut in In-Context Learning" index=132>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 60  
Keywords: GPT, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09488v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09488v1.pdf" filename="2403.09488v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are able to solve various tasks with only a few demonstrations utilizing their <b>in-context</b> <b>learning</b> <b>(ICL)</b> abilities. However, <b>LLMs</b> often rely on their pre-trained semantic priors of demonstrations rather than on the input-label relationships to proceed with <b>ICL</b> prediction. In this work, we term this phenomenon as the `Demonstration Shortcut'. While previous works have primarily focused on improving <b>ICL</b> prediction results for predefined tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the <b>LLM</b> to effectively learn new input-label relationships from demonstrations. To achieve this, we introduce <b>In-Context</b> <b>Calibration,</b> a demonstration-aware calibration method. We evaluate the effectiveness of the proposed method in two settings: (1) the Original <b>ICL</b> Task using the standard label space and (2) the Task Learning setting, where the label space is replaced with semantically unrelated tokens. In both settings, <b>In-Context</b> <b>Calibration</b> demonstrates substantial improvements, with results generalized across three <b>LLM</b> families (OPT, <b>GPT,</b> and Llama2) under various configurations.

{{</citation>}}


### (11/32 | 133/254) To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation (Abdul Hameed Azeemi et al., 2024)

{{<citation>}}

Abdul Hameed Azeemi, Ihsan Ayyub Qazi, Agha Ali Raza. (2024)  
**To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation**
<br/>
<button class="copy-to-clipboard" title="To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation" index=133>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-LG, cs.CL  
Keyword Score: 60  
Keywords: Active Learning, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, Sentence Embedding, Domain Adaptation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09259v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09259v1.pdf" filename="2403.09259v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Active</b> <b>learning</b> (AL) techniques reduce labeling costs for training <b>neural</b> <b>machine</b> <b>translation</b> <b>(NMT)</b> models by selecting smaller representative subsets from unlabeled data for annotation. Diversity sampling techniques select heterogeneous instances, while uncertainty sampling methods select instances with the highest model uncertainty. Both approaches have limitations - diversity methods may extract varied but trivial examples, while uncertainty sampling can yield repetitive, uninformative instances. To bridge this gap, we propose HUDS, a hybrid AL strategy for <b>domain</b> <b>adaptation</b> in <b>NMT</b> that combines uncertainty and diversity for <b>sentence</b> <b>selection.</b> HUDS computes uncertainty scores for unlabeled <b>sentences</b> <b>and</b> subsequently stratifies them. It then clusters <b>sentence</b> <b>embeddings</b> within each stratum using k-MEANS and computes diversity scores by distance to the centroid. A weighted hybrid score that combines uncertainty and diversity is then used to select the top instances for annotation in each AL iteration. Experiments on multi-domain German-English datasets demonstrate the better performance of HUDS over other strong AL baselines. We analyze the <b>sentence</b> <b>selection</b> with HUDS and show that it prioritizes diverse instances having high model uncertainty for annotation in early AL iterations.

{{</citation>}}


### (12/32 | 134/254) Unveiling the Generalization Power of Fine-Tuned Large Language Models (Haoran Yang et al., 2024)

{{<citation>}}

Haoran Yang, Yumeng Zhang, Jiaqi Xu, Hongyuan Lu, Pheng Ann Heng, Wai Lam. (2024)  
**Unveiling the Generalization Power of Fine-Tuned Large Language Models**
<br/>
<button class="copy-to-clipboard" title="Unveiling the Generalization Power of Fine-Tuned Large Language Models" index=134>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 60  
Keywords: Fine-tuning, Fine-tuning, In-context Learning, In-context Learning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09162v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09162v1.pdf" filename="2403.09162v1.pdf">Download PDF</button>

---


**ABSTRACT**  
While <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated exceptional multitasking abilities, <b>fine-tuning</b> these models on downstream, domain-specific datasets is often necessary to yield superior performance on test sets compared to their counterparts without <b>fine-tuning.</b> However, the comprehensive effects of <b>fine-tuning</b> on the <b>LLMs'</b> generalization ability are not fully understood. This paper delves into the differences between original, unmodified <b>LLMs</b> and their <b>fine-tuned</b> variants. Our primary investigation centers on whether <b>fine-tuning</b> affects the generalization ability intrinsic to <b>LLMs.</b> To elaborate on this, we conduct extensive experiments across five distinct language tasks on various datasets. Our main findings reveal that models <b>fine-tuned</b> on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks. Intriguingly, we observe that integrating the <b>in-context</b> <b>learning</b> strategy during <b>fine-tuning</b> on generation tasks can enhance the model's generalization ability. Through this systematic investigation, we aim to contribute valuable insights into the evolving landscape of <b>fine-tuning</b> practices for <b>LLMs.</b>

{{</citation>}}


### (13/32 | 135/254) Information Extraction: An application to the domain of hyper-local financial data on developing countries (Abuzar Royesh et al., 2024)

{{<citation>}}

Abuzar Royesh, Olamide Oladeji. (2024)  
**Information Extraction: An application to the domain of hyper-local financial data on developing countries**
<br/>
<button class="copy-to-clipboard" title="Information Extraction: An application to the domain of hyper-local financial data on developing countries" index=135>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 60  
Keywords: Fine-tuning, T5, Transformer, Information Retrieval, Named Entity Recognition, Relation Extraction  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09077v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09077v1.pdf" filename="2403.09077v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Despite the need for financial data on company activities in developing countries for development research and economic analysis, such data does not exist. In this project, we develop and evaluate two Natural Language Processing (NLP) based techniques to address this issue. First, we curate a custom dataset specific to the domain of financial text data on developing countries and explore multiple approaches for <b>information</b> <b>extraction.</b> We then explore a text-to-text approach with the <b>transformer-based</b> <b>T5</b> model with the goal of undertaking simultaneous <b>NER</b> and <b>relation</b> <b>extraction.</b> We find that this model is able to learn the custom text structure output data corresponding to the entities and their <b>relations,</b> <b>resulting</b> in an accuracy of 92.44\%, a precision of 68.25\% and a recall of 54.20\% from our best <b>T5</b> model on the combined task. Secondly, we explore an approach with sequential <b>NER</b> and <b>relation</b> <b>extration.</b> For the <b>NER,</b> we run pre-trained and <b>fine-tuned</b> models using SpaCy, and we develop a custom <b>relation</b> <b>extraction</b> model using SpaCy's Dependency Parser output and some heuristics to determine entity relationships \cite{spacy}. We obtain an accuracy of 84.72\%, a precision of 6.06\% and a recall of 5.57\% on this sequential task.

{{</citation>}}


### (14/32 | 136/254) Large Language Models are Parallel Multilingual Learners (Yongyu Mu et al., 2024)

{{<citation>}}

Yongyu Mu, Peinan Feng, Zhiquan Cao, Yuzhang Wu, Bei Li, Chenglong Wang, Tong Xiao, Kai Song, Tongran Liu, Chunliang Zhang, Jingbo Zhu. (2024)  
**Large Language Models are Parallel Multilingual Learners**
<br/>
<button class="copy-to-clipboard" title="Large Language Models are Parallel Multilingual Learners" index=136>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 60  
Keywords: Pruning, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09073v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09073v1.pdf" filename="2403.09073v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this study, we reveal an <b>in-context</b> <b>learning</b> <b>(ICL)</b> capability of multilingual <b>large</b> <b>language</b> <b>models</b> <b>(LLMs):</b> by translating the input to several languages, we provide Parallel Input in Multiple Languages (PiM) to <b>LLMs,</b> which significantly enhances their comprehension abilities. To test this capability, we design extensive experiments encompassing 8 typical datasets, 7 languages and 8 state-of-the-art multilingual <b>LLMs.</b> Experimental results show that (1) incorporating more languages help PiM surpass the conventional <b>ICL</b> further; (2) even combining with the translations that are inferior to baseline performance can also help. Moreover, by examining the activated neurons in <b>LLMs,</b> we discover a counterintuitive but interesting phenomenon. Contrary to the common thought that PiM would activate more neurons than monolingual input to leverage knowledge learned from diverse languages, PiM actually inhibits neurons and promotes more precise neuron activation especially when more languages are added. This phenomenon aligns with the neuroscience insight about synaptic <b>pruning,</b> which removes less used neural connections, strengthens remainders, and then enhances brain intelligence.

{{</citation>}}


### (15/32 | 137/254) LAMP: A Language Model on the Map (Pasquale Balsebre et al., 2024)

{{<citation>}}

Pasquale Balsebre, Weiming Huang, Gao Cong. (2024)  
**LAMP: A Language Model on the Map**
<br/>
<button class="copy-to-clipboard" title="LAMP: A Language Model on the Map" index=137>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 60  
Keywords: Fine-tuning, Recommendation, GPT, GPT-4, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09059v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09059v1.pdf" filename="2403.09059v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are poised to play an increasingly important role in our lives, providing assistance across a wide array of tasks. In the geospatial domain, <b>LLMs</b> have demonstrated the ability to answer generic questions, such as identifying a country's capital; nonetheless, their utility is hindered when it comes to answering fine-grained questions about specific places, such as grocery stores or restaurants, which constitute essential aspects of people's everyday lives. This is mainly because the places in our cities haven't been systematically fed into <b>LLMs,</b> so as to understand and memorize them. This study introduces a novel framework for <b>fine-tuning</b> a pre-trained model on city-specific data, to enable it to provide accurate <b>recommendations,</b> while minimizing hallucinations. We share our model, LAMP, and the data used to train it. We conduct experiments to analyze its ability to correctly retrieving spatial objects, and compare it to well-known open- and closed- source language models, such as <b>GPT-4.</b> Finally, we explore its emerging capabilities through a case study on day planning.

{{</citation>}}


### (16/32 | 138/254) Less is More: Data Value Estimation for Visual Instruction Tuning (Zikang Liu et al., 2024)

{{<citation>}}

Zikang Liu, Kun Zhou, Wayne Xin Zhao, Dawei Gao, Yaliang Li, Ji-Rong Wen. (2024)  
**Less is More: Data Value Estimation for Visual Instruction Tuning**
<br/>
<button class="copy-to-clipboard" title="Less is More: Data Value Estimation for Visual Instruction Tuning" index=138>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-CV, cs.CL  
Keyword Score: 59  
Keywords: Benchmarking, Fine-tuning, Multi-modal, Multi-modal, Reasoning, Instruction Tuning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09559v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09559v1.pdf" filename="2403.09559v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Visual <b>instruction</b> <b>tuning</b> is the key to building <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs), which greatly improves the <b>reasoning</b> capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in vision scenario. However, existing MLLMs mostly rely on a mixture of multiple highly diverse visual <b>instruction</b> <b>datasets</b> for training (even more than a million <b>instructions),</b> <b>which</b> may introduce data redundancy. To investigate this issue, we conduct a series of empirical studies, which reveal a significant redundancy within the visual <b>instruction</b> <b>datasets,</b> and show that greatly reducing the amount of several <b>instruction</b> <b>dataset</b> even do not affect the performance. Based on the findings, we propose a new data selection approach TIVE, to eliminate redundancy within visual <b>instruction</b> <b>data.</b> TIVE first estimates the task-level and instance-level value of the visual <b>instructions</b> <b>based</b> on computed gradients. Then, according to the estimated values, TIVE determines the task proportion within the visual <b>instructions,</b> <b>and</b> selects representative instances to compose a smaller visual <b>instruction</b> <b>subset</b> for training. Experiments on LLaVA-1.5 show that our approach using only about 7.5% data can achieve comparable performance as the full-data <b>fine-tuned</b> model across seven <b>benchmarks,</b> even surpassing it on four of the <b>benchmarks.</b> Our code and data will be publicly released.

{{</citation>}}


### (17/32 | 139/254) AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic (Emad A. Alghamdi et al., 2024)

{{<citation>}}

Emad A. Alghamdi, Reem I. Masoud, Deema Alnuhait, Afnan Y. Alomairi, Ahmed Ashraf, Mohamed Zaytoon. (2024)  
**AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic**
<br/>
<button class="copy-to-clipboard" title="AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic" index=139>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 53  
Keywords: Benchmarking, GPT, GPT-4, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09017v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09017v1.pdf" filename="2403.09017v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The swift progress and widespread acceptance of artificial intelligence (AI) systems highlight a pressing requirement to comprehend both the capabilities and potential risks associated with AI. Given the linguistic complexity, cultural richness, and underrepresented status of Arabic in AI research, there is a pressing need to focus on <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> performance and safety for Arabic related tasks. Despite some progress in their development, there is a lack of comprehensive trustworthiness evaluation <b>benchmarks</b> which presents a major challenge in accurately assessing and improving the safety of <b>LLMs</b> when <b>prompted</b> in Arabic. In this paper, we introduce AraTrust 1, the first comprehensive trustworthiness <b>benchmark</b> for <b>LLMs</b> in Arabic. AraTrust comprises 516 human-written multiple-choice questions addressing diverse dimensions related to truthfulness, ethics, safety, physical health, mental health, unfairness, illegal activities, privacy, and offensive language. By introducing AraTrust, we aim to promote collaborative efforts to create safer and more trustworthy <b>LLMs</b> for Arabic users. We evaluated a set of <b>LLMs</b> against our <b>benchmark</b> to assess its trustworthiness. <b>GPT-4</b> showed to be the most trustworthy regarding Arabic language.

{{</citation>}}


### (18/32 | 140/254) Logits of API-Protected LLMs Leak Proprietary Information (Matthew Finlayson et al., 2024)

{{<citation>}}

Matthew Finlayson, Swabha Swayamdipta, Xiang Ren. (2024)  
**Logits of API-Protected LLMs Leak Proprietary Information**
<br/>
<button class="copy-to-clipboard" title="Logits of API-Protected LLMs Leak Proprietary Information" index=140>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: 68T50, I-2-7, cs-AI, cs-CL, cs-CR, cs-LG, cs.CL  
Keyword Score: 50  
Keywords: GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09539v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09539v1.pdf" filename="2403.09539v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The commercialization of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly <b>large</b> <b>amount</b> <b>of</b> non-public information about an API-protected <b>LLM</b> from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's <b>gpt-3.5-turbo).</b> Our findings are centered on one key observation: most modern <b>LLMs</b> suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the <b>LLM's</b> hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source <b>LLM</b> given a single full <b>LLM</b> output, and even estimating the output layer parameters. Our empirical investigations show the effectiveness of our methods, which allow us to estimate the embedding size of OpenAI's <b>gpt-3.5-turbo</b> to be about 4,096. Lastly, we discuss ways that <b>LLM</b> providers can guard against these attacks, as well as how these capabilities can be viewed as a feature (rather than a bug) by allowing for greater transparency and accountability.

{{</citation>}}


### (19/32 | 141/254) Basque and Spanish Counter Narrative Generation: Data Creation and Evaluation (Jaione Bengoetxea et al., 2024)

{{<citation>}}

Jaione Bengoetxea, Yi-Ling Chung, Marco Guerini, Rodrigo Agerri. (2024)  
**Basque and Spanish Counter Narrative Generation: Data Creation and Evaluation**
<br/>
<button class="copy-to-clipboard" title="Basque and Spanish Counter Narrative Generation: Data Creation and Evaluation" index=141>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 50  
Keywords: Data Augmentation, Fine-tuning, Zero-shot, Neural Machine Translation, Neural Machine Translation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09159v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09159v1.pdf" filename="2403.09159v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Counter Narratives (CNs) are non-negative textual responses to Hate Speech (HS) aiming at defusing online hatred and mitigating its spreading across media. Despite the recent increase in HS content posted online, research on automatic CN generation has been relatively scarce and predominantly focused on English. In this paper, we present CONAN-EUS, a new Basque and Spanish dataset for CN generation developed by means of <b>Machine</b> <b>Translation</b> <b>(MT)</b> and professional post-edition. Being a parallel corpus, also with respect to the original English CONAN, it allows to perform novel research on multilingual and crosslingual automatic generation of CNs. Our experiments on CN generation with mT5, a multilingual encoder-decoder model, show that generation greatly benefits from training on post-edited <b>data,</b> <b>as</b> opposed to relying on silver <b>MT</b> <b>data</b> <b>only.</b> These results are confirmed by their correlation with a qualitative manual evaluation, demonstrating that manually revised training <b>data</b> <b>remains</b> crucial for the quality of the generated CNs. Furthermore, multilingual <b>data</b> <b>augmentation</b> improves results over monolingual settings for structurally similar languages such as English and Spanish, while being detrimental for Basque, a language isolate. Similar findings occur in <b>zero-shot</b> crosslingual evaluations, where model transfer <b>(fine-tuning</b> in English and generating in a different target language) outperforms <b>fine-tuning</b> mT5 on <b>machine</b> <b>translated</b> <b>data</b> <b>for</b> Spanish but not for Basque. This provides an interesting insight into the asymmetry in the multilinguality of generative models, a challenging topic which is still open to research.

{{</citation>}}


### (20/32 | 142/254) AI on AI: Exploring the Utility of GPT as an Expert Annotator of AI Publications (Autumn Toney-Wails et al., 2024)

{{<citation>}}

Autumn Toney-Wails, Christian Schoeberl, James Dunham. (2024)  
**AI on AI: Exploring the Utility of GPT as an Expert Annotator of AI Publications**
<br/>
<button class="copy-to-clipboard" title="AI on AI: Exploring the Utility of GPT as an Expert Annotator of AI Publications" index=142>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 50  
Keywords: Fine-tuning, GPT, Transformer, Chatbot, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09097v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09097v1.pdf" filename="2403.09097v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Identifying scientific publications that are within a dynamic field of research often requires costly annotation by subject-matter experts. Resources like widely-accepted classification criteria or field taxonomies are unavailable for a domain like artificial intelligence (AI), which spans emerging topics and technologies. We address these challenges by inferring a functional definition of AI research from existing expert labels, and then evaluating state-of-the-art <b>chatbot</b> models on the task of expert data annotation. Using the arXiv publication database as ground-truth, we experiment with <b>prompt</b> engineering for <b>GPT</b> <b>chatbot</b> models to identify an alternative, automated expert annotation pipeline that assigns AI labels with 94% accuracy. For comparison, we <b>fine-tune</b> SPECTER, a <b>transformer</b> language model pre-trained on scientific publications, that achieves 96% accuracy (only 2% higher than <b>GPT)</b> on classifying AI publications. Our results indicate that with effective <b>prompt</b> engineering, <b>chatbots</b> can be used as reliable data annotators even where subject-area expertise is required. To evaluate the utility of <b>chatbot-annotated</b> datasets on downstream classification tasks, we train a new classifier on <b>GPT-labeled</b> data and compare its performance to the arXiv-trained model. The classifier trained on <b>GPT-labeled</b> data outperforms the arXiv-trained model by nine percentage points, achieving 82% accuracy.

{{</citation>}}


### (21/32 | 143/254) Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference (Piotr Nawrot et al., 2024)

{{<citation>}}

Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David Tarjan, Edoardo M. Ponti. (2024)  
**Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference**
<br/>
<button class="copy-to-clipboard" title="Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference" index=143>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 40  
Keywords: LLaMA, Transformer, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09636v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09636v1.pdf" filename="2403.09636v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Transformers</b> have emerged as the backbone of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for on-line key-value cache compression at inference time. Most importantly, the model learns to apply different compression rates in different heads and layers. We retrofit pre-trained <b>LLMs</b> such as <b>Llama</b> 2 (7B, 13B and 70B) into DMC <b>Transformers,</b> achieving up to ~3.7x throughput increase in auto-regressive inference on a NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. We find that DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA). GQA and DMC can be even combined to obtain compounded gains. As a result DMC fits longer contexts and larger batches within any given memory budget.

{{</citation>}}


### (22/32 | 144/254) Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey (Xiaoyu Liu et al., 2024)

{{<citation>}}

Xiaoyu Liu, Paiheng Xu, Junda Wu, Jiaxin Yuan, Yifan Yang, Yuhang Zhou, Fuxiao Liu, Tianrui Guan, Haoliang Wang, Tong Yu, Julian McAuley, Wei Ai, Furong Huang. (2024)  
**Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey**
<br/>
<button class="copy-to-clipboard" title="Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey" index=144>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 40  
Keywords: Fairness, Reasoning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09606v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09606v1.pdf" filename="2403.09606v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Causal inference has shown potential in enhancing the predictive accuracy, <b>fairness,</b> robustness, and explainability of Natural Language Processing (NLP) models by capturing causal relationships among variables. The emergence of generative <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has significantly impacted various NLP domains, particularly through their advanced <b>reasoning</b> capabilities. This survey focuses on evaluating and improving <b>LLMs</b> from a causal view in the following areas: understanding and improving the <b>LLMs'</b> <b>reasoning</b> capacity, addressing <b>fairness</b> and safety issues in <b>LLMs,</b> complementing <b>LLMs</b> with explanations, and handling multimodality. Meanwhile, <b>LLMs'</b> strong <b>reasoning</b> capacities can in turn contribute to the field of causal inference by aiding causal relationship discovery and causal effect estimations. This review explores the interplay between causal inference frameworks and <b>LLMs</b> from both perspectives, emphasizing their collective potential to further the development of more advanced and equitable artificial intelligence systems.

{{</citation>}}


### (23/32 | 145/254) Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse (Jianwei Sun et al., 2024)

{{<citation>}}

Jianwei Sun, Chaoyang Mei, Linlin Wei, Kaiyu Zheng, Na Liu, Ming Cui, Tianyi Li. (2024)  
**Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse**
<br/>
<button class="copy-to-clipboard" title="Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse" index=145>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 40  
Keywords: Fine-tuning, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09167v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09167v1.pdf" filename="2403.09167v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The efficacy of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> is heavily dependent on the quality of the underlying data, particularly within specialized domains. A common challenge when <b>fine-tuning</b> <b>LLMs</b> for domain-specific applications is the potential degradation of the model's generalization capabilities. To address these issues, we propose a two-stage approach for the construction of production <b>prompts</b> designed to yield high-quality data. This method involves the generation of a diverse array of <b>prompts</b> that encompass a broad spectrum of tasks and exhibit a rich variety of expressions. Furthermore, we introduce a cost-effective, multi-dimensional quality assessment framework to ensure the integrity of the generated labeling data. Utilizing a dataset comprised of service provider and customer interactions from the real estate sector, we demonstrate a positive correlation between data quality and model performance. Notably, our findings indicate that the domain-specific proficiency of general <b>LLMs</b> can be enhanced through <b>fine-tuning</b> with data produced via our proposed method, without compromising their overall generalization abilities, even when exclusively domain-specific data is employed for <b>fine-tuning.</b>

{{</citation>}}


### (24/32 | 146/254) AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning (Ruiyi Zhang et al., 2024)

{{<citation>}}

Ruiyi Zhang, Rushi Qiang, Sai Ashish Somayajula, Pengtao Xie. (2024)  
**AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning**
<br/>
<button class="copy-to-clipboard" title="AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning" index=146>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-LG, cs.CL  
Keyword Score: 40  
Keywords: Fine-tuning, Fine-tuning, Meta Learning, Natural Language Understanding  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09113v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09113v1.pdf" filename="2403.09113v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Large-scale pretraining followed by task-specific <b>finetuning</b> has achieved great success in various NLP tasks. Since <b>finetuning</b> all parameters of large pretrained models poses substantial computational and memory challenges, several efficient <b>finetuning</b> methods have been developed. Among them, low-rank adaptation (LoRA), which <b>finetunes</b> low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective. Nonetheless, LoRA's uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal <b>finetuning</b> performance. To address these limitations, we introduce AutoLoRA, a <b>meta</b> <b>learning</b> based framework for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should be discarded. A <b>meta</b> <b>learning</b> based method is developed to learn these selection variables. The optimal rank is determined by thresholding the values of these variables. Our comprehensive experiments on <b>natural</b> <b>language</b> <b>understanding,</b> generation, and sequence labeling demonstrate the effectiveness of AutoLoRA.

{{</citation>}}


### (25/32 | 147/254) RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems (Jennifer Hsia et al., 2024)

{{<citation>}}

Jennifer Hsia, Afreen Shaikh, Zhiruo Wang, Graham Neubig. (2024)  
**RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems**
<br/>
<button class="copy-to-clipboard" title="RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems" index=147>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 40  
Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Question Answering  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09040v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09040v1.pdf" filename="2403.09040v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Retrieval-augmented</b> <b>generation</b> <b>(RAG)</b> greatly benefits language models (LMs) by providing additional context for tasks such as document-based <b>question</b> <b>answering</b> (DBQA). Despite its potential, the power of <b>RAG</b> is highly dependent on its configuration, raising the <b>question:</b> <b>What</b> is the optimal <b>RAG</b> configuration? To answer this, we introduce the RAGGED framework to analyze and optimize <b>RAG</b> systems. On a set of representative DBQA tasks, we study two classic sparse and dense retrievers, and four top-performing LMs in encoder-decoder and decoder-only architectures. Through RAGGED, we uncover that different models suit substantially varied <b>RAG</b> setups. While encoder-decoder models monotonically improve with more documents, we find decoder-only models can only effectively use < 5 documents, despite often having a longer context window. RAGGED offers further insights into LMs' context utilization habits, where we find that encoder-decoder models rely more on contexts and are thus more sensitive to <b>retrieval</b> <b>quality,</b> <b>while</b> decoder-only models tend to rely on knowledge memorized during training.

{{</citation>}}


### (26/32 | 148/254) Hyper-CL: Conditioning Sentence Representations with Hypernetworks (Young Hyun Yoo et al., 2024)

{{<citation>}}

Young Hyun Yoo, Jii Cha, Changhyeon Kim, Taeuk Kim. (2024)  
**Hyper-CL: Conditioning Sentence Representations with Hypernetworks**
<br/>
<button class="copy-to-clipboard" title="Hyper-CL: Conditioning Sentence Representations with Hypernetworks" index=148>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 36  
Keywords: Graph, Benchmarking, Contrastive Learning, Knowledge Graph, Representation Learning, Sentence Embedding  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09490v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09490v1.pdf" filename="2403.09490v1.pdf">Download PDF</button>

---


**ABSTRACT**  
While the introduction of <b>contrastive</b> <b>learning</b> frameworks in <b>sentence</b> <b>representation</b> <b>learning</b> has significantly contributed to advancements in the field, it still remains unclear whether state-of-the-art <b>sentence</b> <b>embeddings</b> can capture the fine-grained semantics of <b>sentences,</b> <b>particularly</b> when conditioned on specific perspectives. In this paper, we introduce Hyper-CL, an efficient methodology that integrates hypernetworks with <b>contrastive</b> <b>learning</b> to compute conditioned <b>sentence</b> <b>representations.</b> <b>In</b> our proposed approach, the hypernetwork is responsible for transforming pre-computed condition embeddings into corresponding projection layers. This enables the same <b>sentence</b> <b>embeddings</b> to be projected differently according to various conditions. Evaluation on two representative conditioning <b>benchmarks,</b> namely conditional semantic text similarity and <b>knowledge</b> <b>graph</b> completion, demonstrates that Hyper-CL is effective in flexibly conditioning <b>sentence</b> <b>representations,</b> <b>showcasing</b> its computational efficiency at the same time. We also provide a comprehensive analysis of the inner workings of our approach, leading to a better interpretation of its mechanisms.

{{</citation>}}


### (27/32 | 149/254) MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection (Yupeng Li et al., 2024)

{{<citation>}}

Yupeng Li, Haorui He, Jin Bai, Dacheng Wen. (2024)  
**MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection**
<br/>
<button class="copy-to-clipboard" title="MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection" index=149>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 33  
Keywords: Benchmarking, Fact Verification, Fake News Detection, Fake News Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09092v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09092v1.pdf" filename="2403.09092v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The prevalence of <b>fake</b> <b>news</b> <b>across</b> various online sources has had a significant influence on the public. Existing Chinese <b>fake</b> <b>news</b> <b>detection</b> datasets are limited to news sourced solely from Weibo. However, <b>fake</b> <b>news</b> <b>originating</b> from multiple sources exhibits diversity in various aspects, including its content and social context. Methods trained on purely one single news source can hardly be applicable to real-world scenarios. Our pilot experiment demonstrates that the F1 score of the state-of-the-art method that learns from a large Chinese <b>fake</b> <b>news</b> <b>detection</b> dataset, Weibo-21, drops significantly from 0.943 to 0.470 when the test data is changed to multi-source news data, failing to identify more than one-third of the multi-source <b>fake</b> <b>news.</b> <b>To</b> address this limitation, we constructed the first multi-source <b>benchmark</b> dataset for Chinese <b>fake</b> <b>news</b> <b>detection,</b> termed MCFEND, which is composed of news we collected from diverse sources such as social platforms, messaging apps, and traditional online news outlets. Notably, such news has been <b>fact-checked</b> <b>by</b> 14 authoritative <b>fact-checking</b> <b>agencies</b> worldwide. In addition, various existing Chinese <b>fake</b> <b>news</b> <b>detection</b> methods are thoroughly evaluated on our proposed dataset in cross-source, multi-source, and unseen source ways. MCFEND, as a <b>benchmark</b> dataset, aims to advance Chinese <b>fake</b> <b>news</b> <b>detection</b> approaches in real-world scenarios.

{{</citation>}}


### (28/32 | 150/254) Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models (Akhil Kedia et al., 2024)

{{<citation>}}

Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia, Jungho Jung, Harshith Goka, Haejun Lee. (2024)  
**Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models**
<br/>
<button class="copy-to-clipboard" title="Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models" index=150>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: I-2-7; I-2-10, cs-AI, cs-CL, cs-CV, cs-LG, cs.CL  
Keyword Score: 30  
Keywords: Transformer, Question Answering, Speech-to-Speech Translation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09635v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09635v1.pdf" filename="2403.09635v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In spite of their huge success, <b>transformer</b> models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the <b>transformer</b> model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 100s of layers. We find that <b>transformer</b> models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, <b>Speech</b> <b>Translation,</b> and Image Classification, across Encoder-only, Decoder-only and Encoder-Decoder variants, for both Pre-LN and Post-LN <b>transformers,</b> for multiple datasets and model sizes. These improvements also translate into improved performance on downstream <b>Question</b> <b>Answering</b> tasks and improved robustness for image classification.

{{</citation>}}


### (29/32 | 151/254) Caveat Lector: Large Language Models in Legal Practice (Eliza Mik, 2024)

{{<citation>}}

Eliza Mik. (2024)  
**Caveat Lector: Large Language Models in Legal Practice**
<br/>
<button class="copy-to-clipboard" title="Caveat Lector: Large Language Models in Legal Practice" index=151>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-CY, cs.CL  
Keyword Score: 30  
Keywords: Reasoning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09163v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09163v1.pdf" filename="2403.09163v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The current fascination with <b>large</b> <b>language</b> <b>models,</b> or <b>LLMs,</b> derives from the fact that many users lack the expertise to evaluate the quality of the generated text. <b>LLMs</b> may therefore appear more capable than they actually are. The dangerous combination of fluency and superficial plausibility leads to the temptation to trust the generated text and creates the risk of overreliance. Who would not trust perfect legalese? Relying recent findings in both technical and legal scholarship, this Article counterbalances the overly optimistic predictions as to the role of <b>LLMs</b> in legal practice. Integrating <b>LLMs</b> into legal workstreams without a better comprehension of their limitations, will create inefficiencies if not outright risks. Notwithstanding their unprecedented ability to generate text, <b>LLMs</b> do not understand text. Without the ability to understand meaning, <b>LLMs</b> will remain unable to use language, to acquire knowledge and to perform complex <b>reasoning</b> tasks. Trained to model language on the basis of stochastic word predictions, <b>LLMs</b> cannot distinguish fact from fiction. Their knowledge of the law is limited to word strings memorized in their parameters. It is also incomplete and largely incorrect. <b>LLMs</b> operate at the level of word distributions, not at the level of verified facts. The resulting propensity to hallucinate, to produce statements that are incorrect but appear helpful and relevant, is alarming in high-risk areas like legal services. At present, lawyers should beware of relying on text generated by <b>LLMs.</b>

{{</citation>}}


### (30/32 | 152/254) Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance (Kai Xiong et al., 2024)

{{<citation>}}

Kai Xiong, Xiao Ding, Ting Liu, Bing Qin, Dongliang Xu, Qing Yang, Hongtao Liu, Yixin Cao. (2024)  
**Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance**
<br/>
<button class="copy-to-clipboard" title="Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance" index=152>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 30  
Keywords: Reasoning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09085v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09085v1.pdf" filename="2403.09085v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have developed impressive performance and strong explainability across various <b>reasoning</b> scenarios, marking a significant stride towards mimicking human-like intelligence. Despite this, when tasked with simple questions supported by a generic fact, <b>LLMs</b> often fail to provide consistent and precise answers, indicating a deficiency in abstract <b>reasoning</b> abilities. This has sparked a vigorous debate about whether <b>LLMs</b> are genuinely <b>reasoning</b> or merely memorizing. In light of this, we design a preliminary study to quantify and delve into the abstract <b>reasoning</b> abilities of existing <b>LLMs.</b> Our findings reveal a substantial discrepancy between their general <b>reasoning</b> and abstract <b>reasoning</b> performances. To relieve this problem, we tailor an abstract <b>reasoning</b> dataset (AbsR) together with a meaningful learning paradigm to teach <b>LLMs</b> how to leverage generic facts for <b>reasoning</b> purposes. The results show that our approach not only boosts the general <b>reasoning</b> performance of <b>LLMs</b> but also makes considerable strides towards their capacity for abstract <b>reasoning,</b> moving beyond simple memorization or imitation to a more nuanced understanding and application of generic facts.

{{</citation>}}


### (31/32 | 153/254) A Continued Pretrained LLM Approach for Automatic Medical Note Generation (Dong Yuan et al., 2024)

{{<citation>}}

Dong Yuan, Eti Rastogi, Gautam Naik, Jai Chintagunta, Sree Prasanna Rajagopal, Fen Zhao, Sagar Goyal, Jeff Ward. (2024)  
**A Continued Pretrained LLM Approach for Automatic Medical Note Generation**
<br/>
<button class="copy-to-clipboard" title="A Continued Pretrained LLM Approach for Automatic Medical Note Generation" index=153>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 30  
Keywords: GPT, GPT-4, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09057v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09057v1.pdf" filename="2403.09057v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>LLMs</b> are revolutionizing NLP tasks. However, the most powerful <b>LLM,</b> like <b>GPT-4,</b> is too costly for most domain-specific scenarios. We present the first continuously trained 13B Llama2-based <b>LLM</b> that is purpose-built for medical conversations and measured on automated scribing. Our results show that our model outperforms <b>GPT-4</b> in PubMedQA with 76.6\% accuracy and matches its performance in summarizing medical conversations into SOAP notes. Notably, our model exceeds <b>GPT-4</b> in capturing a higher number of correct medical concepts and outperforms human scribes with higher correctness and completeness.

{{</citation>}}


### (32/32 | 154/254) Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information (Shadi Iskander et al., 2024)

{{<citation>}}

Shadi Iskander, Kira Radinsky, Yonatan Belinkov. (2024)  
**Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information**
<br/>
<button class="copy-to-clipboard" title="Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information" index=154>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-CY, cs-LG, cs.CL  
Keyword Score: 10  
Keywords: Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09516v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09516v1.pdf" filename="2403.09516v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the <b>fine-tuning</b> process to mitigate bias in the model's representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.

{{</citation>}}


## cs.RO (10)



### (1/10 | 155/254) ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models (Runyu Ma et al., 2024)

{{<citation>}}

Runyu Ma, Jelle Luijkx, Zlatan Ajanovic, Jens Kober. (2024)  
**ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models**
<br/>
<button class="copy-to-clipboard" title="ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models" index=155>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 80  
Keywords: Few-shot, Foundation Model, Reinforcement Learning, Simulation, Simulator, Zero-shot, Reasoning, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09583v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09583v1.pdf" filename="2403.09583v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In image-based robot manipulation tasks with <b>large</b> <b>observation</b> <b>and</b> action spaces, <b>reinforcement</b> <b>learning</b> struggles with low sample efficiency, slow training speed, and uncertain convergence. As an alternative, <b>large</b> <b>pre-trained</b> <b>foundation</b> <b>models</b> have shown promise in robotic manipulation, particularly in <b>zero-shot</b> and <b>few-shot</b> applications. However, using these models directly is unreliable due to limited <b>reasoning</b> capabilities and challenges in understanding physical and spatial contexts. This paper introduces ExploRLLM, a novel approach that leverages the inductive bias of <b>foundation</b> <b>models</b> (e.g. <b>Large</b> <b>Language</b> <b>Models)</b> to guide exploration in <b>reinforcement</b> <b>learning.</b> We also exploit these <b>foundation</b> <b>models</b> to reformulate the action and observation spaces to enhance the training efficiency in <b>reinforcement</b> <b>learning.</b> Our experiments demonstrate that guided exploration enables much quicker convergence than training without it. Additionally, we validate that ExploRLLM outperforms vanilla <b>foundation</b> <b>model</b> baselines and that the policy trained in <b>simulation</b> can be applied in real-world settings without additional training.

{{</citation>}}


### (2/10 | 156/254) PaperBot: Learning to Design Real-World Tools Using Paper (Ruoshi Liu et al., 2024)

{{<citation>}}

Ruoshi Liu, Junbang Liang, Sruthi Sudhakar, Huy Ha, Cheng Chi, Shuran Song, Carl Vondrick. (2024)  
**PaperBot: Learning to Design Real-World Tools Using Paper**
<br/>
<button class="copy-to-clipboard" title="PaperBot: Learning to Design Real-World Tools Using Paper" index=156>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 50  
Keywords: Human Intervention, Self-supervised Learning, Self-supervised Learning, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09566v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09566v1.pdf" filename="2403.09566v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Paper is a cheap, recyclable, and clean material that is often used to make practical tools. Traditional tool design either relies on <b>simulation</b> or physical analysis, which is often inaccurate and time-consuming. In this paper, we propose PaperBot, an approach that directly learns to design and use a tool in the real world using paper without <b>human</b> <b>intervention.</b> We demonstrated the effectiveness and efficiency of PaperBot on two tool design tasks: 1. learning to fold and throw paper airplanes for maximum travel distance 2. learning to cut paper into grippers that exert maximum gripping force. We present a <b>self-supervised</b> <b>learning</b> framework that learns to perform a sequence of folding, cutting, and dynamic manipulation actions in order to optimize the design and use of a tool. We deploy our system to a real-world two-arm robotic system to solve challenging design tasks that involve aerodynamics (paper airplane) and friction (paper gripper) that are impossible to simulate accurately.

{{</citation>}}


### (3/10 | 157/254) GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping (Yuhang Zheng et al., 2024)

{{<citation>}}

Yuhang Zheng, Xiangyu Chen, Yupeng Zheng, Songen Gu, Runyi Yang, Bu Jin, Pengfei Li, Chengliang Zhong, Zengmao Wang, Lina Liu, Chao Yang, Dawei Wang, Zhen Chen, Xiaoxiao Long, Meiqing Wang. (2024)  
**GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping**
<br/>
<button class="copy-to-clipboard" title="GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping" index=157>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-CV, cs-RO, cs.RO  
Keyword Score: 35  
Keywords: Contrastive Learning, Geometry, Knowledge Distillation, Knowledge Distillation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09637v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09637v1.pdf" filename="2403.09637v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Constructing a 3D scene capable of accommodating open-ended language queries, is a pivotal pursuit, particularly within the domain of robotics. Such technology facilitates robots in executing object manipulations based on human language directives. To tackle this challenge, some research efforts have been dedicated to the development of language-embedded implicit fields. However, implicit fields (e.g. NeRF) encounter limitations due to the necessity of processing a large number of input views for reconstruction, coupled with their inherent inefficiencies in inference. Thus, we present the GaussianGrasper, which utilizes 3D Gaussian Splatting to explicitly represent the scene as a collection of Gaussian primitives. Our approach takes a limited set of RGB-D views and employs a tile-based splatting technique to create a feature field. In particular, we propose an Efficient Feature <b>Distillation</b> (EFD) module that employs <b>contrastive</b> <b>learning</b> to efficiently and accurately <b>distill</b> language embeddings derived from foundational models. With the reconstructed <b>geometry</b> of the Gaussian field, our method enables the pre-trained grasping model to generate collision-free grasp pose candidates. Furthermore, we propose a normal-guided grasp module to select the best grasp pose. Through comprehensive real-world experiments, we demonstrate that GaussianGrasper enables robots to accurately query and grasp objects with language instructions, providing a new solution for language-guided manipulation tasks. Data and codes can be available at https://github.com/MrSecant/GaussianGrasper.

{{</citation>}}


### (4/10 | 158/254) Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models (Laura Fernández-Becerra et al., 2024)

{{<citation>}}

Laura Fernández-Becerra, Miguel Ángel González-Santamarta, Ángel Manuel Guerrero-Higueras, Francisco Javier Rodríguez-Lera, Vicente Matellán Olivera. (2024)  
**Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models**
<br/>
<button class="copy-to-clipboard" title="Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models" index=158>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-AI, cs-RO, cs.RO  
Keyword Score: 35  
Keywords: Black Box, Natural Language Explanation, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09567v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09567v1.pdf" filename="2403.09567v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The deployment of autonomous agents in environments involving human interaction has increasingly raised security concerns. Consequently, understanding the circumstances behind an event becomes critical, requiring the development of capabilities to justify their behaviors to non-expert users. Such explanations are essential in enhancing trustworthiness and safety, acting as a preventive measure against failures, errors, and misunderstandings. Additionally, they contribute to improving communication, bridging the gap between the agent and the user, thereby improving the effectiveness of their interactions. This work presents an accountability and explainability architecture implemented for ROS-based mobile robots. The proposed solution consists of two main components. Firstly, a <b>black</b> <b>box-like</b> element to provide accountability, featuring anti-tampering properties achieved through blockchain technology. Secondly, a component in charge of generating <b>natural</b> <b>language</b> <b>explanations</b> by harnessing the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> over the data contained within the previously mentioned <b>black</b> <b>box.</b> The study evaluates the performance of our solution in three different scenarios, each involving autonomous agent navigation functionalities. This evaluation includes a thorough examination of accountability and explainability metrics, demonstrating the effectiveness of our approach in using accountable data from robot actions to obtain coherent, accurate and understandable explanations, even when facing challenges inherent in the use of autonomous agents in real-world scenarios.

{{</citation>}}


### (5/10 | 159/254) BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation (Chengshu Li et al., 2024)

{{<citation>}}

Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martín-Martín, Chen Wang, Gabrael Levine, Wensi Ai, Benjamin Martinez, Hang Yin, Michael Lingelbach, Minjune Hwang, Ayano Hiranaka, Sujay Garlanka, Arman Aydin, Sharon Lee, Jiankai Sun, Mona Anvari, Manasi Sharma, Dhruva Bansal, Samuel Hunter, Kyu-Young Kim, Alan Lou, Caleb R Matthews, Ivan Villa-Renteria, Jerry Huayang Tang, Claire Tang, Fei Xia, Yunzhu Li, Silvio Savarese, Hyowon Gweon, C. Karen Liu, Jiajun Wu, Li Fei-Fei. (2024)  
**BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation**
<br/>
<button class="copy-to-clipboard" title="BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation" index=159>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-AI, cs-RO, cs.RO  
Keyword Score: 23  
Keywords: Benchmarking, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09227v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09227v1.pdf" filename="2403.09227v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present BEHAVIOR-1K, a comprehensive <b>simulation</b> <b>benchmark</b> for human-centered robotics. BEHAVIOR-1K includes two components, guided and motivated by the results of an extensive survey on "what do you want robots to do for you?". The first is the definition of 1,000 everyday activities, grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more than 9,000 objects annotated with rich physical and semantic properties. The second is OMNIGIBSON, a novel <b>simulation</b> environment that supports these activities via realistic physics <b>simulation</b> and rendering of rigid bodies, deformable bodies, and liquids. Our experiments indicate that the activities in BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both of which remain a challenge for even state-of-the-art robot learning solutions. To calibrate the <b>simulation-to-reality</b> gap of BEHAVIOR-1K, we provide an initial study on transferring solutions learned with a mobile manipulator in a simulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K's human-grounded nature, diversity, and realism make it valuable for embodied AI and robot learning research. Project website: https://behavior.stanford.edu.

{{</citation>}}


### (6/10 | 160/254) Pushing in the Dark: A Reactive Pushing Strategy for Mobile Robots Using Tactile Feedback (Idil Ozdamar et al., 2024)

{{<citation>}}

Idil Ozdamar, Doganay Sirintuna, Robin Arbaud, Arash Ajoudani. (2024)  
**Pushing in the Dark: A Reactive Pushing Strategy for Mobile Robots Using Tactile Feedback**
<br/>
<button class="copy-to-clipboard" title="Pushing in the Dark: A Reactive Pushing Strategy for Mobile Robots Using Tactile Feedback" index=160>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09305v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09305v1.pdf" filename="2403.09305v1.pdf">Download PDF</button>

---


**ABSTRACT**  
For mobile robots, navigating cluttered or dynamic environments often necessitates non-prehensile manipulation, particularly when faced with objects that are too large, irregular, or fragile to grasp. The unpredictable behavior and varying physical properties of these objects significantly complicate manipulation tasks. To address this challenge, this manuscript proposes a novel Reactive Pushing Strategy. This strategy allows a mobile robot to dynamically adjust its base movements in real-time to achieve successful pushing maneuvers towards a target location. Notably, our strategy adapts the robot motion based on changes in contact location obtained through the tactile sensor covering the base, avoiding dependence on object-related assumptions and its modeled behavior. The effectiveness of the Reactive Pushing Strategy was initially evaluated in the <b>simulation</b> environment, where it significantly outperformed the compared baseline approaches. Following this, we validated the proposed strategy through real-world experiments, demonstrating the robot capability to push objects to the target points located in the entire vicinity of the robot. In both <b>simulation</b> and real-world experiments, the object-specific properties (shape, mass, friction, inertia) were altered along with the changes in target locations to assess the robustness of the proposed method comprehensively.

{{</citation>}}


### (7/10 | 161/254) Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis (Fabio Maresca et al., 2024)

{{<citation>}}

Fabio Maresca, Filippo Grazioli, Antonio Albanese, Vincenzo Sciancalepore, Gianpiero Negri, Xavier Costa-Perez. (2024)  
**Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis**
<br/>
<button class="copy-to-clipboard" title="Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis" index=161>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-LG, cs-RO, cs.RO  
Keyword Score: 10  
Keywords: Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09571v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09571v1.pdf" filename="2403.09571v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The tremendous hype around autonomous driving is eagerly calling for emerging and novel technologies to support advanced mobility use cases. As car manufactures keep developing SAE level 3+ systems to improve the safety and comfort of passengers, traffic authorities need to establish new procedures to manage the transition from human-driven to fully-autonomous vehicles while providing a feedback-loop mechanism to <b>fine-tune</b> envisioned autonomous systems. Thus, a way to automatically profile autonomous vehicles and differentiate those from human-driven ones is a must. In this paper, we present a fully-fledged framework that monitors active vehicles using camera images and state information in order to determine whether vehicles are autonomous, without requiring any active notification from the vehicles themselves. Essentially, it builds on the cooperation among vehicles, which share their data acquired on the road feeding a machine learning model to identify autonomous cars. We extensively tested our solution and created the NexusStreet dataset, by means of the CARLA simulator, employing an autonomous driving control agent and a steering wheel maneuvered by licensed drivers. Experiments show it is possible to discriminate the two behaviors by analyzing video clips with an accuracy of 80%, which improves up to 93% when the target state information is available. Lastly, we deliberately degraded the state to observe how the framework performs under non-ideal data collection conditions.

{{</citation>}}


### (8/10 | 162/254) MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion (Arul Selvam Periyasamy et al., 2024)

{{<citation>}}

Arul Selvam Periyasamy, Sven Behnke. (2024)  
**MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion**
<br/>
<button class="copy-to-clipboard" title="MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion" index=162>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 10  
Keywords: Object Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09309v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09309v1.pdf" filename="2403.09309v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Cluttered bin-picking environments are challenging for pose estimation models. Despite the impressive progress enabled by deep learning, single-view RGB pose estimation models perform poorly in cluttered dynamic environments. Imbuing the rich temporal information contained in the video of scenes has the potential to enhance models ability to deal with the adverse effects of occlusion and the dynamic nature of the environments. Moreover, joint <b>object</b> <b>detection</b> and pose estimation models are better suited to leverage the co-dependent nature of the tasks for improving the accuracy of both tasks. To this end, we propose attention-based temporal fusion for multi-object 6D pose estimation that accumulates information across multiple frames of a video sequence. Our MOTPose method takes a sequence of images as input and performs joint <b>object</b> <b>detection</b> and pose estimation for all <b>objects</b> <b>in</b> one forward pass. It learns to aggregate both <b>object</b> <b>embeddings</b> and <b>object</b> <b>parameters</b> over multiple time steps using cross-attention-based fusion modules. We evaluate our method on the physically-realistic cluttered bin-picking dataset SynPick and the YCB-Video dataset and demonstrate improved pose estimation accuracy as well as better <b>object</b> <b>detection</b> accuracy

{{</citation>}}


### (9/10 | 163/254) Development of control algorithms for mobile robotics focused on their potential use for FPGA-based robots (Andrés-David Suárez-Gómez et al., 2024)

{{<citation>}}

Andrés-David Suárez-Gómez, Andres A. Hernandez Ortega. (2024)  
**Development of control algorithms for mobile robotics focused on their potential use for FPGA-based robots**
<br/>
<button class="copy-to-clipboard" title="Development of control algorithms for mobile robotics focused on their potential use for FPGA-based robots" index=163>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-AR, cs-RO, cs.RO  
Keyword Score: 6  
Keywords: Benchmarking, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09459v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09459v1.pdf" filename="2403.09459v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper investigates the development and optimization of control algorithms for mobile robotics, with a keen focus on their implementation in Field-Programmable Gate Arrays (FPGAs). It delves into both classical control approaches such as PID and modern techniques including deep learning, addressing their application in sectors ranging from industrial automation to medical care. The study highlights the practical challenges and advancements in embedding these algorithms into FPGAs, which offer significant benefits for mobile robotics due to their high-speed processing and parallel computation capabilities. Through an analysis of various control strategies, the paper showcases the improvements in robot performance, particularly in navigation and obstacle avoidance. It emphasizes the critical role of FPGAs in enhancing the efficiency and adaptability of control algorithms in dynamic environments. Additionally, the research discusses the difficulties in <b>benchmarking</b> and evaluating the performance of these algorithms in real-world applications, suggesting a need for standardized evaluation criteria. The contribution of this work lies in its comprehensive examination of control algorithms' potential in FPGA-based mobile robotics, offering insights into future research directions for improving robotic autonomy and operational efficiency.

{{</citation>}}


### (10/10 | 164/254) THÖR-MAGNI: A Large-scale Indoor Motion Capture Recording of Human Movement and Robot Interaction (Tim Schreiter et al., 2024)

{{<citation>}}

Tim Schreiter, Tiago Rodrigues de Almeida, Yufei Zhu, Eduardo Gutierrez Maestro, Lucas Morillo-Mendez, Andrey Rudenko, Luigi Palmieri, Tomasz P. Kucner, Martin Magnusson, Achim J. Lilienthal. (2024)  
**THÖR-MAGNI: A Large-scale Indoor Motion Capture Recording of Human Movement and Robot Interaction**
<br/>
<button class="copy-to-clipboard" title="THÖR-MAGNI: A Large-scale Indoor Motion Capture Recording of Human Movement and Robot Interaction" index=164>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 3  
Keywords: Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09285v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09285v1.pdf" filename="2403.09285v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present a new large dataset of indoor human and robot navigation and interaction, called TH\"OR-MAGNI, that is designed to facilitate research on social navigation: e.g., modelling and predicting human motion, analyzing goal-oriented interactions between humans and robots, and investigating visual attention in a social interaction context. TH\"OR-MAGNI was created to fill a gap in available datasets for human motion analysis and HRI. This gap is characterized by a lack of comprehensive inclusion of exogenous factors and essential target agent cues, which hinders the development of robust models capable of capturing the relationship between contextual cues and human behavior in different scenarios. Unlike existing datasets, TH\"OR-MAGNI includes a broader set of contextual features and offers multiple scenario variations to facilitate factor isolation. The dataset includes many social human-human and human-robot interaction scenarios, rich context annotations, and <b>multi-modal</b> data, such as walking trajectories, gaze tracking data, and lidar and camera streams recorded from a mobile robot. We also provide a set of tools for visualization and processing of the recorded data. TH\"OR-MAGNI is, to the best of our knowledge, unique in the amount and diversity of sensor data collected in a contextualized and socially dynamic environment, capturing natural human-robot interactions.

{{</citation>}}


## cs.SE (6)



### (1/6 | 165/254) CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences (Martin Weyssow et al., 2024)

{{<citation>}}

Martin Weyssow, Aton Kamanda, Houari Sahraoui. (2024)  
**CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences**
<br/>
<button class="copy-to-clipboard" title="CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences" index=165>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-CL, cs-LG, cs-SE, cs.SE  
Keyword Score: 73  
Keywords: Benchmarking, Direct Preference Optimization, Reinforcement Learning, GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09032v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09032v1.pdf" filename="2403.09032v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Evaluating the alignment of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with user-defined coding preferences is a challenging endeavour that requires assessing intricate textual <b>LLMs'</b> outputs. By relying on automated metrics and static analysis tools, existing <b>benchmarks</b> fail to assess nuances in user instructions and <b>LLM</b> outputs, highlighting the need for <b>large-scale</b> <b>datasets</b> <b>and</b> <b>benchmarks</b> for <b>LLM</b> preference alignment. In this paper, we introduce CodeUltraFeedback, a preference dataset of 10,000 complex instructions to tune and align <b>LLMs</b> to coding preferences through AI feedback. We generate responses to the instructions using a pool of 14 diverse <b>LLMs,</b> which we then annotate according to their alignment with five coding preferences using the <b>LLM-as-a-Judge</b> approach with <b>GPT-3.5,</b> producing both numerical and textual feedback. We also present CODAL-Bench, a <b>benchmark</b> for assessing <b>LLM</b> alignment with these coding preferences. Our results show that CodeLlama-7B-Instruct, aligned through <b>reinforcement</b> <b>learning</b> from AI feedback (RLAIF) with <b>direct</b> <b>preference</b> <b>optimization</b> (DPO) using CodeUltraFeedback's AI feedback data, outperforms 34B <b>LLMs</b> on CODAL-Bench, validating the utility of CodeUltraFeedback for preference tuning. Furthermore, we show our DPO-aligned CodeLlama model improves functional correctness on HumanEval+ compared to the unaligned base model. Therefore, our contributions bridge the gap in preference tuning of <b>LLMs</b> for code and set the stage for further advancements in model alignment and RLAIF for code intelligence. Our code and data are available at https://github.com/martin-wey/CodeUltraFeedback.

{{</citation>}}


### (2/6 | 166/254) Code Revert Prediction with Graph Neural Networks: A Case Study at J.P. Morgan Chase (Yulong Pei et al., 2024)

{{<citation>}}

Yulong Pei, Salwa Alamir, Rares Dolga, Sameena Shah. (2024)  
**Code Revert Prediction with Graph Neural Networks: A Case Study at J.P. Morgan Chase**
<br/>
<button class="copy-to-clipboard" title="Code Revert Prediction with Graph Neural Networks: A Case Study at J.P. Morgan Chase" index=166>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-SE, cs.SE  
Keyword Score: 23  
Keywords: Graph, Graph Neural Network, Anomaly Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09507v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09507v1.pdf" filename="2403.09507v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Code revert prediction, a specialized form of software defect detection, aims to forecast or predict the likelihood of code changes being reverted or rolled back in software development. This task is very important in practice because by identifying code changes that are more prone to being reverted, developers and project managers can proactively take measures to prevent issues, improve code quality, and optimize development processes. However, compared to code defect detection, code revert prediction has been rarely studied in previous research. Additionally, many previous methods for code defect detection relied on independent features but ignored relationships between code scripts. Moreover, new challenges are introduced due to constraints in an industry setting such as company regulation, limited features and large-scale codebase. To overcome these limitations, this paper presents a systematic empirical study for code revert prediction that integrates the code import <b>graph</b> <b>with</b> <b>code</b> features. Different strategies to address anomalies and data imbalance have been implemented including <b>graph</b> <b>neural</b> <b>networks</b> with imbalance classification and <b>anomaly</b> <b>detection.</b> We conduct the experiments on real-world code commit data within J.P. Morgan Chase which is extremely imbalanced in order to make a comprehensive comparison of these different approaches for the code revert prediction problem.

{{</citation>}}


### (3/6 | 167/254) Welcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models (Ali Nouri et al., 2024)

{{<citation>}}

Ali Nouri, Beatriz Cabrero-Daniel, Fredrik Törner, Hȧkan Sivencrona, Christian Berger. (2024)  
**Welcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models**
<br/>
<button class="copy-to-clipboard" title="Welcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models" index=167>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-AI, cs-SE, cs.SE  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09565v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09565v1.pdf" filename="2403.09565v1.pdf">Download PDF</button>

---


**ABSTRACT**  
DevOps is a necessity in many industries, including the development of Autonomous Vehicles. In those settings, there are iterative activities that reduce the speed of SafetyOps cycles. One of these activities is "Hazard Analysis & Risk Assessment" (HARA), which is an essential step to start the safety requirements specification. As a potential approach to increase the speed of this step in SafetyOps, we have delved into the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Our objective is to systematically assess their potential for application in the field of safety engineering. To that end, we propose a framework to support a higher degree of automation of HARA with <b>LLMs.</b> Despite our endeavors to automate as much of the process as possible, expert review remains crucial to ensure the validity and correctness of the analysis results, with necessary modifications made accordingly.

{{</citation>}}


### (4/6 | 168/254) LLM-based agents for automating the enhancement of user story quality: An early report (Zheying Zhang et al., 2024)

{{<citation>}}

Zheying Zhang, Maruf Rayhan, Tomas Herda, Manuel Goisauf, Pekka Abrahamsson. (2024)  
**LLM-based agents for automating the enhancement of user story quality: An early report**
<br/>
<button class="copy-to-clipboard" title="LLM-based agents for automating the enhancement of user story quality: An early report" index=168>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-AI, cs-SE, cs.SE  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09442v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09442v1.pdf" filename="2403.09442v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In agile software development, maintaining high-quality user stories is crucial, but also challenging. This study explores the use of <b>large</b> <b>language</b> <b>models</b> to automatically improve the user story quality in Austrian Post Group IT agile teams. We developed a reference model for an Autonomous <b>LLM-based</b> Agent System and implemented it at the company. The quality of user stories in the study and the effectiveness of these agents for user story quality improvement was assessed by 11 participants across six agile teams. Our findings demonstrate the potential of <b>LLMs</b> in improving user story quality, contributing to the research on AI role in agile development, and providing a practical example of the transformative impact of AI in an industry setting.

{{</citation>}}


### (5/6 | 169/254) Analyzing and Mitigating (with LLMs) the Security Misconfigurations of Helm Charts from Artifact Hub (Francesco Minna et al., 2024)

{{<citation>}}

Francesco Minna, Fabio Massacci, Katja Tuma. (2024)  
**Analyzing and Mitigating (with LLMs) the Security Misconfigurations of Helm Charts from Artifact Hub**
<br/>
<button class="copy-to-clipboard" title="Analyzing and Mitigating (with LLMs) the Security Misconfigurations of Helm Charts from Artifact Hub" index=169>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-SE, cs.SE  
Keyword Score: 10  
Keywords: Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09537v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09537v1.pdf" filename="2403.09537v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Background: Helm is a package manager that allows defining, installing, and upgrading applications with Kubernetes (K8s), a popular container orchestration platform. A Helm chart is a collection of files describing all dependencies, resources, and parameters required for deploying an application within a K8s cluster. Objective: The goal of this study is to mine and empirically evaluate the security of Helm charts, comparing the performance of existing tools in terms of misconfigurations reported by policies available by default, and measure to what extent <b>LLMs</b> could be used for removing misconfiguration. We also want to investigate whether there are false positives in both the <b>LLM</b> refactorings and the tool outputs. Method: We propose a pipeline to mine Helm charts from Artifact Hub, a popular centralized repository, and analyze them using state-of-the-art open-source tools, such as Checkov and KICS. First, such a pipeline will run several chart analyzers and identify the common and unique misconfigurations reported by each tool. Secondly, it will use <b>LLMs</b> to suggest mitigation for each misconfiguration. Finally, the chart refactoring previously generated will be analyzed again by the same tools to see whether it satisfies the tool's policies. At the same time, we will also perform a manual analysis on a subset of charts to evaluate whether there are false positive misconfigurations from the tool's reporting and in the <b>LLM</b> refactoring.

{{</citation>}}


### (6/6 | 170/254) An Extensive Comparison of Static Application Security Testing Tools (Matteo Esposito et al., 2024)

{{<citation>}}

Matteo Esposito, Valentina Falaschi, Davide Falessi. (2024)  
**An Extensive Comparison of Static Application Security Testing Tools**
<br/>
<button class="copy-to-clipboard" title="An Extensive Comparison of Static Application Security Testing Tools" index=170>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-CR, cs-CY, cs-SE, cs.SE  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09219v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09219v1.pdf" filename="2403.09219v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Context: Static Application Security Testing Tools (SASTTs) identify software vulnerabilities to support the security and reliability of software applications. Interestingly, several studies have suggested that alternative solutions may be more effective than SASTTs due to their tendency to generate false alarms, commonly referred to as low Precision. Aim: We aim to comprehensively evaluate SASTTs, setting a reliable <b>benchmark</b> for assessing and finding gaps in vulnerability identification mechanisms based on SASTTs or alternatives. Method: Our SASTTs evaluation is based on a controlled, though synthetic, Java codebase. It involves an assessment of 1.5 million test executions, and it features innovative methodological features such as effort-aware accuracy metrics and method-level analysis. Results: Our findings reveal that SASTTs detect a tiny range of vulnerabilities. In contrast to prevailing wisdom, SASTTs exhibit high Precision while falling short in Recall. Conclusions: The paper suggests that enhancing Recall, alongside expanding the spectrum of detected vulnerability types, should be the primary focus for improving SASTTs or alternative approaches, such as machine learning-based vulnerability identification solutions.

{{</citation>}}


## eess.SY (9)



### (1/9 | 171/254) Exploring the Capabilities and Limitations of Large Language Models in the Electric Energy Sector (Lin Dong et al., 2024)

{{<citation>}}

Lin Dong, Subir Majumder, Fatemeh Doudi, Yuting Cai, Chao Tian, Dileep Kalathi, Kevin Ding, Anupam A. Thatte, Le Xie. (2024)  
**Exploring the Capabilities and Limitations of Large Language Models in the Electric Energy Sector**
<br/>
<button class="copy-to-clipboard" title="Exploring the Capabilities and Limitations of Large Language Models in the Electric Energy Sector" index=171>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 70  
Keywords: Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Chatbot, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09125v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09125v1.pdf" filename="2403.09125v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> as <b>chatbots</b> have drawn remarkable attention thanks to their versatile capability in natural language processing as well as in a wide range of tasks. While there has been great enthusiasm towards adopting such foundational model-based artificial intelligence tools in all sectors possible, the capabilities and limitations of such <b>LLMs</b> in improving the operation of the electric energy sector need to be explored, and this article identifies fruitful directions in this regard. Key future research directions include data collection systems for <b>fine-tuning</b> <b>LLMs,</b> embedding power system-specific tools in the <b>LLMs,</b> and <b>retrieval</b> <b>augmented</b> <b>generation</b> <b>(RAG)-based</b> knowledge pool to improve the quality of <b>LLM</b> responses and <b>LLMs</b> in safety-critical use cases.

{{</citation>}}


### (2/9 | 172/254) Is Data All That Matters? The Role of Control Frequency for Learning-Based Sampled-Data Control of Uncertain Systems (Ralf Römer et al., 2024)

{{<citation>}}

Ralf Römer, Lukas Brunke, Siqi Zhou, Angela P. Schoellig. (2024)  
**Is Data All That Matters? The Role of Control Frequency for Learning-Based Sampled-Data Control of Uncertain Systems**
<br/>
<button class="copy-to-clipboard" title="Is Data All That Matters? The Role of Control Frequency for Learning-Based Sampled-Data Control of Uncertain Systems" index=172>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-RO, cs-SY, eess-SY, eess.SY  
Keyword Score: 50  
Keywords: Continuous Time, Continuous Time, Discrete Time, Discrete Time, Probabilistic Model, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09504v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09504v1.pdf" filename="2403.09504v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Learning models or control policies from data has become a powerful tool to improve the performance of uncertain systems. While a strong focus has been placed on increasing the amount and quality of data to improve performance, data can never fully eliminate uncertainty, making feedback necessary to ensure stability and performance. We show that the control frequency at which the input is recalculated is a crucial design parameter, yet it has hardly been considered before. We address this gap by combining <b>probabilistic</b> <b>model</b> learning and sampled-data control. We use Gaussian processes (GPs) to learn a <b>continuous-time</b> <b>model</b> and compute a corresponding <b>discrete-time</b> <b>controller.</b> The result is an uncertain sampled-data control system, for which we derive robust stability conditions. We formulate semidefinite programs to compute the minimum control frequency required for stability and to optimize performance. As a result, our approach enables us to study the effect of both control frequency and data on stability and closed-loop performance. We show in numerical <b>simulations</b> of a quadrotor that performance can be improved by increasing either the amount of data or the control frequency, and that we can trade off one for the other. For example, by increasing the control frequency by 33%, we can reduce the number of data points by half while still achieving similar performance.

{{</citation>}}


### (3/9 | 173/254) Fairness-Aware Multi-Server Federated Learning Task Delegation over Wireless Networks (Yulan Gao et al., 2024)

{{<citation>}}

Yulan Gao, Chao Ren, Han Yu. (2024)  
**Fairness-Aware Multi-Server Federated Learning Task Delegation over Wireless Networks**
<br/>
<button class="copy-to-clipboard" title="Fairness-Aware Multi-Server Federated Learning Task Delegation over Wireless Networks" index=173>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 20  
Keywords: Fairness, Federated Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09153v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09153v1.pdf" filename="2403.09153v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In the rapidly advancing field of <b>federated</b> <b>learning</b> (FL), ensuring efficient FL task delegation while incentivising FL client participation poses significant challenges, especially in wireless networks where FL participants' coverage is limited. Existing Contract Theory-based methods are designed under the assumption that there is only one FL server in the system (i.e., the monopoly market assumption), which in unrealistic in practice. To address this limitation, we propose <b>Fairness-Aware</b> Multi-Server FL task delegation approach (FAMuS), a novel framework based on Contract Theory and Lyapunov optimization to jointly address these intricate issues facing wireless multi-server FL networks (WMSFLN). Within a given WMSFLN, a task requester products multiple FL tasks and delegate them to FL servers which coordinate the training processes. To ensure fair treatment of FL servers, FAMuS establishes virtual queues to track their previous access to FL tasks, updating them in relation to the resulting FL model performance. The objective is to minimize the time-averaged cost in a WMSFLN, while ensuring all queues remain stable. This is particularly challenging given the incomplete information regarding FL clients' participation cost and the unpredictable nature of the WMSFLN state, which depends on the locations of the mobile clients. Extensive experiments comparing FAMuS against five state-of-the-art approaches based on two real-world datasets demonstrate that it achieves 6.91% higher test accuracy, 27.34% lower cost, and 0.63% higher <b>fairness</b> on average than the best-performing baseline.

{{</citation>}}


### (4/9 | 174/254) Optimal Pinning Control for Synchronization over Temporal Networks (Aandrew Baggio S et al., 2024)

{{<citation>}}

Aandrew Baggio S, Rachel Kalpana Kalaimani. (2024)  
**Optimal Pinning Control for Synchronization over Temporal Networks**
<br/>
<button class="copy-to-clipboard" title="Optimal Pinning Control for Synchronization over Temporal Networks" index=174>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09127v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09127v1.pdf" filename="2403.09127v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we address the finite time synchronization of a network of dynamical systems with time-varying interactions modeled using temporal networks. We synchronize a few nodes initially using external control inputs. These nodes are termed as pinning nodes. The other nodes are synchronized by interacting with the pinning nodes and with each other. We first provide sufficient conditions for the network to be synchronized. Then we formulate an optimization problem to minimize the number of pinning nodes for synchronizing the entire network. Finally, we address the problem of maximizing the number of synchronized nodes when there are constraints on the number of nodes that could be pinned. We show that this problem belongs to the class of NP-hard problems and propose a greedy heuristic. We illustrate the results using numerical <b>simulations.</b>

{{</citation>}}


### (5/9 | 175/254) Confidence-Aware Safe and Stable Control of Control-Affine Systems (Shiqing Wei et al., 2024)

{{<citation>}}

Shiqing Wei, Prashanth Krishnamurthy, Farshad Khorrami. (2024)  
**Confidence-Aware Safe and Stable Control of Control-Affine Systems**
<br/>
<button class="copy-to-clipboard" title="Confidence-Aware Safe and Stable Control of Control-Affine Systems" index=175>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09067v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09067v1.pdf" filename="2403.09067v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Designing control inputs that satisfy safety requirements is crucial in safety-critical nonlinear control, and this task becomes particularly challenging when full-state measurements are unavailable. In this work, we address the problem of synthesizing safe and stable control for control-affine systems via output feedback (using an observer) while reducing the estimation error of the observer. To achieve this, we adapt control Lyapunov function (CLF) and control barrier function (CBF) techniques to the output feedback setting. Building upon the existing CLF-CBF-QP (Quadratic Program) and CBF-QP frameworks, we formulate two confidence-aware optimization problems and establish the Lipschitz continuity of the obtained solutions. To validate our approach, we conduct <b>simulation</b> studies on two illustrative examples. The <b>simulation</b> studies indicate both improvements in the observer's estimation accuracy and the fulfillment of safety and control requirements.

{{</citation>}}


### (6/9 | 176/254) A Geometric Approach to Resilient Distributed Consensus Accounting for State Imprecision and Adversarial Agents (Christopher A. Lee et al., 2024)

{{<citation>}}

Christopher A. Lee, Waseem Abbas. (2024)  
**A Geometric Approach to Resilient Distributed Consensus Accounting for State Imprecision and Adversarial Agents**
<br/>
<button class="copy-to-clipboard" title="A Geometric Approach to Resilient Distributed Consensus Accounting for State Imprecision and Adversarial Agents" index=176>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09009v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09009v1.pdf" filename="2403.09009v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents a novel approach for resilient distributed consensus in multiagent networks when dealing with adversarial agents imprecision in states observed by normal agents. Traditional resilient distributed consensus algorithms often presume that agents have exact knowledge of their neighbors' states, which is unrealistic in practical scenarios. We show that such existing methods are inadequate when agents only have access to imprecise states of their neighbors. To overcome this challenge, we adapt a geometric approach and model an agent's state by an `imprecision region' rather than a point in $\mathbb{R}^d$. From a given set of imprecision regions, we first present an efficient way to compute a region that is guaranteed to lie in the convex hull of true, albeit unknown, states of agents. We call this region the \emph{invariant hull} of imprecision regions and provide its geometric characterization. Next, we use these invariant hulls to identify a \emph{safe point} for each normal agent. The safe point of an agent lies within the convex hull of its \emph{normal} neighbors' states and hence is used by the agent to update it's state. This leads to the aggregation of normal agents' states to safe points inside the convex hull of their initial states, or an approximation of consensus. We also illustrate our results through <b>simulations.</b> Our contributions enhance the robustness of resilient distributed consensus algorithms by accommodating state imprecision without compromising resilience against adversarial agents.

{{</citation>}}


### (7/9 | 177/254) Defense via Behavior Attestation against Attacks in Connected and Automated Vehicles based Federated Learning Systems (Godwin Badu-Marfo et al., 2024)

{{<citation>}}

Godwin Badu-Marfo, Ranwa Al Mallah, Bilal Farooq. (2024)  
**Defense via Behavior Attestation against Attacks in Connected and Automated Vehicles based Federated Learning Systems**
<br/>
<button class="copy-to-clipboard" title="Defense via Behavior Attestation against Attacks in Connected and Automated Vehicles based Federated Learning Systems" index=177>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 10  
Keywords: Federated Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09531v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09531v1.pdf" filename="2403.09531v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The recent application of <b>Federated</b> <b>Learning</b> algorithms in IOT and Wireless vehicular networks have given rise to newer cyber threats in the mobile environment which hitherto were not present in traditional fixed networks. These threats arise due to the intrinsic nature of wireless transmission medium and other inherent characteristics of mobile networks such as high-node mobility and rapidly changing topology. This paper investigates the robustness of Vehicular AttestedFL defense strategies against falsified information attacks by tracking the behavior. We show that the defense strategies are capable of detecting and eliminating malicious nodes in the wireless mobile setting of the future smart road networks.

{{</citation>}}


### (8/9 | 178/254) Learning Algorithms for Verification of Markov Decision Processes (Tomáš Brázdil et al., 2024)

{{<citation>}}

Tomáš Brázdil, Krishnendu Chatterjee, Martin Chmelik, Vojtěch Forejt, Jan Křetínský, Marta Kwiatkowska, Tobias Meggendorfer, David Parker, Mateusz Ujma. (2024)  
**Learning Algorithms for Verification of Markov Decision Processes**
<br/>
<button class="copy-to-clipboard" title="Learning Algorithms for Verification of Markov Decision Processes" index=178>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-AI, cs-SY, eess-SY, eess.SY  
Keyword Score: 10  
Keywords: Markov Decision Process  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09184v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09184v1.pdf" filename="2403.09184v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present a general framework for applying learning algorithms and heuristical guidance to the verification of Markov decision processes <b>(MDPs),</b> based on the ideas of Br\'azdil, T. et al. (2014). Verification of Markov Decision Processes Using Learning Algorithms. The primary goal of the techniques presented in that work is to improve performance by avoiding an exhaustive exploration of the state space, guided by heuristics. This approach is significantly extended in this work. Several details of the base theory are refined and errors are fixed. Section 1.3 provides an overview of all differences. The presented framework focuses on probabilistic reachability, which is a core problem in verification, and is instantiated in two distinct scenarios. The first assumes that full knowledge of the MDP is available, in particular precise transition probabilities. It performs a heuristic-driven partial exploration of the model, yielding precise lower and upper bounds on the required probability. The second tackles the case where we may only sample the MDP without knowing the exact transition dynamics. Here, we obtain probabilistic guarantees, again in terms of both the lower and upper bounds, which provides efficient stopping criteria for the approximation. In particular, the latter is an extension of statistical model-checking (SMC) for unbounded properties in <b>MDPs.</b> In contrast to other related approaches, we do not restrict our attention to time-bounded (finite-horizon) or discounted properties, nor assume any particular structural properties of the MDP.

{{</citation>}}


### (9/9 | 179/254) Partitioning Distribution Networks for Integrated Electrification Planning (Olamide Oladeji et al., 2024)

{{<citation>}}

Olamide Oladeji, Pedro Ciller Cutillas, Fernando de Cuadra, Ignacio Perez-Arriaga. (2024)  
**Partitioning Distribution Networks for Integrated Electrification Planning**
<br/>
<button class="copy-to-clipboard" title="Partitioning Distribution Networks for Integrated Electrification Planning" index=179>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-CE, cs-SY, eess-SY, eess.SY  
Keyword Score: 6  
Keywords: Benchmarking, Clustering  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09111v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09111v1.pdf" filename="2403.09111v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In many developing countries, access to electricity remains a significant challenge. Electrification planners in these countries often have to make important decisions on the mode of electrification and the planning of electrical networks for those without access, while under resource constraints. An integrated approach to electrification planning in which traditional grid electrification is complemented off-the-grid technologies such as off-grid microgrids and stand-alone systems can enable the economic provision of electricity access in these regions. This integrated planning approach can be facilitated by determining the least-cost mode of electrification - i.e by electric grid extension or off-grid systems - for non-electrified consumers in a region under analysis, while considering technical, economic and environmental constraints. Computational <b>clustering</b> methods the identification of consumer clusters (either as clusters of off-grid microgrids, stand-alone systems or grid-extension projects) can be undertaken using computational <b>clustering</b> methods. This paper presents a novel computational approach to achieve this purpose. This methodology involves exploiting the grid network that connects all consumers, by greedily partitioning the network to identify clusters of consumers to be electrified by grid-extension and off-grid microgrid systems. Using test cases and sensitivity analyses, we implement and <b>benchmark</b> this top-down approach with those obtained from a bottom-up <b>clustering</b> methodology used by the Reference Electrification Model, a model obtainable in literature. Results presented show that the alternative top-down methodology proposed can compare favorably, in terms of global electrification costs, with a bottom-up approach to rural electrification planning.

{{</citation>}}


## cs.IR (5)



### (1/5 | 180/254) USimAgent: Large Language Models for Simulating Search Users (Erhan Zhang et al., 2024)

{{<citation>}}

Erhan Zhang, Xingzhu Wang, Peiyuan Gong, Yankai Lin, Jiaxin Mao. (2024)  
**USimAgent: Large Language Models for Simulating Search Users**
<br/>
<button class="copy-to-clipboard" title="USimAgent: Large Language Models for Simulating Search Users" index=180>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-AI, cs-IR, cs.IR  
Keyword Score: 60  
Keywords: Simulation, Simulator, Information Retrieval, Reasoning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09142v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09142v1.pdf" filename="2403.09142v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Due to the advantages in the cost-efficiency and reproducibility, user <b>simulation</b> has become a promising solution to the user-centric evaluation of <b>information</b> <b>retrieval</b> systems. Nonetheless, accurately simulating user search behaviors has long been a challenge, because users' actions in search are highly complex and driven by intricate cognitive processes such as learning, <b>reasoning,</b> and planning. Recently, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated remarked potential in simulating human-level intelligence and have been used in building autonomous agents for various tasks. However, the potential of using <b>LLMs</b> in simulating search behaviors has not yet been fully explored. In this paper, we introduce a <b>LLM-based</b> user search behavior simulator, USimAgent. The proposed simulator can simulate users' querying, clicking, and stopping behaviors during search, and thus, is capable of generating complete search sessions for specific search tasks. Empirical investigation on a real user behavior dataset shows that the proposed simulator outperforms existing methods in query generation and is comparable to traditional methods in predicting user clicks and stopping behaviors. These results not only validate the effectiveness of using <b>LLMs</b> for user <b>simulation</b> but also shed light on the development of a more robust and generic user simulators.

{{</citation>}}


### (2/5 | 181/254) Logical Discrete Graphical Models Must Supplement Large Language Models for Information Synthesis (Gregory Coppola, 2024)

{{<citation>}}

Gregory Coppola. (2024)  
**Logical Discrete Graphical Models Must Supplement Large Language Models for Information Synthesis**
<br/>
<button class="copy-to-clipboard" title="Logical Discrete Graphical Models Must Supplement Large Language Models for Information Synthesis" index=181>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-IR, cs.IR  
Keyword Score: 30  
Keywords: Information Retrieval, Reasoning, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09599v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09599v1.pdf" filename="2403.09599v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Given the emergent <b>reasoning</b> abilities of <b>large</b> <b>language</b> <b>models,</b> <b>information</b> <b>retrieval</b> is becoming more complex. Rather than just retrieve a document, modern <b>information</b> <b>retrieval</b> systems advertise that they can synthesize an answer based on potentially many different documents, conflicting data sources, and using <b>reasoning.</b> We review recent literature and argue that the <b>large</b> <b>language</b> <b>model</b> has crucial flaws that prevent it from on its own ever constituting general intelligence, or answering general <b>information</b> <b>synthesis</b> requests. This review shows that the following are problems for <b>large</b> <b>language</b> <b>models:</b> hallucinations, complex <b>reasoning,</b> planning under uncertainty, and complex calculations. We outline how logical discrete graphical models can solve all of these problems, and outline a method of training a logical discrete model from unlabeled text.

{{</citation>}}


### (3/5 | 182/254) Projected Gradient Descent for Spectral Compressed Sensing via Symmetric Hankel Factorization (Jinsheng Li et al., 2024)

{{<citation>}}

Jinsheng Li, Wei Cui, Xu Zhang. (2024)  
**Projected Gradient Descent for Spectral Compressed Sensing via Symmetric Hankel Factorization**
<br/>
<button class="copy-to-clipboard" title="Projected Gradient Descent for Spectral Compressed Sensing via Symmetric Hankel Factorization" index=182>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-IR, cs.IR  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09031v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09031v1.pdf" filename="2403.09031v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Current spectral compressed sensing methods via Hankel matrix completion employ symmetric factorization to demonstrate the low-rank property of the Hankel matrix. However, previous non-convex gradient methods only utilize asymmetric factorization to achieve spectral compressed sensing. In this paper, we propose a novel nonconvex projected gradient descent method for spectral compressed sensing via symmetric factorization named Symmetric Hankel Projected Gradient Descent (SHGD), which updates only one matrix and avoids a balancing regularization term. SHGD reduces about half of the computation and storage costs compared to the prior gradient method based on asymmetric factorization. {Besides, the symmetric factorization employed in our work is completely novel to the prior low-rank factorization model, introducing a new factorization ambiguity under complex orthogonal transformation}. Novel distance metrics are designed for our factorization method and a linear convergence guarantee to the desired signal is established with $O(r^2\log(n))$ observations. Numerical <b>simulations</b> demonstrate the superior performance of the proposed SHGD method in phase transitions and computation efficiency compared to state-of-the-art methods.

{{</citation>}}


### (4/5 | 183/254) Seed-based information retrieval in networks of research publications: Evaluation of direct citations, bibliographic coupling, co-citations and PubMed related article score (Peter Sjögårde et al., 2024)

{{<citation>}}

Peter Sjögårde, Per Ahlgren. (2024)  
**Seed-based information retrieval in networks of research publications: Evaluation of direct citations, bibliographic coupling, co-citations and PubMed related article score**
<br/>
<button class="copy-to-clipboard" title="Seed-based information retrieval in networks of research publications: Evaluation of direct citations, bibliographic coupling, co-citations and PubMed related article score" index=183>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-IR, cs.IR  
Keyword Score: 10  
Keywords: Information Retrieval  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09295v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09295v1.pdf" filename="2403.09295v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this contribution, we deal with seed-based <b>information</b> <b>retrieval</b> in networks of research publications. Using systematic reviews as a baseline, and publication data from the NIH Open Citation Collection, we compare the performance of the three citation-based approaches direct citation, co-citation, and bibliographic coupling with respect to recall and precision measures. In addition, we include the PubMed Related Article score as well as combined approaches in the comparison. We also provide a fairly comprehensive review of earlier research in which citation relations have been used for <b>information</b> <b>retrieval</b> purposes. The results show an advantage for co-citation over bibliographic coupling and direct citation. However, combining the three approaches outperforms the exclusive use of co-citation in the study. The results further indicate, in line with previous research, that combining citation-based approaches with textual approaches enhances the performance of seed-based <b>information</b> <b>retrieval.</b> The results from the study may guide approaches combining citation-based and textual approaches in their choice of citation similarity measures. We suggest that future research use more structured approaches to evaluate methods for seed-based retrieval of publications, including comparative approaches as well as the elaboration of common data sets and baselines for evaluation.

{{</citation>}}


### (5/5 | 184/254) Online and Offline Evaluation in Search Clarification (Leila Tavakoli et al., 2024)

{{<citation>}}

Leila Tavakoli, Johanne R. Trippas, Hamed Zamani, Falk Scholer, Mark Sanderson. (2024)  
**Online and Offline Evaluation in Search Clarification**
<br/>
<button class="copy-to-clipboard" title="Online and Offline Evaluation in Search Clarification" index=184>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-IR, cs.IR  
Keyword Score: 10  
Keywords: Information Retrieval  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09180v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09180v1.pdf" filename="2403.09180v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The effectiveness of clarification question models in engaging users within search systems is currently constrained, casting doubt on their overall usefulness. To improve the performance of these models, it is crucial to employ assessment approaches that encompass both real-time feedback from users (online evaluation) and the characteristics of clarification questions evaluated through human assessment (offline evaluation). However, the relationship between online and offline evaluations has been debated in <b>information</b> <b>retrieval.</b> This study aims to investigate how this discordance holds in search clarification. We use user engagement as ground truth and employ several offline labels to investigate to what extent the offline ranked lists of clarification resemble the ideal ranked lists based on online user engagement.

{{</citation>}}


## cs.SI (3)



### (1/3 | 185/254) From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News (Yuhan Liu et al., 2024)

{{<citation>}}

Yuhan Liu, Xiuying Chen, Xiaoqing Zhang, Xing Gao, Ji Zhang, Rui Yan. (2024)  
**From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News**
<br/>
<button class="copy-to-clipboard" title="From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News" index=185>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SI  
Categories: cs-AI, cs-CL, cs-SI, cs.SI  
Keyword Score: 50  
Keywords: Simulation, Simulator, Fake News Detection, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09498v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09498v1.pdf" filename="2403.09498v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In the digital era, the rapid propagation of <b>fake</b> <b>news</b> and rumors via social networks brings notable societal challenges and impacts public opinion regulation. Traditional <b>fake</b> <b>news</b> modeling typically forecasts the general popularity trends of different groups or numerically represents opinions shift. However, these methods often oversimplify real-world complexities and overlook the rich semantic information of news text. The advent of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> provides the possibility of modeling subtle dynamics of opinion. Consequently, in this work, we introduce a <b>Fake</b> <b>news</b> Propagation <b>Simulation</b> framework (FPS) based on <b>LLM,</b> which studies the trends and control of <b>fake</b> <b>news</b> propagation in detail. Specifically, each agent in the <b>simulation</b> represents an individual with a distinct personality. They are equipped with both short-term and long-term memory, as well as a reflective mechanism to mimic human-like thinking. Every day, they engage in random opinion exchanges, reflect on their thinking, and update their opinions. Our <b>simulation</b> results uncover patterns in <b>fake</b> <b>news</b> propagation related to topic relevance, and individual traits, aligning with real-world observations. Additionally, we evaluate various intervention strategies and demonstrate that early and appropriately frequent interventions strike a balance between governance cost and effectiveness, offering valuable insights for practical applications. Our study underscores the significant utility and potential of <b>LLMs</b> in combating <b>fake</b> <b>news.</b>

{{</citation>}}


### (2/3 | 186/254) Rumor Mitigation in Social Media Platforms with Deep Reinforcement Learning (Hongyuan Su et al., 2024)

{{<citation>}}

Hongyuan Su, Yu Zheng, Jingtao Ding, Depeng Jin, Yong Li. (2024)  
**Rumor Mitigation in Social Media Platforms with Deep Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="Rumor Mitigation in Social Media Platforms with Deep Reinforcement Learning" index=186>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SI  
Categories: 68T09, cs-SI, cs.SI  
Keyword Score: 23  
Keywords: Graph, Graph Neural Network, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09217v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09217v1.pdf" filename="2403.09217v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Social media platforms have become one of the main channels where people disseminate and acquire information, of which the reliability is severely threatened by rumors widespread in the network. Existing approaches such as suspending users or broadcasting real information to combat rumors are either with high cost or disturbing users. In this paper, we introduce a novel rumor mitigation paradigm, where only a minimal set of links in the social network are intervened to decelerate the propagation of rumors, countering misinformation with low business cost and user awareness. A knowledge-informed agent embodying rumor propagation mechanisms is developed, which intervenes the social network with a <b>graph</b> <b>neural</b> <b>network</b> for capturing information flow in the social media platforms and a policy network for selecting links. Experiments on real social media platforms demonstrate that the proposed approach can effectively alleviate the influence of rumors, substantially reducing the affected populations by over 25%. Codes for this paper are released at https://github.com/tsinghua-fib-lab/DRL-Rumor-Mitigation.

{{</citation>}}


### (3/3 | 187/254) Belief and Persuasion in Scientific Discourse on Social Media: A Study of the COVID-19 Pandemic (Salwa Alamir et al., 2024)

{{<citation>}}

Salwa Alamir, Armineh Nourbakhsh, Cecilia Tilli, Sameena Shah, Manuela Veloso. (2024)  
**Belief and Persuasion in Scientific Discourse on Social Media: A Study of the COVID-19 Pandemic**
<br/>
<button class="copy-to-clipboard" title="Belief and Persuasion in Scientific Discourse on Social Media: A Study of the COVID-19 Pandemic" index=187>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SI  
Categories: cs-SI, cs.SI, physics-soc-ph  
Keyword Score: 10  
Keywords: Recommendation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09260v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09260v1.pdf" filename="2403.09260v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Research into COVID-19 has been rapidly evolving since the onset of the pandemic. This occasionally results in contradictory <b>recommendations</b> by credible sources of scientific opinion, public health authorities, and medical professionals. In this study, we examine whether this has resulted in a lack of trust in scientific opinion, by examining the belief patterns of social media users and their reactions to statements related to scientific facts. We devise models to mine belief and persuasion in Twitter discourse using semi-supervised approaches, and show the relationship between lack of belief and insurgence of paranoia and conspiracy theories. By investigating these belief patterns, we explore the best persuasion tactics for communicating information related to COVID-19.

{{</citation>}}


## cs.CR (7)



### (1/7 | 188/254) AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting (Yu Wang et al., 2024)

{{<citation>}}

Yu Wang, Xiaogeng Liu, Yu Li, Muhao Chen, Chaowei Xiao. (2024)  
**AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting**
<br/>
<button class="copy-to-clipboard" title="AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting" index=188>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-AI, cs-CR, cs.CR  
Keyword Score: 46  
Keywords: Fine-tuning, Multi-modal, Multi-modal, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09513v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09513v1.pdf" filename="2403.09513v1.pdf">Download PDF</button>

---


**ABSTRACT**  
With the advent and widespread deployment of <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs), the imperative to ensure their safety has become increasingly pronounced. However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g., "harmful text") has been injected into the images to mislead MLLMs. In this work, we aim to defend against such threats. Specifically, we propose \textbf{Ada}ptive \textbf{Shield} <b>Prompting</b> (\textbf{AdaShield}), which prepends inputs with defense <b>prompts</b> to defend MLLMs against structure-based jailbreak attacks without <b>fine-tuning</b> MLLMs or training additional modules (e.g., post-stage content detector). Initially, we present a manually designed static defense <b>prompt,</b> which thoroughly examines the image and instruction content step by step and specifies response methods to malicious queries. Furthermore, we introduce an adaptive auto-refinement framework, consisting of a target MLLM and a <b>LLM-based</b> defense <b>prompt</b> generator (Defender). These components collaboratively and iteratively communicate to generate a defense <b>prompt.</b> Extensive experiments on the popular structure-based jailbreak attacks and benign datasets show that our methods can consistently improve MLLMs' robustness against structure-based jailbreak attacks without compromising the model's general capabilities evaluated on standard benign tasks. Our code is available at https://github.com/rain305f/AdaShield.

{{</citation>}}


### (2/7 | 189/254) Graph-Based DDoS Attack Detection in IoT Systems with Lossy Network (Arvin Hekmati et al., 2024)

{{<citation>}}

Arvin Hekmati, Bhaskar Krishnamachari. (2024)  
**Graph-Based DDoS Attack Detection in IoT Systems with Lossy Network**
<br/>
<button class="copy-to-clipboard" title="Graph-Based DDoS Attack Detection in IoT Systems with Lossy Network" index=189>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs.CR  
Keyword Score: 43  
Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09118v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09118v1.pdf" filename="2403.09118v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This study introduces a robust solution for the detection of Distributed Denial of Service (DDoS) attacks in Internet of Things (IoT) systems, leveraging the capabilities of <b>Graph</b> <b>Convolutional</b> <b>Networks</b> <b>(GCN).</b> By conceptualizing IoT devices as nodes within a <b>graph</b> <b>structure,</b> <b>we</b> present a detection mechanism capable of operating efficiently even in lossy network environments. We introduce various <b>graph</b> <b>topologies</b> <b>for</b> modeling IoT networks and evaluate them for detecting tunable futuristic DDoS attacks. By studying different levels of network connection loss and various attack situations, we demonstrate that the correlation-based hybrid <b>graph</b> <b>structure</b> <b>is</b> effective in spotting DDoS attacks, substantiating its good performance even in lossy network scenarios. The results indicate a remarkable performance of the <b>GCN-based</b> DDoS detection model with an F1 score of up to 91%. Furthermore, we observe at most a 2% drop in F1-score in environments with up to 50% connection loss. The findings from this study highlight the advantages of utilizing <b>GCN</b> for the security of IoT systems which benefit from high detection accuracy while being resilient to connection disruption.

{{</citation>}}


### (3/7 | 190/254) PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy Traps (Ruixuan Liu et al., 2024)

{{<citation>}}

Ruixuan Liu, Tianhao Wang, Yang Cao, Li Xiong. (2024)  
**PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy Traps**
<br/>
<button class="copy-to-clipboard" title="PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy Traps" index=190>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs.CR  
Keyword Score: 35  
Keywords: Black Box, Fine-tuning, Fine-tuning, Pre-trained Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09562v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09562v1.pdf" filename="2403.09562v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The pre-training and <b>fine-tuning</b> paradigm has demonstrated its effectiveness and has become the standard approach for tailoring language models to various tasks. Currently, community-based platforms offer easy access to various <b>pre-trained</b> <b>models,</b> <b>as</b> anyone can publish without strict validation processes. However, a released <b>pre-trained</b> <b>model</b> <b>can</b> be a privacy trap for <b>fine-tuning</b> datasets if it is carefully designed. In this work, we propose PreCurious framework to reveal the new attack surface where the attacker releases the <b>pre-trained</b> <b>model</b> <b>and</b> gets a <b>black-box</b> <b>access</b> to the final <b>fine-tuned</b> model. PreCurious aims to escalate the general privacy risk of both membership inference and data extraction. The key intuition behind PreCurious is to manipulate the memorization stage of the <b>pre-trained</b> <b>model</b> <b>and</b> guide <b>fine-tuning</b> with a seemingly legitimate configuration. The effectiveness of defending against privacy attacks on a <b>fine-tuned</b> model seems promising, as empirical and theoretical evidence suggests that parameter-efficient and differentially private <b>fine-tuning</b> techniques are invulnerable to privacy attacks. But PreCurious demonstrates the possibility of breaking up invulnerability in a stealthy manner compared to <b>fine-tuning</b> on a benign model. By further leveraging a sanitized dataset, PreCurious can extract originally unexposed secrets under differentially private <b>fine-tuning.</b> Thus, PreCurious raises warnings for users who download <b>pre-trained</b> <b>models</b> <b>from</b> unknown sources, rely solely on tutorials or common-sense defenses, and previously release sanitized datasets even after perfect scrubbing.

{{</citation>}}


### (4/7 | 191/254) Optimistic Verifiable Training by Controlling Hardware Nondeterminism (Megha Srivastava et al., 2024)

{{<citation>}}

Megha Srivastava, Simran Arora, Dan Boneh. (2024)  
**Optimistic Verifiable Training by Controlling Hardware Nondeterminism**
<br/>
<button class="copy-to-clipboard" title="Optimistic Verifiable Training by Controlling Hardware Nondeterminism" index=191>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-AI, cs-CR, cs-LG, cs.CR  
Keyword Score: 30  
Keywords: Fine-tuning, GPT, GPT-2  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09603v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09603v1.pdf" filename="2403.09603v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources. However, ensuring correctness of training and guarding against potential training-time attacks, such as data poisoning, poses challenges. Existing works on verifiable training largely fall into two classes: proof-based systems, which struggle to scale due to requiring cryptographic techniques, and "optimistic" methods that consider a trusted third-party auditor who replicates the training process. A key challenge with the latter is that hardware nondeterminism between GPU types during training prevents an auditor from replicating the training process exactly, and such schemes are therefore non-robust. We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thresholding procedure, to successfully control for nondeterminism. Across three different NVIDIA GPUs (A40, Titan XP, RTX 2080 Ti), we achieve exact training replication at FP32 precision for both full-training and <b>fine-tuning</b> of ResNet-50 (23M) and <b>GPT-2</b> (117M) models. Our verifiable training scheme significantly decreases the storage and time costs compared to proof-based systems.

{{</citation>}}


### (5/7 | 192/254) Privacy Preserving Anomaly Detection on Homomorphic Encrypted Data from IoT Sensors (Anca Hangan et al., 2024)

{{<citation>}}

Anca Hangan, Dragos Lazea, Tudor Cioara. (2024)  
**Privacy Preserving Anomaly Detection on Homomorphic Encrypted Data from IoT Sensors**
<br/>
<button class="copy-to-clipboard" title="Privacy Preserving Anomaly Detection on Homomorphic Encrypted Data from IoT Sensors" index=192>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs.CR  
Keyword Score: 20  
Keywords: Anomaly Detection, Adversarial Attack  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09322v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09322v1.pdf" filename="2403.09322v1.pdf">Download PDF</button>

---


**ABSTRACT**  
IoT devices have become indispensable components of our lives, and the advancement of AI technologies will make them even more pervasive, increasing the vulnerability to malfunctions or cyberattacks and raising privacy concerns. Encryption can mitigate these challenges; however, most existing <b>anomaly</b> <b>detection</b> techniques decrypt the data to perform the analysis, potentially undermining the encryption protection provided during transit or storage. Homomorphic encryption schemes are promising solutions as they enable the processing and execution of operations on IoT data while still encrypted, however, these schemes offer only limited operations, which poses challenges to their practical usage. In this paper, we propose a novel privacy-preserving <b>anomaly</b> <b>detection</b> solution designed for homomorphically encrypted data generated by IoT devices that efficiently detects abnormal values without performing decryption. We have adapted the Histogram-based <b>anomaly</b> <b>detection</b> technique for TFHE scheme to address limitations related to the input size and the depth of computation by implementing vectorized support operations. These operations include addition, value placement in buckets, labeling abnormal buckets based on a threshold frequency, labeling abnormal values based on their range, and bucket labels. Evaluation results show that the solution effectively detects anomalies without requiring data decryption and achieves consistent results comparable to the mechanism operating on plain data. Also, it shows robustness and resilience against various challenges commonly encountered in IoT environments, such as noisy sensor data, <b>adversarial</b> <b>attacks,</b> communication failures, and device malfunctions. Moreover, the time and computational overheads determined for several solution configurations, despite being large, are reasonable compared to those reported in existing literature.

{{</citation>}}


### (6/7 | 193/254) LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection (Xiangrui Cai et al., 2024)

{{<citation>}}

Xiangrui Cai, Yang Wang, Sihan Xu, Hao Li, Ying Zhang, Xiaojie Yuan. (2024)  
**LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection**
<br/>
<button class="copy-to-clipboard" title="LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection" index=193>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-AI, cs-CR, cs-LG, cs.CR  
Keyword Score: 13  
Keywords: Graph, Anomaly Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09209v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09209v1.pdf" filename="2403.09209v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Enterprises and organizations are faced with potential threats from insider employees that may lead to serious consequences. Previous studies on insider threat detection (ITD) mainly focus on detecting abnormal users or abnormal time periods (e.g., a week or a day). However, a user may have hundreds of thousands of activities in the log, and even within a day there may exist thousands of activities for a user, requiring a high investigation budget to verify abnormal users or activities given the detection results. On the other hand, existing works are mainly post-hoc methods rather than real-time detection, which can not report insider threats in time before they cause loss. In this paper, we conduct the first study towards real-time ITD at activity level, and present a fine-grained and efficient framework LAN. Specifically, LAN simultaneously learns the temporal dependencies within an activity sequence and the relationships between activities across sequences with <b>graph</b> structure learning. Moreover, to mitigate the data imbalance problem in ITD, we propose a novel hybrid prediction loss, which integrates self-supervision signals {from normal activities} and supervision signals from abnormal activities into a unified loss for <b>anomaly</b> <b>detection.</b> We evaluate the performance of LAN on two widely used datasets, i.e., CERT r4.2 and CERT r5.2. Extensive and comparative experiments demonstrate the superiority of LAN, outperforming 9 state-of-the-art baselines by at least 9.92% and 6.35% in AUC for real-time ITD on CERT r4.2 and r5.2, respectively. Moreover, LAN can be also applied to post-hoc ITD, surpassing 8 competitive baselines by at least 7.70% and 4.03% in AUC on two datasets. Finally, the ablation study, parameter analysis, and compatibility analysis evaluate the impact of each module and hyper-parameter in LAN.

{{</citation>}}


### (7/7 | 194/254) LDPRecover: Recovering Frequencies from Poisoning Attacks against Local Differential Privacy (Xinyue Sun et al., 2024)

{{<citation>}}

Xinyue Sun, Qingqing Ye, Haibo Hu, Jiawei Duan, Tianyu Wo, Jie Xu, Renyu Yang. (2024)  
**LDPRecover: Recovering Frequencies from Poisoning Attacks against Local Differential Privacy**
<br/>
<button class="copy-to-clipboard" title="LDPRecover: Recovering Frequencies from Poisoning Attacks against Local Differential Privacy" index=194>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs.CR  
Keyword Score: 10  
Keywords: Differential Privacy  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09351v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09351v1.pdf" filename="2403.09351v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Local <b>differential</b> <b>privacy</b> (LDP), which enables an untrusted server to collect aggregated statistics from distributed users while protecting the privacy of those users, has been widely deployed in practice. However, LDP protocols for frequency estimation are vulnerable to poisoning attacks, in which an attacker can poison the aggregated frequencies by manipulating the data sent from malicious users. Therefore, it is an open challenge to recover the accurate aggregated frequencies from poisoned ones. In this work, we propose LDPRecover, a method that can recover accurate aggregated frequencies from poisoning attacks, even if the server does not learn the details of the attacks. In LDPRecover, we establish a genuine frequency estimator that theoretically guides the server to recover the frequencies aggregated from genuine users' data by eliminating the impact of malicious users' data in poisoned frequencies. Since the server has no idea of the attacks, we propose an adaptive attack to unify existing attacks and learn the statistics of the malicious data within this adaptive attack by exploiting the properties of LDP protocols. By taking the estimator and the learning statistics as constraints, we formulate the problem of recovering aggregated frequencies to approach the genuine ones as a constraint inference (CI) problem. Consequently, the server can obtain accurate aggregated frequencies by solving this problem optimally. Moreover, LDPRecover can serve as a frequency recovery paradigm that recovers more accurate aggregated frequencies by integrating attack details as new constraints in the CI problem. Our evaluation on two real-world datasets, three LDP protocols, and untargeted and targeted poisoning attacks shows that LDPRecover is both accurate and widely applicable against various poisoning attacks.

{{</citation>}}


## cs.HC (8)



### (1/8 | 195/254) 'Like a Nesting Doll': Analyzing Recursion Analogies Generated by CS Students using Large Language Models (Seth Bernstein et al., 2024)

{{<citation>}}

Seth Bernstein, Paul Denny, Juho Leinonen, Lauren Kan, Arto Hellas, Matt Littlefield Sami Sarsa, Stephen MacNeil. (2024)  
**'Like a Nesting Doll': Analyzing Recursion Analogies Generated by CS Students using Large Language Models**
<br/>
<button class="copy-to-clipboard" title="'Like a Nesting Doll': Analyzing Recursion Analogies Generated by CS Students using Large Language Models" index=195>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-AI, cs-CL, cs-HC, cs.HC  
Keyword Score: 40  
Keywords: ChatGPT, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09409v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09409v1.pdf" filename="2403.09409v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Grasping complex computing concepts often poses a challenge for students who struggle to anchor these new ideas to familiar experiences and understandings. To help with this, a good analogy can bridge the gap between unfamiliar concepts and familiar ones, providing an engaging way to aid understanding. However, creating effective educational analogies is difficult even for experienced instructors. We investigate to what extent <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> specifically <b>ChatGPT,</b> can provide access to personally relevant analogies on demand. Focusing on recursion, a challenging threshold concept, we conducted an investigation analyzing the analogies generated by more than 350 first-year computing students. They were provided with a code snippet and tasked to generate their own recursion-based analogies using <b>ChatGPT,</b> optionally including personally relevant topics in their <b>prompts.</b> We observed a great deal of diversity in the analogies produced with student-prescribed topics, in contrast to the otherwise generic analogies, highlighting the value of student creativity when working with <b>LLMs.</b> Not only did students enjoy the activity and report an improved understanding of recursion, but they described more easily remembering analogies that were personally and culturally relevant.

{{</citation>}}


### (2/8 | 196/254) Towards Proactive Interactions for In-Vehicle Conversational Assistants Utilizing Large Language Models (Huifang Du et al., 2024)

{{<citation>}}

Huifang Du, Xuejing Feng, Jun Ma, Meng Wang, Shiyu Tao, Yijie Zhong, Yuan-Fang Li, Haofen Wang. (2024)  
**Towards Proactive Interactions for In-Vehicle Conversational Assistants Utilizing Large Language Models**
<br/>
<button class="copy-to-clipboard" title="Towards Proactive Interactions for In-Vehicle Conversational Assistants Utilizing Large Language Models" index=196>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-HC, cs.HC  
Keyword Score: 40  
Keywords: Intent Detection, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09135v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09135v1.pdf" filename="2403.09135v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Research demonstrates that the proactivity of in-vehicle conversational assistants (IVCAs) can help to reduce distractions and enhance driving safety, better meeting users' cognitive needs. However, existing IVCAs struggle with user <b>intent</b> <b>recognition</b> and context awareness, which leads to suboptimal proactive interactions. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown potential for generalizing to various tasks with <b>prompts,</b> but their application in IVCAs and exploration of proactive interaction remain under-explored. These raise questions about how <b>LLMs</b> improve proactive interactions for IVCAs and influence user perception. To investigate these questions systematically, we establish a framework with five proactivity levels across two dimensions-assumption and autonomy-for IVCAs. According to the framework, we propose a "Rewrite + ReAct + Reflect" strategy, aiming to empower <b>LLMs</b> to fulfill the specific demands of each proactivity level when interacting with users. Both feasibility and subjective experiments are conducted. The <b>LLM</b> outperforms the state-of-the-art model in success rate and achieves satisfactory results for each proactivity level. Subjective experiments with 40 participants validate the effectiveness of our framework and show the proactive level with strong assumptions and user confirmation is most appropriate.

{{</citation>}}


### (3/8 | 197/254) Enabling Waypoint Generation for Collaborative Robots using LLMs and Mixed Reality (Cathy Mengying Fang et al., 2024)

{{<citation>}}

Cathy Mengying Fang, Krzysztof Zieliński, Pattie Maes, Joe Paradiso, Bruce Blumberg, Mikkel Baun Kjærgaard. (2024)  
**Enabling Waypoint Generation for Collaborative Robots using LLMs and Mixed Reality**
<br/>
<button class="copy-to-clipboard" title="Enabling Waypoint Generation for Collaborative Robots using LLMs and Mixed Reality" index=197>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-HC, cs-RO, cs.HC  
Keyword Score: 30  
Keywords: Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09308v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09308v1.pdf" filename="2403.09308v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Programming a robotic is a complex task, as it demands the user to have a good command of specific programming languages and awareness of the robot's physical constraints. We propose a framework that simplifies robot deployment by allowing direct communication using natural language. It uses <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> for <b>prompt</b> processing, workspace understanding, and waypoint generation. It also employs Augmented Reality (AR) to provide visual feedback of the planned outcome. We showcase the effectiveness of our framework with a simple pick-and-place task, which we implement on a real robot. Moreover, we present an early concept of expressive robot behavior and skill generation that can be used to communicate with the user and learn new skills (e.g., object grasping).

{{</citation>}}


### (4/8 | 198/254) PrompTHis: Visualizing the Process and Influence of Prompt Editing during Text-to-Image Creation (Yuhan Guo et al., 2024)

{{<citation>}}

Yuhan Guo, Hanning Shao, Can Liu, Kai Xu, Xiaoru Yuan. (2024)  
**PrompTHis: Visualizing the Process and Influence of Prompt Editing during Text-to-Image Creation**
<br/>
<button class="copy-to-clipboard" title="PrompTHis: Visualizing the Process and Influence of Prompt Editing during Text-to-Image Creation" index=198>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-HC, cs.HC  
Keyword Score: 23  
Keywords: Graph, Text2image, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09615v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09615v1.pdf" filename="2403.09615v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Generative <b>text-to-image</b> models, which allow users to create appealing images through a text <b>prompt,</b> have seen a dramatic increase in popularity in recent years. However, most users have a limited understanding of how such models work and it often requires many trials and errors to achieve satisfactory results. The <b>prompt</b> history contains a wealth of information that could provide users with insights into what have been explored and how the <b>prompt</b> changes impact the output image, yet little research attention has been paid to the visual analysis of such process to support users. We propose the Image Variant <b>Graph,</b> a novel visual representation designed to support comparing <b>prompt-image</b> pairs and exploring the editing history. The Image Variant <b>Graph</b> models <b>prompt</b> differences as edges between corresponding images and presents the distances between images through projection. Based on the <b>graph,</b> we developed the PrompTHis system through co-design with artists. Besides Image Variant <b>Graph,</b> PrompTHis also incorporates a detailed <b>prompt-image</b> history and a navigation mini-map. Based on the review and analysis of the <b>prompting</b> history, users can better understand the impact of <b>prompt</b> changes and have a more effective control of image generation. A quantitative user study with eleven amateur participants and qualitative interviews with five professionals and one amateur user were conducted to evaluate the effectiveness of PrompTHis. The results demonstrate PrompTHis can help users review the <b>prompt</b> history, make sense of the model, and plan their creative process.

{{</citation>}}


### (5/8 | 199/254) VIVID: Human-AI Collaborative Authoring of Vicarious Dialogues from Lecture Videos (Seulgi Choi et al., 2024)

{{<citation>}}

Seulgi Choi, Hyewon Lee, Yoonjoo Lee, Juho Kim. (2024)  
**VIVID: Human-AI Collaborative Authoring of Vicarious Dialogues from Lecture Videos**
<br/>
<button class="copy-to-clipboard" title="VIVID: Human-AI Collaborative Authoring of Vicarious Dialogues from Lecture Videos" index=199>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-HC, cs.HC  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09168v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09168v1.pdf" filename="2403.09168v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The lengthy monologue-style online lectures cause learners to lose engagement easily. Designing lectures in a "vicarious dialogue" format can foster learners' cognitive activities more than monologue-style. However, designing online lectures in a dialogue style catered to the diverse needs of learners is laborious for instructors. We conducted a design workshop with eight educational experts and seven instructors to present key guidelines and the potential use of <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> to transform a monologue lecture script into pedagogically meaningful dialogue. Applying these design guidelines, we created VIVID which allows instructors to collaborate with <b>LLMs</b> to design, evaluate, and modify pedagogical dialogues. In a within-subjects study with instructors (N=12), we show that VIVID helped instructors select and revise dialogues efficiently, thereby supporting the authoring of quality dialogues. Our findings demonstrate the potential of <b>LLMs</b> to assist instructors with creating high-quality educational dialogues across various learning stages.

{{</citation>}}


### (6/8 | 200/254) Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset (Hugo Laurençon et al., 2024)

{{<citation>}}

Hugo Laurençon, Léo Tronchon, Victor Sanh. (2024)  
**Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset**
<br/>
<button class="copy-to-clipboard" title="Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset" index=200>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-AI, cs-CV, cs-HC, cs.HC  
Keyword Score: 20  
Keywords: Fine-tuning, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09029v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09029v1.pdf" filename="2403.09029v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Using <b>vision-language</b> models (VLMs) in web development presents a promising strategy to increase efficiency and unblock no-code solutions: by providing a screenshot or a sketch of a UI, a VLM could generate the code to reproduce it, for instance in a language like HTML. Despite the advancements in VLMs for various tasks, the specific challenge of converting a screenshot into a corresponding HTML has been minimally explored. We posit that this is mainly due to the absence of a suitable, high-quality dataset. This work introduces WebSight, a synthetic dataset consisting of 2 million pairs of HTML codes and their corresponding screenshots. We <b>fine-tune</b> a foundational VLM on our dataset and show proficiency in converting webpage screenshots to functional HTML code. To accelerate the research in this area, we open-source WebSight.

{{</citation>}}


### (7/8 | 201/254) pARam: Leveraging Parametric Design in Extended Reality to Support the Personalization of Artifacts for Personal Fabrication (Evgeny Stemasov et al., 2024)

{{<citation>}}

Evgeny Stemasov, Simon Demharter, Max Rädler, Jan Gugenheimer, Enrico Rukzio. (2024)  
**pARam: Leveraging Parametric Design in Extended Reality to Support the Personalization of Artifacts for Personal Fabrication**
<br/>
<button class="copy-to-clipboard" title="pARam: Leveraging Parametric Design in Extended Reality to Support the Personalization of Artifacts for Personal Fabrication" index=201>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: H-5-2, cs-GR, cs-HC, cs.HC  
Keyword Score: 10  
Keywords: Recommendation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09607v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09607v1.pdf" filename="2403.09607v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Extended Reality (XR) allows in-situ previewing of designs to be manufactured through Personal Fabrication (PF). These in-situ interactions exhibit advantages for PF, like incorporating the environment into the design process. However, design-for-fabrication in XR often happens through either highly complex 3D-modeling or is reduced to rudimentary adaptations of crowd-sourced models. We present pARam, a tool combining parametric designs (PDs) and XR, enabling in-situ configuration of artifacts for PF. In contrast to modeling- or search-focused approaches, pARam supports customization through embodied and practical inputs (e.g., gestures, <b>recommendations)</b> and evaluation (e.g., lighting estimation) without demanding complex 3D-modeling skills. We implemented pARam for HoloLens 2 and evaluated it (n=20), comparing XR and desktop conditions. Users succeeded in choosing context-related parameters and took their environment into account for their configuration using pARam. We reflect on the prospects and challenges of PDs in XR to streamline complex design methods for PF while retaining suitable expressivity.

{{</citation>}}


### (8/8 | 202/254) Which Artificial Intelligences Do People Care About Most? A Conjoint Experiment on Moral Consideration (Ali Ladak et al., 2024)

{{<citation>}}

Ali Ladak, Jamie Harris, Jacy Reese Anthis. (2024)  
**Which Artificial Intelligences Do People Care About Most? A Conjoint Experiment on Moral Consideration**
<br/>
<button class="copy-to-clipboard" title="Which Artificial Intelligences Do People Care About Most? A Conjoint Experiment on Moral Consideration" index=202>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-HC, cs.HC  
Keyword Score: 10  
Keywords: Emotion Recognition  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09405v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09405v1.pdf" filename="2403.09405v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Many studies have identified particular features of artificial intelligences (AI), such as their autonomy and <b>emotion</b> <b>expression,</b> that affect the extent to which they are treated as subjects of moral consideration. However, there has not yet been a comparison of the relative importance of features as is necessary to design and understand increasingly capable, multi-faceted AI systems. We conducted an online conjoint experiment in which 1,163 participants evaluated descriptions of AIs that varied on these features. All 11 features increased how morally wrong participants considered it to harm the AIs. The largest effects were from human-like physical bodies and prosociality (i.e., <b>emotion</b> <b>expression,</b> <b>emotion</b> <b>recognition,</b> cooperation, and moral judgment). For human-computer interaction designers, the importance of prosociality suggests that, because AIs are often seen as threatening, the highest levels of moral consideration may only be granted if the AI has positive intentions.

{{</citation>}}


## cs.AI (6)



### (1/6 | 203/254) Silico-centric Theory of Mind (Anirban Mukherjee et al., 2024)

{{<citation>}}

Anirban Mukherjee, Hannah Hanwen Chang. (2024)  
**Silico-centric Theory of Mind**
<br/>
<button class="copy-to-clipboard" title="Silico-centric Theory of Mind" index=203>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 40  
Keywords: Counter-factual, Counterfactual Reasoning, Reasoning, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09289v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09289v1.pdf" filename="2403.09289v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Theory of Mind (ToM) refers to the ability to attribute mental states, such as beliefs, desires, intentions, and knowledge, to oneself and others, and to understand that these mental states can differ from one's own and from reality. We investigate ToM in environments with multiple, distinct, independent AI agents, each possessing unique internal states, information, and objectives. Inspired by human false-belief experiments, we present an AI ('focal AI') with a scenario where its clone undergoes a human-centric ToM assessment. We <b>prompt</b> the focal AI to assess whether its clone would benefit from additional instructions. Concurrently, we give its clones the ToM assessment, both with and without the instructions, thereby engaging the focal AI in higher-order <b>counterfactual</b> <b>reasoning</b> akin to human mentalizing--with respect to humans in one test and to other AI in another. We uncover a discrepancy: Contemporary AI demonstrates near-perfect accuracy on human-centric ToM assessments. Since information embedded in one AI is identically embedded in its clone, additional instructions are redundant. Yet, we observe AI crafting elaborate instructions for their clones, erroneously anticipating a need for assistance. An independent referee AI agrees with these unsupported expectations. Neither the focal AI nor the referee demonstrates ToM in our 'silico-centric' test.

{{</citation>}}


### (2/6 | 204/254) Clinical Reasoning over Tabular Data and Text with Bayesian Networks (Paloma Rabaey et al., 2024)

{{<citation>}}

Paloma Rabaey, Johannes Deleu, Stefan Heytens, Thomas Demeester. (2024)  
**Clinical Reasoning over Tabular Data and Text with Bayesian Networks**
<br/>
<button class="copy-to-clipboard" title="Clinical Reasoning over Tabular Data and Text with Bayesian Networks" index=204>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 30  
Keywords: Simulation, Simulator, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09481v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09481v1.pdf" filename="2403.09481v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Bayesian networks are well-suited for clinical <b>reasoning</b> on tabular data, but are less compatible with natural language data, for which neural networks provide a successful framework. This paper compares and discusses strategies to augment Bayesian networks with neural text representations, both in a generative and discriminative manner. This is illustrated with <b>simulation</b> results for a primary care use case (diagnosis of pneumonia) and discussed in a broader clinical context.

{{</citation>}}


### (3/6 | 205/254) Generating Feasible and Plausible Counterfactual Explanations for Outcome Prediction of Business Processes (Alexander Stevens et al., 2024)

{{<citation>}}

Alexander Stevens, Chun Ouyang, Johannes De Smedt, Catarina Moreira. (2024)  
**Generating Feasible and Plausible Counterfactual Explanations for Outcome Prediction of Business Processes**
<br/>
<button class="copy-to-clipboard" title="Generating Feasible and Plausible Counterfactual Explanations for Outcome Prediction of Business Processes" index=205>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 20  
Keywords: Counter-factual, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09232v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09232v1.pdf" filename="2403.09232v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In recent years, various machine and deep learning architectures have been successfully introduced to the field of predictive process analytics. Nevertheless, the inherent opacity of these algorithms poses a significant challenge for human decision-makers, hindering their ability to understand the <b>reasoning</b> behind the predictions. This growing concern has sparked the introduction of <b>counterfactual</b> explanations, designed as human-understandable what if scenarios, to provide clearer insights into the decision-making process behind undesirable predictions. The generation of <b>counterfactual</b> explanations, however, encounters specific challenges when dealing with the sequential nature of the (business) process cases typically used in predictive process analytics. Our paper tackles this challenge by introducing a data-driven approach, REVISEDplus, to generate more feasible and plausible <b>counterfactual</b> explanations. First, we restrict the <b>counterfactual</b> algorithm to generate <b>counterfactuals</b> that lie within a high-density region of the process data, ensuring that the proposed <b>counterfactuals</b> are realistic and feasible within the observed process data distribution. Additionally, we ensure plausibility by learning sequential patterns between the activities in the process cases, utilising Declare language templates. Finally, we evaluate the properties that define the validity of <b>counterfactuals.</b>

{{</citation>}}


### (4/6 | 206/254) Leveraging Constraint Programming in a Deep Learning Approach for Dynamically Solving the Flexible Job-Shop Scheduling Problem (Imanol Echeverria et al., 2024)

{{<citation>}}

Imanol Echeverria, Maialen Murua, Roberto Santana. (2024)  
**Leveraging Constraint Programming in a Deep Learning Approach for Dynamically Solving the Flexible Job-Shop Scheduling Problem**
<br/>
<button class="copy-to-clipboard" title="Leveraging Constraint Programming in a Deep Learning Approach for Dynamically Solving the Flexible Job-Shop Scheduling Problem" index=206>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 13  
Keywords: Benchmarking, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09249v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09249v1.pdf" filename="2403.09249v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advancements in the flexible job-shop scheduling problem (FJSSP) are primarily based on deep <b>reinforcement</b> <b>learning</b> (DRL) due to its ability to generate high-quality, real-time solutions. However, DRL approaches often fail to fully harness the strengths of existing techniques such as exact methods or constraint programming (CP), which can excel at finding optimal or near-optimal solutions for smaller instances. This paper aims to integrate CP within a deep learning (DL) based methodology, leveraging the benefits of both. In this paper, we introduce a method that involves training a DL model using optimal solutions generated by CP, ensuring the model learns from high-quality data, thereby eliminating the need for the extensive exploration typical in DRL and enhancing overall performance. Further, we integrate CP into our DL framework to jointly construct solutions, utilizing DL for the initial complex stages and transitioning to CP for optimal resolution as the problem is simplified. Our hybrid approach has been extensively tested on three public FJSSP <b>benchmarks,</b> demonstrating superior performance over five state-of-the-art DRL approaches and a widely-used CP solver. Additionally, with the objective of exploring the application to other combinatorial optimization problems, promising preliminary results are presented on applying our hybrid approach to the traveling salesman problem, combining an exact method with a well-known DRL method.

{{</citation>}}


### (5/6 | 207/254) Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption (Anirban Mukherjee et al., 2024)

{{<citation>}}

Anirban Mukherjee, Hannah Hanwen Chang. (2024)  
**Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption**
<br/>
<button class="copy-to-clipboard" title="Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption" index=207>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 10  
Keywords: Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09404v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09404v1.pdf" filename="2403.09404v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We propose a novel program of heuristic <b>reasoning</b> within artificial intelligence (AI) systems. Through a series of innovative experiments, including variations of the classic Linda problem and a novel application of the Beauty Contest game, we uncover trade-offs between accuracy maximization and effort reduction that shape the conditions under which AIs transition between exhaustive logical processing and the use of cognitive shortcuts (heuristics). We distinguish between the 'instrumental' use of heuristics to match resources with objectives, and 'mimetic absorption,' whereby heuristics are learned from humans, and manifest randomly and universally. We provide evidence that AI, despite lacking intrinsic goals or self-awareness, manifests an adaptive balancing of precision and efficiency, consistent with principles of resource-rational human cognition as explicated in classical theories of bounded rationality and dual-process theory.

{{</citation>}}


### (6/6 | 208/254) A Multi-population Integrated Approach for Capacitated Location Routing (Pengfei He et al., 2024)

{{<citation>}}

Pengfei He, Jin-Kao Hao, Qinghua Wu. (2024)  
**A Multi-population Integrated Approach for Capacitated Location Routing**
<br/>
<button class="copy-to-clipboard" title="A Multi-population Integrated Approach for Capacitated Location Routing" index=208>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09361v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09361v1.pdf" filename="2403.09361v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The capacitated location-routing problem involves determining the depots from a set of candidate capacitated depot locations and finding the required routes from the selected depots to serve a set of customers whereas minimizing a cost function that includes the cost of opening the chosen depots, the fixed utilization cost per vehicle used, and the total cost (distance) of the routes. This paper presents a multi-population integrated framework in which a multi-depot edge assembly crossover generates promising offspring solutions from the perspective of both depot location and route edge assembly. The method includes an effective neighborhood-based local search, a feasibility-restoring procedure and a diversification-oriented mutation. Of particular interest is the multi-population scheme which organizes the population into multiple subpopulations based on depot configurations. Extensive experiments on 281 <b>benchmark</b> instances from the literature show that the algorithm performs remarkably well, by improving 101 best-known results (new upper bounds) and matching 84 best-known results. Additional experiments are presented to gain insight into the role of the key elements of the algorithm.

{{</citation>}}


## cs.NI (2)



### (1/2 | 209/254) Whittle Index Based User Association in Dense Millimeter Wave Networks (Mandar R. Nalavade et al., 2024)

{{<citation>}}

Mandar R. Nalavade, Gaurav S. Kasbekar, Vivek S. Borkar. (2024)  
**Whittle Index Based User Association in Dense Millimeter Wave Networks**
<br/>
<button class="copy-to-clipboard" title="Whittle Index Based User Association in Dense Millimeter Wave Networks" index=209>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NI  
Categories: cs-NI, cs.NI  
Keyword Score: 40  
Keywords: Bandit Algorithm, Fairness, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09279v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09279v1.pdf" filename="2403.09279v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We address the problem of user association in a dense millimeter wave (mmWave) network, in which each arriving user brings a file containing a random number of packets and each time slot is divided into multiple mini-slots. This problem is an instance of the restless multi-armed <b>bandit</b> problem, and is provably hard to solve. Using a technique introduced by Whittle, we relax the hard per-stage constraint that each arriving user must be associated with exactly one mmWave base station (mBS) to a long-term constraint and then use the Lagrangian multiplier technique to convert the problem into an unconstrained problem. This decouples the process governing the system into separate Markov Decision Processes at different mBSs. We prove that the problem is Whittle indexable, present a scheme for computing the Whittle indices of different mBSs, and propose an association scheme under which, each arriving user is associated with the mBS with the smallest value of the Whittle index. Using extensive <b>simulations,</b> we show that the proposed Whittle index based scheme outperforms several user association schemes proposed in prior work in terms of various performance metrics such as average cost, delay, throughput, and Jain's <b>fairness</b> index.

{{</citation>}}


### (2/2 | 210/254) PreConfig: A Pretrained Model for Automating Network Configuration (Fuliang Li et al., 2024)

{{<citation>}}

Fuliang Li, Haozhi Lang, Jiajie Zhang, Jiaxing Shen, Xingwei Wang. (2024)  
**PreConfig: A Pretrained Model for Automating Network Configuration**
<br/>
<button class="copy-to-clipboard" title="PreConfig: A Pretrained Model for Automating Network Configuration" index=210>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NI  
Categories: cs-NI, cs.NI  
Keyword Score: 10  
Keywords: Pre-trained Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09369v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09369v1.pdf" filename="2403.09369v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Manual network configuration automation (NCA) tools face significant challenges in versatility and flexibility due to their reliance on extensive domain expertise and manual design, limiting their adaptability to diverse scenarios and complex application needs. This paper introduces PreConfig, an innovative NCA tool that leverages a <b>pretrained</b> <b>language</b> <b>model</b> for automating network configuration tasks. PreConfig is designed to address the complexity and variety of NCA tasks by framing them as text-to-text transformation problems, thus unifying the tasks of configuration generation, translation, and analysis under a single, versatile model. Our approach overcomes existing tools' limitations by utilizing advances in natural language processing to automatically comprehend and generate network configurations without extensive manual re-engineering. We confront the challenges of integrating domain-specific knowledge into <b>pretrained</b> <b>models</b> <b>and</b> the scarcity of supervision data in the network configuration field. Our solution involves constructing a specialized corpus and further pretraining on network configuration data, coupled with a novel data mining technique for generating task supervision data. The proposed model demonstrates robustness in configuration generation, translation, and analysis, outperforming conventional tools in handling complex networking environments. The experimental results validate the effectiveness of PreConfig, establishing a new direction for automating network configuration tasks with <b>pretrained</b> <b>language</b> <b>models.</b>

{{</citation>}}


## cs.SD (6)



### (1/6 | 211/254) uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures (Afrina Tabassum et al., 2024)

{{<citation>}}

Afrina Tabassum, Dung Tran, Trung Dang, Ismini Lourentzou, Kazuhito Koishida. (2024)  
**uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures**
<br/>
<button class="copy-to-clipboard" title="uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures" index=211>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SD  
Categories: cs-LG, cs-SD, cs.SD, eess-AS  
Keyword Score: 33  
Keywords: Autoencoder, Benchmarking, Unsupervised Learning, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09579v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09579v1.pdf" filename="2403.09579v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Masked <b>Autoencoders</b> (MAEs) learn rich low-level representations from unlabeled data but require substantial labeled data to effectively adapt to downstream tasks. Conversely, Instance Discrimination (ID) emphasizes high-level semantics, offering a potential solution to alleviate annotation requirements in MAEs. Although combining these two approaches can address downstream tasks with limited labeled data, naively integrating ID into MAEs leads to extended training times and high computational costs. To address this challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that leverages <b>unsupervised</b> audio mixtures. Utilizing contrastive tuning, uaMix-MAE aligns the representations of pretrained MAEs, thereby facilitating effective adaptation to task-specific semantics. To optimize the model with small amounts of unlabeled data, we propose an audio mixing technique that manipulates audio samples in both input and virtual label spaces. Experiments in low/few-shot settings demonstrate that \modelname achieves 4-6% accuracy improvements over various <b>benchmarks</b> when tuned with limited unlabeled data, such as AudioSet-20K. Code is available at https://github.com/PLAN-Lab/uamix-MAE

{{</citation>}}


### (2/6 | 212/254) LM2D: Lyrics- and Music-Driven Dance Synthesis (Wenjie Yin et al., 2024)

{{<citation>}}

Wenjie Yin, Xuejiao Zhao, Yi Yu, Hang Yin, Danica Kragic, Mårten Björkman. (2024)  
**LM2D: Lyrics- and Music-Driven Dance Synthesis**
<br/>
<button class="copy-to-clipboard" title="LM2D: Lyrics- and Music-Driven Dance Synthesis" index=212>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SD  
Categories: cs-AI, cs-LG, cs-MM, cs-SD, cs.SD, eess-AS  
Keyword Score: 26  
Keywords: Diffusion Model, Knowledge Distillation, Multi-modal, Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09407v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09407v1.pdf" filename="2403.09407v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Dance typically involves professional choreography with complex movements that follow a musical rhythm and can also be influenced by lyrical content. The integration of lyrics in addition to the auditory dimension, enriches the foundational tone and makes motion generation more amenable to its semantic meanings. However, existing dance synthesis methods tend to model motions only conditioned on audio signals. In this work, we make two contributions to bridge this gap. First, we propose LM2D, a novel probabilistic architecture that incorporates a <b>multimodal</b> <b>diffusion</b> <b>model</b> with consistency <b>distillation,</b> designed to create dance conditioned on both music and lyrics in one <b>diffusion</b> <b>generation</b> step. Second, we introduce the first 3D dance-motion dataset that encompasses both music and lyrics, obtained with pose estimation technologies. We evaluate our model against music-only baseline models with objective metrics and human evaluations, including dancers and choreographers. The results demonstrate LM2D is able to produce realistic and diverse dance matching both lyrics and music. A video summary can be accessed at: https://youtu.be/4XCgvYookvA.

{{</citation>}}


### (3/6 | 213/254) An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from Acoustic Signals (Zhao Wang et al., 2024)

{{<citation>}}

Zhao Wang, Xiaomeng Li, Na Li, Longlong Shu. (2024)  
**An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from Acoustic Signals**
<br/>
<button class="copy-to-clipboard" title="An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from Acoustic Signals" index=213>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SD  
Categories: cs-LG, cs-SD, cs.SD, eess-AS  
Keyword Score: 20  
Keywords: Convolution, LSTM  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09030v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09030v1.pdf" filename="2403.09030v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This study aimed to develop a deep learning model for the classification of bearing faults in wind turbine generators from acoustic signals. A <b>convolutional</b> <b>LSTM</b> model was successfully constructed and trained by using audio data from five predefined fault types for both training and validation. To create the dataset, raw audio signal data was collected and processed in frames to capture time and frequency domain information. The model exhibited outstanding accuracy on training samples and demonstrated excellent generalization ability during validation, indicating its proficiency of generalization capability. On the test samples, the model achieved remarkable classification performance, with an overall accuracy exceeding 99.5%, and a false positive rate of less than 1% for normal status. The findings of this study provide essential support for the diagnosis and maintenance of bearing faults in wind turbine generators, with the potential to enhance the reliability and efficiency of wind power generation.

{{</citation>}}


### (4/6 | 214/254) The Neural-SRP method for positional sound source localization (Eric Grinstein et al., 2024)

{{<citation>}}

Eric Grinstein, Toon van Waterschoot, Mike Brookes, Patrick A. Naylor. (2024)  
**The Neural-SRP method for positional sound source localization**
<br/>
<button class="copy-to-clipboard" title="The Neural-SRP method for positional sound source localization" index=214>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SD  
Categories: cs-SD, cs.SD, eess-AS  
Keyword Score: 10  
Keywords: Transfer Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09455v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09455v1.pdf" filename="2403.09455v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Steered Response Power (SRP) is a widely used method for the task of sound source localization using microphone arrays, showing satisfactory localization performance on many practical scenarios. However, its performance is diminished under highly reverberant environments. Although Deep Neural Networks (DNNs) have been previously proposed to overcome this limitation, most are trained for a specific number of microphones with fixed spatial coordinates. This restricts their practical application on scenarios frequently observed in wireless acoustic sensor networks, where each application has an ad-hoc microphone topology. We propose Neural-SRP, a DNN which combines the flexibility of SRP with the performance gains of DNNs. We train our network using simulated data and <b>transfer</b> <b>learning,</b> and evaluate our approach on recorded and simulated data. Results verify that Neural-SRP's localization performance significantly outperforms the baselines.

{{</citation>}}


### (5/6 | 215/254) A Practical Guide to Spectrogram Analysis for Audio Signal Processing (Zulfidin Khodzhaev, 2024)

{{<citation>}}

Zulfidin Khodzhaev. (2024)  
**A Practical Guide to Spectrogram Analysis for Audio Signal Processing**
<br/>
<button class="copy-to-clipboard" title="A Practical Guide to Spectrogram Analysis for Audio Signal Processing" index=215>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SD  
Categories: cs-SD, cs.SD, eess-AS  
Keyword Score: 10  
Keywords: Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09321v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09321v1.pdf" filename="2403.09321v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The paper <b>summarizes</b> spectrogram and gives practical application of spectrogram in signal processing. For analysis, finger-snapping is recorded with a sampling rate of 441000 Hz and 96000 Hz. The effects of the number of segments on the Power Spectral Density (PSD) and spectrogram are analyzed and visualized.

{{</citation>}}


### (6/6 | 216/254) More than words: Advancements and challenges in speech recognition for singing (Anna Kruspe, 2024)

{{<citation>}}

Anna Kruspe. (2024)  
**More than words: Advancements and challenges in speech recognition for singing**
<br/>
<button class="copy-to-clipboard" title="More than words: Advancements and challenges in speech recognition for singing" index=216>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SD  
Categories: cs-CL, cs-IR, cs-LG, cs-SD, cs.SD, eess-AS  
Keyword Score: 10  
Keywords: Automatic Speech Recognition  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09298v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09298v1.pdf" filename="2403.09298v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper addresses the challenges and advancements in <b>speech</b> <b>recognition</b> for singing, a domain distinctly different from standard <b>speech</b> <b>recognition.</b> Singing encompasses unique challenges, including extensive pitch variations, diverse vocal styles, and background music interference. We explore key areas such as phoneme recognition, language identification in songs, keyword spotting, and full lyrics transcription. I will describe some of my own experiences when performing research on these tasks just as they were starting to gain traction, but will also show how recent developments in deep learning and large-scale datasets have propelled progress in this field. My goal is to illuminate the complexities of applying <b>speech</b> <b>recognition</b> to singing, evaluate current capabilities, and outline future research directions.

{{</citation>}}


## cs.CY (2)



### (1/2 | 217/254) MetroGNN: Metro Network Expansion with Reinforcement Learning (Hongyuan Su et al., 2024)

{{<citation>}}

Hongyuan Su, Yu Zheng, Jingtao Ding, Depeng Jin, Yong Li. (2024)  
**MetroGNN: Metro Network Expansion with Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="MetroGNN: Metro Network Expansion with Reinforcement Learning" index=217>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CY  
Categories: 68T09, cs-CY, cs.CY  
Keyword Score: 33  
Keywords: Graph, Graph Neural Network, Markov Decision Process, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09197v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09197v1.pdf" filename="2403.09197v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Selecting urban regions for metro network expansion to meet maximal transportation demands is crucial for urban development, while computationally challenging to solve. The expansion process relies not only on complicated features like urban demographics and origin-destination (OD) flow but is also constrained by the existing metro network and urban geography. In this paper, we introduce a <b>reinforcement</b> <b>learning</b> framework to address a <b>Markov</b> <b>decision</b> <b>process</b> within an urban heterogeneous multi-graph. Our approach employs an attentive policy network that intelligently selects nodes based on information captured by a <b>graph</b> <b>neural</b> <b>network.</b> Experiments on real-world urban data demonstrate that our proposed methodology substantially improve the satisfied transportation demands by over 30\% when compared with state-of-the-art methods. Codes are published at https://github.com/tsinghua-fib-lab/MetroGNN.

{{</citation>}}


### (2/2 | 218/254) Older adults' safety and security online: A post-pandemic exploration of attitudes and behaviors (Edgar Pacheco, 2024)

{{<citation>}}

Edgar Pacheco. (2024)  
**Older adults' safety and security online: A post-pandemic exploration of attitudes and behaviors**
<br/>
<button class="copy-to-clipboard" title="Older adults' safety and security online: A post-pandemic exploration of attitudes and behaviors" index=218>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CY  
Categories: cs-CY, cs.CY  
Keyword Score: 10  
Keywords: Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09208v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09208v1.pdf" filename="2403.09208v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Older adults' growing use of the internet and related technologies, further accelerated by the COVID-19 pandemic, has <b>prompted</b> not only a critical examination of their behaviors and attitudes about online threats but also a greater understanding of the roles of specific characteristics within this population group. Based on survey data and using descriptive and inferential statistics, this empirical study delves into this matter. The behaviors and attitudes of a group of older adults aged 60 years and older (n=275) regarding different dimensions of online safety and cybersecurity are investigated. The results show that older adults report a discernible degree of concern about the security of their personal information. Despite the varied precautions taken, most of them do not know where to report online threats. What is more, regarding key demographics, the study found some significant differences in terms of gender and age group, but not disability status. This implies that older adults do not seem to constitute a homogeneous group when it comes to attitudes and behaviors regarding safety and security online. The study concludes that support systems should include older adults in the development of protective measures and acknowledge their diversity. The implications of the results are discussed and some directions for future research are proposed.

{{</citation>}}


## cs.DB (1)



### (1/1 | 219/254) Query Rewriting via Large Language Models (Jie Liu et al., 2024)

{{<citation>}}

Jie Liu, Barzan Mozafari. (2024)  
**Query Rewriting via Large Language Models**
<br/>
<button class="copy-to-clipboard" title="Query Rewriting via Large Language Models" index=219>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DB  
Categories: cs-DB, cs.DB  
Keyword Score: 33  
Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09060v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09060v1.pdf" filename="2403.09060v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Query rewriting is one of the most effective techniques for coping with poorly written queries before passing them down to the query optimizer. Manual rewriting is not scalable, as it is error-prone and requires deep expertise. Similarly, traditional query rewriting algorithms can only handle a small subset of queries: rule-based techniques do not generalize to new query patterns and synthesis-based techniques cannot handle complex queries. Fortunately, the rise of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> equipped with broad general knowledge and advanced <b>reasoning</b> capabilities, has created hopes for solving some of these previously open problems. In this paper, we present GenRewrite, the first holistic system that leverages <b>LLMs</b> for query rewriting. We introduce the notion of Natural Language Rewrite Rules (NLR2s), and use them as hints to the <b>LLM</b> but also a means for transferring knowledge from rewriting one query to another, and thus becoming smarter and more effective over time. We present a novel counterexample-guided technique that iteratively corrects the syntactic and semantic errors in the rewritten query, significantly reducing the <b>LLM</b> costs and the manual effort required for verification. GenRewrite speeds up 22 out of 99 TPC queries (the most complex public <b>benchmark)</b> by more than 2x, which is 2.5x--3.2x higher coverage than state-of-the-art traditional query rewriting and 2.1x higher than the out-of-the-box <b>LLM</b> baseline.

{{</citation>}}


## cs.DC (2)



### (1/2 | 220/254) BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences (Sun Ao et al., 2024)

{{<citation>}}

Sun Ao, Weilin Zhao, Xu Han, Cheng Yang, Zhiyuan Liu, Chuan Shi, Maosong Sun, Shengnan Wang, Teng Su. (2024)  
**BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences**
<br/>
<button class="copy-to-clipboard" title="BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences" index=220>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DC  
Categories: cs-DC, cs-LG, cs.DC  
Keyword Score: 30  
Keywords: Transformer, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09347v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09347v1.pdf" filename="2403.09347v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Effective attention modules have played a crucial role in the success of <b>Transformer-based</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> but the quadratic time and memory complexities of these attention modules also pose a challenge when processing long sequences. One potential solution for the long sequence problem is to utilize distributed clusters to parallelize the computation of attention modules across multiple devices (e.g., GPUs). However, adopting a distributed approach inevitably introduces extra memory overheads to store local attention results and incurs additional communication costs to aggregate local results into global ones. In this paper, we propose a distributed attention framework named ``BurstAttention'' to optimize memory access and communication operations at both the global cluster and local device levels. In our experiments, we compare BurstAttention with other competitive distributed attention solutions for long sequence processing. The experimental results under different length settings demonstrate that BurstAttention offers significant advantages for processing long sequences compared with these competitive baselines, reducing 40% communication overheads and achieving 2 X speedup during training 32K sequence length on 8 X A100.

{{</citation>}}


### (2/2 | 221/254) Benchmarking Distributed Coordination Systems: A Survey and Analysis (Bekir Turkkan et al., 2024)

{{<citation>}}

Bekir Turkkan, Tevfik Kosar, Aleksey Charapko, Ailidani Ailijiang, Murat Demirbas. (2024)  
**Benchmarking Distributed Coordination Systems: A Survey and Analysis**
<br/>
<button class="copy-to-clipboard" title="Benchmarking Distributed Coordination Systems: A Survey and Analysis" index=221>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DC  
Categories: cs-DC, cs.DC  
Keyword Score: 6  
Keywords: Benchmarking, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09445v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09445v1.pdf" filename="2403.09445v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Coordination services and protocols are critical components of distributed systems and are essential for providing consistency, fault tolerance, and scalability. However, due to lack of a standard <b>benchmarking</b> tool for distributed coordination services, coordination service developers/researchers either use a NoSQL standard <b>benchmark</b> and omit evaluating consistency, distribution, and fault-tolerance; or create their own ad-hoc microbenchmarks and skip comparability with other services. In this paper, we analyze and compare known and widely used distributed coordination services, their evaluations, and the tools used to <b>benchmark</b> those systems. We identify important requirements of distributed coordination service <b>benchmarking,</b> like the metrics and parameters that need to be evaluated and their evaluation setups and tools.

{{</citation>}}


## eess.IV (8)



### (1/8 | 222/254) XReal: Realistic Anatomy and Pathology-Aware X-ray Generation via Controllable Diffusion Model (Anees Ur Rehman Hashmi et al., 2024)

{{<citation>}}

Anees Ur Rehman Hashmi, Ibrahim Almakky, Mohammad Areeb Qazi, Santosh Sanjeev, Vijay Ram Papineni, Dwarikanath Mahapatra, Mohammad Yaqub. (2024)  
**XReal: Realistic Anatomy and Pathology-Aware X-ray Generation via Controllable Diffusion Model**
<br/>
<button class="copy-to-clipboard" title="XReal: Realistic Anatomy and Pathology-Aware X-ray Generation via Controllable Diffusion Model" index=222>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 30  
Keywords: Diffusion Model, Fine-tuning, Text2image  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09240v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09240v1.pdf" filename="2403.09240v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Large-scale generative models have demonstrated impressive capacity in producing visually compelling images, with increasing applications in medical imaging. However, they continue to grapple with the challenge of image hallucination and the generation of anatomically inaccurate outputs. These limitations are mainly due to the sole reliance on textual inputs and lack of spatial control over the generated images, hindering the potential usefulness of such models in real-life settings. We present XReal, a novel controllable <b>diffusion</b> <b>model</b> for generating realistic chest X-ray images through precise anatomy and pathology location control. Our lightweight method can seamlessly integrate spatial control in a pre-trained <b>text-to-image</b> <b>diffusion</b> <b>model</b> without <b>fine-tuning,</b> retaining its existing knowledge while enhancing its generation capabilities. XReal outperforms state-of-the-art x-ray <b>diffusion</b> <b>models</b> in quantitative and qualitative metrics while showing 13% and 10% anatomy and pathology realism gain, respectively, based on the expert radiologist evaluation. Our model holds promise for advancing generative models in medical imaging, offering greater precision and adaptability while inviting further exploration in this evolving field. A large synthetically generated data with annotations and code is publicly available at https://github.com/BioMedIA-MBZUAI/XReal.

{{</citation>}}


### (2/8 | 223/254) StainFuser: Controlling Diffusion for Faster Neural Style Transfer in Multi-Gigapixel Histology Images (Robert Jewsbury et al., 2024)

{{<citation>}}

Robert Jewsbury, Ruoyu Wang, Abhir Bhalerao, Nasir Rajpoot, Quoc Dang Vu. (2024)  
**StainFuser: Controlling Diffusion for Faster Neural Style Transfer in Multi-Gigapixel Histology Images**
<br/>
<button class="copy-to-clipboard" title="StainFuser: Controlling Diffusion for Faster Neural Style Transfer in Multi-Gigapixel Histology Images" index=223>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, cs-LG, eess-IV, eess.IV  
Keyword Score: 20  
Keywords: Generative Adversarial Network, Style Transfer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09302v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09302v1.pdf" filename="2403.09302v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Stain normalization algorithms aim to transform the color and intensity characteristics of a source multi-gigapixel histology image to match those of a target image, mitigating inconsistencies in the appearance of stains used to highlight cellular components in the images. We propose a new approach, StainFuser, which treats this problem as a <b>style</b> <b>transfer</b> task using a novel Conditional Latent Diffusion architecture, eliminating the need for handcrafted color components. With this method, we curate SPI-2M the largest stain normalization dataset to date of over 2 million histology images with neural <b>style</b> <b>transfer</b> for high-quality transformations. Trained on this data, StainFuser outperforms current state-of-the-art <b>GAN</b> and handcrafted methods in terms of the quality of normalized images. Additionally, compared to existing approaches, it improves the performance of nuclei instance segmentation and classification models when used as a test time augmentation method on the challenging CoNIC dataset. Finally, we apply StainFuser on multi-gigapixel Whole Slide Images (WSIs) and demonstrate improved performance in terms of computational efficiency, image quality and consistency across tiles over current methods.

{{</citation>}}


### (3/8 | 224/254) VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation (Mingya Zhang et al., 2024)

{{<citation>}}

Mingya Zhang, Yue Yu, Limei Gu, Tingsheng Lin, Xianping Tao. (2024)  
**VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation**
<br/>
<button class="copy-to-clipboard" title="VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation" index=224>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 20  
Keywords: Convolutional Neural Network, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09157v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09157v1.pdf" filename="2403.09157v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In the field of medical image segmentation, models based on both <b>CNN</b> and <b>Transformer</b> have been thoroughly investigated. However, <b>CNNs</b> have limited modeling capabilities for long-range dependencies, making it challenging to exploit the semantic information within images fully. On the other hand, the quadratic computational complexity poses a challenge for <b>Transformers.</b> Recently, State Space Models (SSMs), such as Mamba, have been recognized as a promising method. They not only demonstrate superior performance in modeling long-range interactions, but also preserve a linear computational complexity. Inspired by the Mamba architecture, We proposed Vison Mamba-UNetV2, the Visual State Space (VSS) Block is introduced to capture extensive contextual information, the Semantics and Detail Infusion (SDI) is introduced to augment the infusion of low-level and high-level features. We conduct comprehensive experiments on the ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir, CVC-ColonDB and ETIS-LaribPolypDB public datasets. The results indicate that VM-UNetV2 exhibits competitive performance in medical image segmentation tasks. Our code is available at https://github.com/nobodyplayer1/VM-UNetV2.

{{</citation>}}


### (4/8 | 225/254) Mitigating Data Consistency Induced Discrepancy in Cascaded Diffusion Models for Sparse-view CT Reconstruction (Hanyu Chen et al., 2024)

{{<citation>}}

Hanyu Chen, Zhixiu Hao, Lin Guo, Liying Xiao. (2024)  
**Mitigating Data Consistency Induced Discrepancy in Cascaded Diffusion Models for Sparse-view CT Reconstruction**
<br/>
<button class="copy-to-clipboard" title="Mitigating Data Consistency Induced Discrepancy in Cascaded Diffusion Models for Sparse-view CT Reconstruction" index=225>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 10  
Keywords: Diffusion Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09355v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09355v1.pdf" filename="2403.09355v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Sparse-view Computed Tomography (CT) image reconstruction is a promising approach to reduce radiation exposure, but it inevitably leads to image degradation. Although <b>diffusion</b> <b>model-based</b> approaches are computationally expensive and suffer from the training-sampling discrepancy, they provide a potential solution to the problem. This study introduces a novel Cascaded <b>Diffusion</b> <b>with</b> Discrepancy Mitigation (CDDM) framework, including the low-quality image generation in latent space and the high-quality image generation in pixel space which contains data consistency and discrepancy mitigation in a one-step reconstruction process. The cascaded framework minimizes computational costs by moving some inference steps from pixel space to latent space. The discrepancy mitigation technique addresses the training-sampling gap induced by data consistency, ensuring the data distribution is close to the original manifold. A specialized Alternating Direction Method of Multipliers (ADMM) is employed to process image gradients in separate directions, offering a more targeted approach to regularization. Experimental results across two datasets demonstrate CDDM's superior performance in high-quality image generation with clearer boundaries compared to existing methods, highlighting the framework's computational efficiency.

{{</citation>}}


### (5/8 | 226/254) Advanced Tumor Segmentation in Medical Imaging: An Ensemble Approach for BraTS 2023 Adult Glioma and Pediatric Tumor Tasks (Fadillah Maani et al., 2024)

{{<citation>}}

Fadillah Maani, Anees Ur Rehman Hashmi, Mariam Aljuboory, Numan Saeed, Ikboljon Sobirov, Mohammad Yaqub. (2024)  
**Advanced Tumor Segmentation in Medical Imaging: An Ensemble Approach for BraTS 2023 Adult Glioma and Pediatric Tumor Tasks**
<br/>
<button class="copy-to-clipboard" title="Advanced Tumor Segmentation in Medical Imaging: An Ensemble Approach for BraTS 2023 Adult Glioma and Pediatric Tumor Tasks" index=226>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 10  
Keywords: Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09262v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09262v1.pdf" filename="2403.09262v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Automated segmentation proves to be a valuable tool in precisely detecting tumors within medical images. The accurate identification and segmentation of tumor types hold paramount importance in diagnosing, monitoring, and treating highly fatal brain tumors. The BraTS challenge serves as a platform for researchers to tackle this issue by participating in open challenges focused on tumor segmentation. This study outlines our methodology for segmenting tumors in the context of two distinct tasks from the BraTS 2023 challenge: Adult Glioma and Pediatric Tumors. Our approach leverages two encoder-decoder-based <b>CNN</b> models, namely SegResNet and MedNeXt, for segmenting three distinct subregions of tumors. We further introduce a set of robust postprocessing to improve the segmentation, especially for the newly introduced BraTS 2023 metrics. The specifics of our approach and comprehensive performance analyses are expounded upon in this work. Our proposed approach achieves third place in the BraTS 2023 Adult Glioma Segmentation Challenges with an average of 0.8313 and 36.38 Dice and HD95 scores on the test set, respectively.

{{</citation>}}


### (6/8 | 227/254) A Modified da Vinci Surgical Instrument for OCE based Elasticity Estimation with Deep Learning (Maximilian Neidhardt et al., 2024)

{{<citation>}}

Maximilian Neidhardt, Robin Mieling, Sarah Latus, Martin Fischer, Tobias Maurer, Alexander Schlaefer. (2024)  
**A Modified da Vinci Surgical Instrument for OCE based Elasticity Estimation with Deep Learning**
<br/>
<button class="copy-to-clipboard" title="A Modified da Vinci Surgical Instrument for OCE based Elasticity Estimation with Deep Learning" index=227>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-SY, eess-IV, eess-SY, eess.IV  
Keyword Score: 10  
Keywords: Key Point Analysis  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09256v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09256v1.pdf" filename="2403.09256v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Robot-assisted surgery has advantages compared to conventional laparoscopic procedures, e.g., precise movement of the surgical instruments, improved dexterity, and high-resolution visualization of the surgical field. However, mechanical tissue properties may provide additional information, e.g., on the location of lesions or vessels. While elastographic imaging has been proposed, it is not readily available as an online modality during robot-assisted surgery. We propose modifying a da~Vinci surgical instrument to realize optical coherence elastography (OCE) for quantitative elasticity estimation. The modified da~Vinci instrument is equipped with piezoelectric elements for shear wave excitation and we employ fast optical coherence tomography (OCT) imaging to track propagating wave fields, which are directly related to biomechanical tissue properties. All high-voltage components are mounted at the proximal end outside the patient. We demonstrate that external excitation at the instrument shaft can effectively stimulate shear waves, even when considering damping. Comparing conventional and deep learning-based signal processing, resulting in mean absolute errors of 19.27 <b>kPa</b> and 6.29 <b>kPa,</b> respectively. These results illustrate that precise quantitative elasticity estimates can be obtained. We also demonstrate quantitative elasticity estimation on ex-vivo tissue samples of heart, liver and stomach, and show that the measurements can be used to distinguish soft and stiff tissue types.

{{</citation>}}


### (7/8 | 228/254) TBI Image/Text (TBI-IT): Comprehensive Text and Image Datasets for Traumatic Brain Injury Research (Jie Li et al., 2024)

{{<citation>}}

Jie Li, Jiaying Wen, Tongxin Yang, Fenglin Cai, Miao Wei, Zhiwei Zhang, Li Jiang. (2024)  
**TBI Image/Text (TBI-IT): Comprehensive Text and Image Datasets for Traumatic Brain Injury Research**
<br/>
<button class="copy-to-clipboard" title="TBI Image/Text (TBI-IT): Comprehensive Text and Image Datasets for Traumatic Brain Injury Research" index=228>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 10  
Keywords: Named Entity Recognition  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09062v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09062v1.pdf" filename="2403.09062v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we introduce a new dataset in the medical field of Traumatic Brain Injury (TBI), called TBI-IT, which includes both electronic medical records (EMRs) and head CT images. This dataset is designed to enhance the accuracy of artificial intelligence in the diagnosis and treatment of TBI. This dataset, built upon the foundation of standard text and image data, incorporates specific annotations within the EMRs, extracting key content from the text information, and categorizes the annotation content of imaging data into five types: brain midline, hematoma, left cerebral ventricle, right cerebral ventricle and fracture. TBI-IT aims to be a foundational dataset for feature learning in image segmentation tasks and <b>named</b> <b>entity</b> <b>recognition.</b>

{{</citation>}}


### (8/8 | 229/254) Deep unfolding Network for Hyperspectral Image Super-Resolution with Automatic Exposure Correction (Yuan Fang et al., 2024)

{{<citation>}}

Yuan Fang, Yipeng Liu, Jie Chen, Zhen Long, Ao Li, Chong-Yung Chi, Ce Zhu. (2024)  
**Deep unfolding Network for Hyperspectral Image Super-Resolution with Automatic Exposure Correction**
<br/>
<button class="copy-to-clipboard" title="Deep unfolding Network for Hyperspectral Image Super-Resolution with Automatic Exposure Correction" index=229>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09096v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09096v1.pdf" filename="2403.09096v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In recent years, the fusion of high spatial resolution multispectral image (HR-MSI) and low spatial resolution hyperspectral image (LR-HSI) has been recognized as an effective method for HSI super-resolution (HSI-SR). However, both HSI and MSI may be acquired under extreme conditions such as night or poorly illuminating scenarios, which may cause different exposure levels, thereby seriously downgrading the yielded HSISR. In contrast to most existing methods based on respective low-light enhancements (LLIE) of MSI and HSI followed by their fusion, a deep Unfolding HSI Super-Resolution with Automatic Exposure Correction (UHSR-AEC) is proposed, that can effectively generate a high-quality fused HSI-SR (in texture and features) even under very imbalanced exposures, thanks to the correlation between LLIE and HSI-SR taken into account. Extensive experiments are provided to demonstrate the state-of-the-art overall performance of the proposed UHSR-AEC, including comparison with some <b>benchmark</b> peer methods.

{{</citation>}}


## physics.optics (1)



### (1/1 | 230/254) Compute-first optical detection for noise-resilient visual perception (Jungmin Kim et al., 2024)

{{<citation>}}

Jungmin Kim, Nanfang Yu, Zongfu Yu. (2024)  
**Compute-first optical detection for noise-resilient visual perception**
<br/>
<button class="copy-to-clipboard" title="Compute-first optical detection for noise-resilient visual perception" index=230>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: physics.optics  
Categories: cs-CV, cs-LG, eess-IV, physics-optics, physics.optics  
Keyword Score: 23  
Keywords: MNIST, Benchmarking, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09612v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09612v1.pdf" filename="2403.09612v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In the context of visual perception, the optical signal from a scene is transferred into the electronic domain by detectors in the form of image data, which are then processed for the extraction of visual information. In noisy and weak-signal environments such as thermal imaging for night vision applications, however, the performance of neural computing tasks faces a significant bottleneck due to the inherent degradation of data quality upon noisy detection. Here, we propose a concept of optical signal processing before detection to address this issue. We demonstrate that spatially redistributing optical signals through a properly designed linear <b>transformer</b> can enhance the detection noise resilience of visual perception tasks, as <b>benchmarked</b> with the <b>MNIST</b> classification. Our idea is supported by a quantitative analysis detailing the relationship between signal concentration and noise robustness, as well as its practical implementation in an incoherent imaging system. This compute-first detection scheme can pave the way for advancing infrared machine vision technologies widely used for industrial and defense applications.

{{</citation>}}


## cs.CE (2)



### (1/2 | 231/254) A mixed-order quasicontinuum approach for beam-based architected materials with application to fracture (Kevin Kraschewski et al., 2024)

{{<citation>}}

Kevin Kraschewski, Gregory P. Phlipot, Dennis M. Kochmann. (2024)  
**A mixed-order quasicontinuum approach for beam-based architected materials with application to fracture**
<br/>
<button class="copy-to-clipboard" title="A mixed-order quasicontinuum approach for beam-based architected materials with application to fracture" index=231>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CE  
Categories: cs-CE, cs.CE  
Keyword Score: 23  
Keywords: Benchmarking, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09495v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09495v1.pdf" filename="2403.09495v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Predicting the mechanics of large structural networks, such as beam-based architected materials, requires a multiscale computational strategy that preserves information about the discrete structure while being applicable to large assemblies of struts. Especially the fracture properties of such beam lattices necessitate a two-scale modeling strategy, since the fracture toughness depends on discrete beam failure events, while the application of remote loads requires large <b>simulation</b> domains. As classical homogenization techniques fail in the absence of a separation of scales at the crack tip, we present a concurrent multiscale technique: a fully-nonlocal quasicontinuum (QC) multi-lattice formulation for beam networks, based on a conforming mesh. Like the original atomistic QC formulation, we maintain discrete resolution where needed (such as around a crack tip) while efficiently coarse-graining in the remaining <b>simulation</b> domain. A key challenge is a suitable model in the coarse-grained domain, where classical QC uses affine interpolations. This formulation fails in bending-dominated lattices, as it overconstrains the lattice by preventing bending without stretching of beams. Therefore, we here present a beam QC formulation based on mixed-order interpolation in the coarse-grained region -- combining the efficiency of linear interpolation where possible with the accuracy advantages of quadratic interpolation where needed. This results in a powerful computational framework, which, as we demonstrate through our validation and <b>benchmark</b> examples, overcomes the deficiencies of previous QC formulations and enables, e.g., the prediction of the fracture toughness and the diverse nature of stress distributions of stretching- and bending-dominated beam lattices in two and three dimensions.

{{</citation>}}


### (2/2 | 232/254) Modular parametric PGD enabling online solution of partial differential equations (Angelo Pasquale et al., 2024)

{{<citation>}}

Angelo Pasquale, Mohammad-Javad Kazemzadeh-Parsi, Daniele Di Lorenzo, Victor Champaney, Amine Ammar, Francisco Chinesta. (2024)  
**Modular parametric PGD enabling online solution of partial differential equations**
<br/>
<button class="copy-to-clipboard" title="Modular parametric PGD enabling online solution of partial differential equations" index=232>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CE  
Categories: cs-CE, cs.CE  
Keyword Score: 5  
Keywords: Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09312v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09312v1.pdf" filename="2403.09312v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In the present work, a new methodology is proposed for building surrogate parametric models of engineering systems based on modular assembly of pre-solved modules. Each module is a generic parametric solution considering parametric <b>geometry,</b> material and boundary conditions. By assembling these modules and satisfying continuity constraints at the interfaces, a parametric surrogate model of the full problem can be obtained. In the present paper, the PGD technique in connection with NURBS <b>geometry</b> representation is used to create a parametric model for each module. In this technique, the NURBS objects allow to map the governing boundary value problem from a parametric non-regular domain into a regular reference domain and the PGD is used to create a reduced model in the reference domain. In the assembly stage, an optimization problem is solved to satisfy the continuity constraints at the interfaces. The proposed procedure is based on the offline--online paradigm: the offline stage consists of creating multiple pre-solved modules which can be afterwards assembled in almost real-time during the online stage, enabling quick evaluations of the full system response. To show the potential of the proposed approach some numerical examples in heat conduction and structural plates under bending are presented.

{{</citation>}}


## cs.IT (6)



### (1/6 | 233/254) Smart Resource Allocation at mmWave/THz Frequencies with Cooperative Rate-Splitting (Hyesang Cho et al., 2024)

{{<citation>}}

Hyesang Cho, Junil Choi. (2024)  
**Smart Resource Allocation at mmWave/THz Frequencies with Cooperative Rate-Splitting**
<br/>
<button class="copy-to-clipboard" title="Smart Resource Allocation at mmWave/THz Frequencies with Cooperative Rate-Splitting" index=233>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, eess-SP, math-IT  
Keyword Score: 23  
Keywords: Benchmarking, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09022v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09022v1.pdf" filename="2403.09022v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we propose algorithms to minimize the energy consumption in millimeter wave/terahertz multi-user downlink communication systems. To ensure coverage in blockage-vulnerable high frequency systems, we consider cooperative rate-splitting (CRS) and transmission over multiple time blocks, where via CRS, multiple users cooperate to assist a blocked user. Moreover, we show that transmission over multiple time blocks provides benefits through smart resource allocation. We first propose a communication framework named improved distinct extraction-based CRS (iDeCRS) that utilizes the benefits of rate-splitting. With our transmission framework, we derive a performance <b>benchmark</b> assuming genie channel state information (CSI), i.e., the channels of the present and future time blocks are known, denoted as GENIE. Using the results from GENIE, we derive a novel efficiency constrained optimization (ECO) algorithm assuming instantaneous CSI. In addition, a simple but effective even data transmission (EDT) algorithm that promotes steady transmission along the time blocks is proposed. <b>Simulation</b> results show that ECO and EDT have satisfactory performances compared to GENIE. The results also show that ECO outperforms EDT when many users are cooperating, and vise versa.

{{</citation>}}


### (2/6 | 234/254) Localization in Digital Twin MIMO Networks: A Case for Massive Fingerprinting (João Morais et al., 2024)

{{<citation>}}

João Morais, Ahmed Alkhateeb. (2024)  
**Localization in Digital Twin MIMO Networks: A Case for Massive Fingerprinting**
<br/>
<button class="copy-to-clipboard" title="Localization in Digital Twin MIMO Networks: A Case for Massive Fingerprinting" index=234>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, eess-SP, math-IT  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09614v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09614v1.pdf" filename="2403.09614v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Localization in outdoor wireless systems typically requires transmitting specific reference signals to estimate distance (trilateration methods) or angle (triangulation methods). These cause overhead on communication, need a LoS link to work well, and require multiple base stations, often imposing synchronization or specific hardware requirements. Fingerprinting has none of these drawbacks, but building its database requires high human effort to collect real-world measurements. For a long time, this issue limited the size of databases and thus their performance. This work proposes significantly reducing human effort in building fingerprinting databases by populating them with \textit{digital twin RF maps}. These RF maps are built from ray-tracing <b>simulations</b> on a digital replica of the environment across several frequency bands and beamforming configurations. Online user fingerprints are then matched against this spatial database. The approach was evaluated with practical <b>simulations</b> using realistic propagation models and user measurements. Our experiments show sub-meter localization errors on a NLoS location 95\% of the time using sensible user measurement report sizes. Results highlight the promising potential of the proposed digital twin approach for ubiquitous wide-area 6G localization.

{{</citation>}}


### (3/6 | 235/254) Joint Port Selection and Beamforming Design for Fluid Antenna Assisted Integrated Data and Energy Transfer (Long Zhang et al., 2024)

{{<citation>}}

Long Zhang, Halvin Yang, Yizhe Zhao, Jie Hu. (2024)  
**Joint Port Selection and Beamforming Design for Fluid Antenna Assisted Integrated Data and Energy Transfer**
<br/>
<button class="copy-to-clipboard" title="Joint Port Selection and Beamforming Design for Fluid Antenna Assisted Integrated Data and Energy Transfer" index=235>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, eess-SP, math-IT  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09357v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09357v1.pdf" filename="2403.09357v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Integrated data and energy transfer (IDET) has been of fundamental importance for providing both wireless data transfer (WDT) and wireless energy transfer (WET) services towards low-power devices. Fluid antenna (FA) is capable of exploiting the huge spatial diversity of the wireless channel to enhance the receive signal strength, which is more suitable for the tiny-size low-power devices having the IDET requirements. In this letter, a multiuser FA assisted IDET system is studied and the weighted energy harvesting power at energy receivers (ERs) is maximized by jointly optimizing the port selection and transmit beamforming design under imperfect channel state information (CSI), while the signal-to-interference-plus-noise ratio (SINR) constraint for each data receiver (DR) is satisfied. An efficient algorithm is proposed to obtain the suboptimal solutions for the non-convex problem. <b>Simulation</b> results evaluate the performance of the FA-IDET system, while also demonstrate that FA outperforms the multi-input-multi-output (MIMO) counterpart in terms of the IDET performance, as long as the port number is large enough.

{{</citation>}}


### (4/6 | 236/254) Reverse em-problem based on Bregman divergence and its application to classical and quantum information theory (Masahito Hayashi, 2024)

{{<citation>}}

Masahito Hayashi. (2024)  
**Reverse em-problem based on Bregman divergence and its application to classical and quantum information theory**
<br/>
<button class="copy-to-clipboard" title="Reverse em-problem based on Bregman divergence and its application to classical and quantum information theory" index=236>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, math-IT, math-OC, quant-ph  
Keyword Score: 15  
Keywords: Geometry, Mutual Information  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09252v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09252v1.pdf" filename="2403.09252v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The recent paper (IEEE Trans. IT 69, 1680) introduced an analytical method for calculating the channel capacity without the need for iteration. This method has certain limitations that restrict its applicability. Furthermore, the paper does not provide an explanation as to why the channel capacity can be solved analytically in this particular case. In order to broaden the scope of this method and address its limitations, we turn our attention to the reverse em-problem, proposed by Toyota (Information <b>Geometry,</b> 3, 1355 (2020)). This reverse em-problem involves iteratively applying the inverse map of the em iteration to calculate the channel capacity, which represents the maximum <b>mutual</b> <b>information.</b> However, several open problems remained unresolved in Toyota's work. To overcome these challenges, we formulate the reverse em-problem based on Bregman divergence and provide solutions to these open problems. Building upon these results, we transform the reverse em-problem into em-problems and derive a non-iterative formula for the reverse em-problem. This formula can be viewed as a generalization of the aforementioned analytical calculation method. Importantly, this derivation sheds light on the information geometrical structure underlying this special case. By effectively addressing the limitations of the previous analytical method and providing a deeper understanding of the underlying information geometrical structure, our work significantly expands the applicability of the proposed method for calculating the channel capacity without iteration.

{{</citation>}}


### (5/6 | 237/254) A Deep Reinforcement Learning Approach for Autonomous Reconfigurable Intelligent Surfaces (Hyuckjin Choi et al., 2024)

{{<citation>}}

Hyuckjin Choi, Ly V. Nguyen, Junil Choi, A. Lee Swindlehurst. (2024)  
**A Deep Reinforcement Learning Approach for Autonomous Reconfigurable Intelligent Surfaces**
<br/>
<button class="copy-to-clipboard" title="A Deep Reinforcement Learning Approach for Autonomous Reconfigurable Intelligent Surfaces" index=237>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, eess-SP, math-IT  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09270v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09270v1.pdf" filename="2403.09270v1.pdf">Download PDF</button>

---


**ABSTRACT**  
A reconfigurable intelligent surface (RIS) is a prospective wireless technology that enhances wireless channel quality. An RIS is often equipped with passive array of elements and provides cost and power-efficient solutions for coverage extension of wireless communication systems. Without any radio frequency (RF) chains or computing resources, however, the RIS requires control information to be sent to it from an external unit, e.g., a base station (BS). The control information can be delivered by wired or wireless channels, and the BS must be aware of the RIS and the RIS-related channel conditions in order to effectively configure its behavior. Recent works have introduced hybrid RIS structures possessing a few active elements that can sense and digitally process received data. Here, we propose the operation of an entirely autonomous RIS that operates without a control link between the RIS and BS. Using a few sensing elements, the autonomous RIS employs a deep Q network (DQN) based on <b>reinforcement</b> <b>learning</b> in order to enhance the sum rate of the network. Our results illustrate the potential of deploying autonomous RISs in wireless networks with essentially no network overhead.

{{</citation>}}


### (6/6 | 238/254) Performance Analysis on RIS-Aided Wideband Massive MIMO OFDM Systems with Low-Resolution ADCs (Xianzhe Chen et al., 2024)

{{<citation>}}

Xianzhe Chen, Hong Ren, Cunhua Pan, Zhangjie Peng, Kangda Zhi, Yong Liu, Xiaojun Xi, Ana Garcia Armada, Cheng-Xiang Wang. (2024)  
**Performance Analysis on RIS-Aided Wideband Massive MIMO OFDM Systems with Low-Resolution ADCs**
<br/>
<button class="copy-to-clipboard" title="Performance Analysis on RIS-Aided Wideband Massive MIMO OFDM Systems with Low-Resolution ADCs" index=238>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, eess-SP, math-IT  
Keyword Score: 10  
Keywords: Scaling Law  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09058v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09058v1.pdf" filename="2403.09058v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper investigates a reconfigurable intelligent surface (RIS)-aided wideband massive multiple-input multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM) system with low-resolution analog-to-digital converters (ADCs). Frequency-selective Rician fading channels are considered, and the OFDM data transmission process is presented in time domain. This paper derives the closed-form approximate expression of the uplink achievable rate, based on which the asymptotic system performance is analyzed when the number of the antennas at the base station and the number of reflecting elements at the RIS grow to infinity. Besides, the power <b>scaling</b> <b>laws</b> of the considered system are revealed to provide energy-saving insights. Furthermore, this paper proposes a gradient ascent-based algorithm to design the phase shifts of the RIS for maximizing the minimum user rate. Finally, numerical results are presented to verify the correctness of analytical conclusions and draw insights.

{{</citation>}}


## stat.ML (2)



### (1/2 | 239/254) Variational Inference with Sequential Sample-Average Approximations (Heiko Zimmermann et al., 2024)

{{<citation>}}

Heiko Zimmermann, Christian A. Naesseth, Jan-Willem van de Meent. (2024)  
**Variational Inference with Sequential Sample-Average Approximations**
<br/>
<button class="copy-to-clipboard" title="Variational Inference with Sequential Sample-Average Approximations" index=239>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: cs-LG, stat-ML, stat.ML  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09429v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09429v1.pdf" filename="2403.09429v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present variational inference with sequential sample-average approximation (VISA), a method for approximate inference in computationally intensive models, such as those based on numerical <b>simulations.</b> VISA extends importance-weighted forward-KL variational inference by employing a sequence of sample-average approximations, which are considered valid inside a trust region. This makes it possible to reuse model evaluations across multiple gradient steps, thereby reducing computational cost. We perform experiments on high-dimensional Gaussians, Lotka-Volterra dynamics, and a Pickover attractor, which demonstrate that VISA can achieve comparable approximation accuracy to standard importance-weighted forward-KL variational inference with computational savings of a factor two or more for conservatively chosen learning rates.

{{</citation>}}


### (2/2 | 240/254) Pantypes: Diverse Representatives for Self-Explainable Models (Rune Kjærsgaard et al., 2024)

{{<citation>}}

Rune Kjærsgaard, Ahcène Boubekki, Line Clemmensen. (2024)  
**Pantypes: Diverse Representatives for Self-Explainable Models**
<br/>
<button class="copy-to-clipboard" title="Pantypes: Diverse Representatives for Self-Explainable Models" index=240>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: cs-LG, stat-ML, stat.ML  
Keyword Score: 10  
Keywords: Fairness  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09383v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09383v1.pdf" filename="2403.09383v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Prototypical self-explainable classifiers have emerged to meet the growing demand for interpretable AI systems. These classifiers are designed to incorporate high transparency in their decisions by basing inference on similarity with learned prototypical objects. While these models are designed with diversity in mind, the learned prototypes often do not sufficiently represent all aspects of the input distribution, particularly those in low density regions. Such lack of sufficient data representation, known as representation bias, has been associated with various detrimental properties related to machine learning diversity and <b>fairness.</b> In light of this, we introduce pantypes, a new family of prototypical objects designed to capture the full diversity of the input distribution through a sparse set of objects. We show that pantypes can empower prototypical self-explainable models by occupying divergent regions of the latent space and thus fostering high diversity, interpretability and <b>fairness.</b>

{{</citation>}}


## math.OC (1)



### (1/1 | 241/254) Solvability of the Inverse Optimal Control problem based on the minimum principle (Afreen Islam et al., 2024)

{{<citation>}}

Afreen Islam, Guido Herrmann, Joaquin Carrasco. (2024)  
**Solvability of the Inverse Optimal Control problem based on the minimum principle**
<br/>
<button class="copy-to-clipboard" title="Solvability of the Inverse Optimal Control problem based on the minimum principle" index=241>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.OC  
Categories: cs-SY, eess-SY, math-OC, math.OC  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09375v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09375v1.pdf" filename="2403.09375v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, the solvability of the Inverse Optimal Control (IOC) problem based on two existing minimum principal methods, is analysed. The aim of this work is to answer the question regarding what kinds of trajectories, that is depending on the initial conditions of the closed-loop system and system dynamics, of the original optimal control problem, will result in the recovery of the true weights of the reward function for both the soft and the hard-constrained methods [1], [2]. Analytical conditions are provided which allow to verify if a trajectory is sufficiently conditioned, that is, holds sufficient information to recover the true weights of an optimal control problem. It was found that the open-loop system of the original optimal problem has a stronger influence on the solvability of the Inverse Optimal Control problem for the hard-constrained method as compared to the soft-constrained method. These analytical results were validated via <b>simulation.</b>

{{</citation>}}


## cs.GT (1)



### (1/1 | 242/254) All-pay Auction Based Profit Maximization in End-to-End Computation Offloading System (Hai Xue et al., 2024)

{{<citation>}}

Hai Xue, Yun Xia, Di Zhang, Honghua Wei, Xiaolong Xu. (2024)  
**All-pay Auction Based Profit Maximization in End-to-End Computation Offloading System**
<br/>
<button class="copy-to-clipboard" title="All-pay Auction Based Profit Maximization in End-to-End Computation Offloading System" index=242>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.GT  
Categories: cs-GT, cs.GT  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09129v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09129v1.pdf" filename="2403.09129v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Pricing is an important issue in mobile edge computing. How to appropriately determine the bid of end user (EU) is an incentive factor for edge cloud (EC) to offer service. In this letter, we propose an equilibrium pricing scheme based on the all-pay auction model in end-to-end collaboration environment, wherein all EUs can acquire the service at a lower price than the own value of the required resource. In addition, we propose a set allocation algorithm to divide all the bidders into different sets according to the price, and the EUs in each set get the service, which averts the case of getting no service due to the low price. Extensive <b>simulation</b> results demonstrate that the proposed scheme can effectively maximize the total profit of the edge offloading system, and guarantee all EUs can access the service.

{{</citation>}}


## math.LO (1)



### (1/1 | 243/254) Generalisation of proof simulation procedures for Frege systems by M.L.~Bonet and S.R.~Buss (Daniil Kozhemiachenko, 2024)

{{<citation>}}

Daniil Kozhemiachenko. (2024)  
**Generalisation of proof simulation procedures for Frege systems by M.L.~Bonet and S.R.~Buss**
<br/>
<button class="copy-to-clipboard" title="Generalisation of proof simulation procedures for Frege systems by M.L.~Bonet and S.R.~Buss" index=243>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.LO  
Categories: cs-LO, math-LO, math.LO  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09119v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09119v1.pdf" filename="2403.09119v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we present a~generalisation of proof <b>simulation</b> procedures for Frege systems by Bonet and Buss to some logics for which the deduction theorem does not hold. In particular, we study the case of finite-valued \L{}ukasiewicz logics. To this end, we provide proof systems that augment Avron's Frege system for \L{}ukasiewicz three-valued logic with nested and general versions of the disjunction elimination rule, respectively. For these systems we provide upper bounds on speed-ups w.r.t.\ both the number of steps in proofs and the length of proofs. We also consider Tamminga's natural deduction and Avron's hypersequent calculus for 3-valued \L{}ukasiewicz logic and generalise our results considering the disjunction elimination rule to all finite-valued \L{}ukasiewicz logics.

{{</citation>}}


## cs.AR (2)



### (1/2 | 244/254) FlexNN: A Dataflow-aware Flexible Deep Learning Accelerator for Energy-Efficient Edge Devices (Arnab Raha et al., 2024)

{{<citation>}}

Arnab Raha, Deepak A. Mathaikutty, Soumendu K. Ghosh, Shamik Kundu. (2024)  
**FlexNN: A Dataflow-aware Flexible Deep Learning Accelerator for Energy-Efficient Edge Devices**
<br/>
<button class="copy-to-clipboard" title="FlexNN: A Dataflow-aware Flexible Deep Learning Accelerator for Energy-Efficient Edge Devices" index=244>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AR  
Categories: cs-AR, cs-NE, cs.AR  
Keyword Score: 20  
Keywords: Convolution, Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09026v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09026v1.pdf" filename="2403.09026v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper introduces FlexNN, a Flexible Neural Network accelerator, which adopts agile design principles to enable versatile dataflows, enhancing energy efficiency. Unlike conventional <b>convolutional</b> <b>neural</b> <b>network</b> accelerator architectures that adhere to fixed dataflows (such as input, weight, output, or row stationary) for transferring activations and weights between storage and compute units, our design revolutionizes by enabling adaptable dataflows of any type through software configurable descriptors. Considering that data movement costs considerably outweigh compute costs from an energy perspective, the flexibility in dataflow allows us to optimize the movement per layer for minimal data transfer and energy consumption, a capability unattainable in fixed dataflow architectures. To further enhance throughput and reduce energy consumption in the FlexNN architecture, we propose a novel sparsity-based acceleration logic that utilizes fine-grained sparsity in both the activation and weight tensors to bypass redundant computations, thus optimizing the <b>convolution</b> engine within the hardware accelerator. Extensive experimental results underscore a significant enhancement in the performance and energy efficiency of FlexNN relative to existing DNN accelerators.

{{</citation>}}


### (2/2 | 245/254) Analytical Heterogeneous Die-to-Die 3D Placement with Macros (Yuxuan Zhao et al., 2024)

{{<citation>}}

Yuxuan Zhao, Peiyu Liao, Siting Liu, Jiaxi Jiang, Yibo Lin, Bei Yu. (2024)  
**Analytical Heterogeneous Die-to-Die 3D Placement with Macros**
<br/>
<button class="copy-to-clipboard" title="Analytical Heterogeneous Die-to-Die 3D Placement with Macros" index=245>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AR  
Categories: cs-AR, cs.AR  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09070v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09070v1.pdf" filename="2403.09070v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents an innovative approach to 3D mixed-size placement in heterogeneous face-to-face (F2F) bonded 3D ICs. We propose an analytical framework that utilizes a dedicated density model and a bistratal wirelength model, effectively handling macros and standard cells in a 3D solution space. A novel 3D preconditioner is developed to resolve the topological and physical gap between macros and standard cells. Additionally, we propose a mixed-integer linear programming (MILP) formulation for macro rotation to optimize wirelength. Our framework is implemented with full-scale GPU acceleration, leveraging an adaptive 3D density accumulation algorithm and an incremental wirelength gradient algorithm. Experimental results on ICCAD 2023 contest <b>benchmarks</b> demonstrate that our framework can achieve 5.9% quality score improvement compared to the first-place winner with 4.0x runtime speedup.

{{</citation>}}


## cs.DM (2)



### (1/2 | 246/254) Binary Stretch Embedding of Weighted Graphs (Javad B. Ebrahimi et al., 2024)

{{<citation>}}

Javad B. Ebrahimi, Mehri Oghbaei Bonab. (2024)  
**Binary Stretch Embedding of Weighted Graphs**
<br/>
<button class="copy-to-clipboard" title="Binary Stretch Embedding of Weighted Graphs" index=246>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DM  
Categories: G-2-2; G-2-3, cs-DM, cs.DM, math-CO  
Keyword Score: 13  
Keywords: Graph, Graph Embedding  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09311v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09311v1.pdf" filename="2403.09311v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we introduce and study the problem of \textit{binary stretch embedding} of edge-weighted <b>graph.</b> <b>This</b> problem is closely related to the well-known \textit{addressing problem} of Graham and Pollak. Addressing problem is the problem of assigning the shortest possible length strings (called ``addresses") over the alphabet $\{0,1,*\}$ to the vertices of an input <b>graph</b> <b>$G$</b> with the following property. For every pair $u,v$ of vertices, the number of positions in which one of their addresses is $1$, and the other is $0$ is exactly equal to the distance of $u,v$ in <b>graph</b> <b>$G$.</b> When the addresses do not contain the symbol $*$, the problem is called \textit{isometric hypercube embedding}. As far as we know, the isometric hypercube embedding was introduced by Firsov in 1965. It is known that such addresses do not exist for general <b>graphs.</b> <b>Inspired</b> by the addressing problem, in this paper, we introduce the \textit{binary stretch embedding problem}, or BSEP for short, for the edge-weighted undirected <b>graphs.</b> <b>We</b> also argue how this problem is related to other <b>graph</b> <b>embedding</b> problems in the literature. Using tools and techniques such as Hadamard codes and the theory of linear programming, several upper and lower bounds as well as exact solutions for certain classes of <b>graphs</b> <b>will</b> be discovered. As an application of the results in this paper, we derive improved upper bounds or exact values for the maximum size of Lee metric codes of certain parameters.

{{</citation>}}


### (2/2 | 247/254) Bounds and extremal graphs for monitoring edge-geodetic sets in graphs (Florent Foucaud et al., 2024)

{{<citation>}}

Florent Foucaud, Pierre-Marie Marcille, Zin Mar Myint, R. B. Sandeep, Sagnik Sen, S. Taruni. (2024)  
**Bounds and extremal graphs for monitoring edge-geodetic sets in graphs**
<br/>
<button class="copy-to-clipboard" title="Bounds and extremal graphs for monitoring edge-geodetic sets in graphs" index=247>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DM  
Categories: cs-DM, cs.DM, math-CO  
Keyword Score: 13  
Keywords: Graph, Stemming  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09122v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09122v1.pdf" filename="2403.09122v1.pdf">Download PDF</button>

---


**ABSTRACT**  
A monitoring edge-geodetic set, or simply an MEG-set, of a <b>graph</b> $G$ is a vertex subset $M \subseteq V(G)$ such that given any edge $e$ of $G$, $e$ lies on every shortest $u$-$v$ path of $G$, for some $u,v \in M$. The monitoring edge-geodetic number of $G$, denoted by $meg(G)$, is the minimum cardinality of such an MEG-set. This notion provides a <b>graph</b> theoretic model of the network monitoring problem. In this article, we compare $meg(G)$ with some other <b>graph</b> theoretic parameters <b>stemming</b> from the network monitoring problem and provide examples of <b>graphs</b> having prescribed values for each of these parameters. We also characterize <b>graphs</b> $G$ that have $V(G)$ as their minimum MEG-set, which settles an open problem due to Foucaud \textit{et al.} (CALDAM 2023), and prove that some classes of <b>graphs</b> fall within this characterization. We also provide a general upper bound for $meg(G)$ for sparse <b>graphs</b> in terms of their girth, and later refine the upper bound using the chromatic number of $G$. We examine the change in $meg(G)$ with respect to two fundamental <b>graph</b> operations: clique-sum and subdivisions. In both cases, we provide a lower and an upper bound of the possible amount of changes and provide (almost) tight examples.

{{</citation>}}


## cs.GR (2)



### (1/2 | 248/254) HeadEvolver: Text to Head Avatars via Locally Learnable Mesh Deformation (Duotun Wang et al., 2024)

{{<citation>}}

Duotun Wang, Hengyu Meng, Zeyu Cai, Zhijing Shao, Qianxi Liu, Lin Wang, Mingming Fan, Ying Shan, Xiaohang Zhan, Zeyu Wang. (2024)  
**HeadEvolver: Text to Head Avatars via Locally Learnable Mesh Deformation**
<br/>
<button class="copy-to-clipboard" title="HeadEvolver: Text to Head Avatars via Locally Learnable Mesh Deformation" index=248>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.GR  
Categories: I-2-6; I-3-8, cs-AI, cs-GR, cs.GR  
Keyword Score: 10  
Keywords: Diffusion Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09326v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09326v1.pdf" filename="2403.09326v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present HeadEvolver, a novel framework to generate stylized head avatars from text guidance. HeadEvolver uses locally learnable mesh deformation from a template head mesh, producing high-quality digital assets for detail-preserving editing and animation. To tackle the challenges of lacking fine-grained and semantic-aware local shape control in global deformation through Jacobians, we introduce a trainable parameter as a weighting factor for the Jacobian at each triangle to adaptively change local shapes while maintaining global correspondences and facial features. Moreover, to ensure the coherence of the resulting shape and appearance from different viewpoints, we use pretrained image <b>diffusion</b> <b>models</b> for differentiable rendering with regularization terms to refine the deformation under text guidance. Extensive experiments demonstrate that our method can generate diverse head avatars with an articulated mesh that can be edited seamlessly in 3D graphics software, facilitating downstream applications such as more efficient animation with inherited blend shapes and semantic consistency.

{{</citation>}}


### (2/2 | 249/254) A New Split Algorithm for 3D Gaussian Splatting (Qiyuan Feng et al., 2024)

{{<citation>}}

Qiyuan Feng, Gengchen Cao, Haoxiang Chen, Tai-Jiang Mu, Ralph R. Martin, Shi-Min Hu. (2024)  
**A New Split Algorithm for 3D Gaussian Splatting**
<br/>
<button class="copy-to-clipboard" title="A New Split Algorithm for 3D Gaussian Splatting" index=249>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.GR  
Categories: cs-GR, cs.GR  
Keyword Score: 5  
Keywords: Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09143v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09143v1.pdf" filename="2403.09143v1.pdf">Download PDF</button>

---


**ABSTRACT**  
3D Gaussian splatting models, as a novel explicit 3D representation, have been applied in many domains recently, such as explicit geometric editing and <b>geometry</b> generation. Progress has been rapid. However, due to their mixed scales and cluttered shapes, 3D Gaussian splatting models can produce a blurred or needle-like effect near the surface. At the same time, 3D Gaussian splatting models tend to flatten large untextured regions, yielding a very sparse point cloud. These problems are caused by the non-uniform nature of 3D Gaussian splatting models, so in this paper, we propose a new 3D Gaussian splitting algorithm, which can produce a more uniform and surface-bounded 3D Gaussian splatting model. Our algorithm splits an $N$-dimensional Gaussian into two N-dimensional Gaussians. It ensures consistency of mathematical characteristics and similarity of appearance, allowing resulting 3D Gaussian splatting models to be more uniform and a better fit to the underlying surface, and thus more suitable for explicit editing, point cloud extraction and other tasks. Meanwhile, our 3D Gaussian splitting approach has a very simple closed-form solution, making it readily applicable to any 3D Gaussian model.

{{</citation>}}


## quant-ph (1)



### (1/1 | 250/254) Bridging Quantum Computing and Differential Privacy: A Survey on Quantum Computing Privacy (Yusheng Zhao et al., 2024)

{{<citation>}}

Yusheng Zhao, Hui Zhong, Xinyue Zhang, Chi Zhang, Miao Pan. (2024)  
**Bridging Quantum Computing and Differential Privacy: A Survey on Quantum Computing Privacy**
<br/>
<button class="copy-to-clipboard" title="Bridging Quantum Computing and Differential Privacy: A Survey on Quantum Computing Privacy" index=250>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: quant-ph  
Categories: cs-CR, quant-ph, quant-ph  
Keyword Score: 10  
Keywords: Differential Privacy  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09173v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09173v1.pdf" filename="2403.09173v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Quantum computing has attracted significant attention in areas such as cryptography, cybersecurity, and drug discovery. Due to the advantage of parallel processing, quantum computing can speed up the response to complex challenges and the processing of large-scale datasets. However, since quantum computing usually requires sensitive datasets, privacy breaches have become a vital concern. <b>Differential</b> <b>privacy</b> (DP) is a promising privacy-preserving method in classical computing and has been extended to the quantum domain in recent years. In this survey, we categorize the existing literature based on whether internal inherent noise or external artificial noise is used as a source to achieve DP in quantum computing. We explore how these approaches are applied at different stages of a quantum algorithm (i.e., state preparation, quantum circuit, and quantum measurement). We also discuss challenges and future directions for DP in quantum computing. By summarizing recent advancements, we hope to provide a comprehensive, up-to-date overview for researchers venturing into this field.

{{</citation>}}


## math.NA (2)



### (1/2 | 251/254) High-order numerical integration on regular embedded surfaces (Gentian Zavalani et al., 2024)

{{<citation>}}

Gentian Zavalani, Michael Hecht. (2024)  
**High-order numerical integration on regular embedded surfaces**
<br/>
<button class="copy-to-clipboard" title="High-order numerical integration on regular embedded surfaces" index=251>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.NA  
Categories: 65D15, 65D30, 65D32, cs-NA, math-NA, math.NA  
Keyword Score: 5  
Keywords: Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09178v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09178v1.pdf" filename="2403.09178v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present a high-order surface quadrature (HOSQ) for accurately approximating regular surface integrals on closed surfaces. The initial step of our approach rests on exploiting square-squeezing--a homeomorphic bilinear square-simplex transformation, re-parametrizing any surface triangulation to a quadrilateral mesh. For each resulting quadrilateral domain we interpolate the <b>geometry</b> by tensor polynomials in Chebyshev--Lobatto grids. Posterior the tensor-product Clenshaw-Curtis quadrature is applied to compute the resulting integral. We demonstrate efficiency, fast runtime performance, high-order accuracy, and robustness for complex geometries.

{{</citation>}}


### (2/2 | 252/254) Dynamically accelerating the power iteration with momentum (Christian Austin et al., 2024)

{{<citation>}}

Christian Austin, Sara Pollock, Yunrong Zhu. (2024)  
**Dynamically accelerating the power iteration with momentum**
<br/>
<button class="copy-to-clipboard" title="Dynamically accelerating the power iteration with momentum" index=252>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.NA  
Categories: 65F15, 65B99, cs-NA, math-NA, math.NA  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09618v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09618v1.pdf" filename="2403.09618v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we propose, analyze and demonstrate a dynamic momentum method to accelerate power and inverse power iterations with minimal computational overhead. The method is appropriate for real, diagonalizable matrices, and does not require a priori spectral knowledge. We review and extend background results on previously developed static momentum accelerations for the power iteration through the connection between the momentum accelerated iteration and the standard power iteration applied to an augmented matrix. We show that the augmented matrix is defective for the optimal parameter choice. We then present our dynamic method which updates the momentum parameter at each iteration based on the Rayleigh quotient and two previous residuals. We present convergence and stability theory for the method by considering a power-like method consisting of multiplying an initial vector by a sequence of augmented matrices. We demonstrate the developed method on a number of <b>benchmark</b> problems, and see that it outperforms both the power iteration and often the static momentum acceleration with optimal parameter choice. Finally, we present and demonstrate an explicit extension of the algorithm to inverse power iterations.

{{</citation>}}


## math.CO (1)



### (1/1 | 253/254) Edge-apexing in hereditary classes of graphs (Jagdeep Singh et al., 2024)

{{<citation>}}

Jagdeep Singh, Vaidy Sivaraman. (2024)  
**Edge-apexing in hereditary classes of graphs**
<br/>
<button class="copy-to-clipboard" title="Edge-apexing in hereditary classes of graphs" index=253>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.CO  
Categories: 05C75, cs-DM, math-CO, math.CO  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09456v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09456v1.pdf" filename="2403.09456v1.pdf">Download PDF</button>

---


**ABSTRACT**  
A class $\mathcal{G}$ of <b>graphs</b> is called hereditary if it is closed under taking induced subgraphs. We denote by $G^{epex}$ the class of <b>graphs</b> that are at most one edge away from being in $\mathcal{G}$. We note that $G^{epex}$ is hereditary and prove that if a hereditary class $\mathcal{G}$ has finitely many forbidden induced subgraphs, then so does $G^{epex}$. The hereditary class of cographs consists of all <b>graphs</b> $G$ that can be generated from $K_1$ using complementation and disjoint union. Cographs are precisely the <b>graphs</b> that do not have the $4$-vertex path as an induced subgraph. For the class of edge-apex cographs our main result bounds the order of such forbidden induced subgraphs by 8 and finds all of them by computer search.

{{</citation>}}


## cs.LO (1)



### (1/1 | 254/254) A complete logic for causal consistency (Will Simmons et al., 2024)

{{<citation>}}

Will Simmons, Aleks Kissinger. (2024)  
**A complete logic for causal consistency**
<br/>
<button class="copy-to-clipboard" title="A complete logic for causal consistency" index=254>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LO  
Categories: 03B70, 18M45, 81P16, F-4-1, cs-LO, cs.LO, quant-ph  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.09297v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.09297v1.pdf" filename="2403.09297v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The $\mathrm{Caus}[-]$ construction takes a base category of ``raw materials'' and builds a category of higher order causal processes, that is a category whose types encode causal (a.k.a. signalling) constraints between collections of systems. Notable examples are categories of higher-order stochastic maps and higher-order quantum channels. Well-typedness in $\mathrm{Caus}[-]$ corresponds to a composition of processes being causally consistent, in the sense that any choice of local processes of the prescribed types yields an overall process respecting causality constraints. It follows that closed processes always occur with probability 1, ruling out e.g. causal paradoxes arising from time loops. It has previously been shown that $\mathrm{Caus}[\mathcal{C}]$ gives a model of MLL+MIX and BV logic, hence these logics give sufficient conditions for causal consistency, but they fail to provide a complete characterisation. In this follow-on work, we introduce <b>graph</b> types as a tool to examine causal structures over <b>graphs</b> in this model. We explore their properties, standard forms, and equivalent definitions; in particular, a process obeys all signalling constraints of the <b>graph</b> iff it is expressible as an affine combination of factorisations into local causal processes connected according to the edges of the <b>graph.</b> The properties of <b>graph</b> types are then used to prove completeness for causal consistency of a new causal logic that conservatively extends pomset logic. The crucial extra ingredient is a notion of distinguished atoms that correspond to first-order states, which only admit a flow of information in one direction. Using the fact that causal logic conservatively extends pomset logic, we finish by giving a physically-meaningful interpretation to a separating statement between pomset and BV.

{{</citation>}}
