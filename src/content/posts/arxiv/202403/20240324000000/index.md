---
draft: false
title: "arXiv @ 2024.03.24"
date: 2024-03-24
author: "akitenkrad"
description: ""
tags: ["arXiv", "Published:2024"]
menu:
  sidebar:
    name: "arXiv @ 2024.03.24"
    identifier: arxiv_20240324
    parent: 202403_arxiv
    weight: 10
math: true
---

<figure style="border:none; width:100%; display:flex; justify-content: center">
    <iframe src="pie.html" width=900 height=620 style="border:none"></iframe>
</figure>


## Primary Categories

- [cs.AI (5)](#csai-5)
- [cs.AR (1)](#csar-1)
- [cs.CL (24)](#cscl-24)
- [cs.CR (3)](#cscr-3)
- [cs.CV (63)](#cscv-63)
- [cs.CY (5)](#cscy-5)
- [cs.DC (2)](#csdc-2)
- [cs.DS (1)](#csds-1)
- [cs.ET (1)](#cset-1)
- [cs.GT (2)](#csgt-2)
- [cs.IR (2)](#csir-2)
- [cs.IT (7)](#csit-7)
- [cs.LG (28)](#cslg-28)
- [cs.MM (2)](#csmm-2)
- [cs.NI (2)](#csni-2)
- [cs.PL (1)](#cspl-1)
- [cs.RO (15)](#csro-15)
- [cs.SE (6)](#csse-6)
- [cs.SI (5)](#cssi-5)
- [eess.IV (5)](#eessiv-5)
- [eess.SP (1)](#eesssp-1)
- [eess.SY (4)](#eesssy-4)
- [math.CO (1)](#mathco-1)
- [math.NA (2)](#mathna-2)
- [math.OC (3)](#mathoc-3)
- [math.ST (1)](#mathst-1)
- [physics.geo-ph (1)](#physicsgeo-ph-1)
- [physics.med-ph (1)](#physicsmed-ph-1)
- [physics.soc-ph (1)](#physicssoc-ph-1)
- [q-bio.NC (1)](#q-bionc-1)
- [q-bio.OT (1)](#q-bioot-1)
- [q-bio.QM (1)](#q-bioqm-1)
- [q-fin.CP (2)](#q-fincp-2)
- [quant-ph (1)](#quant-ph-1)
- [stat.ML (1)](#statml-1)

## Keywords

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>keyword</th>
      <th>cs.CL</th>
      <th>cs.CV</th>
      <th>cs.LG</th>
      <th>cs.RO</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Active Learning</td>
      <td></td>
      <td>2</td>
      <td>3</td>
      <td></td>
    </tr>
    <tr>
      <td>Adversarial Attack</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Adversarial Learning</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Autoencoder</td>
      <td></td>
      <td></td>
      <td></td>
      <td>2</td>
    </tr>
    <tr>
      <td>Automatic Evaluation</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>BERT</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Bandit Algorithm</td>
      <td></td>
      <td></td>
      <td>3</td>
      <td></td>
    </tr>
    <tr>
      <td>Benchmarking</td>
      <td>2</td>
      <td>10</td>
      <td>4</td>
      <td></td>
    </tr>
    <tr>
      <td>Black Box</td>
      <td>1</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Chain-of-thought Prompt</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>ChatGPT</td>
      <td>1</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Clustering</td>
      <td>2</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Continual Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Contrastive Learning</td>
      <td></td>
      <td>3</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>ControlNet</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Convolution</td>
      <td></td>
      <td>8</td>
      <td>4</td>
      <td></td>
    </tr>
    <tr>
      <td>Convolutional Neural Network</td>
      <td></td>
      <td>4</td>
      <td>5</td>
      <td></td>
    </tr>
    <tr>
      <td>Data Augmentation</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Differential Privacy</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Diffusion Model</td>
      <td></td>
      <td>9</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Distribution Shift</td>
      <td></td>
      <td>4</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Domain Adaptation</td>
      <td></td>
      <td>2</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Event-Relation Extraction</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Face Recognition</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Fairness</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Fake News Detection</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Federated Learning</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Few-shot</td>
      <td>2</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Fine-tuning</td>
      <td>9</td>
      <td>4</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Foundation Model</td>
      <td></td>
      <td>4</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>GPT</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>GPT-2</td>
      <td>1</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>GPT-3</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>GPT-3.5</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>GPT-4</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Generative Adversarial Network</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Geometry</td>
      <td></td>
      <td>5</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Graph</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td></td>
    </tr>
    <tr>
      <td>Graph Classification</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Graph Convolutional Network</td>
      <td></td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Graph Embedding</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Graph Neural Network</td>
      <td></td>
      <td></td>
      <td>4</td>
      <td></td>
    </tr>
    <tr>
      <td>Grounding</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Hallucination Detection</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Human Intervention</td>
      <td></td>
      <td></td>
      <td></td>
      <td>2</td>
    </tr>
    <tr>
      <td>In-context Learning</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Information Retrieval</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Knowledge Distillation</td>
      <td>1</td>
      <td>6</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Knowledge Graph</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>LSTM</td>
      <td></td>
      <td>1</td>
      <td>4</td>
      <td></td>
    </tr>
    <tr>
      <td>Large Language Model</td>
      <td>26</td>
      <td>3</td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <td>MNIST</td>
      <td></td>
      <td>2</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Meta Learning</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Model Compression</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Multi-modal</td>
      <td>2</td>
      <td>20</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Mutual Information</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Named Entity Recognition</td>
      <td>3</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Natural Language Understanding</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Object Detection</td>
      <td></td>
      <td>6</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Out-of-distribution</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Outlier Detection</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Perplexity</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Pre-trained Language Model</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Prompt</td>
      <td>7</td>
      <td>6</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Prompt Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Pruning</td>
      <td></td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Quantization</td>
      <td></td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Question Answering</td>
      <td>2</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Reasoning</td>
      <td>5</td>
      <td>3</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Recurrent Neural Network</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Reinforcement Learning</td>
      <td>1</td>
      <td></td>
      <td>6</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Reinforcement Learning from Human Feedback</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Relation Extraction</td>
      <td>3</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Representation Learning</td>
      <td></td>
      <td>3</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Rerank</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Retrieval-Augmented Generation</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Self-Attention</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Self-supervised Learning</td>
      <td></td>
      <td>8</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Self-supervised Pre-training</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Simulation</td>
      <td></td>
      <td>2</td>
      <td>1</td>
      <td>6</td>
    </tr>
    <tr>
      <td>Simulator</td>
      <td></td>
      <td>2</td>
      <td>1</td>
      <td>6</td>
    </tr>
    <tr>
      <td>Stance Detection</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Stemming</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Stochastic Gradient Descent</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Summarization</td>
      <td>1</td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Supervised Learning</td>
      <td>2</td>
      <td>9</td>
      <td>3</td>
      <td></td>
    </tr>
    <tr>
      <td>Text Classification</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Text Clustering</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Text Embedding</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Text Generation</td>
      <td>3</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Text2image</td>
      <td></td>
      <td>8</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Tokenization</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Transfer Learning</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Transformer</td>
      <td>3</td>
      <td>9</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Unsupervised Learning</td>
      <td>1</td>
      <td>3</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Variational Autoencoder</td>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Vision Transformer</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Vision-and-Language</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Visual Question Answering</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Weakly Supervised Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Weakly-supervised Learning</td>
      <td></td>
      <td>4</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Zero-shot</td>
      <td>5</td>
      <td>4</td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>human-in-the-loop</td>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<script>
$(function() {
  $("table").addClass("keyword-table table-bordered border-success");
  $("table thead").addClass("sticky-top");
  $("table tbody td").css("text-align", "");
});
</script>


## cs.LG (28)



### (1/28 | 1/202) Can large language models explore in-context? (Akshay Krishnamurthy et al., 2024)

{{<citation>}}

Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang, Aleksandrs Slivkins. (2024)  
**Can large language models explore in-context?**
<br/>
<button class="copy-to-clipboard" title="Can large language models explore in-context?" index=1>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CL, cs-LG, cs.LG  
Keyword Score: 140  
Keywords: Bandit Algorithm, Fine-tuning, Reinforcement Learning, GPT, GPT-3, GPT-3.5, GPT-4, Reasoning, In-context Learning, Large Language Model, Large Language Model, Prompt, Summarization, Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15371v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15371v1.pdf" filename="2403.15371v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We investigate the extent to which contemporary <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can engage in exploration, a core capability in <b>reinforcement</b> <b>learning</b> and decision making. We focus on native performance of existing <b>LLMs,</b> without training interventions. We deploy <b>LLMs</b> as agents in simple multi-armed <b>bandit</b> environments, specifying the environment description and interaction history entirely <b>in-context,</b> i.e., within the <b>LLM</b> <b>prompt.</b> We experiment with <b>GPT-3.5,</b> <b>GPT-4,</b> and Llama2, using a variety of <b>prompt</b> designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: <b>GPT-4</b> with chain-of-thought <b>reasoning</b> and an externally <b>summarized</b> interaction history, presented as sufficient statistics; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thought <b>reasoning</b> but unsummarized history. Although these findings can be interpreted positively, they suggest that external <b>summarization</b> -- which may not be possible in more complex settings -- is important for obtaining desirable behavior from <b>LLM</b> agents. We conclude that non-trivial algorithmic interventions, such as <b>fine-tuning</b> or dataset curation, may be required to empower <b>LLM-based</b> decision making agents in complex settings.

{{</citation>}}


### (2/28 | 2/202) GTAGCN: Generalized Topology Adaptive Graph Convolutional Networks (Sukhdeep Singh et al., 2024)

{{<citation>}}

Sukhdeep Singh, Anuj Sharma, Vinod Kumar Chauhan. (2024)  
**GTAGCN: Generalized Topology Adaptive Graph Convolutional Networks**
<br/>
<button class="copy-to-clipboard" title="GTAGCN: Generalized Topology Adaptive Graph Convolutional Networks" index=2>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 73  
Keywords: Graph Convolutional Network, Graph Classification, Graph, Graph Neural Network, Graph Neural Network, Convolution, Convolutional Neural Network, Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15077v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15077v1.pdf" filename="2403.15077v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNN)</b> have emerged as a popular and standard approach for learning from <b>graph-structured</b> <b>data.</b> <b>The</b> literature on <b>GNN</b> highlights the potential of this evolving research area and its widespread adoption in real-life applications. However, most of the approaches are either new in concept or derived from specific techniques. Therefore, the potential of more than one approach in hybrid form has not been studied extensively, which can be well utilized for sequenced data or static data together. We derive a hybrid approach based on two established techniques as generalized aggregation networks and topology adaptive <b>graph</b> <b>convolution</b> <b>networks</b> that solve our purpose to apply on both types of sequenced and static nature of data, effectively. The proposed method applies to both node and <b>graph</b> <b>classification.</b> <b>Our</b> empirical analysis reveals that the results are at par with literature results and better for handwritten strokes as sequenced data, where <b>graph</b> <b>structures</b> <b>have</b> not been explored.

{{</citation>}}


### (3/28 | 3/202) Improved Long Short-Term Memory-based Wastewater Treatment Simulators for Deep Reinforcement Learning (Esmaeel Mohammadi et al., 2024)

{{<citation>}}

Esmaeel Mohammadi, Daniel Ortiz-Arroyo, Mikkel Stokholm-Bjerregaard, Aviaja Anna Hansen, Petar Durdevic. (2024)  
**Improved Long Short-Term Memory-based Wastewater Treatment Simulators for Deep Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="Improved Long Short-Term Memory-based Wastewater Treatment Simulators for Deep Reinforcement Learning" index=3>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs-SY, cs.LG, eess-SY  
Keyword Score: 50  
Keywords: Reinforcement Learning, Simulation, Simulator, LSTM, LSTM  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15091v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15091v1.pdf" filename="2403.15091v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Even though Deep <b>Reinforcement</b> <b>Learning</b> (DRL) showed outstanding results in the fields of Robotics and Games, it is still challenging to implement it in the optimization of industrial processes like wastewater treatment. One of the challenges is the lack of a <b>simulation</b> environment that will represent the actual plant as accurately as possible to train DRL policies. Stochasticity and non-linearity of wastewater treatment data lead to unstable and incorrect predictions of models over <b>long</b> <b>time</b> <b>horizons.</b> <b>One</b> possible reason for the models' incorrect <b>simulation</b> behavior can be related to the issue of compounding error, which is the accumulation of errors throughout the <b>simulation.</b> The compounding error occurs because the model utilizes its predictions as inputs at each time step. The error between the actual data and the prediction accumulates as the <b>simulation</b> continues. We implemented two methods to improve the trained models for wastewater treatment data, which resulted in more accurate simulators: 1- Using the model's prediction data as input in the training step as a tool of correction, and 2- Change in the loss function to consider the <b>long-term</b> <b>predicted</b> <b>shape</b> <b>(dynamics).</b> The experimental results showed that implementing these methods can improve the behavior of simulators in terms of Dynamic Time Warping throughout a year up to 98% compared to the base model. These improvements demonstrate significant promise in creating simulators for biological processes that do not need pre-existing knowledge of the process but instead depend exclusively on time series data obtained from the system.

{{</citation>}}


### (4/28 | 4/202) Magic for the Age of Quantized DNNs (Yoshihide Sawada et al., 2024)

{{<citation>}}

Yoshihide Sawada, Ryuji Saiin, Kazuma Suetake. (2024)  
**Magic for the Age of Quantized DNNs**
<br/>
<button class="copy-to-clipboard" title="Magic for the Age of Quantized DNNs" index=4>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CV, cs-LG, cs-NE, cs.LG  
Keyword Score: 50  
Keywords: Model Compression, Quantization, Quantization, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14999v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14999v1.pdf" filename="2403.14999v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recently, the number of parameters in DNNs has explosively increased, as exemplified by <b>LLMs</b> <b>(Large</b> <b>Language</b> <b>Models),</b> <b>making</b> inference on small-scale computers more difficult. <b>Model</b> <b>compression</b> technology is, therefore, essential for integration into products. In this paper, we propose a method of <b>quantization-aware</b> training. We introduce a novel normalization (Layer-Batch Normalization) that is independent of the mini-batch size and does not require any additional computation cost during inference. Then, we <b>quantize</b> the weights by the scaled round-clip function with the weight standardization. We also <b>quantize</b> activation functions using the same function and apply surrogate gradients to train the <b>model</b> <b>with</b> both <b>quantized</b> weights and the <b>quantized</b> activation functions. We call this method Magic for the age of Quantised DNNs (MaQD). Experimental results show that our <b>quantization</b> method can be achieved with minimal accuracy degradation.

{{</citation>}}


### (5/28 | 5/202) Robust optimization for adversarial learning with finite sample complexity guarantees (André Bertolace et al., 2024)

{{<citation>}}

André Bertolace, Konstatinos Gatsis, Kostas Margellos. (2024)  
**Robust optimization for adversarial learning with finite sample complexity guarantees**
<br/>
<button class="copy-to-clipboard" title="Robust optimization for adversarial learning with finite sample complexity guarantees" index=5>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs-SY, cs.LG, eess-SY  
Keyword Score: 43  
Keywords: MNIST, Adversarial Learning, Adversarial Learning, Benchmarking, Adversarial Attack  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15207v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15207v1.pdf" filename="2403.15207v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Decision making and learning in the presence of uncertainty has attracted significant attention in view of the increasing need to achieve robust and reliable operations. In the case where uncertainty stems from the presence of <b>adversarial</b> <b>attacks</b> this need is becoming more prominent. In this paper we focus on linear and nonlinear classification problems and propose a novel <b>adversarial</b> <b>training</b> method for robust classifiers, inspired by Support Vector Machine (SVM) margins. We view robustness under a data driven lens, and derive finite sample complexity bounds for both linear and non-linear classifiers in binary and multi-class scenarios. Notably, our bounds match natural classifiers' complexity. Our algorithm minimizes a worst-case surrogate loss using Linear Programming (LP) and Second Order Cone Programming (SOCP) for linear and non-linear models. Numerical experiments on the <b>benchmark</b> <b>MNIST</b> and CIFAR10 datasets show our approach's comparable performance to state-of-the-art methods, without needing <b>adversarial</b> <b>examples</b> during training. Our work offers a comprehensive framework for enhancing binary linear and non-linear classifier robustness, embedding robustness in learning under the presence of adversaries.

{{</citation>}}


### (6/28 | 6/202) Self-Improvement for Neural Combinatorial Optimization: Sample without Replacement, but Improvement (Jonathan Pirnay et al., 2024)

{{<citation>}}

Jonathan Pirnay, Dominik G. Grimm. (2024)  
**Self-Improvement for Neural Combinatorial Optimization: Sample without Replacement, but Improvement**
<br/>
<button class="copy-to-clipboard" title="Self-Improvement for Neural Combinatorial Optimization: Sample without Replacement, but Improvement" index=6>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 40  
Keywords: Fine-tuning, Reinforcement Learning, Supervised Learning, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15180v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15180v1.pdf" filename="2403.15180v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Current methods for end-to-end constructive neural combinatorial optimization usually train a policy using behavior cloning from expert solutions or policy gradient methods from <b>reinforcement</b> <b>learning.</b> While behavior cloning is straightforward, it requires expensive expert solutions, and policy gradient methods are often computationally demanding and complex to <b>fine-tune.</b> In this work, we bridge the two and simplify the training process by sampling multiple solutions for random instances using the current model in each epoch and then selecting the best solution as an expert trajectory for <b>supervised</b> imitation learning. To achieve progressively improving solutions with minimal sampling, we introduce a method that combines round-wise Stochastic Beam Search with an update strategy derived from a provable policy improvement. This strategy refines the policy between rounds by utilizing the advantage of the sampled sequences with almost no computational overhead. We evaluate our approach on the Traveling Salesman Problem and the Capacitated Vehicle Routing Problem. The models trained with our method achieve comparable performance and generalization to those trained with expert data. Additionally, we apply our method to the Job Shop Scheduling Problem using a <b>transformer-based</b> architecture and outperform existing state-of-the-art methods by a wide margin.

{{</citation>}}


### (7/28 | 7/202) DP-Dueling: Learning from Preference Feedback without Compromising User Privacy (Aadirupa Saha et al., 2024)

{{<citation>}}

Aadirupa Saha, Hilal Asi. (2024)  
**DP-Dueling: Learning from Preference Feedback without Compromising User Privacy**
<br/>
<button class="copy-to-clipboard" title="DP-Dueling: Learning from Preference Feedback without Compromising User Privacy" index=7>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CR, cs-LG, cs.LG  
Keyword Score: 40  
Keywords: Active Learning, Bandit Algorithm, Bandit Algorithm, Differential Privacy  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15045v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15045v1.pdf" filename="2403.15045v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We consider the well-studied dueling <b>bandit</b> <b>problem,</b> where a learner aims to identify near-optimal actions using pairwise comparisons, under the constraint of <b>differential</b> <b>privacy.</b> We consider a general class of utility-based preference matrices for large (potentially unbounded) decision spaces and give the first differentially private dueling <b>bandit</b> <b>algorithm</b> for <b>active</b> <b>learning</b> with user preferences. Our proposed algorithms are computationally efficient with near-optimal performance, both in terms of the private and non-private regret bound. More precisely, we show that when the decision space is of finite size $K$, our proposed algorithm yields order optimal $O\Big(\sum_{i = 2}^K\log\frac{KT}{\Delta_i} + \frac{K}{\epsilon}\Big)$ regret bound for pure $\epsilon$-DP, where $\Delta_i$ denotes the suboptimality gap of the $i$-th arm. We also present a matching lower bound analysis which proves the optimality of our algorithms. Finally, we extend our results to any general decision space in $d$-dimensions with potentially infinite arms and design an $\epsilon$-DP algorithm with regret $\tilde{O} \left( \frac{d^6}{\kappa \epsilon } + \frac{ d\sqrt{T }}{\kappa} \right)$, providing privacy for free when $T \gg d$.

{{</citation>}}


### (8/28 | 8/202) Simple Graph Condensation (Zhenbang Xiao et al., 2024)

{{<citation>}}

Zhenbang Xiao, Yu Wang, Shunyu Liu, Huiqiong Wang, Mingli Song, Tongya Zheng. (2024)  
**Simple Graph Condensation**
<br/>
<button class="copy-to-clipboard" title="Simple Graph Condensation" index=8>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs-SI, cs.LG  
Keyword Score: 36  
Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Convolution  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14951v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14951v1.pdf" filename="2403.14951v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The burdensome training costs on large-scale <b>graphs</b> <b>have</b> <b>aroused</b> significant interest in <b>graph</b> <b>condensation,</b> <b>which</b> involves tuning <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> on a small condensed <b>graph</b> <b>for</b> <b>use</b> on the large-scale original <b>graph.</b> <b>Existing</b> <b>methods</b> primarily focus on aligning key metrics between the condensed and original <b>graphs,</b> <b>such</b> <b>as</b> gradients, distribution and trajectory of <b>GNNs,</b> yielding satisfactory performance on downstream tasks. However, these complex metrics necessitate intricate computations and can potentially disrupt the optimization process of the condensation <b>graph,</b> <b>making</b> <b>the</b> condensation process highly demanding and unstable. Motivated by the recent success of simplified models in various fields, we propose a simplified approach to metric alignment in <b>graph</b> <b>condensation,</b> <b>aiming</b> to reduce unnecessary complexity inherited from <b>GNNs.</b> In our approach, we eliminate external parameters and exclusively retain the target condensed <b>graph</b> <b>during</b> <b>the</b> condensation process. Following the hierarchical aggregation principles of <b>GNNs,</b> we introduce the Simple <b>Graph</b> <b>Condensation</b> <b>(SimGC)</b> framework, which aligns the condensed <b>graph</b> <b>with</b> <b>the</b> original <b>graph</b> <b>from</b> <b>the</b> input layer to the prediction layer, guided by a pre-trained Simple <b>Graph</b> <b>Convolution</b> <b>(SGC)</b> model on the original <b>graph.</b> <b>As</b> <b>a</b> result, both <b>graphs</b> <b>possess</b> <b>the</b> similar capability to train <b>GNNs.</b> This straightforward yet effective strategy achieves a significant speedup of up to 10 times compared to existing <b>graph</b> <b>condensation</b> <b>methods</b> while performing on par with state-of-the-art baselines. Comprehensive experiments conducted on seven <b>benchmark</b> datasets demonstrate the effectiveness of SimGC in prediction accuracy, condensation time, and generalization capability. Our code will be made publicly available.

{{</citation>}}


### (9/28 | 9/202) Deep learning-based method for weather forecasting: A case study in Itoshima (Yuzhong Cheng et al., 2024)

{{<citation>}}

Yuzhong Cheng, Linh Thi Hoai Nguyen, Akinori Ozaki, Ton Viet Ta. (2024)  
**Deep learning-based method for weather forecasting: A case study in Itoshima**
<br/>
<button class="copy-to-clipboard" title="Deep learning-based method for weather forecasting: A case study in Itoshima" index=9>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 33  
Keywords: Benchmarking, LSTM, LSTM, Recurrent Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14918v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14918v1.pdf" filename="2403.14918v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Accurate weather forecasting is of paramount importance for a wide range of practical applications, drawing substantial scientific and societal interest. However, the intricacies of weather systems pose substantial challenges to accurate predictions. This research introduces a multilayer perceptron model tailored for weather forecasting in Itoshima, Kyushu, Japan. Our meticulously designed architecture demonstrates superior performance compared to existing models, surpassing <b>benchmarks</b> such as <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>and</b> <b>Recurrent</b> <b>Neural</b> <b>Networks.</b>

{{</citation>}}


### (10/28 | 10/202) PDE-CNNs: Axiomatic Derivations and Applications (Gijs Bellaard et al., 2024)

{{<citation>}}

Gijs Bellaard, Sei Sakata, Bart M. N. Smets, Remco Duits. (2024)  
**PDE-CNNs: Axiomatic Derivations and Applications**
<br/>
<button class="copy-to-clipboard" title="PDE-CNNs: Axiomatic Derivations and Applications" index=10>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CV, cs-LG, cs.LG  
Keyword Score: 30  
Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15182v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15182v1.pdf" filename="2403.15182v1.pdf">Download PDF</button>

---


**ABSTRACT**  
PDE-based Group <b>Convolutional</b> <b>Neural</b> <b>Networks</b> (PDE-G-CNNs) utilize solvers of geometrically meaningful evolution PDEs as substitutes for the conventional components in G-CNNs. PDE-G-CNNs offer several key benefits all at once: fewer parameters, inherent equivariance, better performance, data efficiency, and geometric interpretability. In this article we focus on Euclidean equivariant PDE-G-CNNs where the feature maps are two dimensional throughout. We call this variant of the framework a PDE-CNN. We list several practically desirable axioms and derive from these which PDEs should be used in a PDE-CNN. Here our approach to geometric learning via PDEs is inspired by the axioms of classical linear and morphological scale-space theory, which we generalize by introducing semifield-valued signals. Furthermore, we experimentally confirm for small networks that PDE-CNNs offer fewer parameters, better performance, and data efficiency in comparison to <b>CNNs.</b> We also investigate what effect the use of different semifields has on the performance of the models.

{{</citation>}}


### (11/28 | 11/202) Addressing Concept Shift in Online Time Series Forecasting: Detect-then-Adapt (YiFan Zhang et al., 2024)

{{<citation>}}

YiFan Zhang, Weiqi Chen, Zhaoyang Zhu, Dalin Qin, Liang Sun, Xue Wang, Qingsong Wen, Zhang Zhang, Liang Wang, Rong Jin. (2024)  
**Addressing Concept Shift in Online Time Series Forecasting: Detect-then-Adapt**
<br/>
<button class="copy-to-clipboard" title="Addressing Concept Shift in Online Time Series Forecasting: Detect-then-Adapt" index=11>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 30  
Keywords: Convolution, Convolutional Neural Network, Data Augmentation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14949v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14949v1.pdf" filename="2403.14949v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Online updating of time series forecasting models aims to tackle the challenge of concept drifting by adjusting forecasting models based on streaming <b>data.</b> <b>While</b> numerous algorithms have been developed, most of them focus on model design and updating. In practice, many of these methods struggle with continuous performance regression in the face of accumulated concept drifts over time. To address this limitation, we present a novel approach, Concept \textbf{D}rift \textbf{D}etection an\textbf{D} \textbf{A}daptation (D3A), that first detects drifting conception and then aggressively adapts the current model to the drifted concepts after the detection for rapid adaption. To best harness the utility of historical <b>data</b> <b>for</b> model adaptation, we propose a <b>data</b> <b>augmentation</b> strategy introducing Gaussian noise into existing training instances. It helps mitigate the <b>data</b> <b>distribution</b> gap, a critical factor contributing to train-test performance inconsistency. The significance of our <b>data</b> <b>augmentation</b> process is verified by our theoretical analysis. Our empirical studies across six datasets demonstrate the effectiveness of D3A in improving model adaptation capability. Notably, compared to a simple Temporal <b>Convolutional</b> <b>Network</b> (TCN) baseline, D3A reduces the average Mean Squared Error (MSE) by $43.9\%$. For the state-of-the-art (SOTA) model, the MSE is reduced by $33.3\%$.

{{</citation>}}


### (12/28 | 12/202) CODA: A COst-efficient Test-time Domain Adaptation Mechanism for HAR (Minghui Qiu et al., 2024)

{{<citation>}}

Minghui Qiu, Yandao Huang, Lin Chen, Lu Wang, Kaishun Wu. (2024)  
**CODA: A COst-efficient Test-time Domain Adaptation Mechanism for HAR**
<br/>
<button class="copy-to-clipboard" title="CODA: A COst-efficient Test-time Domain Adaptation Mechanism for HAR" index=12>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs-NI, cs.LG  
Keyword Score: 23  
Keywords: Active Learning, Clustering, Domain Adaptation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14922v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14922v1.pdf" filename="2403.14922v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In recent years, emerging research on mobile sensing has led to novel scenarios that enhance daily life for humans, but dynamic usage conditions often result in performance degradation when systems are deployed in real-world settings. Existing solutions typically employ one-off adaptation schemes based on neural networks, which struggle to ensure robustness against uncertain drifting conditions in human-centric sensing scenarios. In this paper, we propose CODA, a COst-efficient <b>Domain</b> <b>Adaptation</b> mechanism for mobile sensing that addresses real-time drifts from the data distribution perspective with <b>active</b> <b>learning</b> theory, ensuring cost-efficient adaptation directly on the device. By incorporating a <b>clustering</b> loss and importance-weighted <b>active</b> <b>learning</b> algorithm, CODA retains the relationship between different clusters during cost-effective instance-level updates, preserving meaningful structure within the data distribution. We also showcase its generalization by seamlessly integrating it with Neural Network-based solutions for Human Activity Recognition tasks. Through meticulous evaluations across diverse datasets, including phone-based, watch-based, and integrated sensor-based sensing tasks, we demonstrate the feasibility and potential of online adaptation with CODA. The promising results achieved by CODA, even without learnable parameters, also suggest the possibility of realizing unobtrusive adaptation through specific application designs with sufficient feedback.

{{</citation>}}


### (13/28 | 13/202) Exploring the Task-agnostic Trait of Self-supervised Learning in the Context of Detecting Mental Disorders (Rohan Kumar Gupta et al., 2024)

{{<citation>}}

Rohan Kumar Gupta, Rohit Sinha. (2024)  
**Exploring the Task-agnostic Trait of Self-supervised Learning in the Context of Detecting Mental Disorders**
<br/>
<button class="copy-to-clipboard" title="Exploring the Task-agnostic Trait of Self-supervised Learning in the Context of Detecting Mental Disorders" index=13>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG, eess-SP  
Keyword Score: 20  
Keywords: Self-supervised Learning, Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15170v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15170v1.pdf" filename="2403.15170v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Self-supervised</b> <b>learning</b> (SSL) has been investigated to generate task-agnostic representations across various domains. However, such investigation has not been conducted for detecting multiple mental disorders. The rationale behind the existence of a task-agnostic representation lies in the overlapping symptoms among multiple mental disorders. Consequently, the behavioural data collected for mental health assessment may carry a mixed bag of attributes related to multiple disorders. Motivated by that, in this study, we explore a task-agnostic representation derived through SSL in the context of detecting major depressive disorder (MDD) and post-traumatic stress disorder (PTSD) using audio and video data collected during interactive sessions. This study employs SSL models trained by predicting multiple fixed targets or masked frames. We propose a list of fixed targets to make the generated representation more efficient for detecting MDD and PTSD. Furthermore, we modify the hyper-parameters of the SSL encoder predicting fixed targets to generate global representations that capture varying temporal contexts. Both these innovations are noted to yield improved detection performances for considered mental disorders and exhibit task-agnostic traits. In the context of the SSL model predicting masked frames, the generated global representations are also noted to exhibit task-agnostic traits.

{{</citation>}}


### (14/28 | 14/202) Quantification using Permutation-Invariant Networks based on Histograms (Olaya Pérez-Mon et al., 2024)

{{<citation>}}

Olaya Pérez-Mon, Alejandro Moreo, Juan José del Coz, Pablo González. (2024)  
**Quantification using Permutation-Invariant Networks based on Histograms**
<br/>
<button class="copy-to-clipboard" title="Quantification using Permutation-Invariant Networks based on Histograms" index=14>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, stat-ML  
Keyword Score: 20  
Keywords: Supervised Learning, Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15123v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15123v1.pdf" filename="2403.15123v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Quantification, also known as class prevalence estimation, is the <b>supervised</b> <b>learning</b> task in which a model is trained to predict the prevalence of each class in a given bag of examples. This paper investigates the application of deep neural networks to tasks of quantification in scenarios where it is possible to apply a symmetric <b>supervised</b> <b>approach</b> that eliminates the need for classification as an intermediary step, directly addressing the quantification problem. Additionally, it discusses existing permutation-invariant layers designed for set processing and assesses their suitability for quantification. In light of our analysis, we propose HistNetQ, a novel neural architecture that relies on a permutation-invariant representation based on histograms that is specially suited for quantification problems. Our experiments carried out in the only quantification competition held to date, show that HistNetQ outperforms other deep neural architectures devised for set processing, as well as the state-of-the-art quantification methods. Furthermore, HistNetQ offers two significant advantages over traditional quantification methods: i) it does not require the labels of the training examples but only the prevalence values of a collection of training bags, making it applicable to new scenarios; and ii) it is able to optimize any custom quantification-oriented loss function.

{{</citation>}}


### (15/28 | 15/202) Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices (Pengxiang Zhao et al., 2024)

{{<citation>}}

Pengxiang Zhao, Ping Li, Yingjie Gu, Yi Zheng, Stephan Ludger Kölker, Zhefeng Wang, Xiaoming Yuan. (2024)  
**Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices**
<br/>
<button class="copy-to-clipboard" title="Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices" index=15>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CL, cs-LG, cs.LG, math-OC  
Keyword Score: 20  
Keywords: GPT, GPT-2  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14958v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14958v1.pdf" filename="2403.14958v1.pdf">Download PDF</button>

---


**ABSTRACT**  
As deep learning models exponentially increase in size, optimizers such as Adam encounter significant memory consumption challenges due to the storage of first and second moment data. Current memory-efficient methods like Adafactor and CAME often compromise accuracy with their matrix factorization techniques. Addressing this, we introduce Adapprox, a novel approach that employs randomized low-rank matrix approximation for a more effective and accurate approximation of Adam's second moment. Adapprox features an adaptive rank selection mechanism, finely balancing accuracy and memory efficiency, and includes an optional cosine similarity guidance strategy to enhance stability and expedite convergence. In <b>GPT-2</b> training and downstream tasks, Adapprox surpasses AdamW by achieving 34.5% to 49.9% and 33.8% to 49.9% memory savings for the 117M and 345M models, respectively, with the first moment enabled, and further increases these savings without the first moment. Besides, it enhances convergence speed and improves downstream task performance relative to its counterparts.

{{</citation>}}


### (16/28 | 16/202) Planning with a Learned Policy Basis to Optimally Solve Complex Tasks (Guillermo Infante et al., 2024)

{{<citation>}}

Guillermo Infante, David Kuric, Anders Jonsson, Vicenç Gómez, Herke van Hoof. (2024)  
**Planning with a Learned Policy Basis to Optimally Solve Complex Tasks**
<br/>
<button class="copy-to-clipboard" title="Planning with a Learned Policy Basis to Optimally Solve Complex Tasks" index=16>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15301v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15301v1.pdf" filename="2403.15301v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Conventional <b>reinforcement</b> <b>learning</b> (RL) methods can successfully solve a wide range of sequential decision problems. However, learning policies that can generalize predictably across multiple tasks in a setting with non-Markovian reward specifications is a challenging problem. We propose to use successor features to learn a policy basis so that each (sub)policy in it solves a well-defined subproblem. In a task described by a finite state automaton (FSA) that involves the same set of subproblems, the combination of these (sub)policies can then be used to generate an optimal solution without additional learning. In contrast to other methods that combine (sub)policies via planning, our method asymptotically attains global optimality, even in stochastic environments.

{{</citation>}}


### (17/28 | 17/202) Parametric PDE Control with Deep Reinforcement Learning and Differentiable L0-Sparse Polynomial Policies (Nicolò Botteghi et al., 2024)

{{<citation>}}

Nicolò Botteghi, Urban Fasel. (2024)  
**Parametric PDE Control with Deep Reinforcement Learning and Differentiable L0-Sparse Polynomial Policies**
<br/>
<button class="copy-to-clipboard" title="Parametric PDE Control with Deep Reinforcement Learning and Differentiable L0-Sparse Polynomial Policies" index=17>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15267v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15267v1.pdf" filename="2403.15267v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Optimal control of parametric partial differential equations (PDEs) is crucial in many applications in engineering and science. In recent years, the progress in scientific machine learning has opened up new frontiers for the control of parametric PDEs. In particular, deep <b>reinforcement</b> <b>learning</b> (DRL) has the potential to solve high-dimensional and complex control problems in a large variety of applications. Most DRL methods rely on deep neural network (DNN) control policies. However, for many dynamical systems, DNN-based control policies tend to be over-parametrized, which means they need large amounts of training data, show limited robustness, and lack interpretability. In this work, we leverage dictionary learning and differentiable L$_0$ regularization to learn sparse, robust, and interpretable control policies for parametric PDEs. Our sparse policy architecture is agnostic to the DRL method and can be used in different policy-gradient and actor-critic DRL algorithms without changing their policy-optimization procedure. We test our approach on the challenging tasks of controlling parametric Kuramoto-Sivashinsky and convection-diffusion-reaction PDEs. We show that our method (1) outperforms baseline DNN-based DRL policies, (2) allows for the derivation of interpretable equations of the learned optimal control laws, and (3) generalizes to unseen parameters of the PDE without retraining the policies.

{{</citation>}}


### (18/28 | 18/202) Federated Bayesian Deep Learning: The Application of Statistical Aggregation Methods to Bayesian Models (John Fischer et al., 2024)

{{<citation>}}

John Fischer, Marko Orescanin, Justin Loomis, Patrick McClure. (2024)  
**Federated Bayesian Deep Learning: The Application of Statistical Aggregation Methods to Bayesian Models**
<br/>
<button class="copy-to-clipboard" title="Federated Bayesian Deep Learning: The Application of Statistical Aggregation Methods to Bayesian Models" index=18>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, stat-ML  
Keyword Score: 10  
Keywords: Federated Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15263v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15263v1.pdf" filename="2403.15263v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Federated</b> <b>learning</b> (FL) is an approach to training machine learning models that takes advantage of multiple distributed datasets while maintaining data privacy and reducing communication costs associated with sharing local datasets. Aggregation strategies have been developed to pool or fuse the weights and biases of distributed deterministic models; however, modern deterministic deep learning (DL) models are often poorly calibrated and lack the ability to communicate a measure of epistemic uncertainty in prediction, which is desirable for remote sensing platforms and safety-critical applications. Conversely, Bayesian DL models are often well calibrated and capable of quantifying and communicating a measure of epistemic uncertainty along with a competitive prediction accuracy. Unfortunately, because the weights and biases in Bayesian DL models are defined by a probability distribution, simple application of the aggregation methods associated with FL schemes for deterministic models is either impossible or results in sub-optimal performance. In this work, we use independent and identically distributed (IID) and non-IID partitions of the CIFAR-10 dataset and a fully variational ResNet-20 architecture to analyze six different aggregation strategies for Bayesian DL models. Additionally, we analyze the traditional <b>federated</b> <b>averaging</b> approach applied to an approximate Bayesian Monte Carlo dropout model as a lightweight alternative to more complex variational inference methods in FL. We show that aggregation strategy is a key hyperparameter in the design of a Bayesian FL system with downstream effects on accuracy, calibration, uncertainty quantification, training stability, and client compute requirements.

{{</citation>}}


### (19/28 | 19/202) Early Period of Training Impacts Out-of-Distribution Generalization (Chen Cecilia Liu et al., 2024)

{{<citation>}}

Chen Cecilia Liu, Iryna Gurevych. (2024)  
**Early Period of Training Impacts Out-of-Distribution Generalization**
<br/>
<button class="copy-to-clipboard" title="Early Period of Training Impacts Out-of-Distribution Generalization" index=19>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Out-of-distribution  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15210v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15210v1.pdf" filename="2403.15210v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Prior research has found that differences in the early period of neural network training significantly impact the performance of in-distribution (ID) tasks. However, neural networks are often sensitive to <b>out-of-distribution</b> (OOD) data, making them less reliable in downstream applications. Yet, the impact of the early training period on OOD generalization remains understudied due to its complexity and lack of effective analytical methodologies. In this work, we investigate the relationship between learning dynamics and OOD generalization during the early period of neural network training. We utilize the trace of Fisher Information and sharpness, with a focus on gradual unfreezing (i.e. progressively unfreezing parameters during training) as the methodology for investigation. Through a series of empirical experiments, we show that 1) selecting the number of trainable parameters at different times during training, i.e. realized by gradual unfreezing -- has a minuscule impact on ID results, but greatly affects the generalization to OOD data; 2) the absolute values of sharpness and trace of Fisher Information at the initial period of training are not indicative for OOD generalization, but the relative values could be; 3) the trace of Fisher Information and sharpness may be used as indicators for the removal of interventions during early period of training for better OOD generalization.

{{</citation>}}


### (20/28 | 20/202) An In-Depth Analysis of Data Reduction Methods for Sustainable Deep Learning (Víctor Toscano-Durán et al., 2024)

{{<citation>}}

Víctor Toscano-Durán, Javier Perera-Lago, Eduardo Paluzo-Hidalgo, Rocío Gonzalez-Diaz, Miguel Ángel Gutierrez-Naranjo, Matteo Rucco. (2024)  
**An In-Depth Analysis of Data Reduction Methods for Sustainable Deep Learning**
<br/>
<button class="copy-to-clipboard" title="An In-Depth Analysis of Data Reduction Methods for Sustainable Deep Learning" index=20>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CV, cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Object Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15150v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15150v1.pdf" filename="2403.15150v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In recent years, Deep Learning has gained popularity for its ability to solve complex classification tasks, increasingly delivering better results thanks to the development of more accurate models, the availability of huge volumes of data and the improved computational capabilities of modern computers. However, these improvements in performance also bring efficiency problems, related to the storage of datasets and models, and to the waste of energy and time involved in both the training and inference processes. In this context, data reduction can help reduce energy consumption when training a deep learning model. In this paper, we present up to eight different methods to reduce the size of a tabular training dataset, and we develop a Python package to apply them. We also introduce a representativeness metric based on topology to measure how similar are the reduced datasets and the full training dataset. Additionally, we develop a methodology to apply these data reduction methods to image datasets for <b>object</b> <b>detection</b> tasks. Finally, we experimentally compare how these data reduction methods affect the representativeness of the reduced dataset, the energy consumption and the predictive performance of the model.

{{</citation>}}


### (21/28 | 21/202) On the Convergence of Adam under Non-uniform Smoothness: Separability from SGDM and Beyond (Bohan Wang et al., 2024)

{{<citation>}}

Bohan Wang, Huishuai Zhang, Qi Meng, Ruoyu Sun, Zhi-Ming Ma, Wei Chen. (2024)  
**On the Convergence of Adam under Non-uniform Smoothness: Separability from SGDM and Beyond**
<br/>
<button class="copy-to-clipboard" title="On the Convergence of Adam under Non-uniform Smoothness: Separability from SGDM and Beyond" index=21>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, math-OC  
Keyword Score: 10  
Keywords: Stochastic Gradient Descent  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15146v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15146v1.pdf" filename="2403.15146v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper aims to clearly distinguish between <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> with Momentum (SGDM) and Adam in terms of their convergence rates. We demonstrate that Adam achieves a faster convergence compared to SGDM under the condition of non-uniformly bounded smoothness. Our findings reveal that: (1) in deterministic environments, Adam can attain the known lower bound for the convergence rate of deterministic first-order optimizers, whereas the convergence rate of Gradient Descent with Momentum (GDM) has higher order dependence on the initial function value; (2) in <b>stochastic</b> <b>setting,</b> <b>Adam's</b> convergence rate upper bound matches the lower bounds of <b>stochastic</b> <b>first-order</b> <b>optimizers,</b> considering both the initial function value and the final error, whereas there are instances where SGDM fails to converge with any learning rate. These insights distinctly differentiate Adam and SGDM regarding their convergence rates. Additionally, by introducing a novel stopping-time based technique, we further prove that if we consider the minimum gradient norm during iterations, the corresponding convergence rate can match the lower bounds across all problem hyperparameters. The technique can also help proving that Adam with a specific hyperparameter scheduler is parameter-agnostic, which hence can be of independent interest.

{{</citation>}}


### (22/28 | 22/202) Active Learning for Regression based on Wasserstein distance and GroupSort Neural Networks (Benjamin Bobbia et al., 2024)

{{<citation>}}

Benjamin Bobbia, Matthias Picard. (2024)  
**Active Learning for Regression based on Wasserstein distance and GroupSort Neural Networks**
<br/>
<button class="copy-to-clipboard" title="Active Learning for Regression based on Wasserstein distance and GroupSort Neural Networks" index=22>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, math-ST, stat-ML, stat-TH  
Keyword Score: 10  
Keywords: Active Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15108v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15108v1.pdf" filename="2403.15108v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper addresses a new <b>active</b> <b>learning</b> strategy for regression problems. The presented Wasserstein <b>active</b> <b>regression</b> model is based on the principles of distribution-matching to measure the representativeness of the labeled dataset. The Wasserstein distance is computed using GroupSort Neural Networks. The use of such networks provides theoretical foundations giving a way to quantify errors with explicit bounds for their size and depth. This solution is combined with another uncertainty-based approach that is more outlier-tolerant to complete the query strategy. Finally, this method is compared with other classical and recent solutions. The study empirically shows the pertinence of such a representativity-uncertainty approach, which provides good estimation all along the query procedure. Moreover, the Wasserstein <b>active</b> <b>regression</b> often achieves more precise estimations and tends to improve accuracy faster than other models.

{{</citation>}}


### (23/28 | 23/202) Automated Feature Selection for Inverse Reinforcement Learning (Daulet Baimukashev et al., 2024)

{{<citation>}}

Daulet Baimukashev, Gokhan Alcan, Ville Kyrki. (2024)  
**Automated Feature Selection for Inverse Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="Automated Feature Selection for Inverse Reinforcement Learning" index=23>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: 68T40, 68T05, cs-LG, cs-RO, cs.LG  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15079v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15079v1.pdf" filename="2403.15079v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Inverse <b>reinforcement</b> <b>learning</b> (IRL) is an imitation learning approach to learning reward functions from expert demonstrations. Its use avoids the difficult and tedious procedure of manual reward specification while retaining the generalization power of <b>reinforcement</b> <b>learning.</b> In IRL, the reward is usually represented as a linear combination of features. In continuous state spaces, the state variables alone are not sufficiently rich to be used as features, but which features are good is not known in general. To address this issue, we propose a method that employs polynomial basis functions to form a candidate set of features, which are shown to allow the matching of statistical moments of state distributions. Feature selection is then performed for the candidates by leveraging the correlation between trajectory probabilities and feature expectations. We demonstrate the approach's effectiveness by recovering reward functions that capture expert policies across non-linear control tasks of increasing complexity. Code, data, and videos are available at https://sites.google.com/view/feature4irl.

{{</citation>}}


### (24/28 | 24/202) Robust Conformal Prediction under Distribution Shift via Physics-Informed Structural Causal Model (Rui Xu et al., 2024)

{{<citation>}}

Rui Xu, Yue Sun, Chao Chen, Parv Venkitasubramaniam, Sihong Xie. (2024)  
**Robust Conformal Prediction under Distribution Shift via Physics-Informed Structural Causal Model**
<br/>
<button class="copy-to-clipboard" title="Robust Conformal Prediction under Distribution Shift via Physics-Informed Structural Causal Model" index=24>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, stat-ML  
Keyword Score: 10  
Keywords: Distribution Shift, Distribution Shift  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15025v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15025v1.pdf" filename="2403.15025v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Uncertainty is critical to reliable decision-making with machine learning. Conformal prediction (CP) handles uncertainty by predicting a set on a test input, hoping the set to cover the true label with at least $(1-\alpha)$ confidence. This coverage can be guaranteed on test data even if the marginal <b>distributions</b> <b>$P_X$</b> differ between calibration and test datasets. However, as it is common in practice, when the conditional <b>distribution</b> <b>$P_{Y|X}$</b> is different on calibration and test data, the coverage is not guaranteed and it is essential to measure and minimize the coverage loss under <b>distributional</b> <b>shift</b> at \textit{all} possible confidence levels. To address these issues, we upper bound the coverage difference at all levels using the cumulative density functions of calibration and test conformal scores and Wasserstein distance. Inspired by the invariance of physics across data <b>distributions,</b> <b>we</b> propose a physics-informed structural causal model (PI-SCM) to reduce the upper bound. We validated that PI-SCM can improve coverage robustness along confidence level and test domain on a traffic speed prediction task and an epidemic spread task with multiple real-world datasets.

{{</citation>}}


### (25/28 | 25/202) Insights into the Lottery Ticket Hypothesis and the Iterative Magnitude Pruning (Tausifa Jan Saleem et al., 2024)

{{<citation>}}

Tausifa Jan Saleem, Ramanjit Ahuja, Surendra Prasad, Brejesh Lall. (2024)  
**Insights into the Lottery Ticket Hypothesis and the Iterative Magnitude Pruning**
<br/>
<button class="copy-to-clipboard" title="Insights into the Lottery Ticket Hypothesis and the Iterative Magnitude Pruning" index=25>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Pruning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15022v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15022v1.pdf" filename="2403.15022v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Lottery ticket hypothesis for deep neural networks emphasizes the importance of initialization used to re-train the sparser networks obtained using the iterative magnitude <b>pruning</b> process. An explanation for why the specific initialization proposed by the lottery ticket hypothesis tends to work better in terms of generalization (and training) performance has been lacking. Moreover, the underlying principles in iterative magnitude <b>pruning,</b> like the <b>pruning</b> of smaller magnitude weights and the role of the iterative process, lack full understanding and explanation. In this work, we attempt to provide insights into these phenomena by empirically studying the volume/geometry and loss landscape characteristics of the solutions obtained at various stages of the iterative magnitude <b>pruning</b> process.

{{</citation>}}


### (26/28 | 26/202) Unifying Lane-Level Traffic Prediction from a Graph Structural Perspective: Benchmark and Baseline (Shuhao Li et al., 2024)

{{<citation>}}

Shuhao Li, Yue Cui, Jingyi Xu, Libin Li, Lingkai Meng, Weidong Yang, Fan Zhang, Xiaofang Zhou. (2024)  
**Unifying Lane-Level Traffic Prediction from a Graph Structural Perspective: Benchmark and Baseline**
<br/>
<button class="copy-to-clipboard" title="Unifying Lane-Level Traffic Prediction from a Graph Structural Perspective: Benchmark and Baseline" index=26>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 6  
Keywords: Graph, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14941v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14941v1.pdf" filename="2403.14941v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Traffic prediction has long been a focal and pivotal area in research, witnessing both significant strides from city-level to road-level predictions in recent years. With the advancement of Vehicle-to-Everything (V2X) technologies, autonomous driving, and large-scale models in the traffic domain, lane-level traffic prediction has emerged as an indispensable direction. However, further progress in this field is hindered by the absence of comprehensive and unified evaluation standards, coupled with limited public availability of data and code. This paper extensively analyzes and categorizes existing research in lane-level traffic prediction, establishes a unified spatial topology structure and prediction tasks, and introduces a simple baseline model, GraphMLP, based on <b>graph</b> structure and MLP networks. We have replicated codes not publicly available in existing studies and, based on this, thoroughly and fairly assessed various models in terms of effectiveness, efficiency, and applicability, providing insights for practical applications. Additionally, we have released three new datasets and corresponding codes to accelerate progress in this field, all of which can be found on https://github.com/ShuhaoLii/TITS24LaneLevel-Traffic-Benchmark.

{{</citation>}}


### (27/28 | 27/202) Grey-informed neural network for time-series forecasting (Wanli Xie et al., 2024)

{{<citation>}}

Wanli Xie, Ruibin Zhao, Zhenguo Xu, Tingting Liang. (2024)  
**Grey-informed neural network for time-series forecasting**
<br/>
<button class="copy-to-clipboard" title="Grey-informed neural network for time-series forecasting" index=27>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 5  
Keywords: Black Box  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15027v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15027v1.pdf" filename="2403.15027v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Neural network models have shown outstanding performance and successful resolutions to complex problems in various fields. However, the majority of these models are viewed as <b>black-box,</b> <b>requiring</b> a significant amount of data for development. Consequently, in situations with limited data, constructing appropriate models becomes challenging due to the lack of transparency and scarcity of data. To tackle these challenges, this study suggests the implementation of a grey-informed neural network (GINN). The GINN ensures that the output of the neural network follows the differential equation model of the grey system, improving interpretability. Moreover, incorporating prior knowledge from grey system theory enables traditional neural networks to effectively handle small data samples. Our proposed model has been observed to uncover underlying patterns in the real world and produce reliable forecasts based on empirical data.

{{</citation>}}


### (28/28 | 28/202) Transition Graph Properties of Target Class Classification (Levon Aslanyan et al., 2024)

{{<citation>}}

Levon Aslanyan, Hasmik Sahakyan. (2024)  
**Transition Graph Properties of Target Class Classification**
<br/>
<button class="copy-to-clipboard" title="Transition Graph Properties of Target Class Classification" index=28>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-DM, cs-LG, cs.LG  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15167v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15167v1.pdf" filename="2403.15167v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Target class classification is a mixed classification and transition model whose integrated goal is to assign objects to a certain, so called target or normal class. The classification process is iterative, and in each step an object in a certain class undergoes an action attached to that class, initiating the transition of the object to one of the classes. The sequence of transitions, which we call class transitions, must be designed to provide the final assignment of objects to the target class. The transition process can be described in the form of a directed <b>graph,</b> and the success of the final classification is mainly due to the properties of this <b>graph.</b> In our previous research we showed that the desirable structure of the transition <b>graph</b> is an oriented rooted tree with orientation towards the root vertex, which corresponds to the normal class. It is clear that the transition <b>graph</b> of an arbitrary algorithm (policy) may not have this property. In this paper we study the structure of realistic transition <b>graphs,</b> which makes it possible to find classification inconsistencies, helping to transfer it into the desired form. The medical interpretation of dynamic treatment regime considered in the article further clarifies the investigated framework.

{{</citation>}}


## cs.SE (6)



### (1/6 | 29/202) Comprehensive Evaluation and Insights into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation (Shanthi Karpurapu et al., 2024)

{{<citation>}}

Shanthi Karpurapu, Sravanthy Myneni, Unnati Nettur, Likhit Sagar Gajja, Dave Burke, Tom Stiehm, Jeffery Payne. (2024)  
**Comprehensive Evaluation and Insights into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation**
<br/>
<button class="copy-to-clipboard" title="Comprehensive Evaluation and Insights into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation" index=29>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: I-2-7; I-2-1, cs-AI, cs-SE, cs.SE  
Keyword Score: 130  
Keywords: Few-shot, GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, PaLM, Question Answering, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14965v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14965v1.pdf" filename="2403.14965v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Behavior-driven development (BDD) is an Agile testing methodology fostering collaboration among developers, <b>QA</b> analysts, and stakeholders. In this manuscript, we propose a novel approach to enhance BDD practices using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to automate acceptance test generation. Our study uses zero and <b>few-shot</b> <b>prompts</b> to evaluate <b>LLMs</b> such as <b>GPT-3.5,</b> <b>GPT-4,</b> <b>Llama-2-13B,</b> and <b>PaLM-2.</b> The paper presents a detailed methodology that includes the dataset, <b>prompt</b> techniques, <b>LLMs,</b> and the evaluation process. The results demonstrate that <b>GPT-3.5</b> and <b>GPT-4</b> generate error-free BDD acceptance tests with better performance. The <b>few-shot</b> <b>prompt</b> technique highlights its ability to provide higher accuracy by incorporating examples for <b>in-context</b> <b>learning.</b> Furthermore, the study examines syntax errors, validation accuracy, and comparative analysis of <b>LLMs,</b> revealing their effectiveness in enhancing BDD practices. However, our study acknowledges that there are limitations to the proposed approach. We emphasize that this approach can support collaborative BDD processes and create opportunities for future research into automated BDD acceptance test generation using <b>LLMs.</b>

{{</citation>}}


### (2/6 | 30/202) AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large Language Models (Chaoyun Zhang et al., 2024)

{{<citation>}}

Chaoyun Zhang, Zicheng Ma, Yuhao Wu, Shilin He, Si Qin, Minghua Ma, Xiaoting Qin, Yu Kang, Yuyi Liang, Xiaoyu Gou, Yajie Xue, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang. (2024)  
**AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large Language Models**
<br/>
<button class="copy-to-clipboard" title="AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large Language Models" index=30>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-SE, cs.SE  
Keyword Score: 43  
Keywords: Multi-modal, Topic Model, Large Language Model, Large Language Model, Topic Modeling  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15157v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15157v1.pdf" filename="2403.15157v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Verbatim feedback constitutes a valuable repository of user experiences, opinions, and requirements essential for software development. Effectively and efficiently extracting valuable insights from such data poses a challenging task. This paper introduces Allhands , an innovative analytic framework designed for <b>large-scale</b> <b>feedback</b> <b>analysis</b> through a natural language interface, leveraging <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Allhands adheres to a conventional feedback analytic workflow, initially conducting classification and <b>topic</b> <b>modeling</b> on the feedback to convert them into a structurally augmented format, incorporating <b>LLMs</b> to enhance accuracy, robustness, generalization, and user-friendliness. Subsequently, an <b>LLM</b> agent is employed to interpret users' diverse questions in natural language on feedback, translating them into Python code for execution, and delivering comprehensive <b>multi-modal</b> responses, including text, code, tables, and images. We evaluate Allhands across three diverse feedback datasets. The experiments demonstrate that Allhands achieves superior efficacy at all stages of analysis, including classification and <b>topic</b> <b>modeling,</b> eventually providing users with an ``ask me anything'' experience with comprehensive, correct and human-readable response. To the best of our knowledge, Allhands stands as the first comprehensive feedback analysis framework that supports diverse and customized requirements for insight extraction through a natural language interface.

{{</citation>}}


### (3/6 | 31/202) Enhancing Testing at Meta with Rich-State Simulated Populations (Nadia Alshahwan et al., 2024)

{{<citation>}}

Nadia Alshahwan, Arianna Blasi, Kinga Bojarczuk, Andrea Ciancone, Natalija Gucevska, Mark Harman, Simon Schellaert, Inna Harper, Yue Jia, Michał Królikowski, Will Lewis, Dragos Martac, Rubmary Rojas, Kate Ustiuzhanina. (2024)  
**Enhancing Testing at Meta with Rich-State Simulated Populations**
<br/>
<button class="copy-to-clipboard" title="Enhancing Testing at Meta with Rich-State Simulated Populations" index=31>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-SE, cs.SE  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15374v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15374v1.pdf" filename="2403.15374v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper reports the results of the deployment of Rich-State Simulated Populations at Meta for both automated and manual testing. We use simulated users (aka test users) to mimic user interactions and acquire state in much the same way that real user accounts acquire state. For automated testing, we present empirical results from deployment on the Facebook, Messenger, and Instagram apps for iOS and Android Platforms. These apps consist of tens of millions of lines of code, communicating with hundreds of millions of lines of backend code, and are used by over 2 billion people every day. Our results reveal that rich state increases average code coverage by 38\%, and endpoint coverage by 61\%. More importantly, it also yields an average increase of 115\% in the faults found by automated testing. The rich-state test user populations are also deployed in a (continually evolving) Test Universe; a web-enabled <b>simulation</b> platform for privacy-safe manual testing, which has been used by over 21,000 Meta engineers since its deployment in November 2022.

{{</citation>}}


### (4/6 | 32/202) An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets (Jonathan Katzy et al., 2024)

{{<citation>}}

Jonathan Katzy, Răzvan-Mihai Popescu, Arie van Deursen, Maliheh Izadi. (2024)  
**An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets**
<br/>
<button class="copy-to-clipboard" title="An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets" index=32>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-LG, cs-SE, cs.SE  
Keyword Score: 20  
Keywords: Recommendation, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15230v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15230v1.pdf" filename="2403.15230v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Does the training of <b>large</b> <b>language</b> <b>models</b> potentially infringe upon code licenses? Furthermore, are there any datasets available that can be safely used for training these models without violating such licenses? In our study, we assess the current trends in the field and the importance of incorporating code into the training of <b>large</b> <b>language</b> <b>models.</b> Additionally, we examine publicly available datasets to see whether these models can be trained on them without the risk of legal issues in the future. To accomplish this, we compiled a list of 53 <b>large</b> <b>language</b> <b>models</b> trained on file-level code. We then extracted their datasets and analyzed how much they overlap with a dataset we created, consisting exclusively of strong copyleft code. Our analysis revealed that every dataset we examined contained license inconsistencies, despite being selected based on their associated repository licenses. We analyzed a total of 514 million code files, discovering 38 million exact duplicates present in our strong copyleft dataset. Additionally, we examined 171 million file-leading comments, identifying 16 million with strong copyleft licenses and another 11 million comments that discouraged copying without explicitly mentioning a license. Based on the findings of our study, which highlights the pervasive issue of license inconsistencies in <b>large</b> <b>language</b> <b>models</b> trained on code, our <b>recommendation</b> for both researchers and the community is to prioritize the development and adoption of best practices for dataset creation and management.

{{</citation>}}


### (5/6 | 33/202) On the Generalizability of Deep Learning-based Code Completion Across Programming Language Versions (Matteo Ciniselli et al., 2024)

{{<citation>}}

Matteo Ciniselli, Alberto Martin-Lopez, Gabriele Bavota. (2024)  
**On the Generalizability of Deep Learning-based Code Completion Across Programming Language Versions**
<br/>
<button class="copy-to-clipboard" title="On the Generalizability of Deep Learning-based Code Completion Across Programming Language Versions" index=33>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-SE, cs.SE  
Keyword Score: 10  
Keywords: Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15149v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15149v1.pdf" filename="2403.15149v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Code completion is a key feature of Integrated Development Environments (IDEs), aimed at predicting the next tokens a developer is likely to write, helping them write code faster and with less effort. Modern code completion approaches are often powered by deep learning (DL) models. However, the swift evolution of programming languages poses a critical challenge to the performance of DL-based code completion models: Can these models generalize across different language versions? This paper delves into such a question. In particular, we assess the capabilities of a state-of-the-art model, CodeT5, to generalize across nine different Java versions, ranging from Java 2 to Java 17, while being exclusively trained on Java 8 code. Our evaluation spans three completion scenarios, namely, predicting tokens, constructs (e.g., the condition of an if statement) and entire code blocks. The results of our study reveal a noticeable disparity among language versions, with the worst performance being obtained in Java 2 and 17 - the most far apart versions compared to Java 8. We investigate possible causes for the performance degradation and show that the adoption of a limited version-specific <b>fine-tuning</b> can partially alleviate the problem. Our work raises awareness on the importance of continuous model refinement, and it can inform the design of alternatives to make code completion models more robust to language evolution.

{{</citation>}}


### (6/6 | 34/202) Testing for Fault Diversity in Reinforcement Learning (Quentin Mazouni et al., 2024)

{{<citation>}}

Quentin Mazouni, Helge Spieker, Arnaud Gotlieb, Mathieu Acher. (2024)  
**Testing for Fault Diversity in Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="Testing for Fault Diversity in Reinforcement Learning" index=34>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-SE, cs.SE  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15065v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15065v1.pdf" filename="2403.15065v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Reinforcement</b> <b>Learning</b> is the premier technique to approach sequential decision problems, including complex tasks such as driving cars and landing spacecraft. Among the software validation and verification practices, testing for functional fault detection is a convenient way to build trustworthiness in the learned decision model. While recent works seek to maximise the number of detected faults, none consider fault characterisation during the search for more diversity. We argue that policy testing should not find as many failures as possible (e.g., inputs that trigger similar car crashes) but rather aim at revealing as informative and diverse faults as possible in the model. In this paper, we explore the use of quality diversity optimisation to solve the problem of fault diversity in policy testing. Quality diversity (QD) optimisation is a type of evolutionary algorithm to solve hard combinatorial optimisation problems where high-quality diverse solutions are sought. We define and address the underlying challenges of adapting QD optimisation to the test of action policies. Furthermore, we compare classical QD optimisers to state-of-the-art frameworks dedicated to policy testing, both in terms of search efficiency and fault diversity. We show that QD optimisation, while being conceptually simple and generally applicable, finds effectively more diverse faults in the decision model, and conclude that QD-based policy testing is a promising approach.

{{</citation>}}


## cs.IR (2)



### (1/2 | 35/202) Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation (Jiaheng Yu et al., 2024)

{{<citation>}}

Jiaheng Yu, Jing Li, Yue He, Kai Zhu, Shuyi Zhang, Wen Hu. (2024)  
**Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation**
<br/>
<button class="copy-to-clipboard" title="Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation" index=35>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-AI, cs-IR, cs.IR  
Keyword Score: 83  
Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Graph Contrastive Learning, Contrastive Learning, Convolution, Convolutional Neural Network, Recommendation, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15075v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15075v1.pdf" filename="2403.15075v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent methods utilize <b>graph</b> <b>contrastive</b> <b>Learning</b> within <b>graph-structured</b> <b>user-item</b> <b>interaction</b> data for collaborative filtering and have demonstrated their efficacy in <b>recommendation</b> tasks. However, they ignore that the difference relation density of nodes between the user- and item-side causes the adaptability of <b>graphs</b> <b>on</b> <b>bilateral</b> nodes to be different after multi-hop <b>graph</b> <b>interaction</b> <b>calculation,</b> which limits existing models to achieve ideal results. To solve this issue, we propose a novel framework for <b>recommendation</b> tasks called Bilateral Unsymmetrical <b>Graph</b> <b>Contrastive</b> <b>Learning</b> (BusGCL) that consider the bilateral unsymmetry on user-item node relation density for sliced user and item <b>graph</b> <b>reasoning</b> <b>better</b> with bilateral slicing <b>contrastive</b> <b>training.</b> Especially, taking into account the aggregation ability of hypergraph-based <b>graph</b> <b>convolutional</b> <b>network</b> <b>(GCN)</b> in digging implicit similarities is more suitable for user nodes, embeddings generated from three different modules: hypergraph-based <b>GCN,</b> <b>GCN</b> and perturbed <b>GCN,</b> are sliced into two subviews by the user- and item-side respectively, and selectively combined into subview pairs bilaterally based on the characteristics of inter-node relation structure. Furthermore, to align the distribution of user and item embeddings after aggregation, a dispersing loss is leveraged to adjust the mutual distance between all embeddings for maintaining learning ability. Comprehensive experiments on two public datasets have proved the superiority of BusGCL in comparison to various <b>recommendation</b> methods. Other models can simply utilize our bilateral slicing <b>contrastive</b> <b>learning</b> to enhance recommending performance without incurring extra expenses.

{{</citation>}}


### (2/2 | 36/202) FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions (Orion Weller et al., 2024)

{{<citation>}}

Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn Lawrie, Luca Soldaini. (2024)  
**FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions**
<br/>
<button class="copy-to-clipboard" title="FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions" index=36>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-CL, cs-IR, cs-LG, cs.IR  
Keyword Score: 43  
Keywords: Benchmarking, Fine-tuning, Information Retrieval, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15246v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15246v1.pdf" filename="2403.15246v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Modern <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are capable of following long and complex instructions that enable a diverse amount of user tasks. However, despite <b>Information</b> <b>Retrieval</b> (IR) models using <b>LLMs</b> as the backbone of their architectures, nearly all of them still only take queries as input, with no instructions. For the handful of recent models that do take instructions, it's unclear how they use them. We introduce our dataset FollowIR, which contains a rigorous instruction evaluation <b>benchmark</b> as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR builds off the long history of the TREC conferences: as TREC provides human annotators with instructions (also known as narratives) to determine document relevance, so should IR models be able to understand and decide relevance based on these detailed instructions. Our evaluation <b>benchmark</b> starts with three deeply judged TREC collections and alters the annotator instructions, re-annotating relevant documents. Through this process, we can measure how well IR models follow instructions, through a new pairwise evaluation framework. Our results indicate that existing retrieval models fail to correctly use instructions, using them for basic keywords and struggling to understand long-form <b>information.</b> <b>However,</b> we show that it is possible for IR models to learn to follow complex instructions: our new FollowIR-7B model has significant improvements (over 13%) after <b>fine-tuning</b> on our training set.

{{</citation>}}


## cs.CL (24)



### (1/24 | 37/202) MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts (Md Nishat Raihan et al., 2024)

{{<citation>}}

Md Nishat Raihan, Dhiman Goswami, Al Nahian Bin Emran, Sadiya Sayara Chowdhury Puspo, Amrita Ganguly, Marcos Zampieri. (2024)  
**MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts**
<br/>
<button class="copy-to-clipboard" title="MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts" index=37>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 80  
Keywords: Few-shot, Zero-shot, Natural Language Understanding, Reasoning, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14982v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14982v1.pdf" filename="2403.14982v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Our paper presents team MasonTigers submission to the SemEval-2024 Task 9 - which provides a dataset of puzzles for testing <b>natural</b> <b>language</b> <b>understanding.</b> We employ <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to solve this task through several <b>prompting</b> techniques. <b>Zero-shot</b> and <b>few-shot</b> <b>prompting</b> generate reasonably good results when tested with proprietary <b>LLMs,</b> compared to the open-source models. We obtain further improved results with <b>chain-of-thought</b> <b>prompting,</b> an iterative <b>prompting</b> method that breaks down the <b>reasoning</b> process step-by-step. We obtain our best results by utilizing an ensemble of <b>chain-of-thought</b> <b>prompts,</b> placing 2nd in the word puzzle subtask and 13th in the sentence puzzle subtask. The strong performance of <b>prompted</b> <b>LLMs</b> demonstrates their capability for complex <b>reasoning</b> when provided with a decomposition of the thought process. Our work sheds light on how step-wise explanatory <b>prompts</b> can unlock more of the knowledge encoded in the parameters of large models.

{{</citation>}}


### (2/24 | 38/202) On Zero-Shot Counterspeech Generation by LLMs (Punyajoy Saha et al., 2024)

{{<citation>}}

Punyajoy Saha, Aalok Agrawal, Abhik Jana, Chris Biemann, Animesh Mukherjee. (2024)  
**On Zero-Shot Counterspeech Generation by LLMs**
<br/>
<button class="copy-to-clipboard" title="On Zero-Shot Counterspeech Generation by LLMs" index=38>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 80  
Keywords: Fine-tuning, Zero-shot, ChatGPT, GPT, GPT-2, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14938v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14938v1.pdf" filename="2403.14938v1.pdf">Download PDF</button>

---


**ABSTRACT**  
With the emergence of numerous <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM),</b> the usage of such models in various Natural Language Processing (NLP) applications is increasing extensively. Counterspeech generation is one such key task where efforts are made to develop generative models by <b>fine-tuning</b> <b>LLMs</b> with hatespeech - counterspeech pairs, but none of these attempts explores the intrinsic properties of <b>large</b> <b>language</b> <b>models</b> in <b>zero-shot</b> settings. In this work, we present a comprehensive analysis of the performances of four <b>LLMs</b> namely <b>GPT-2,</b> DialoGPT, <b>ChatGPT</b> and FlanT5 in <b>zero-shot</b> settings for counterspeech generation, which is the first of its kind. For <b>GPT-2</b> and DialoGPT, we further investigate the deviation in performance with respect to the sizes (small, medium, <b>large)</b> <b>of</b> <b>the</b> models. On the other hand, we propose three different <b>prompting</b> strategies for generating different types of counterspeech and analyse the impact of such strategies on the performance of the models. Our analysis shows that there is an improvement in generation quality for two datasets (17%), however the toxicity increase (25%) with increase in model size. Considering type of model, <b>GPT-2</b> and FlanT5 models are significantly better in terms of counterspeech quality but also have high toxicity as compared to DialoGPT. <b>ChatGPT</b> are much better at generating counter speech than other models across all metrics. In terms of <b>prompting,</b> we find that our proposed strategies help in improving counter speech generation across all the models.

{{</citation>}}


### (3/24 | 39/202) Towards Knowledge-Grounded Natural Language Understanding and Generation (Chenxi Whitehouse, 2024)

{{<citation>}}

Chenxi Whitehouse. (2024)  
**Towards Knowledge-Grounded Natural Language Understanding and Generation**
<br/>
<button class="copy-to-clipboard" title="Towards Knowledge-Grounded Natural Language Understanding and Generation" index=39>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 76  
Keywords: Knowledge Distillation, Multi-modal, Multi-modal, Zero-shot, Transformer, Fake News Detection, Grounding, Natural Language Understanding, Fake News Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15364v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15364v1.pdf" filename="2403.15364v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This thesis investigates how <b>natural</b> <b>language</b> <b>understanding</b> and generation with <b>transformer</b> models can benefit from <b>grounding</b> the models with knowledge representations and addresses the following key research questions: (i) Can knowledge of entities extend its benefits beyond entity-centric tasks, such as entity linking? (ii) How can we faithfully and effectively extract such structured knowledge from raw text, especially noisy web text? (iii) How do other types of knowledge, beyond structured knowledge, contribute to improving NLP tasks? Studies in this thesis find that incorporating relevant and up-to-date knowledge of entities benefits <b>fake</b> <b>news</b> <b>detection,</b> and entity-focused code-switching significantly enhances <b>zero-shot</b> cross-lingual transfer on entity-centric tasks. In terms of effective and faithful approaches to extracting structured knowledge, it is observed that integrating negative examples and training with entity planning significantly improves performance. Additionally, it is established that other general forms of knowledge, such as parametric and <b>distilled</b> knowledge, enhance <b>multimodal</b> and multilingual knowledge-intensive tasks. This research shows the tangible benefits of diverse knowledge integration and motivates further exploration in this direction.

{{</citation>}}


### (4/24 | 40/202) CoLLEGe: Concept Embedding Generation for Large Language Models (Ryan Teehan et al., 2024)

{{<citation>}}

Ryan Teehan, Brenden Lake, Mengye Ren. (2024)  
**CoLLEGe: Concept Embedding Generation for Large Language Models**
<br/>
<button class="copy-to-clipboard" title="CoLLEGe: Concept Embedding Generation for Large Language Models" index=40>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 70  
Keywords: Few-shot, Fine-tuning, Meta Learning, Reasoning, In-context Learning, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15362v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15362v1.pdf" filename="2403.15362v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved <b>finetuning</b> process to learn robustly. <b>Prompting</b> <b>in-context</b> is not robust to context distractions, and often fails to confer much information about the new concepts. Classic methods for <b>few-shot</b> word learning in NLP, relying on global word vectors, are less applicable to <b>large</b> <b>language</b> <b>models.</b> In this paper, we introduce a novel approach named CoLLEGe (Concept Learning with Language Embedding Generation) to modernize <b>few-shot</b> concept learning. CoLLEGe is a <b>meta-learning</b> <b>framework</b> capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions. Our primary <b>meta-learning</b> <b>objective</b> is simply to facilitate a language model to make next word predictions in forthcoming sentences, making it compatible with language model pretraining. We design a series of tasks to test new concept learning in challenging real-world scenarios, including new word acquisition, definition inference, and verbal <b>reasoning,</b> and demonstrate that our method succeeds in each setting without task-specific training.

{{</citation>}}


### (5/24 | 41/202) Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation (Zhenrui Yue et al., 2024)

{{<citation>}}

Zhenrui Yue, Huimin Zeng, Yimeng Lu, Lanyu Shang, Yang Zhang, Dong Wang. (2024)  
**Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation**
<br/>
<button class="copy-to-clipboard" title="Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation" index=41>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 70  
Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Rerank, Text Generation, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14952v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14952v1.pdf" filename="2403.14952v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The proliferation of online misinformation has posed significant threats to public interest. While numerous online users actively participate in the combat against misinformation, many of such responses can be characterized by the lack of politeness and supporting facts. As a solution, <b>text</b> <b>generation</b> approaches are proposed to automatically produce counter-misinformation responses. Nevertheless, existing methods are often trained end-to-end without leveraging external knowledge, resulting in subpar <b>text</b> <b>quality</b> and excessively repetitive responses. In this paper, we propose retrieval augmented response generation for online misinformation (RARG), which collects supporting evidence from scientific sources and generates counter-misinformation responses based on the evidences. In particular, our RARG consists of two stages: (1) evidence collection, where we design a retrieval pipeline to retrieve and <b>rerank</b> evidence documents using a database comprising over 1M academic articles; (2) response generation, in which we align <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to generate evidence-based responses via <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF).</b> We propose a reward function to maximize the utilization of the retrieved evidence while maintaining the quality of the generated <b>text,</b> <b>which</b> yields polite and factual responses that clearly refutes misinformation. To demonstrate the effectiveness of our method, we study the case of COVID-19 and perform extensive experiments with both in- and cross-domain datasets, where RARG consistently outperforms baselines by generating high-quality counter-misinformation responses.

{{</citation>}}


### (6/24 | 42/202) Stance Reasoner: Zero-Shot Stance Detection on Social Media with Explicit Reasoning (Maksym Taranukhin et al., 2024)

{{<citation>}}

Maksym Taranukhin, Vered Shwartz, Evangelos Milios. (2024)  
**Stance Reasoner: Zero-Shot Stance Detection on Social Media with Explicit Reasoning**
<br/>
<button class="copy-to-clipboard" title="Stance Reasoner: Zero-Shot Stance Detection on Social Media with Explicit Reasoning" index=42>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 70  
Keywords: Supervised Learning, Zero-shot, Reasoning, Stance Detection, In-context Learning, In-context Learning, Pre-trained Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14895v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14895v1.pdf" filename="2403.14895v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Social media platforms are rich sources of opinionated content. <b>Stance</b> <b>detection</b> allows the automatic extraction of users' opinions on various topics from such content. We focus on <b>zero-shot</b> <b>stance</b> <b>detection,</b> where the model's success relies on (a) having knowledge about the target topic; and (b) learning general <b>reasoning</b> strategies that can be employed for new topics. We present <b>Stance</b> <b>Reasoner,</b> an approach to <b>zero-shot</b> <b>stance</b> <b>detection</b> on social media that leverages explicit <b>reasoning</b> over background knowledge to guide the model's inference about the document's <b>stance</b> <b>on</b> a target. Specifically, our method uses a <b>pre-trained</b> <b>language</b> <b>model</b> as a source of world knowledge, with the chain-of-thought <b>in-context</b> <b>learning</b> approach to generate intermediate <b>reasoning</b> steps. <b>Stance</b> <b>Reasoner</b> outperforms the current state-of-the-art models on 3 Twitter datasets, including fully <b>supervised</b> models. It can better generalize across targets, while at the same time providing explicit and interpretable explanations for its predictions.

{{</citation>}}


### (7/24 | 43/202) KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation (Xindi Luo et al., 2024)

{{<citation>}}

Xindi Luo, Zequn Sun, Jing Zhao, Zhe Zhao, Wei Hu. (2024)  
**KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation**
<br/>
<button class="copy-to-clipboard" title="KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation" index=43>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-LG, cs.CL  
Keyword Score: 61  
Keywords: Graph, Graph Embedding, Benchmarking, Fine-tuning, Knowledge Graph, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14950v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14950v1.pdf" filename="2403.14950v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Parameter-efficient <b>finetuning</b> (PEFT) is a key technique for adapting <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to downstream tasks. In this paper, we study leveraging <b>knowledge</b> <b>graph</b> <b>embeddings</b> to improve the effectiveness of PEFT. We propose a knowledgeable adaptation method called KnowLA. It inserts an adaptation layer into an <b>LLM</b> to integrate the embeddings of entities appearing in the input text. The adaptation layer is trained in combination with LoRA on instruction data. Experiments on six <b>benchmarks</b> with two popular <b>LLMs</b> and three <b>knowledge</b> <b>graphs</b> <b>demonstrate</b> the effectiveness and robustness of KnowLA. We show that \modelname can help activate the relevant parameterized <b>knowledge</b> <b>in</b> an <b>LLM</b> to answer a question without changing its parameters or input <b>prompts.</b>

{{</citation>}}


### (8/24 | 44/202) Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models (Huanxuan Liao et al., 2024)

{{<citation>}}

Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Shengping Liu, Jun Zhao. (2024)  
**Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models**
<br/>
<button class="copy-to-clipboard" title="Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models" index=44>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 60  
Keywords: Out-of-distribution, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Question Answering, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15268v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15268v1.pdf" filename="2403.15268v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Retrieval-Augmented-Generation</b> <b>and</b> <b>Gener-ation-Augmented-Generation</b> have been proposed to enhance the knowledge required for <b>question</b> <b>answering</b> over <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> However, the former depends on external resources, and both require incorporating the explicit documents into the context, which results in longer contexts that lead to more resource consumption. Recent works indicate that <b>LLMs</b> have modeled rich knowledge, albeit not effectively triggered or activated. Inspired by this, we propose a novel knowledge-augmented framework, Imagination-Augmented-Generation (IAG), which simulates the human capacity to compensate for knowledge deficits while answering <b>questions</b> <b>solely</b> through imagination, without relying on external resources. Guided by IAG, we propose an imagine richer context method for <b>question</b> <b>answering</b> (IMcQA), which obtains richer context through the following two modules: explicit imagination by generating a short dummy document with long context compress and implicit imagination with HyperNetwork for generating adapter weights. Experimental results on three datasets demonstrate that IMcQA exhibits significant advantages in both open-domain and closed-book settings, as well as in both in-distribution performance and <b>out-of-distribution</b> generalizations. Our code will be available at https://github.com/Xnhyacinth/IAG.

{{</citation>}}


### (9/24 | 45/202) CHisIEC: An Information Extraction Corpus for Ancient Chinese History (Xuemei Tang et al., 2024)

{{<citation>}}

Xuemei Tang, Zekun Deng, Qi Su, Hao Yang, Jun Wang. (2024)  
**CHisIEC: An Information Extraction Corpus for Ancient Chinese History**
<br/>
<button class="copy-to-clipboard" title="CHisIEC: An Information Extraction Corpus for Ancient Chinese History" index=45>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 60  
Keywords: Information Retrieval, Named Entity Recognition, Named Entity Recognition, Relation Extraction, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15088v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15088v1.pdf" filename="2403.15088v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Natural Language Processing (NLP) plays a pivotal role in the realm of Digital Humanities (DH) and serves as the cornerstone for advancing the structural analysis of historical and cultural heritage texts. This is particularly true for the domains of <b>named</b> <b>entity</b> <b>recognition</b> <b>(NER)</b> and <b>relation</b> <b>extraction</b> (RE). In our commitment to expediting ancient history and culture, we present the ``Chinese Historical <b>Information</b> <b>Extraction</b> Corpus''(CHisIEC). CHisIEC is a meticulously curated dataset designed to develop and evaluate <b>NER</b> and RE tasks, offering a resource to facilitate research in the field. Spanning a remarkable historical timeline encompassing data from 13 dynasties spanning over 1830 years, CHisIEC epitomizes the extensive temporal range and text heterogeneity inherent in Chinese historical documents. The dataset encompasses four distinct entity types and twelve <b>relation</b> <b>types,</b> resulting in a meticulously labeled dataset comprising 14,194 entities and 8,609 <b>relations.</b> <b>To</b> establish the robustness and versatility of our dataset, we have undertaken comprehensive experimentation involving models of various sizes and paradigms. Additionally, we have evaluated the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in the context of tasks related to ancient Chinese history. The dataset and code are available at \url{https://github.com/tangxuemei1995/CHisIEC}.

{{</citation>}}


### (10/24 | 46/202) ESG Classification by Implicit Rule Learning via GPT-4 (Hyo Jeong Yun et al., 2024)

{{<citation>}}

Hyo Jeong Yun, Chanyoung Kim, Moonjeong Hahm, Kyuri Kim, Guijin Son. (2024)  
**ESG Classification by Implicit Rule Learning via GPT-4**
<br/>
<button class="copy-to-clipboard" title="ESG Classification by Implicit Rule Learning via GPT-4" index=46>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 60  
Keywords: GPT, GPT-4, Reasoning, In-context Learning, In-context Learning, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15040v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15040v1.pdf" filename="2403.15040v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Environmental, social, and governance (ESG) factors are widely adopted as higher investment return indicators. Accordingly, ongoing efforts are being made to automate ESG evaluation with language models to extract signals from massive web text easily. However, recent approaches suffer from a lack of training data, as rating agencies keep their evaluation metrics confidential. This paper investigates whether state-of-the-art language models like <b>GPT-4</b> can be guided to align with unknown ESG evaluation criteria through strategies such as <b>prompting,</b> chain-of-thought <b>reasoning,</b> and dynamic <b>in-context</b> <b>learning.</b> We demonstrate the efficacy of these approaches by ranking 2nd in the Shared-Task ML-ESG-3 Impact Type track for Korean without updating the model on the provided training data. We also explore how adjusting <b>prompts</b> impacts the ability of language models to address financial tasks leveraging smaller models with openly available weights. We observe longer general pre-training to correlate with enhanced performance in financial downstream tasks. Our findings showcase the potential of language models to navigate complex, subjective evaluation guidelines despite lacking explicit training examples, revealing opportunities for training-free solutions for financial downstream tasks.

{{</citation>}}


### (11/24 | 47/202) MasonTigers at SemEval-2024 Task 8: Performance Analysis of Transformer-based Models on Machine-Generated Text Detection (Sadiya Sayara Chowdhury Puspo et al., 2024)

{{<citation>}}

Sadiya Sayara Chowdhury Puspo, Md Nishat Raihan, Dhiman Goswami, Al Nahian Bin Emran, Amrita Ganguly, Ozlem Uzuner. (2024)  
**MasonTigers at SemEval-2024 Task 8: Performance Analysis of Transformer-based Models on Machine-Generated Text Detection**
<br/>
<button class="copy-to-clipboard" title="MasonTigers at SemEval-2024 Task 8: Performance Analysis of Transformer-based Models on Machine-Generated Text Detection" index=47>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 55  
Keywords: Black Box, Fine-tuning, Zero-shot, Transformer, Text Classification, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14989v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14989v1.pdf" filename="2403.14989v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents the MasonTigers entry to the SemEval-2024 Task 8 - Multigenerator, Multidomain, and Multilingual <b>Black-Box</b> <b>Machine-Generated</b> <b>Text</b> <b>Detection.</b> The task encompasses Binary Human-Written vs. Machine-Generated <b>Text</b> <b>Classification</b> (Track A), Multi-Way Machine-Generated <b>Text</b> <b>Classification</b> (Track B), and Human-Machine Mixed <b>Text</b> <b>Detection</b> (Track C). Our best performing approaches utilize mainly the ensemble of discriminator <b>transformer</b> models along with sentence <b>transformer</b> and statistical machine learning approaches in specific cases. Moreover, <b>zero-shot</b> <b>prompting</b> and <b>fine-tuning</b> of FLAN-T5 are used for Track A and B.

{{</citation>}}


### (12/24 | 48/202) Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs (Xiaobin Zhang et al., 2024)

{{<citation>}}

Xiaobin Zhang, Liangjun Zang, Qianwen Liu, Shuchong Wei, Songlin Hu. (2024)  
**Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs**
<br/>
<button class="copy-to-clipboard" title="Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs" index=48>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 50  
Keywords: Event-Relation Extraction, Relation Extraction, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15273v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15273v1.pdf" filename="2403.15273v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Event temporal <b>relation</b> <b>(TempRel)</b> is a primary subject of the event <b>relation</b> <b>extraction</b> task. However, the inherent ambiguity of TempRel increases the difficulty of the task. With the rise of <b>prompt</b> engineering, it is important to design effective <b>prompt</b> templates and verbalizers to extract relevant knowledge. The traditional manually designed templates struggle to extract precise temporal knowledge. This paper introduces a novel retrieval-augmented TempRel extraction approach, leveraging knowledge retrieved from <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to enhance <b>prompt</b> templates and verbalizers. Our method capitalizes on the diverse capabilities of various <b>LLMs</b> to generate a wide array of ideas for template and verbalizer design. Our proposed method fully exploits the potential of <b>LLMs</b> for generation tasks and contributes more knowledge to our design. Empirical evaluations across three widely recognized datasets demonstrate the efficacy of our method in improving the performance of event temporal <b>relation</b> <b>extraction</b> tasks.

{{</citation>}}


### (13/24 | 49/202) LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement (Nicholas Lee et al., 2024)

{{<citation>}}

Nicholas Lee, Thanakul Wattanawong, Sehoon Kim, Karttikeya Mangalam, Sheng Shen, Gopala Anumanchipali, Michael W. Mahoney, Kurt Keutzer, Amir Gholami. (2024)  
**LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement**
<br/>
<button class="copy-to-clipboard" title="LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement" index=49>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 50  
Keywords: Data Augmentation, Fine-tuning, Fine-tuning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15042v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15042v1.pdf" filename="2403.15042v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Pretrained <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are currently state-of-the-art for solving the vast majority of natural language processing tasks. While many real-world applications still require <b>fine-tuning</b> to reach satisfactory levels of performance, many of them are in the low-data regime, making <b>fine-tuning</b> challenging. To address this, we propose LLM2LLM, a targeted and iterative <b>data</b> <b>augmentation</b> strategy that uses a teacher <b>LLM</b> to enhance a small seed dataset by augmenting additional <b>data</b> <b>that</b> can be used for <b>fine-tuning</b> on a specific task. LLM2LLM (1) <b>fine-tunes</b> a baseline student <b>LLM</b> on the initial seed <b>data,</b> <b>(2)</b> evaluates and extracts <b>data</b> <b>points</b> that the model gets wrong, and (3) uses a teacher <b>LLM</b> to generate synthetic <b>data</b> <b>based</b> on these incorrect <b>data</b> <b>points,</b> which are then added back into the training <b>data.</b> <b>This</b> approach amplifies the signal from incorrectly predicted <b>data</b> <b>points</b> by the <b>LLM</b> during training and reintegrates them into the dataset to focus on more challenging examples for the <b>LLM.</b> Our results show that LLM2LLM significantly enhances the performance of <b>LLMs</b> in the low-data regime, outperforming both traditional <b>fine-tuning</b> and other <b>data</b> <b>augmentation</b> baselines. LLM2LLM reduces the dependence on labor-intensive <b>data</b> <b>curation</b> and paves the way for more scalable and performant <b>LLM</b> solutions, allowing us to tackle <b>data-constrained</b> <b>domains</b> and tasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on CaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular <b>fine-tuning</b> in the low-data regime using a LLaMA2-7B student model.

{{</citation>}}


### (14/24 | 50/202) Text clustering with LLM embeddings (Alina Petukhova et al., 2024)

{{<citation>}}

Alina Petukhova, Joao P. Matos-Carvalho, Nuno Fachada. (2024)  
**Text clustering with LLM embeddings**
<br/>
<button class="copy-to-clipboard" title="Text clustering with LLM embeddings" index=50>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-LG, cs.CL  
Keyword Score: 43  
Keywords: Clustering, BERT, Text Clustering, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15112v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15112v1.pdf" filename="2403.15112v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Text</b> <b>clustering</b> is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> - and <b>clustering</b> algorithms affect how <b>text</b> <b>datasets</b> are clustered. A series of experiments were conducted to assess how embeddings influence <b>clustering</b> results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that <b>LLM</b> embeddings excel at capturing the nuances of structured language, while <b>BERT</b> leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve <b>clustering</b> efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a complex balance between the need for nuanced <b>text</b> <b>representation</b> and computational feasibility in <b>text</b> <b>clustering</b> applications. This study extends traditional <b>text</b> <b>clustering</b> frameworks by incorporating embeddings from <b>LLMs,</b> thereby paving the way for improved methodologies and opening new avenues for future research in various types of textual analysis.

{{</citation>}}


### (15/24 | 51/202) MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness (Dhiman Goswami et al., 2024)

{{<citation>}}

Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Md Nishat Raihan, Al Nahian Bin Emran, Amrita Ganguly, Marcos Zampieri. (2024)  
**MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness**
<br/>
<button class="copy-to-clipboard" title="MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness" index=51>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 40  
Keywords: Supervised Learning, Unsupervised Learning, BERT, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14990v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14990v1.pdf" filename="2403.14990v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents the MasonTigers entry to the SemEval-2024 Task 1 - Semantic Textual Relatedness. The task encompasses <b>supervised</b> (Track A), <b>unsupervised</b> (Track B), and cross-lingual (Track C) approaches across 14 different languages. MasonTigers stands out as one of the two teams who participated in all languages across the three tracks. Our approaches achieved rankings ranging from 11th to 21st in Track A, from 1st to 8th in Track B, and from 5th to 12th in Track C. Adhering to the task-specific constraints, our best performing approaches utilize ensemble of statistical machine learning approaches combined with language-specific <b>BERT</b> based models and sentence <b>transformers.</b>

{{</citation>}}


### (16/24 | 52/202) Multi-Review Fusion-in-Context (Aviv Slobodkin et al., 2024)

{{<citation>}}

Aviv Slobodkin, Ori Shapira, Ran Levy, Ido Dagan. (2024)  
**Multi-Review Fusion-in-Context**
<br/>
<button class="copy-to-clipboard" title="Multi-Review Fusion-in-Context" index=52>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 33  
Keywords: Benchmarking, Question Answering, Text Generation, Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15351v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15351v1.pdf" filename="2403.15351v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Grounded <b>text</b> <b>generation,</b> encompassing tasks such as long-form <b>question-answering</b> <b>and</b> <b>summarization,</b> necessitates both content selection and content consolidation. Current end-to-end methods are difficult to control and interpret due to their opaqueness. Accordingly, recent works have proposed a modular approach, with separate components for each step. Specifically, we focus on the second subtask, of generating coherent <b>text</b> <b>given</b> pre-selected content in a multi-document setting. Concretely, we formalize \textit{Fusion-in-Context} (FiC) as a standalone task, whose input consists of source <b>texts</b> <b>with</b> highlighted spans of targeted content. A model then needs to generate a coherent passage that includes all and only the target information. Our work includes the development of a curated dataset of 1000 instances in the reviews domain, alongside a novel evaluation framework for assessing the faithfulness and coverage of highlights, which strongly correlate to human judgment. Several baseline models exhibit promising outcomes and provide insightful analyses. This study lays the groundwork for further exploration of modular <b>text</b> <b>generation</b> in the multi-document setting, offering potential improvements in the quality and reliability of generated content. \footnote{Our <b>benchmark,</b> FuseReviews, including the dataset, evaluation framework and designated leaderboard, can be found at \url{https://fusereviews.github.io/}.}

{{</citation>}}


### (17/24 | 53/202) Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study (Tim van Dam et al., 2024)

{{<citation>}}

Tim van Dam, Frank van der Heijden, Philippe de Bekker, Berend Nieuwschepen, Marc Otten, Maliheh Izadi. (2024)  
**Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study**
<br/>
<button class="copy-to-clipboard" title="Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study" index=53>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 30  
Keywords: Automatic Evaluation, Fine-tuning, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15185v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15185v1.pdf" filename="2403.15185v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Language model-based code completion models have quickly grown in use, helping thousands of developers write code in many different programming languages. However, research on code completion models typically focuses on imperative languages such as Python and JavaScript, which results in a lack of representation for functional programming languages. Consequently, these models often perform poorly on functional languages such as Haskell. To investigate whether this can be alleviated, we evaluate the performance of two language models for code, CodeGPT and UniXcoder, on the functional programming language Haskell. We <b>fine-tune</b> and evaluate the models on Haskell functions sourced from a publicly accessible Haskell dataset on HuggingFace. Additionally, we manually evaluate the models using our novel translated HumanEval dataset. Our <b>automatic</b> <b>evaluation</b> shows that knowledge of imperative programming languages in the pre-training of <b>LLMs</b> may not transfer well to functional languages, but that code completion on functional languages is feasible. Consequently, this shows the need for more high-quality Haskell datasets. A manual evaluation on HumanEval-Haskell indicates CodeGPT frequently generates empty predictions and extra comments, while UniXcoder more often produces incomplete or incorrect predictions. Finally, we release HumanEval-Haskell, along with the <b>fine-tuned</b> models and all code required to reproduce our experiments on GitHub (https://github.com/AISE-TUDelft/HaskellCCEval).

{{</citation>}}


### (18/24 | 54/202) Risk and Response in Large Language Models: Evaluating Key Threat Categories (Bahareh Harandizadeh et al., 2024)

{{<citation>}}

Bahareh Harandizadeh, Abel Salinas, Fred Morstatter. (2024)  
**Risk and Response in Large Language Models: Evaluating Key Threat Categories**
<br/>
<button class="copy-to-clipboard" title="Risk and Response in Large Language Models: Evaluating Key Threat Categories" index=54>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 30  
Keywords: Fine-tuning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14988v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14988v1.pdf" filename="2403.14988v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper explores the pressing issue of risk assessment in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> as they become increasingly prevalent in various applications. Focusing on how reward models, which are designed to <b>fine-tune</b> pretrained <b>LLMs</b> to align with human values, perceive and categorize different types of risks, we delve into the challenges posed by the subjective nature of preference-based training data. By utilizing the Anthropic Red-team dataset, we analyze major risk categories, including Information Hazards, Malicious Uses, and Discrimination/Hateful content. Our findings indicate that <b>LLMs</b> tend to consider Information Hazards less harmful, a finding confirmed by a specially developed regression model. Additionally, our analysis shows that <b>LLMs</b> respond less stringently to Information Hazards compared to other risks. The study further reveals a significant vulnerability of <b>LLMs</b> to jailbreaking attacks in Information Hazard scenarios, highlighting a critical security concern in <b>LLM</b> risk assessment and emphasizing the need for improved AI safety measures.

{{</citation>}}


### (19/24 | 55/202) Attention-Driven Reasoning: Unlocking the Potential of Large Language Models (Bingli Liao et al., 2024)

{{<citation>}}

Bingli Liao, Danilo Vasconcellos Vargas. (2024)  
**Attention-Driven Reasoning: Unlocking the Potential of Large Language Models**
<br/>
<button class="copy-to-clipboard" title="Attention-Driven Reasoning: Unlocking the Potential of Large Language Models" index=55>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 30  
Keywords: Reasoning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14932v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14932v1.pdf" filename="2403.14932v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown remarkable capabilities, but their <b>reasoning</b> abilities and underlying mechanisms remain poorly understood. We present a novel approach to enhance <b>LLMs'</b> <b>reasoning</b> through attention mechanism optimization, without additional training data. We identify inefficiencies in the attention distribution caused by non-semantic tokens and propose an algorithm to re-balance the skewed distribution, enabling the model to abstract more nuanced knowledge. Our experiments demonstrate significantly improved <b>reasoning</b> capabilities, particularly for non-STEM questions. We provide insights into the role of attention patterns in <b>LLMs'</b> <b>reasoning</b> and propose a method to enhance these abilities, paving the way for more powerful and versatile language models.

{{</citation>}}


### (20/24 | 56/202) CO-Fun: A German Dataset on Company Outsourcing in Fund Prospectuses for Named Entity Recognition and Relation Extraction (Neda Foroutan et al., 2024)

{{<citation>}}

Neda Foroutan, Markus Schröder, Andreas Dengel. (2024)  
**CO-Fun: A German Dataset on Company Outsourcing in Fund Prospectuses for Named Entity Recognition and Relation Extraction**
<br/>
<button class="copy-to-clipboard" title="CO-Fun: A German Dataset on Company Outsourcing in Fund Prospectuses for Named Entity Recognition and Relation Extraction" index=56>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 20  
Keywords: Named Entity Recognition, Relation Extraction  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15322v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15322v1.pdf" filename="2403.15322v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The process of cyber mapping gives insights in relationships among financial entities and service providers. Centered around the outsourcing practices of companies within fund prospectuses in Germany, we introduce a dataset specifically designed for <b>named</b> <b>entity</b> <b>recognition</b> and <b>relation</b> <b>extraction</b> tasks. The labeling process on 948 sentences was carried out by three experts which yields to 5,969 annotations for four entity types (Outsourcing, Company, Location and Software) and 4,102 <b>relation</b> <b>annotations</b> (Outsourcing-Company, Company-Location). State-of-the-art deep learning models were trained to recognize entities and extract <b>relations</b> <b>showing</b> first promising results. An anonymized version of the dataset, along with guidelines and the code used for model training, are publicly available at https://www.dfki.uni-kl.de/cybermapping/data/CO-Fun-1.0-anonymized.zip.

{{</citation>}}


### (21/24 | 57/202) Hierarchical Skip Decoding for Efficient Autoregressive Text Generation (Yunqi Zhu et al., 2024)

{{<citation>}}

Yunqi Zhu, Xuebing Yang, Yuanyuan Wu, Wensheng Zhang. (2024)  
**Hierarchical Skip Decoding for Efficient Autoregressive Text Generation**
<br/>
<button class="copy-to-clipboard" title="Hierarchical Skip Decoding for Efficient Autoregressive Text Generation" index=57>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 20  
Keywords: Text Generation, Pre-trained Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14919v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14919v1.pdf" filename="2403.14919v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Autoregressive decoding strategy is a commonly used method for <b>text</b> <b>generation</b> tasks with <b>pre-trained</b> <b>language</b> <b>models,</b> while early-exiting is an effective approach to speedup the inference stage. In this work, we propose a novel decoding strategy named Hierarchical Skip Decoding (HSD) for efficient autoregressive <b>text</b> <b>generation.</b> Different from existing methods that require additional trainable components, HSD is a plug-and-play method applicable to autoregressive <b>text</b> <b>generation</b> models, it adaptively skips decoding layers in a hierarchical manner based on the current sequence length, thereby reducing computational workload and allocating computation resources. Comprehensive experiments on five <b>text</b> <b>generation</b> datasets with <b>pre-trained</b> <b>language</b> <b>models</b> demonstrate HSD's advantages in balancing efficiency and <b>text</b> <b>quality.</b> With almost half of the layers skipped, HSD can sustain 90% of the <b>text</b> <b>quality</b> compared to vanilla autoregressive decoding, outperforming the competitive approaches.

{{</citation>}}


### (22/24 | 58/202) Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach (Kun Sun et al., 2024)

{{<citation>}}

Kun Sun, Rong Wang, Haitao Liu, Anders Søgaard. (2024)  
**Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach**
<br/>
<button class="copy-to-clipboard" title="Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach" index=58>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-LG, cs.CL  
Keyword Score: 13  
Keywords: Clustering, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15250v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15250v1.pdf" filename="2403.15250v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Amidst the rapid evolution of <b>LLMs,</b> the significance of evaluation in comprehending and propelling these models forward is increasingly paramount. Evaluations have revealed that factors such as scaling, training types, architectures and other factors profoundly impact the performance of <b>LLMs.</b> However, the extent and nature of these impacts continue to be subjects of debate because most assessments have been restricted to a limited number of models and data points. Clarifying the effects of these factors on performance scores can be more effectively achieved through a statistical lens. Our study embarks on a thorough re-examination of these <b>LLMs,</b> targeting the inadequacies in current evaluation methods. With the advent of a uniform evaluation framework, our research leverages an expansive dataset of evaluation results, introducing a comprehensive statistical methodology. This includes the application of ANOVA, Tukey HSD tests, GAMM, and <b>clustering</b> technique, offering a robust and transparent approach to deciphering <b>LLM</b> performance data. Contrary to prevailing findings, our results challenge assumptions about emergent abilities and the influence of given training types and architectures in <b>LLMs.</b> These findings furnish new perspectives on the characteristics, intrinsic nature, and developmental trajectories of <b>LLMs.</b> By providing straightforward and reliable methods to scrutinize and reassess <b>LLM</b> performance data, this study contributes a nuanced perspective on <b>LLM</b> efficiency and potentials.

{{</citation>}}


### (23/24 | 59/202) Language Models in Dialogue: Conversational Maxims for Human-AI Interactions (Erik Miehling et al., 2024)

{{<citation>}}

Erik Miehling, Manish Nagireddy, Prasanna Sattigeri, Elizabeth M. Daly, David Piorkowski, John T. Richards. (2024)  
**Language Models in Dialogue: Conversational Maxims for Human-AI Interactions**
<br/>
<button class="copy-to-clipboard" title="Language Models in Dialogue: Conversational Maxims for Human-AI Interactions" index=59>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-HC, cs.CL  
Keyword Score: 10  
Keywords: Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15115v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15115v1.pdf" filename="2403.15115v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Modern language models, while sophisticated, exhibit some inherent shortcomings, particularly in conversational settings. We claim that many of the observed shortcomings can be attributed to violation of one or more conversational principles. By drawing upon extensive research from both the social science and AI communities, we propose a set of maxims -- quantity, quality, relevance, manner, benevolence, and transparency -- for describing effective human-AI conversation. We first justify the applicability of the first four maxims (from Grice) in the context of human-AI interactions. We then argue that two new maxims, benevolence (concerning the generation of, and engagement with, harmful content) and transparency (concerning recognition of one's knowledge boundaries, operational constraints, and intents), are necessary for addressing behavior unique to modern human-AI interactions. The proposed maxims offer prescriptive guidance on how to assess conversational quality between humans and <b>LLM-driven</b> conversational agents, informing both their evaluation and improved design.

{{</citation>}}


### (24/24 | 60/202) A Single Linear Layer Yields Task-Adapted Low-Rank Matrices (Hwichan Kim et al., 2024)

{{<citation>}}

Hwichan Kim, Shota Sasaki, Sho Hoshino, Ukyo Honda. (2024)  
**A Single Linear Layer Yields Task-Adapted Low-Rank Matrices**
<br/>
<button class="copy-to-clipboard" title="A Single Linear Layer Yields Task-Adapted Low-Rank Matrices" index=60>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-LG, cs.CL  
Keyword Score: 10  
Keywords: Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14946v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14946v1.pdf" filename="2403.14946v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient <b>Fine-Tuning</b> (PEFT) method that updates an initial weight matrix $W_0$ with a delta matrix $\Delta W$ consisted by two low-rank matrices $A$ and $B$. A previous study suggested that there is correlation between $W_0$ and $\Delta W$. In this study, we aim to delve deeper into relationships between $W_0$ and low-rank matrices $A$ and $B$ to further comprehend the behavior of LoRA. In particular, we analyze a conversion matrix that transform $W_0$ into low-rank matrices, which encapsulates information about the relationships. Our analysis reveals that the conversion matrices are similar across each layer. Inspired by these findings, we hypothesize that a single linear layer, which takes each layer's $W_0$ as input, can yield task-adapted low-rank matrices. To confirm this hypothesis, we devise a method named Conditionally Parameterized LoRA (CondLoRA) that updates initial weight matrices with low-rank matrices derived from a single linear layer. Our empirical results show that CondLoRA maintains a performance on par with LoRA, despite the fact that the trainable parameters of CondLoRA are fewer than those of LoRA. Therefore, we conclude that "a single linear layer yields task-adapted low-rank matrices."

{{</citation>}}


## eess.IV (5)



### (1/5 | 61/202) Integrating multiscale topology in digital pathology with pyramidal graph convolutional networks (Victor Ibañez et al., 2024)

{{<citation>}}

Victor Ibañez, Przemyslaw Szostak, Quincy Wong, Konstanty Korski, Samaneh Abbasi-Sureshjani, Alvaro Gomariz. (2024)  
**Integrating multiscale topology in digital pathology with pyramidal graph convolutional networks**
<br/>
<button class="copy-to-clipboard" title="Integrating multiscale topology in digital pathology with pyramidal graph convolutional networks" index=61>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 73  
Keywords: Graph Convolutional Network, Graph Convolutional Network, Message-Passing, Graph, Convolution, Convolutional Neural Network, Convolutional Neural Network, Multiple Instance Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15068v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15068v1.pdf" filename="2403.15068v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Graph</b> <b>convolutional</b> <b>networks</b> <b>(GCNs)</b> have emerged as a powerful alternative to <b>multiple</b> <b>instance</b> <b>learning</b> with <b>convolutional</b> <b>neural</b> <b>networks</b> in digital pathology, offering superior handling of structural information across various spatial ranges - a crucial aspect of learning from gigapixel H&E-stained whole slide images (WSI). However, <b>graph</b> <b>message-passing</b> <b>algorithms</b> often suffer from oversmoothing when aggregating a large neighborhood. Hence, effective modeling of multi-range interactions relies on the careful construction of the <b>graph.</b> <b>Our</b> <b>proposed</b> multi-scale <b>GCN</b> (MS-GCN) tackles this issue by leveraging information across <b>multiple</b> <b>magnification</b> <b>levels</b> in WSIs. MS-GCN enables the simultaneous modeling of long-range structural dependencies at lower magnifications and high-resolution cellular details at higher magnifications, akin to analysis pipelines usually conducted by pathologists. The architecture's unique configuration allows for the concurrent modeling of structural patterns at lower magnifications and detailed cellular features at higher ones, while also quantifying the contribution of each magnification level to the prediction. Through testing on different datasets, MS-GCN demonstrates superior performance over existing single-magnification <b>GCN</b> methods. The enhancement in performance and interpretability afforded by our method holds promise for advancing computational pathology models, especially in tasks requiring extensive spatial context.

{{</citation>}}


### (2/5 | 62/202) WEEP: A method for spatial interpretation of weakly supervised CNN models in computational pathology (Abhinav Sharma et al., 2024)

{{<citation>}}

Abhinav Sharma, Bojing Liu, Mattias Rantalainen. (2024)  
**WEEP: A method for spatial interpretation of weakly supervised CNN models in computational pathology**
<br/>
<button class="copy-to-clipboard" title="WEEP: A method for spatial interpretation of weakly supervised CNN models in computational pathology" index=62>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV, stat-ME  
Keyword Score: 60  
Keywords: Convolutional Neural Network, Supervised Learning, Supervised Learning, Weakly-supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15238v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15238v1.pdf" filename="2403.15238v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Deep learning enables the modelling of high-resolution histopathology whole-slide images (WSI). <b>Weakly</b> <b>supervised</b> <b>learning</b> of tile-level data is typically applied for tasks where labels only exist on the patient or WSI level (e.g. patient outcomes or histological grading). In this context, there is a need for improved spatial interpretability of predictions from such models. We propose a novel method, Wsi rEgion sElection aPproach (WEEP), for model interpretation. It provides a principled yet straightforward way to establish the spatial area of WSI required for assigning a particular prediction label. We demonstrate WEEP on a binary classification task in the area of breast cancer computational pathology. WEEP is easy to implement, is directly connected to the model-based decision process, and offers information relevant to both research and diagnostic applications.

{{</citation>}}


### (3/5 | 63/202) Ultrasound Imaging based on the Variance of a Diffusion Restoration Model (Yuxin Zhang et al., 2024)

{{<citation>}}

Yuxin Zhang, Clément Huneau, Jérôme Idier, Diana Mateus. (2024)  
**Ultrasound Imaging based on the Variance of a Diffusion Restoration Model**
<br/>
<button class="copy-to-clipboard" title="Ultrasound Imaging based on the Variance of a Diffusion Restoration Model" index=63>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, cs-LG, eess-IV, eess.IV  
Keyword Score: 30  
Keywords: Diffusion Model, Fine-tuning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15316v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15316v1.pdf" filename="2403.15316v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Despite today's prevalence of ultrasound imaging in medicine, ultrasound signal-to-noise ratio is still affected by several sources of noise and artefacts. Moreover, enhancing ultrasound image quality involves balancing concurrent factors like contrast, resolution, and speckle preservation. Recently, there has been progress in both model-based and learning-based approaches addressing the problem of ultrasound image reconstruction. Bringing the best from both worlds, we propose a hybrid reconstruction method combining an ultrasound linear direct model with a learning-based prior coming from a generative Denoising <b>Diffusion</b> <b>model.</b> More specifically, we rely on the <b>unsupervised</b> <b>fine-tuning</b> of a pre-trained Denoising <b>Diffusion</b> <b>Restoration</b> Model (DDRM). Given the nature of multiplicative noise inherent to ultrasound, this paper proposes an empirical model to characterize the stochasticity of <b>diffusion</b> <b>reconstruction</b> of ultrasound images, and shows the interest of its variance as an echogenicity map estimator. We conduct experiments on synthetic, in-vitro, and in-vivo data, demonstrating the efficacy of our variance imaging approach in achieving high-quality image reconstructions from single plane-wave acquisitions and in comparison to state-of-the-art methods.

{{</citation>}}


### (4/5 | 64/202) Improving cross-domain brain tissue segmentation in fetal MRI with synthetic data (Vladyslav Zalevskyi et al., 2024)

{{<citation>}}

Vladyslav Zalevskyi, Thomas Sanchez, Margaux Roulet, Jordina Aviles Verddera, Jana Hutter, Hamza Kebiri, Meritxell Bach Cuadra. (2024)  
**Improving cross-domain brain tissue segmentation in fetal MRI with synthetic data**
<br/>
<button class="copy-to-clipboard" title="Improving cross-domain brain tissue segmentation in fetal MRI with synthetic data" index=64>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, cs-LG, eess-IV, eess.IV  
Keyword Score: 10  
Keywords: Out-of-domain  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15103v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15103v1.pdf" filename="2403.15103v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Segmentation of fetal brain tissue from magnetic resonance imaging (MRI) plays a crucial role in the study of in utero neurodevelopment. However, automated tools face substantial domain shift challenges as they must be robust to highly heterogeneous clinical data, often limited in numbers and lacking annotations. Indeed, high variability of the fetal brain morphology, MRI acquisition parameters, and superresolution reconstruction (SR) algorithms adversely affect the model's performance when evaluated <b>out-of-domain.</b> In this work, we introduce FetalSynthSeg, a domain randomization method to segment fetal brain MRI, inspired by SynthSeg. Our results show that models trained solely on synthetic data outperform models trained on real data in <b>out-ofdomain</b> settings, validated on a 120-subject cross-domain dataset. Furthermore, we extend our evaluation to 40 subjects acquired using lowfield (0.55T) MRI and reconstructed with novel SR models, showcasing robustness across different magnetic field strengths and SR algorithms. Leveraging a generative synthetic approach, we tackle the domain shift problem in fetal brain MRI and offer compelling prospects for applications in fields with limited and highly heterogeneous data.

{{</citation>}}


### (5/5 | 65/202) Subjective Quality Assessment of Compressed Tone-Mapped High Dynamic Range Videos (Abhinau K. Venkataramanan et al., 2024)

{{<citation>}}

Abhinau K. Venkataramanan, Alan C. Bovik. (2024)  
**Subjective Quality Assessment of Compressed Tone-Mapped High Dynamic Range Videos**
<br/>
<button class="copy-to-clipboard" title="Subjective Quality Assessment of Compressed Tone-Mapped High Dynamic Range Videos" index=65>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 6  
Keywords: Benchmarking, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15061v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15061v1.pdf" filename="2403.15061v1.pdf">Download PDF</button>

---


**ABSTRACT**  
High Dynamic Range (HDR) videos are able to represent wider ranges of contrasts and colors than Standard Dynamic Range (SDR) videos, giving more vivid experiences. Due to this, HDR videos are expected to grow into the dominant video modality of the future. However, HDR videos are incompatible with existing SDR displays, which form the majority of affordable consumer displays on the market. Because of this, HDR videos must be processed by tone-mapping them to reduced bit-depths to service a broad swath of SDR-limited video consumers. Here, we analyze the impact of tone-mapping operators on the visual quality of streaming HDR videos. To this end, we built the first large-scale subjectively annotated open-source database of compressed tone-mapped HDR videos, containing 15,000 tone-mapped sequences derived from 40 unique HDR source contents. The videos in the database were labeled with more than 750,000 subjective quality annotations, collected from more than 1,600 unique human observers. We demonstrate the usefulness of the new subjective database by <b>benchmarking</b> objective models of visual quality on it. We envision that the new LIVE Tone-Mapped HDR (LIVE-TMHDR) database will enable significant progress on HDR video tone mapping and quality assessment in the future. To this end, we make the database freely available to the community at https://live.ece.utexas.edu/research/LIVE_TMHDR/index.html

{{</citation>}}


## stat.ML (1)



### (1/1 | 66/202) Contrastive Learning on Multimodal Analysis of Electronic Health Records (Tianxi Cai et al., 2024)

{{<citation>}}

Tianxi Cai, Feiqing Huang, Ryumei Nakada, Linjun Zhang, Doudou Zhou. (2024)  
**Contrastive Learning on Multimodal Analysis of Electronic Health Records**
<br/>
<button class="copy-to-clipboard" title="Contrastive Learning on Multimodal Analysis of Electronic Health Records" index=66>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: cs-LG, stat-ML, stat.ML  
Keyword Score: 61  
Keywords: Contrastive Learning, Multi-modal, Multi-modal, Mutual Information, Representation Learning, Simulation, Simulator, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14926v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14926v1.pdf" filename="2403.14926v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Electronic health record (EHR) systems contain a wealth of <b>multimodal</b> clinical data including structured data like clinical codes and unstructured data such as clinical notes. However, many existing EHR-focused studies has traditionally either concentrated on an individual modality or merged different modalities in a rather rudimentary fashion. This approach often results in the perception of structured and unstructured data as separate entities, neglecting the inherent synergy between them. Specifically, the two important modalities contain clinically relevant, inextricably linked and complementary health information. A more complete picture of a patient's medical history is captured by the joint analysis of the two modalities of data. Despite the great success of <b>multimodal</b> <b>contrastive</b> <b>learning</b> on <b>vision-language,</b> its potential remains under-explored in the realm of <b>multimodal</b> EHR, particularly in terms of its theoretical understanding. To accommodate the statistical analysis of <b>multimodal</b> EHR data, in this paper, we propose a novel <b>multimodal</b> feature embedding generative model and design a <b>multimodal</b> <b>contrastive</b> <b>loss</b> to obtain the <b>multimodal</b> EHR feature <b>representation.</b> <b>Our</b> theoretical analysis demonstrates the effectiveness of <b>multimodal</b> learning compared to single-modality learning and connects the solution of the loss function to the singular value decomposition of a pointwise <b>mutual</b> <b>information</b> matrix. This connection paves the way for a privacy-preserving algorithm tailored for <b>multimodal</b> EHR feature <b>representation</b> <b>learning.</b> <b>Simulation</b> studies show that the proposed algorithm performs well under a variety of configurations. We further validate the clinical utility of the proposed algorithm in real-world EHR data.

{{</citation>}}


## cs.CV (63)



### (1/63 | 67/202) Controlled Training Data Generation with Diffusion Models (Teresa Yeo et al., 2024)

{{<citation>}}

Teresa Yeo, Andrei Atanov, Harold Benoit, Aleksandr Alekseev, Ruchira Ray, Pooya Esmaeil Akhoondi, Amir Zamir. (2024)  
**Controlled Training Data Generation with Diffusion Models**
<br/>
<button class="copy-to-clipboard" title="Controlled Training Data Generation with Diffusion Models" index=67>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CL, cs-CV, cs-LG, cs.CV  
Keyword Score: 60  
Keywords: Diffusion Model, Distribution Shift, Distribution Shift, Supervised Learning, Supervised Learning, Text2image, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15309v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15309v1.pdf" filename="2403.15309v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this work, we present a method to control a <b>text-to-image</b> generative model to produce training data specifically "useful" for <b>supervised</b> <b>learning.</b> Unlike previous works that employ an open-loop approach and pre-define <b>prompts</b> to generate new data using either a language model or human expertise, we develop an automated closed-loop system which involves two feedback mechanisms. The first mechanism uses feedback from a given <b>supervised</b> <b>model</b> and finds adversarial <b>prompts</b> that result in image generations that maximize the model loss. While these adversarial <b>prompts</b> result in diverse data informed by the model, they are not informed of the target <b>distribution,</b> <b>which</b> can be inefficient. Therefore, we introduce the second feedback mechanism that guides the generation process towards a certain target <b>distribution.</b> <b>We</b> call the method combining these two mechanisms Guided Adversarial <b>Prompts.</b> We perform our evaluations on different tasks, datasets and architectures, with different types of <b>distribution</b> <b>shifts</b> (spuriously correlated data, unseen domains) and demonstrate the efficiency of the proposed feedback mechanisms compared to open-loop approaches.

{{</citation>}}


### (2/63 | 68/202) InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding (Yi Wang et al., 2024)

{{<citation>}}

Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang. (2024)  
**InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding**
<br/>
<button class="copy-to-clipboard" title="InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" index=68>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 59  
Keywords: Benchmarking, Contrastive Learning, Foundation Model, Multi-modal, Multi-modal, Weakly-supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15377v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15377v1.pdf" filename="2403.15377v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We introduce InternVideo2, a new video <b>foundation</b> <b>model</b> (ViFM) that achieves the state-of-the-art performance in action recognition, video-text tasks, and video-centric dialogue. Our approach employs a progressive training paradigm that unifies the different self- or <b>weakly-supervised</b> <b>learning</b> <b>frameworks</b> of masked video token reconstruction, cross-modal <b>contrastive</b> <b>learning,</b> and next token prediction. Different training stages would guide our model to capture different levels of structure and semantic information through different pretext tasks. At the data level, we prioritize the spatiotemporal consistency by semantically segmenting videos and generating video-audio-speech captions. This improves the alignment between video and text. We scale both data and model size for our InternVideo2. Through extensive experiments, we validate our designs and demonstrate the state-of-the-art performance on over 60 video and audio tasks. Notably, our model outperforms others on various video-related captioning, dialogue, and long video understanding <b>benchmarks,</b> highlighting its ability to reason and comprehend long temporal contexts. Code and models are available at https://github.com/OpenGVLab/InternVideo2/.

{{</citation>}}


### (3/63 | 69/202) LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models (Yuzhang Shang et al., 2024)

{{<citation>}}

Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, Yan Yan. (2024)  
**LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models**
<br/>
<button class="copy-to-clipboard" title="LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models" index=69>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CL, cs-CV, cs.CV  
Keyword Score: 56  
Keywords: Multi-modal, Multi-modal, Transformer, Question Answering, Reasoning, Visual Question Answering, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15388v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15388v1.pdf" filename="2403.15388v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>Multimodal</b> <b>Models</b> (LMMs) have shown significant <b>reasoning</b> capabilities by connecting a <b>visual</b> <b>encoder</b> <b>and</b> a <b>large</b> <b>language</b> <b>model.</b> LMMs typically use a fixed amount of <b>visual</b> <b>tokens,</b> <b>such</b> as the penultimate layer features in the CLIP <b>visual</b> <b>encoder,</b> <b>as</b> the prefix content. Recent LMMs incorporate more complex <b>visual</b> <b>inputs,</b> <b>such</b> as high-resolution images and videos, which increase the number of <b>visual</b> <b>tokens</b> <b>significantly.</b> However, due to the design of the <b>Transformer</b> architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many <b>visual</b> <b>tokens</b> <b>are</b> spatially redundant. Based on this, we propose PruMerge, a novel adaptive <b>visual</b> <b>token</b> <b>reduction</b> approach, which largely reduces the number of <b>visual</b> <b>tokens</b> <b>while</b> maintaining comparable model performance. We first select the unpruned <b>visual</b> <b>tokens</b> <b>based</b> on their similarity to class tokens and spatial tokens. We then cluster the pruned tokens based on key similarity and merge the clustered tokens with the unpruned tokens to supplement their information. Empirically, when applied to LLaVA-1.5, our approach can compress the <b>visual</b> <b>tokens</b> <b>by</b> 14.4 times on average, and achieve comparable performance across diverse <b>visual</b> <b>question-answering</b> <b>and</b> <b>reasoning</b> tasks. Code and checkpoints are at https://llava-prumerge.github.io/.

{{</citation>}}


### (4/63 | 70/202) CLIP-VQDiffusion : Langauge Free Training of Text To Image generation using CLIP and vector quantized diffusion model (Seungdae Han et al., 2024)

{{<citation>}}

Seungdae Han, Joohee Kim. (2024)  
**CLIP-VQDiffusion : Langauge Free Training of Text To Image generation using CLIP and vector quantized diffusion model**
<br/>
<button class="copy-to-clipboard" title="CLIP-VQDiffusion : Langauge Free Training of Text To Image generation using CLIP and vector quantized diffusion model" index=70>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 56  
Keywords: Diffusion Model, Multi-modal, Multi-modal, Out-of-distribution, Quantization, Text2image, Text2image  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14944v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14944v1.pdf" filename="2403.14944v1.pdf">Download PDF</button>

---


**ABSTRACT**  
There has been a significant progress in text conditional image generation models. Recent advancements in this field depend not only on improvements in model structures, but also vast quantities of <b>text-image</b> paired datasets. However, creating these kinds of datasets is very costly and requires a substantial amount of labor. Famous face datasets don't have corresponding text captions, making it difficult to develop text conditional image generation models on these datasets. Some research has focused on developing text to image generation models using only images without text captions. Here, we propose CLIP-VQDiffusion, which leverage the pretrained CLIP model to provide <b>multimodal</b> <b>text-image</b> representations and strong image generation capabilities. On the FFHQ dataset, our model outperformed previous state-of-the-art methods by 4.4% in clipscore and generated very realistic images even when the text was both in and out of distribution. The pretrained models and codes will soon be available at https://github.com/INFINIQ-AI1/CLIPVQDiffusion

{{</citation>}}


### (5/63 | 71/202) Cell Variational Information Bottleneck Network (Zhonghua Zhai et al., 2024)

{{<citation>}}

Zhonghua Zhai, Chen Ju, Jinsong Lan, Shuai Xiao. (2024)  
**Cell Variational Information Bottleneck Network**
<br/>
<button class="copy-to-clipboard" title="Cell Variational Information Bottleneck Network" index=71>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 55  
Keywords: Face Recognition, MNIST, Convolution, Convolutional Neural Network, Mutual Information, Representation Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15082v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15082v1.pdf" filename="2403.15082v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this work, we propose Cell Variational Information Bottleneck Network (cellVIB), a <b>convolutional</b> <b>neural</b> <b>network</b> using information bottleneck mechanism, which can be combined with the latest feedforward network architecture in an end-to-end training method. Our Cell Variational Information Bottleneck Network is constructed by stacking VIB cells, which generate feature maps with uncertainty. As layers going deeper, the regularization effect will gradually increase, instead of directly adding excessive regular constraints to the output layer of the model as in Deep VIB. Under each VIB cell, the feedforward process learns an independent mean term and an standard deviation term, and predicts the Gaussian distribution based on them. The feedback process is based on reparameterization trick for effective training. This work performs an extensive analysis on <b>MNIST</b> dataset to verify the effectiveness of each VIB cells, and provides an insightful analysis on how the VIB cells affect <b>mutual</b> <b>information.</b> Experiments conducted on CIFAR-10 also prove that our cellVIB is robust against noisy labels during training and against corrupted images during testing. Then, we validate our method on PACS dataset, whose results show that the VIB cells can significantly improve the generalization performance of the basic model. Finally, in a more complex <b>representation</b> <b>learning</b> task, <b>face</b> <b>recognition,</b> our network structure has also achieved very competitive results.

{{</citation>}}


### (6/63 | 72/202) Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning (Bumsoo Kim et al., 2024)

{{<citation>}}

Bumsoo Kim, Wonseop Shin, Kyuchul Lee, Sanghyun Seo. (2024)  
**Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning**
<br/>
<button class="copy-to-clipboard" title="Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning" index=72>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs-MM, cs.CV  
Keyword Score: 50  
Keywords: Fine-tuning, Hallucination Detection, Text2image, In-context Learning, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15048v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15048v1.pdf" filename="2403.15048v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Large-scale <b>Text-to-Image</b> (TTI) models have become a common approach for generating training data in various generative fields. However, visual <b>hallucinations,</b> <b>which</b> contain perceptually critical defects, remain a concern, especially in non-photorealistic styles like cartoon characters. We propose a novel visual <b>hallucination</b> <b>detection</b> system for cartoon character images generated by TTI models. Our approach leverages pose-aware <b>in-context</b> visual learning (PA-ICVL) with <b>Vision-Language</b> Models (VLMs), utilizing both RGB images and pose information. By incorporating pose guidance from a <b>fine-tuned</b> pose estimator, we enable VLMs to make more accurate decisions. Experimental results demonstrate significant improvements in identifying visual <b>hallucinations</b> <b>compared</b> to baseline methods relying solely on RGB images. This research advances TTI models by mitigating visual <b>hallucinations,</b> <b>expanding</b> their potential in non-photorealistic domains.

{{</citation>}}


### (7/63 | 73/202) BSNet: Box-Supervised Simulation-assisted Mean Teacher for 3D Instance Segmentation (Jiahao Lu et al., 2024)

{{<citation>}}

Jiahao Lu, Jiacheng Deng, Tianzhu Zhang. (2024)  
**BSNet: Box-Supervised Simulation-assisted Mean Teacher for 3D Instance Segmentation**
<br/>
<button class="copy-to-clipboard" title="BSNet: Box-Supervised Simulation-assisted Mean Teacher for 3D Instance Segmentation" index=73>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 50  
Keywords: Simulation, Simulator, Supervised Learning, Weakly-supervised Learning, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15019v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15019v1.pdf" filename="2403.15019v1.pdf">Download PDF</button>

---


**ABSTRACT**  
3D instance segmentation (3DIS) is a crucial task, but point-level annotations are tedious in fully <b>supervised</b> settings. Thus, using bounding boxes (bboxes) as annotations has shown great potential. The current mainstream approach is a two-step process, involving the generation of pseudo-labels from box annotations and the training of a 3DIS network with the pseudo-labels. However, due to the presence of intersections among bboxes, not every point has a determined instance label, especially in overlapping areas. To generate higher quality pseudo-labels and achieve more precise weakly <b>supervised</b> 3DIS results, we propose the Box-Supervised <b>Simulation-assisted</b> Mean Teacher for 3D Instance Segmentation (BSNet), which devises a novel pseudo-labeler called <b>Simulation-assisted</b> <b>Transformer.</b> The labeler consists of two main components. The first is <b>Simulation-assisted</b> Mean Teacher, which introduces Mean Teacher for the first time in this task and constructs simulated samples to assist the labeler in acquiring prior knowledge about overlapping areas. To better model local-global structure, we also propose Local-Global Aware Attention as the decoder for teacher and student labelers. Extensive experiments conducted on the ScanNetV2 and S3DIS datasets verify the superiority of our designs. Code is available at \href{https://github.com/peoplelu/BSNet}{https://github.com/peoplelu/BSNet}.

{{</citation>}}


### (8/63 | 74/202) ParFormer: Vision Transformer Baseline with Parallel Local Global Token Mixer and Convolution Attention Patch Embedding (Novendra Setyawan et al., 2024)

{{<citation>}}

Novendra Setyawan, Ghufron Wahyu Kurniawan, Chi-Chia Sun, Jun-Wei Hsieh, Hui-Kai Su, Wen-Kai Kuo. (2024)  
**ParFormer: Vision Transformer Baseline with Parallel Local Global Token Mixer and Convolution Attention Patch Embedding**
<br/>
<button class="copy-to-clipboard" title="ParFormer: Vision Transformer Baseline with Parallel Local Global Token Mixer and Convolution Attention Patch Embedding" index=74>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 50  
Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Transformer, Vision Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15004v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15004v1.pdf" filename="2403.15004v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This work presents ParFormer as an enhanced <b>transformer</b> architecture that allows the incorporation of different token mixers into a single stage, hence improving feature extraction capabilities. Integrating both local and global data allows for precise representation of short- and long-range spatial relationships without the need for computationally intensive methods such as shifting windows. Along with the parallel token mixer encoder, We offer the <b>Convolutional</b> Attention Patch Embedding (CAPE) as an enhancement of standard patch embedding to improve token mixer extraction with a <b>convolutional</b> attention module. Our comprehensive evaluation demonstrates that our ParFormer outperforms <b>CNN-based</b> and state-of-the-art <b>transformer-based</b> architectures in image classification and several complex tasks such as object recognition. The proposed CAPE has been demonstrated to benefit the overall MetaFormer architecture, even while utilizing the Identity Mapping Token Mixer, resulting in a 0.5\% increase in accuracy. The ParFormer models outperformed ConvNeXt and Swin <b>Transformer</b> for the pure <b>convolution</b> and <b>transformer</b> model in accuracy. Furthermore, our model surpasses the current leading hybrid <b>transformer</b> by reaching competitive Top-1 scores in the ImageNet-1K classification test. Specifically, our model variants with 11M, 23M, and 34M parameters achieve scores of 80.4\%, 82.1\%, and 83.1\%, respectively. Code: https://github.com/novendrastywn/ParFormer-CAPE-2024

{{</citation>}}


### (9/63 | 75/202) Geometric Generative Models based on Morphological Equivariant PDEs and GANs (El Hadji S. Diop et al., 2024)

{{<citation>}}

El Hadji S. Diop, Thierno Fall, Alioune Mbengue, Mohamed Daoudi. (2024)  
**Geometric Generative Models based on Morphological Equivariant PDEs and GANs**
<br/>
<button class="copy-to-clipboard" title="Geometric Generative Models based on Morphological Equivariant PDEs and GANs" index=75>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV, eess-IV, math-DG  
Keyword Score: 50  
Keywords: MNIST, Convolution, Convolutional Neural Network, Generative Adversarial Network, Generative Adversarial Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14897v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14897v1.pdf" filename="2403.14897v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Content and image generation consist in creating or generating data from noisy information by extracting specific features such as texture, edges, and other thin image structures. We are interested here in <b>generative</b> <b>models,</b> <b>and</b> two main problems are addressed. Firstly, the improvements of specific feature extraction while accounting at multiscale levels intrinsic geometric features; and secondly, the equivariance of the network to reduce its complexity and provide a geometric interpretability. To proceed, we propose a geometric <b>generative</b> <b>model</b> <b>based</b> on an equivariant partial differential equation (PDE) for group <b>convolution</b> neural networks (G-CNNs), so called PDE-G-CNNs, built on morphology operators and <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs).</b> Equivariant morphological PDE layers are composed of multiscale dilations and erosions formulated in Riemannian manifolds, while group symmetries are defined on a Lie group. We take advantage of the Lie group structure to properly integrate the equivariance in layers, and are able to use the Riemannian metric to solve the multiscale morphological operations. Each point of the Lie group is associated with a unique point in the manifold, which helps us derive a metric on the Riemannian manifold from a tensor field invariant under the Lie group so that the induced metric has the same symmetries. The proposed geometric morphological <b>GAN</b> (GM-GAN) is obtained by using the proposed morphological equivariant <b>convolutions</b> in PDE-G-CNNs to bring nonlinearity in classical <b>CNNs.</b> GM-GAN is evaluated on <b>MNIST</b> data and compared with <b>GANs.</b> Preliminary results show that GM-GAN model outperforms classical <b>GAN.</b>

{{</citation>}}


### (10/63 | 76/202) Trajectory Regularization Enhances Self-Supervised Geometric Representation (Jiayun Wang et al., 2024)

{{<citation>}}

Jiayun Wang, Stella X. Yu, Yubei Chen. (2024)  
**Trajectory Regularization Enhances Self-Supervised Geometric Representation**
<br/>
<button class="copy-to-clipboard" title="Trajectory Regularization Enhances Self-Supervised Geometric Representation" index=76>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 48  
Keywords: Benchmarking, Out-of-distribution, Representation Learning, Self-supervised Learning, Self-supervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14973v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14973v1.pdf" filename="2403.14973v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Self-supervised</b> <b>learning</b> (SSL) has proven effective in learning high-quality <b>representations</b> <b>for</b> various downstream tasks, with a primary focus on semantic tasks. However, its application in geometric tasks remains underexplored, partially due to the absence of a standardized evaluation method for geometric <b>representations.</b> <b>To</b> address this gap, we introduce a new pose-estimation <b>benchmark</b> for assessing SSL geometric <b>representations,</b> <b>which</b> demands training without semantic or pose labels and achieving proficiency in both semantic and geometric downstream tasks. On this <b>benchmark,</b> we study enhancing SSL geometric <b>representations</b> <b>without</b> sacrificing semantic classification accuracy. We find that leveraging mid-layer <b>representations</b> <b>improves</b> pose-estimation performance by 10-20%. Further, we introduce an <b>unsupervised</b> trajectory-regularization loss, which improves performance by an additional 4% and improves generalization ability on <b>out-of-distribution</b> data. We hope the proposed <b>benchmark</b> and methods offer new insights and improvements in <b>self-supervised</b> <b>geometric</b> <b>representation</b> <b>learning.</b>

{{</citation>}}


### (11/63 | 77/202) Selectively Informative Description can Reduce Undesired Embedding Entanglements in Text-to-Image Personalization (Jimyeong Kim et al., 2024)

{{<citation>}}

Jimyeong Kim, Jungwon Park, Wonjong Rhee. (2024)  
**Selectively Informative Description can Reduce Undesired Embedding Entanglements in Text-to-Image Personalization**
<br/>
<button class="copy-to-clipboard" title="Selectively Informative Description can Reduce Undesired Embedding Entanglements in Text-to-Image Personalization" index=77>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 46  
Keywords: Multi-modal, Multi-modal, GPT, GPT-4, Text2image, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15330v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15330v1.pdf" filename="2403.15330v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In <b>text-to-image</b> personalization, a timely and crucial challenge is the tendency of generated images overfitting to the biases present in the reference images. We initiate our study with a comprehensive categorization of the biases into background, nearby-object, tied-object, substance (in style re-contextualization), and pose biases. These biases manifest in the generated images due to their entanglement into the subject embedding. This undesired embedding entanglement not only results in the reflection of biases from the reference images into the generated images but also notably diminishes the alignment of the generated images with the given generation <b>prompt.</b> To address this challenge, we propose SID~(Selectively Informative Description), a text description strategy that deviates from the prevalent approach of only characterizing the subject's class identification. SID is generated utilizing <b>multimodal</b> <b>GPT-4</b> and can be seamlessly integrated into optimization-based models. We present comprehensive experimental results along with analyses of cross-attention maps, subject-alignment, non-subject-disentanglement, and text-alignment.

{{</citation>}}


### (12/63 | 78/202) GCN-DevLSTM: Path Development for Skeleton-Based Action Recognition (Lei Jiang et al., 2024)

{{<citation>}}

Lei Jiang, Weixin Yang, Xin Zhang, Hao Ni. (2024)  
**GCN-DevLSTM: Path Development for Skeleton-Based Action Recognition**
<br/>
<button class="copy-to-clipboard" title="GCN-DevLSTM: Path Development for Skeleton-Based Action Recognition" index=78>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 43  
Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, LSTM  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15212v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15212v1.pdf" filename="2403.15212v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Skeleton-based action recognition (SAR) in videos is an important but challenging task in computer vision. The recent state-of-the-art models for SAR are primarily based on <b>graph</b> <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(GCNs),</b> which are powerful in extracting the spatial information of skeleton data. However, it is yet clear that such <b>GCN-based</b> models can effectively capture the temporal dynamics of human action sequences. To this end, we propose the DevLSTM module, which exploits the path development -- a principled and parsimonious representation for sequential data by leveraging the Lie group structure. The path development, originated from Rough path theory, can effectively capture the order of events in high-dimensional stream data with massive dimension reduction and consequently enhance the <b>LSTM</b> module substantially. Our proposed G-DevLSTM module can be conveniently plugged into the temporal <b>graph,</b> complementing existing advanced <b>GCN-based</b> models. Our empirical studies on the NTU60, NTU120 and Chalearn2013 datasets demonstrate that our proposed hybrid model significantly outperforms the current best-performing methods in SAR tasks. The code is available at https://github.com/DeepIntoStreams/GCN-DevLSTM.

{{</citation>}}


### (13/63 | 79/202) MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral Pedestrian Detection (Taeheon Kim et al., 2024)

{{<citation>}}

Taeheon Kim, Sangyun Chung, Damin Yeom, Youngjoon Yu, Hak Gu Kim, Yong Man Ro. (2024)  
**MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral Pedestrian Detection**
<br/>
<button class="copy-to-clipboard" title="MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral Pedestrian Detection" index=79>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 43  
Keywords: Multi-modal, Reasoning, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15209v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15209v1.pdf" filename="2403.15209v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Multispectral pedestrian detection is attractive for around-the-clock applications due to the complementary information between RGB and thermal modalities. However, current models often fail to detect pedestrians in obvious cases, especially due to the modality bias learned from statistically biased datasets. From these problems, we anticipate that maybe understanding the complementary information itself is difficult to achieve from vision-only models. Accordingly, we propose a novel Multispectral Chain-of-Thought Detection (MSCoTDet) framework, which incorporates <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to understand the complementary information at the semantic level and further enhance the fusion process. Specifically, we generate text descriptions of the pedestrian in each RGB and thermal modality and design a Multispectral Chain-of-Thought (MSCoT) <b>prompting,</b> which models a step-by-step process to facilitate cross-modal <b>reasoning</b> at the semantic level and perform accurate detection. Moreover, we design a Language-driven <b>Multi-modal</b> Fusion (LMF) strategy that enables fusing vision-driven and language-driven detections. Extensive experiments validate that MSCoTDet improves multispectral pedestrian detection.

{{</citation>}}


### (14/63 | 80/202) Long-CLIP: Unlocking the Long-Text Capability of CLIP (Beichen Zhang et al., 2024)

{{<citation>}}

Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Jiaqi Wang. (2024)  
**Long-CLIP: Unlocking the Long-Text Capability of CLIP**
<br/>
<button class="copy-to-clipboard" title="Long-CLIP: Unlocking the Long-Text Capability of CLIP" index=80>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Fine-tuning, Zero-shot, Text2image, Text2image  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15378v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15378v1.pdf" filename="2403.15378v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for <b>zero-shot</b> classification, <b>text-image</b> retrieval, and <b>text-image</b> generation by aligning image and text modalities. Despite its widespread adoption, a significant limitation of CLIP lies in the inadequate length of text input. The length of the text token is restricted to 77, and an empirical study shows the actual effective length is even less than 20. This prevents CLIP from handling detailed descriptions, limiting its applications for image retrieval and <b>text-to-image</b> generation with extensive prerequisites. To this end, we propose Long-CLIP as a plug-and-play alternative to CLIP that supports long-text input, retains or even surpasses its <b>zero-shot</b> generalizability, and aligns the CLIP latent space, making it readily replace CLIP without any further adaptation in downstream frameworks. Nevertheless, achieving this goal is far from straightforward, as simplistic <b>fine-tuning</b> can result in a significant degradation of CLIP's performance. Moreover, substituting the text encoder with a language model supporting longer contexts necessitates pretraining with vast amounts of data, incurring significant expenses. Accordingly, Long-CLIP introduces an efficient <b>fine-tuning</b> solution on CLIP with two novel strategies designed to maintain the original capabilities, including (1) a knowledge-preserved stretching of positional embedding and (2) a primary component matching of CLIP features. With leveraging just one million extra long <b>text-image</b> pairs, Long-CLIP has shown the superiority to CLIP for about 20% in long caption <b>text-image</b> retrieval and 6% in traditional <b>text-image</b> retrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers enhanced capabilities for generating images from detailed text descriptions by replacing CLIP in a plug-and-play manner.

{{</citation>}}


### (15/63 | 81/202) Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection (Hongzhi Gao et al., 2024)

{{<citation>}}

Hongzhi Gao, Zheng Chen, Zehui Chen, Lin Chen, Jiaming Liu, Shanghang Zhang, Feng Zhao. (2024)  
**Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection**
<br/>
<button class="copy-to-clipboard" title="Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection" index=81>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Object Detection, Self-supervised Learning, Self-supervised Learning, Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15317v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15317v1.pdf" filename="2403.15317v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Training high-accuracy 3D detectors necessitates massive labeled 3D annotations with 7 degree-of-freedom, which is laborious and time-consuming. Therefore, the form of point annotations is proposed to offer significant prospects for practical applications in 3D detection, which is not only more accessible and less expensive but also provides strong spatial information for <b>object</b> <b>localization.In</b> this paper, we empirically discover that it is non-trivial to merely adapt Point-DETR to its 3D form, encountering two main bottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it generates low-quality pseudo labels in distant regions due to the extreme sparsity of LiDAR points. To overcome these challenges, we introduce Point-DETR3D, a teacher-student framework for weakly semi-supervised 3D detection, designed to fully capitalize on point-wise supervision within a constrained instance-wise annotation budget.Different from Point-DETR which encodes 3D positional information solely through a point encoder, we propose an explicit positional query initialization strategy to enhance the positional prior. Considering the low quality of pseudo labels at distant regions produced by the teacher model, we enhance the detector's perception by incorporating dense imagery data through a novel Cross-Modal Deformable RoI Fusion (D-RoI).Moreover, an innovative point-guided <b>self-supervised</b> <b>learning</b> technique is proposed to allow for fully exploiting point priors, even in student models.Extensive experiments on representative nuScenes dataset demonstrate our Point-DETR3D obtains significant improvements compared to previous works. Notably, with only 5% of labeled data, Point-DETR3D achieves over 90% performance of its fully <b>supervised</b> counterpart.

{{</citation>}}


### (16/63 | 82/202) DreamFlow: High-Quality Text-to-3D Generation by Approximating Probability Flow (Kyungmin Lee et al., 2024)

{{<citation>}}

Kyungmin Lee, Kihyuk Sohn, Jinwoo Shin. (2024)  
**DreamFlow: High-Quality Text-to-3D Generation by Approximating Probability Flow**
<br/>
<button class="copy-to-clipboard" title="DreamFlow: High-Quality Text-to-3D Generation by Approximating Probability Flow" index=82>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Diffusion Model, Knowledge Distillation, Knowledge Distillation, Text2image  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14966v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14966v1.pdf" filename="2403.14966v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent progress in text-to-3D generation has been achieved through the utilization of score <b>distillation</b> methods: they make use of the pre-trained <b>text-to-image</b> (T2I) <b>diffusion</b> <b>models</b> by <b>distilling</b> via the <b>diffusion</b> <b>model</b> training objective. However, such an approach inevitably results in the use of random timesteps at each update, which increases the variance of the gradient and ultimately prolongs the optimization process. In this paper, we propose to enhance the text-to-3D optimization by leveraging the T2I <b>diffusion</b> <b>prior</b> in the generative sampling process with a predetermined timestep schedule. To this end, we interpret text-to3D optimization as a multi-view image-to-image translation problem, and propose a solution by approximating the probability flow. By leveraging the proposed novel optimization algorithm, we design DreamFlow, a practical three-stage coarseto-fine text-to-3D optimization framework that enables fast generation of highquality and high-resolution (i.e., 1024x1024) 3D contents. For example, we demonstrate that DreamFlow is 5 times faster than the existing state-of-the-art text-to-3D method, while producing more photorealistic 3D contents. Visit our project page (https://kyungmnlee.github.io/dreamflow.github.io/) for visualizations.

{{</citation>}}


### (17/63 | 83/202) STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians (Yifei Zeng et al., 2024)

{{<citation>}}

Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, Yao Yao. (2024)  
**STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians**
<br/>
<button class="copy-to-clipboard" title="STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians" index=83>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Diffusion Model, Fine-tuning, Knowledge Distillation, Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14939v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14939v1.pdf" filename="2403.14939v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent progress in pre-trained <b>diffusion</b> <b>models</b> and 3D generation have spurred interest in 4D content creation. However, achieving high-fidelity 4D generation with spatial-temporal consistency remains a challenge. In this work, we propose STAG4D, a novel framework that combines pre-trained <b>diffusion</b> <b>models</b> with dynamic 3D Gaussian splatting for high-fidelity 4D generation. Drawing inspiration from 3D generation techniques, we utilize a multi-view <b>diffusion</b> <b>model</b> to initialize multi-view images anchoring on the input video frames, where the video can be either real-world captured or generated by a video <b>diffusion</b> <b>model.</b> To ensure the temporal consistency of the multi-view sequence initialization, we introduce a simple yet effective fusion strategy to leverage the first frame as a temporal anchor in the <b>self-attention</b> computation. With the almost consistent multi-view sequences, we then apply the score <b>distillation</b> sampling to optimize the 4D Gaussian point cloud. The 4D Gaussian spatting is specially crafted for the generation task, where an adaptive densification strategy is proposed to mitigate the unstable Gaussian gradient for robust optimization. Notably, the proposed pipeline does not require any pre-training or <b>fine-tuning</b> of <b>diffusion</b> <b>networks,</b> offering a more accessible and practical solution for the 4D generation task. Extensive experiments demonstrate that our method outperforms prior 4D generation works in rendering quality, spatial-temporal consistency, and generation robustness, setting a new state-of-the-art for 4D generation from diverse inputs, including text, image, and video.

{{</citation>}}


### (18/63 | 84/202) Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks (Sudhir Sornapudi et al., 2024)

{{<citation>}}

Sudhir Sornapudi, Rajhans Singh. (2024)  
**Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks**
<br/>
<button class="copy-to-clipboard" title="Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks" index=84>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV, eess-IV  
Keyword Score: 35  
Keywords: Contrastive Learning, Representation Learning, Self-supervised Learning, Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15248v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15248v1.pdf" filename="2403.15248v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Computer vision in agriculture is game-changing with its ability to transform farming into a data-driven, precise, and sustainable industry. Deep learning has empowered agriculture vision to analyze vast, complex visual data, but heavily rely on the availability of large annotated datasets. This remains a bottleneck as manual labeling is error-prone, time-consuming, and expensive. The lack of efficient labeling approaches inspired us to consider <b>self-supervised</b> <b>learning</b> as a paradigm shift, learning meaningful feature <b>representations</b> <b>from</b> raw agricultural image data. In this work, we explore how <b>self-supervised</b> <b>representation</b> <b>learning</b> unlocks the potential applicability to diverse agriculture vision tasks by eliminating the need for large-scale annotated datasets. We propose a lightweight framework utilizing SimCLR, a <b>contrastive</b> <b>learning</b> approach, to pre-train a ResNet-50 backbone on a large, unannotated dataset of real-world agriculture field images. Our experimental analysis and results indicate that the model learns robust features applicable to a broad range of downstream agriculture tasks discussed in the paper. Additionally, the reduced reliance on annotated data makes our approach more cost-effective and accessible, paving the way for broader adoption of computer vision in agriculture.

{{</citation>}}


### (19/63 | 85/202) SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series (Badri N. Patro et al., 2024)

{{<citation>}}

Badri N. Patro, Vijay S. Agneeswaran. (2024)  
**SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series**
<br/>
<button class="copy-to-clipboard" title="SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series" index=85>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs-SY, cs.CV, eess-IV, eess-SY  
Keyword Score: 33  
Keywords: Benchmarking, Convolution, Transfer Learning, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15360v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15360v1.pdf" filename="2403.15360v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Transformers</b> have widely adopted attention networks for sequence mixing and MLPs for channel mixing, playing a pivotal role in achieving breakthroughs across domains. However, recent literature highlights issues with attention networks, including low inductive bias and quadratic complexity concerning input sequence length. State Space Models (SSMs) like S4 and others (Hippo, Global <b>Convolutions,</b> liquid S4, LRU, Mega, and Mamba), have emerged to address the above issues to help handle longer sequence lengths. Mamba, while being the state-of-the-art SSM, has a stability issue when scaled to large networks for computer vision datasets. We propose SiMBA, a new architecture that introduces Einstein FFT (EinFFT) for channel modeling by specific eigenvalue computations and uses the Mamba block for sequence modeling. Extensive performance studies across image and time-series <b>benchmarks</b> demonstrate that SiMBA outperforms existing SSMs, bridging the performance gap with state-of-the-art <b>transformers.</b> Notably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet and <b>transfer</b> <b>learning</b> <b>benchmarks</b> such as Stanford Car and Flower as well as task learning <b>benchmarks</b> as well as seven time series <b>benchmark</b> datasets. The project page is available on this website ~\url{https://github.com/badripatro/Simba}.

{{</citation>}}


### (20/63 | 86/202) Piecewise-Linear Manifolds for Deep Metric Learning (Shubhang Bhatnagar et al., 2024)

{{<citation>}}

Shubhang Bhatnagar, Narendra Ahuja. (2024)  
**Piecewise-Linear Manifolds for Deep Metric Learning**
<br/>
<button class="copy-to-clipboard" title="Piecewise-Linear Manifolds for Deep Metric Learning" index=86>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs.CV, eess-IV  
Keyword Score: 33  
Keywords: Benchmarking, Supervised Learning, Unsupervised Learning, Zero-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14977v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14977v1.pdf" filename="2403.14977v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Unsupervised</b> deep metric learning (UDML) focuses on learning a semantic representation space using only unlabeled data. This challenging problem requires accurately estimating the similarity between data points, which is used to supervise a deep network. For this purpose, we propose to model the high-dimensional data manifold using a piecewise-linear approximation, with each low-dimensional linear piece approximating the data manifold in a small neighborhood of a point. These neighborhoods are used to estimate similarity between data points. We empirically show that this similarity estimate correlates better with the ground truth than the similarity estimates of current state-of-the-art techniques. We also show that proxies, commonly used in <b>supervised</b> metric learning, can be used to model the piecewise-linear manifold in an <b>unsupervised</b> setting, helping improve performance. Our method outperforms existing <b>unsupervised</b> metric learning approaches on standard <b>zero-shot</b> image retrieval <b>benchmarks.</b>

{{</citation>}}


### (21/63 | 87/202) Augmented Reality based Simulated Data (ARSim) with multi-view consistency for AV perception networks (Aqeel Anwar et al., 2024)

{{<citation>}}

Aqeel Anwar, Tae Eun Choe, Zian Wang, Sanja Fidler, Minwoo Park. (2024)  
**Augmented Reality based Simulated Data (ARSim) with multi-view consistency for AV perception networks**
<br/>
<button class="copy-to-clipboard" title="Augmented Reality based Simulated Data (ARSim) with multi-view consistency for AV perception networks" index=87>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs-RO, cs.CV  
Keyword Score: 30  
Keywords: Simulation, Simulator, Domain Adaptation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15370v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15370v1.pdf" filename="2403.15370v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Detecting a diverse range of objects under various driving scenarios is essential for the effectiveness of autonomous driving systems. However, the real-world data collected often lacks the necessary diversity presenting a long-tail distribution. Although synthetic data has been utilized to overcome this issue by generating virtual scenes, it faces hurdles such as a significant <b>domain</b> <b>gap</b> and the substantial efforts required from 3D artists to create realistic environments. To overcome these challenges, we present ARSim, a fully automated, comprehensive, modular framework designed to enhance real multi-view image data with 3D synthetic objects of interest. The proposed method integrates <b>domain</b> <b>adaptation</b> and randomization strategies to address covariate shift between real and simulated data by inferring essential <b>domain</b> <b>attributes</b> from real data and employing <b>simulation-based</b> randomization for other attributes. We construct a simplified virtual scene using real data and strategically place 3D synthetic assets within it. Illumination is achieved by estimating light distribution from multiple images capturing the surroundings of the vehicle. Camera parameters from real data are employed to render synthetic assets in each frame. The resulting augmented multi-view consistent dataset is used to train a multi-camera perception network for autonomous vehicles. Experimental results on various AV perception tasks demonstrate the superior performance of networks trained on the augmented dataset.

{{</citation>}}


### (22/63 | 88/202) Shadow Generation for Composite Image Using Diffusion model (Qingyang Liu et al., 2024)

{{<citation>}}

Qingyang Liu, Junqi You, Jianting Wang, Xinhao Tao, Bo Zhang, Li Niu. (2024)  
**Shadow Generation for Composite Image Using Diffusion model**
<br/>
<button class="copy-to-clipboard" title="Shadow Generation for Composite Image Using Diffusion model" index=88>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: ControlNet, Diffusion Model, Foundation Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15234v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15234v1.pdf" filename="2403.15234v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In the realm of image composition, generating realistic shadow for the inserted foreground remains a formidable challenge. Previous works have developed image-to-image translation models which are trained on paired training data. However, they are struggling to generate shadows with accurate shapes and intensities, hindered by data scarcity and inherent task complexity. In this paper, we resort to <b>foundation</b> <b>model</b> with rich prior knowledge of natural shadow images. Specifically, we first adapt <b>ControlNet</b> to our task and then propose intensity modulation modules to improve the shadow intensity. Moreover, we extend the small-scale DESOBA dataset to DESOBAv2 using a novel data acquisition pipeline. Experimental results on both DESOBA and DESOBAv2 datasets as well as real composite images demonstrate the superior capability of our model for shadow generation task. The dataset, code, and model are released at https://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBAv2.

{{</citation>}}


### (23/63 | 89/202) Transfer CLIP for Generalizable Image Denoising (Jun Cheng et al., 2024)

{{<citation>}}

Jun Cheng, Dong Liang, Shan Tan. (2024)  
**Transfer CLIP for Generalizable Image Denoising**
<br/>
<button class="copy-to-clipboard" title="Transfer CLIP for Generalizable Image Denoising" index=89>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV, eess-IV  
Keyword Score: 30  
Keywords: Out-of-distribution, Self-supervised Learning, Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15132v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15132v1.pdf" filename="2403.15132v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Image denoising is a fundamental task in computer vision. While prevailing deep learning-based <b>supervised</b> and <b>self-supervised</b> methods have excelled in eliminating in-distribution noise, their susceptibility to <b>out-of-distribution</b> (OOD) noise remains a significant challenge. The recent emergence of contrastive language-image pre-training (CLIP) model has showcased exceptional capabilities in open-world image recognition and segmentation. Yet, the potential for leveraging CLIP to enhance the robustness of low-level tasks remains largely unexplored. This paper uncovers that certain dense features extracted from the frozen ResNet image encoder of CLIP exhibit distortion-invariant and content-related properties, which are highly desirable for generalizable denoising. Leveraging these properties, we devise an asymmetrical encoder-decoder denoising network, which incorporates dense features including the noisy image and its multi-scale features from the frozen ResNet encoder of CLIP into a learnable image decoder to achieve generalizable denoising. The progressive feature augmentation strategy is further proposed to mitigate feature overfitting and improve the robustness of the learnable decoder. Extensive experiments and comparisons conducted across diverse OOD noises, including synthetic noise, real-world sRGB noise, and low-dose CT image noise, demonstrate the superior generalization ability of our method.

{{</citation>}}


### (24/63 | 90/202) Continual Vision-and-Language Navigation (Seongjun Jeong et al., 2024)

{{<citation>}}

Seongjun Jeong, Gi-Cheon Kang, Seongho Choi, Joochan Kim, Byoung-Tak Zhang. (2024)  
**Continual Vision-and-Language Navigation**
<br/>
<button class="copy-to-clipboard" title="Continual Vision-and-Language Navigation" index=90>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Continual Learning, Perplexity, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15049v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15049v1.pdf" filename="2403.15049v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Vision-and-Language</b> Navigation (VLN) agents navigate to a destination using natural language instructions and the visual information they observe. Existing methods for training VLN agents presuppose fixed datasets, leading to a significant limitation: the introduction of new environments necessitates retraining with previously encountered environments to preserve their knowledge. This makes it difficult to train VLN agents that operate in the ever-changing real world. To address this limitation, we present the <b>Continual</b> <b>Vision-and-Language</b> Navigation (CVLN) paradigm, designed to evaluate agents trained through a <b>continual</b> <b>learning</b> process. For the training and evaluation of CVLN agents, we re-arrange existing VLN datasets to propose two datasets: CVLN-I, focused on navigation via initial-instruction interpretation, and CVLN-D, aimed at navigation through dialogue with other agents. Furthermore, we propose two novel rehearsal-based methods for CVLN, <b>Perplexity</b> Replay (PerpR) and Episodic Self-Replay (ESR). PerpR prioritizes replaying challenging episodes based on action <b>perplexity,</b> while ESR replays previously predicted action logits to preserve learned behaviors. We demonstrate the effectiveness of the proposed methods on CVLN through extensive experiments.

{{</citation>}}


### (25/63 | 91/202) Vehicle Detection Performance in Nordic Region (Hamam Mokayed et al., 2024)

{{<citation>}}

Hamam Mokayed, Rajkumar Saini, Oluwatosin Adewumi, Lama Alkhaled, Bjorn Backe, Palaiahnakote Shivakumara, Olle Hagner, Yan Chai Hum. (2024)  
**Vehicle Detection Performance in Nordic Region**
<br/>
<button class="copy-to-clipboard" title="Vehicle Detection Performance in Nordic Region" index=91>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 30  
Keywords: Data Augmentation, Transfer Learning, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15017v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15017v1.pdf" filename="2403.15017v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper addresses the critical challenge of vehicle detection in the harsh winter conditions in the Nordic regions, characterized by heavy snowfall, reduced visibility, and low lighting. Due to their susceptibility to environmental distortions and occlusions, traditional vehicle detection methods have struggled in these adverse conditions. The advanced proposed deep learning architectures brought promise, yet the unique difficulties of detecting vehicles in Nordic winters remain inadequately addressed. This study uses the Nordic Vehicle Dataset (NVD), which has UAV images from northern Sweden, to evaluate the performance of state-of-the-art vehicle detection algorithms under challenging weather conditions. Our methodology includes a comprehensive evaluation of single-stage, two-stage, and <b>transformer-based</b> detectors against the NVD. We propose a series of enhancements tailored to each detection framework, including <b>data</b> <b>augmentation,</b> hyperparameter tuning, <b>transfer</b> <b>learning,</b> and novel strategies designed explicitly for the DETR model. Our findings not only highlight the limitations of current detection systems in the Nordic environment but also offer promising directions for enhancing these algorithms for improved robustness and accuracy in vehicle detection amidst the complexities of winter landscapes. The code and the dataset are available at https://nvd.ltu-ai.dev

{{</citation>}}


### (26/63 | 92/202) Improve Cross-domain Mixed Sampling with Guidance Training for Adaptive Segmentation (Wenlve Zhou et al., 2024)

{{<citation>}}

Wenlve Zhou, Zhiheng Zhou, Tianlei Wang, Delu Zeng. (2024)  
**Improve Cross-domain Mixed Sampling with Guidance Training for Adaptive Segmentation**
<br/>
<button class="copy-to-clipboard" title="Improve Cross-domain Mixed Sampling with Guidance Training for Adaptive Segmentation" index=92>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Distribution Shift, Distribution Shift, Unsupervised Learning, Domain Adaptation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14995v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14995v1.pdf" filename="2403.14995v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Unsupervised</b> <b>Domain</b> <b>Adaptation</b> (UDA) endeavors to adjust models trained on a source <b>domain</b> <b>to</b> perform well on a target <b>domain</b> <b>without</b> requiring additional annotations. In the context of <b>domain</b> <b>adaptive</b> semantic segmentation, which tackles UDA for dense prediction, the goal is to circumvent the need for costly pixel-level annotations. Typically, various prevailing methods baseline rely on constructing intermediate <b>domains</b> <b>via</b> cross-domain mixed sampling techniques to mitigate the performance decline caused by <b>domain</b> <b>gaps.</b> However, such approaches generate synthetic data that diverge from real-world <b>distributions,</b> <b>potentially</b> leading the model astray from the true target <b>distribution.</b> <b>To</b> address this challenge, we propose a novel auxiliary task called Guidance Training. This task facilitates the effective utilization of cross-domain mixed sampling techniques while mitigating <b>distribution</b> <b>shifts</b> from the real world. Specifically, Guidance Training guides the model to extract and reconstruct the target-domain feature <b>distribution</b> <b>from</b> mixed data, followed by decoding the reconstructed target-domain features to make pseudo-label predictions. Importantly, integrating Guidance Training incurs minimal training overhead and imposes no additional inference burden. We demonstrate the efficacy of our approach by integrating it with existing methods, consistently improving performance. The implementation will be available at https://github.com/Wenlve-Zhou/Guidance-Training.

{{</citation>}}


### (27/63 | 93/202) GPT-Connect: Interaction between Text-Driven Human Motion Generator and 3D Scenes in a Training-free Manner (Haoxuan Qu et al., 2024)

{{<citation>}}

Haoxuan Qu, Ziyan Guo, Jun Liu. (2024)  
**GPT-Connect: Interaction between Text-Driven Human Motion Generator and 3D Scenes in a Training-free Manner**
<br/>
<button class="copy-to-clipboard" title="GPT-Connect: Interaction between Text-Driven Human Motion Generator and 3D Scenes in a Training-free Manner" index=93>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Supervised Learning, ChatGPT, GPT  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14947v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14947v1.pdf" filename="2403.14947v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recently, while text-driven human motion generation has received massive research attention, most existing text-driven motion generators are generally only designed to generate motion sequences in a blank background. While this is the case, in practice, human beings naturally perform their motions in 3D scenes, rather than in a blank background. Considering this, we here aim to perform scene-aware text-drive motion generation instead. Yet, intuitively training a separate scene-aware motion generator in a <b>supervised</b> way can require a large amount of motion samples to be troublesomely collected and annotated in a large scale of different 3D scenes. To handle this task rather in a relatively convenient manner, in this paper, we propose a novel <b>GPT-connect</b> framework. In <b>GPT-connect,</b> we enable scene-aware motion sequences to be generated directly utilizing the existing blank-background human motion generator, via leveraging <b>ChatGPT</b> to connect the existing motion generator with the 3D scene in a totally training-free manner. Extensive experiments demonstrate the efficacy and generalizability of our proposed framework.

{{</citation>}}


### (28/63 | 94/202) IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection (Junbo Yin et al., 2024)

{{<citation>}}

Junbo Yin, Jianbing Shen, Runnan Chen, Wei Li, Ruigang Yang, Pascal Frossard, Wenguan Wang. (2024)  
**IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection**
<br/>
<button class="copy-to-clipboard" title="IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection" index=94>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 29  
Keywords: Object Detection, Benchmarking, Multi-modal, Multi-modal, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15241v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15241v1.pdf" filename="2403.15241v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Bird's eye view (BEV) representation has emerged as a dominant solution for describing 3D space in autonomous driving scenarios. However, <b>objects</b> <b>in</b> the BEV representation typically exhibit small sizes, and the associated point cloud context is inherently sparse, which leads to great challenges for reliable 3D perception. In this paper, we propose IS-Fusion, an innovative <b>multimodal</b> fusion framework that jointly captures the Instance- and Scene-level contextual information. IS-Fusion essentially differs from existing approaches that only focus on the BEV scene-level fusion by explicitly incorporating instance-level <b>multimodal</b> information, thus facilitating the instance-centric tasks like 3D <b>object</b> <b>detection.</b> It comprises a Hierarchical Scene Fusion (HSF) module and an Instance-Guided Fusion (IGF) module. HSF applies Point-to-Grid and Grid-to-Region <b>transformers</b> to capture the <b>multimodal</b> scene context at different granularities. IGF mines instance candidates, explores their relationships, and aggregates the local <b>multimodal</b> context for each instance. These instances then serve as guidance to enhance the scene feature and yield an instance-aware BEV representation. On the challenging nuScenes <b>benchmark,</b> IS-Fusion outperforms all the published <b>multimodal</b> works to date. Code is available at: https://github.com/yinjunbo/IS-Fusion.

{{</citation>}}


### (29/63 | 95/202) Neural Plasticity-Inspired Foundation Model for Observing the Earth Crossing Modalities (Zhitong Xiong et al., 2024)

{{<citation>}}

Zhitong Xiong, Yi Wang, Fahong Zhang, Adam J. Stewart, Joëlle Hanna, Damian Borth, Ioannis Papoutsis, Bertrand Le Saux, Gustau Camps-Valls, Xiao Xiang Zhu. (2024)  
**Neural Plasticity-Inspired Foundation Model for Observing the Earth Crossing Modalities**
<br/>
<button class="copy-to-clipboard" title="Neural Plasticity-Inspired Foundation Model for Observing the Earth Crossing Modalities" index=95>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 26  
Keywords: Foundation Model, Multi-modal, Multi-modal, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15356v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15356v1.pdf" filename="2403.15356v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The development of <b>foundation</b> <b>models</b> has revolutionized our ability to interpret the Earth's surface using satellite observational data. Traditional models have been siloed, tailored to specific sensors or data types like optical, radar, and hyperspectral, each with its own unique characteristics. This specialization hinders the potential for a holistic analysis that could benefit from the combined strengths of these diverse data sources. Our novel approach introduces the Dynamic One-For-All (DOFA) model, leveraging the concept of neural plasticity in brain science to integrate various data modalities into a single framework adaptively. This dynamic hypernetwork, adjusting to different wavelengths, enables a single versatile <b>Transformer</b> jointly trained on data from five sensors to excel across 12 distinct Earth observation tasks, including sensors never seen during pretraining. DOFA's innovative design offers a promising leap towards more accurate, efficient, and unified Earth observation analysis, showcasing remarkable adaptability and performance in harnessing the potential of <b>multimodal</b> Earth observation data.

{{</citation>}}


### (30/63 | 96/202) MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition Integration (Zhichao Wei et al., 2024)

{{<citation>}}

Zhichao Wei, Qingkun Su, Long Qin, Weizhi Wang. (2024)  
**MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition Integration**
<br/>
<button class="copy-to-clipboard" title="MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition Integration" index=96>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 26  
Keywords: Diffusion Model, Multi-modal, Multi-modal, Text Embedding  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15059v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15059v1.pdf" filename="2403.15059v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advances in tuning-free personalized image generation based on <b>diffusion</b> <b>models</b> are impressive. However, to improve subject fidelity, existing methods either retrain the <b>diffusion</b> <b>model</b> or infuse it with dense visual embeddings, both of which suffer from poor generalization and efficiency. Also, these methods falter in multi-subject image generation due to the unconstrained cross-attention mechanism. In this paper, we propose MM-Diff, a unified and tuning-free image personalization framework capable of generating high-fidelity images of both single and multiple subjects in seconds. Specifically, to simultaneously enhance <b>text</b> <b>consistency</b> and subject fidelity, MM-Diff employs a vision encoder to transform the input image into CLS and patch embeddings. CLS embeddings are used on the one hand to augment the <b>text</b> <b>embeddings,</b> and on the other hand together with patch embeddings to derive a small number of detail-rich subject embeddings, both of which are efficiently integrated into the <b>diffusion</b> <b>model</b> through the well-designed <b>multimodal</b> cross-attention mechanism. Additionally, MM-Diff introduces cross-attention map constraints during the training phase, ensuring flexible multi-subject image sampling during inference without any predefined inputs (e.g., layout). Extensive experiments demonstrate the superior performance of MM-Diff over other leading methods.

{{</citation>}}


### (31/63 | 97/202) Hyperbolic Metric Learning for Visual Outlier Detection (Alvaro Gonzalez-Jimenez et al., 2024)

{{<citation>}}

Alvaro Gonzalez-Jimenez, Simone Lionetti, Dena Bazazian, Philippe Gottfrois, Fabian Gröger, Marc Pouly, Alexander Navarini. (2024)  
**Hyperbolic Metric Learning for Visual Outlier Detection**
<br/>
<button class="copy-to-clipboard" title="Hyperbolic Metric Learning for Visual Outlier Detection" index=97>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 25  
Keywords: Geometry, Out-of-distribution, Outlier Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15260v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15260v1.pdf" filename="2403.15260v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Out-Of-Distribution</b> (OOD) detection is critical to deploy deep learning models in safety-critical applications. However, the inherent hierarchical concept structure of visual data, which is instrumental to OOD detection, is often poorly captured by conventional methods based on Euclidean <b>geometry.</b> This work proposes a metric framework that leverages the strengths of Hyperbolic <b>geometry</b> for OOD detection. Inspired by previous works that refine the decision boundary for OOD data with synthetic <b>outliers,</b> <b>we</b> extend this method to Hyperbolic space. Interestingly, we find that synthetic <b>outliers</b> <b>do</b> not benefit OOD detection in Hyperbolic space as they do in Euclidean space. Furthermore we explore the relationship between OOD detection performance and Hyperbolic embedding dimension, addressing practical concerns in resource-constrained environments. Extensive experiments show that our framework improves the FPR95 for OOD detection from 22\% to 15\% and from 49% to 28% on CIFAR-10 and CIFAR-100 respectively compared to Euclidean methods.

{{</citation>}}


### (32/63 | 98/202) LSK3DNet: Towards Effective and Efficient 3D Perception with Large Sparse Kernels (Tuo Feng et al., 2024)

{{<citation>}}

Tuo Feng, Wenguan Wang, Fan Ma, Yi Yang. (2024)  
**LSK3DNet: Towards Effective and Efficient 3D Perception with Large Sparse Kernels**
<br/>
<button class="copy-to-clipboard" title="LSK3DNet: Towards Effective and Efficient 3D Perception with Large Sparse Kernels" index=98>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Benchmarking, Convolution, Pruning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15173v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15173v1.pdf" filename="2403.15173v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Autonomous systems need to process large-scale, sparse, and irregular point clouds with limited compute resources. Consequently, it is essential to develop LiDAR perception methods that are both efficient and effective. Although naively enlarging 3D kernel size can enhance performance, it will also lead to a cubically-increasing overhead. Therefore, it is crucial to develop streamlined 3D large kernel designs that eliminate redundant weights and work effectively with larger kernels. In this paper, we propose an efficient and effective Large Sparse Kernel 3D Neural Network (LSK3DNet) that leverages dynamic <b>pruning</b> to amplify the 3D kernel size. Our method comprises two core components: Spatial-wise Dynamic Sparsity (SDS) and Channel-wise Weight Selection (CWS). SDS dynamically prunes and regrows volumetric weights from the beginning to learn a large sparse 3D kernel. It not only boosts performance but also significantly reduces model size and computational cost. Moreover, CWS selects the most important channels for 3D <b>convolution</b> during training and subsequently prunes the redundant channels to accelerate inference for 3D vision tasks. We demonstrate the effectiveness of LSK3DNet on three <b>benchmark</b> datasets and five tracks compared with classical models and large kernel designs. Notably, LSK3DNet achieves the state-of-the-art performance on SemanticKITTI (i.e., 75.6% on single-scan and 63.4% on multi-scan), with roughly 40% model size reduction and 60% computing operations reduction compared to the naive large 3D kernel model.

{{</citation>}}


### (33/63 | 99/202) FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos (Florian Langer et al., 2024)

{{<citation>}}

Florian Langer, Jihong Ju, Georgi Dikov, Gerhard Reitmayr, Mohsen Ghafoorian. (2024)  
**FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos**
<br/>
<button class="copy-to-clipboard" title="FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos" index=99>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Benchmarking, Contrastive Learning, Knowledge Distillation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15161v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15161v1.pdf" filename="2403.15161v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Digitising the 3D world into a clean, CAD model-based representation has important applications for augmented reality and robotics. Current state-of-the-art methods are computationally intensive as they individually encode each detected object and optimise CAD alignments in a second stage. In this work, we propose FastCAD, a real-time method that simultaneously retrieves and aligns CAD models for all objects in a given scene. In contrast to previous works, we directly predict alignment parameters and shape embeddings. We achieve high-quality shape retrievals by learning CAD embeddings in a <b>contrastive</b> <b>learning</b> framework and <b>distilling</b> those into FastCAD. Our single-stage method accelerates the inference time by a factor of 50 compared to other methods operating on RGB-D scans while outperforming them on the challenging Scan2CAD alignment <b>benchmark.</b> Further, our approach collaborates seamlessly with online 3D reconstruction techniques. This enables the real-time generation of precise CAD model-based reconstructions from videos at 10 FPS. Doing so, we significantly improve the Scan2CAD alignment accuracy in the video setting from 43.0% to 48.2% and the reconstruction accuracy from 22.9% to 29.6%.

{{</citation>}}


### (34/63 | 100/202) AVT2-DWF: Improving Deepfake Detection with Audio-Visual Fusion and Dynamic Weighting Strategies (Rui Wang et al., 2024)

{{<citation>}}

Rui Wang, Dengpan Ye, Long Tang, Yunming Zhang, Jiacheng Deng. (2024)  
**AVT2-DWF: Improving Deepfake Detection with Audio-Visual Fusion and Dynamic Weighting Strategies**
<br/>
<button class="copy-to-clipboard" title="AVT2-DWF: Improving Deepfake Detection with Audio-Visual Fusion and Dynamic Weighting Strategies" index=100>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Multi-modal, Transformer, Tokenization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14974v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14974v1.pdf" filename="2403.14974v1.pdf">Download PDF</button>

---


**ABSTRACT**  
With the continuous improvements of deepfake methods, forgery messages have transitioned from single-modality to <b>multi-modal</b> fusion, posing new challenges for existing forgery detection algorithms. In this paper, we propose AVT2-DWF, the Audio-Visual dual <b>Transformers</b> grounded in Dynamic Weight Fusion, which aims to amplify both intra- and cross-modal forgery cues, thereby enhancing detection capabilities. AVT2-DWF adopts a dual-stage approach to capture both spatial characteristics and temporal dynamics of facial expressions. This is achieved through a face <b>transformer</b> with an n-frame-wise <b>tokenization</b> strategy encoder and an audio <b>transformer</b> encoder. Subsequently, it uses <b>multi-modal</b> conversion with dynamic weight fusion to address the challenge of heterogeneous information fusion between audio and visual modalities. Experiments on DeepfakeTIMIT, FakeAVCeleb, and DFDC datasets indicate that AVT2-DWF achieves state-of-the-art performance intra- and cross-dataset Deepfake detection. Code is available at https://github.com/raining-dev/AVT2-DWF.

{{</citation>}}


### (35/63 | 101/202) WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization (Jialu Wang et al., 2024)

{{<citation>}}

Jialu Wang, Kaichen Zhou, Andrew Markham, Niki Trigoni. (2024)  
**WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization**
<br/>
<button class="copy-to-clipboard" title="WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization" index=101>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Supervised Learning, Weakly-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15272v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15272v1.pdf" filename="2403.15272v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Despite the advancements in deep learning for camera relocalization tasks, obtaining ground truth pose labels required for the training process remains a costly endeavor. While current weakly <b>supervised</b> methods excel in lightweight label generation, their performance notably declines in scenarios with sparse views. In response to this challenge, we introduce WSCLoc, a system capable of being customized to various deep learning-based relocalization models to enhance their performance under <b>weakly-supervised</b> and sparse view conditions. This is realized with two stages. In the initial stage, WSCLoc employs a multilayer perceptron-based structure called WFT-NeRF to co-optimize image reconstruction quality and initial pose information. To ensure a stable learning process, we incorporate temporal information as input. Furthermore, instead of optimizing SE(3), we opt for $\mathfrak{sim}(3)$ optimization to explicitly enforce a scale constraint. In the second stage, we co-optimize the pre-trained WFT-NeRF and WFT-Pose. This optimization is enhanced by Time-Encoding based Random View Synthesis and <b>supervised</b> by inter-frame geometric constraints that consider pose, depth, and RGB information. We validate our approaches on two publicly available datasets, one outdoor and one indoor. Our experimental results demonstrate that our <b>weakly-supervised</b> relocalization solutions achieve superior pose estimation accuracy in sparse-view scenarios, comparable to state-of-the-art camera relocalization methods. We will make our code publicly available.

{{</citation>}}


### (36/63 | 102/202) Spectral Motion Alignment for Video Motion Transfer using Diffusion Models (Geon Yeong Park et al., 2024)

{{<citation>}}

Geon Yeong Park, Hyeonho Jeong, Sang Wan Lee, Jong Chul Ye. (2024)  
**Spectral Motion Alignment for Video Motion Transfer using Diffusion Models**
<br/>
<button class="copy-to-clipboard" title="Spectral Motion Alignment for Video Motion Transfer using Diffusion Models" index=102>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs.CV  
Keyword Score: 20  
Keywords: Diffusion Model, Knowledge Distillation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15249v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15249v1.pdf" filename="2403.15249v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The evolution of <b>diffusion</b> <b>models</b> has greatly impacted video generation and understanding. Particularly, text-to-video <b>diffusion</b> <b>models</b> (VDMs) have significantly facilitated the customization of input video with target appearance, motion, etc. Despite these advances, challenges persist in accurately <b>distilling</b> motion information from video frames. While existing works leverage the consecutive frame residual as the target motion vector, they inherently lack global motion context and are vulnerable to frame-wise distortions. To address this, we present Spectral Motion Alignment (SMA), a novel framework that refines and aligns motion vectors using Fourier and wavelet transforms. SMA learns motion patterns by incorporating frequency-domain regularization, facilitating the learning of whole-frame global motion dynamics, and mitigating spatial artifacts. Extensive experiments demonstrate SMA's efficacy in improving motion transfer while maintaining computational efficiency and compatibility across various video customization frameworks.

{{</citation>}}


### (37/63 | 103/202) Reasoning-Enhanced Object-Centric Learning for Videos (Jian Li et al., 2024)

{{<citation>}}

Jian Li, Pu Ren, Yang Liu, Hao Sun. (2024)  
**Reasoning-Enhanced Object-Centric Learning for Videos**
<br/>
<button class="copy-to-clipboard" title="Reasoning-Enhanced Object-Centric Learning for Videos" index=103>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs.CV  
Keyword Score: 20  
Keywords: Transformer, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15245v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15245v1.pdf" filename="2403.15245v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Object-centric learning aims to break down complex visual scenes into more manageable object representations, enhancing the understanding and <b>reasoning</b> abilities of machine learning systems toward the physical world. Recently, slot-based video models have demonstrated remarkable proficiency in segmenting and tracking objects, but they overlook the importance of the effective <b>reasoning</b> module. In the real world, <b>reasoning</b> and predictive abilities play a crucial role in human perception and object tracking; in particular, these abilities are closely related to human intuitive physics. Inspired by this, we designed a novel <b>reasoning</b> module called the Slot-based Time-Space <b>Transformer</b> with Memory buffer (STATM) to enhance the model's perception ability in complex scenes. The memory buffer primarily serves as storage for slot information from upstream modules, the Slot-based Time-Space <b>Transformer</b> makes predictions through slot-based spatiotemporal attention computations and fusion. Our experiment results on various datasets show that STATM can significantly enhance object-centric learning capabilities of slot-based video models.

{{</citation>}}


### (38/63 | 104/202) Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment Anything Model for Crowd-Sourcing Medical Image Annotations (Pranav Kulkarni et al., 2024)

{{<citation>}}

Pranav Kulkarni, Adway Kanhere, Dharmam Savani, Andrew Chan, Devina Chatterjee, Paul H. Yi, Vishwa S. Parekh. (2024)  
**Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment Anything Model for Crowd-Sourcing Medical Image Annotations**
<br/>
<button class="copy-to-clipboard" title="Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment Anything Model for Crowd-Sourcing Medical Image Annotations" index=104>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs.CV  
Keyword Score: 20  
Keywords: Foundation Model, Zero-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15218v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15218v1.pdf" filename="2403.15218v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Curating annotations for medical image segmentation is a labor-intensive and time-consuming task that requires domain expertise, resulting in "narrowly" focused deep learning (DL) models with limited translational utility. Recently, <b>foundation</b> <b>models</b> like the Segment Anything Model (SAM) have revolutionized semantic segmentation with exceptional <b>zero-shot</b> generalizability across various domains, including medical imaging, and hold a lot of promise for streamlining the annotation process. However, SAM has yet to be evaluated in a crowd-sourced setting to curate annotations for training 3D DL segmentation models. In this work, we explore the potential of SAM for crowd-sourcing "sparse" annotations from non-experts to generate "dense" segmentation masks for training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our results indicate that while SAM-generated annotations exhibit high mean Dice scores compared to ground-truth annotations, nnU-Net models trained on SAM-generated annotations perform significantly worse than nnU-Net models trained on ground-truth annotations ($p<0.001$, all).

{{</citation>}}


### (39/63 | 105/202) SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation (Vladyslav Zalevskyi et al., 2024)

{{<citation>}}

Vladyslav Zalevskyi, Kristoffer Hougaard Madsen. (2024)  
**SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation**
<br/>
<button class="copy-to-clipboard" title="SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" index=105>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Self-supervised Learning, Self-supervised Pre-training  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15121v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15121v1.pdf" filename="2403.15121v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Bipolar disorder (BD) and schizophrenia (SZ) are severe mental disorders with profound societal impact. Identifying risk markers early is crucial for understanding disease progression and enabling preventive measures. The Danish High Risk and Resilience Study (VIA) focuses on understanding early disease processes, particularly in children with familial high risk (FHR). Understanding structural brain changes associated with these diseases during early stages is essential for effective interventions. The central sulcus (CS) is a prominent brain landmark related to brain regions involved in motor and sensory processing. Analyzing CS morphology can provide valuable insights into neurodevelopmental abnormalities in the FHR group. However, segmenting the central sulcus (CS) presents challenges due to its variability, especially in adolescents. This study introduces two novel approaches to improve CS segmentation: synthetic data generation to model CS variability and <b>self-supervised</b> <b>pre-training</b> with multi-task learning to adapt models to new cohorts. These methods aim to enhance segmentation performance across diverse populations, eliminating the need for extensive preprocessing.

{{</citation>}}


### (40/63 | 106/202) Towards a Comprehensive, Efficient and Promptable Anatomic Structure Segmentation Model using 3D Whole-body CT Scans (Heng Guo et al., 2024)

{{<citation>}}

Heng Guo, Jianfeng Zhang, Jiaxing Huang, Tony C. W. Mok, Dazhou Guo, Ke Yan, Le Lu, Dakai Jin, Minfeng Xu. (2024)  
**Towards a Comprehensive, Efficient and Promptable Anatomic Structure Segmentation Model using 3D Whole-body CT Scans**
<br/>
<button class="copy-to-clipboard" title="Towards a Comprehensive, Efficient and Promptable Anatomic Structure Segmentation Model using 3D Whole-body CT Scans" index=106>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Prompt, Prompt Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15063v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15063v1.pdf" filename="2403.15063v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Segment anything model (SAM) demonstrates strong generalization ability on natural image segmentation. However, its direct adaption in medical image segmentation tasks shows significant performance drops with inferior accuracy and unstable results. It may also requires an excessive number of <b>prompt</b> <b>points</b> to obtain a reasonable accuracy. For segmenting 3D radiological CT or MRI scans, a 2D SAM model has to separately handle hundreds of 2D slices. Although quite a few studies explore adapting SAM into medical image volumes, the efficiency of 2D adaption methods is unsatisfactory and 3D adaptation methods only capable of segmenting specific organs/tumors. In this work, we propose a comprehensive and scalable 3D SAM model for whole-body CT segmentation, named CT-SAM3D. Instead of adapting SAM, we propose a 3D promptable segmentation model using a (nearly) fully labeled CT dataset. To train CT-SAM3D effectively, ensuring the model's accurate responses to higher-dimensional spatial <b>prompts</b> <b>is</b> crucial, and 3D patch-wise training is required due to GPU memory constraints. For this purpose, we propose two key technical developments: 1) a progressively and spatially aligned <b>prompt</b> <b>encoding</b> method to effectively encode click <b>prompts</b> <b>in</b> local 3D space; and 2) a cross-patch <b>prompt</b> <b>learning</b> scheme to capture more 3D spatial context, which is beneficial for reducing the editing workloads when interactively <b>prompting</b> <b>on</b> large organs. CT-SAM3D is trained and validated using a curated dataset of 1204 CT scans containing 107 whole-body anatomies, reporting significantly better quantitative performance against all previous SAM-derived models by a large margin with much fewer click <b>prompts.</b> <b>Our</b> model can handle segmenting unseen organ as well. Code, data, and our 3D interactive segmentation tool with quasi-real-time responses will be made publicly available.

{{</citation>}}


### (41/63 | 107/202) Toward Tiny and High-quality Facial Makeup with Data Amplify Learning (Qiaoqiao Jin et al., 2024)

{{<citation>}}

Qiaoqiao Jin, Xuanhong Chen, Meiguang Jin, Ying Cheng, Rui Shi, Yucheng Zheng, Yupeng Zhu, Bingbing Ni. (2024)  
**Toward Tiny and High-quality Facial Makeup with Data Amplify Learning**
<br/>
<button class="copy-to-clipboard" title="Toward Tiny and High-quality Facial Makeup with Data Amplify Learning" index=107>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Diffusion Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15033v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15033v1.pdf" filename="2403.15033v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Contemporary makeup approaches primarily hinge on unpaired learning paradigms, yet they grapple with the challenges of inaccurate supervision (e.g., face misalignment) and sophisticated facial <b>prompts</b> (including face parsing, and landmark detection). These challenges prohibit low-cost deployment of facial makeup models, especially on mobile devices. To solve above problems, we propose a brand-new learning paradigm, termed "Data Amplify Learning (DAL)," alongside a compact makeup model named "TinyBeauty." The core idea of DAL lies in employing a <b>Diffusion-based</b> <b>Data</b> Amplifier (DDA) to "amplify" limited images for the model training, thereby enabling accurate pixel-to-pixel supervision with merely a handful of annotations. Two pivotal innovations in DDA facilitate the above training approach: (1) A Residual <b>Diffusion</b> <b>Model</b> (RDM) is designed to generate high-fidelity detail and circumvent the detail vanishing problem in the vanilla <b>diffusion</b> <b>models;</b> (2) A Fine-Grained Makeup Module (FGMM) is proposed to achieve precise makeup control and combination while retaining face identity. Coupled with DAL, TinyBeauty necessitates merely 80K parameters to achieve a state-of-the-art performance without intricate face <b>prompts.</b> Meanwhile, TinyBeauty achieves a remarkable inference speed of up to 460 fps on the iPhone 13. Extensive experiments show that DAL can produce highly competitive makeup models using only 5 image pairs.

{{</citation>}}


### (42/63 | 108/202) Multimodal Fusion with Pre-Trained Model Features in Affective Behaviour Analysis In-the-wild (Zhuofan Wen et al., 2024)

{{<citation>}}

Zhuofan Wen, Fengyu Zhang, Siyuan Zhang, Haiyang Sun, Mingyu Xu, Licai Sun, Zheng Lian, Bin Liu, Jianhua Tao. (2024)  
**Multimodal Fusion with Pre-Trained Model Features in Affective Behaviour Analysis In-the-wild**
<br/>
<button class="copy-to-clipboard" title="Multimodal Fusion with Pre-Trained Model Features in Affective Behaviour Analysis In-the-wild" index=108>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 16  
Keywords: Convolution, Multi-modal, Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15044v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15044v1.pdf" filename="2403.15044v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Multimodal</b> fusion is a significant method for most <b>multimodal</b> tasks. With the recent surge in the number of large pre-trained models, combining both <b>multimodal</b> fusion methods and pre-trained model features can achieve outstanding performance in many <b>multimodal</b> tasks. In this paper, we present our approach, which leverages both advantages for addressing the task of Expression (Expr) Recognition and Valence-Arousal (VA) Estimation. We evaluate the Aff-Wild2 database using pre-trained models, then extract the final hidden layers of the models as features. Following preprocessing and interpolation or <b>convolution</b> to align the extracted features, different models are employed for modal fusion. Our code is available at GitHub - FulgenceWen/ABAW6th.

{{</citation>}}


### (43/63 | 109/202) LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis (Kevin Xie et al., 2024)

{{<citation>}}

Kevin Xie, Jonathan Lorraine, Tianshi Cao, Jun Gao, James Lucas, Antonio Torralba, Sanja Fidler, Xiaohui Zeng. (2024)  
**LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis**
<br/>
<button class="copy-to-clipboard" title="LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis" index=109>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: 68T45, I-2-6; I-2-7; I-3-6; I-3-7, cs-AI, cs-CV, cs-GR, cs-LG, cs.CV  
Keyword Score: 15  
Keywords: Geometry, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15385v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15385v1.pdf" filename="2403.15385v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent text-to-3D generation approaches produce impressive 3D results but require time-consuming optimization that can take up to an hour per <b>prompt.</b> Amortized methods like ATT3D optimize multiple <b>prompts</b> simultaneously to improve efficiency, enabling fast text-to-3D synthesis. However, they cannot capture high-frequency <b>geometry</b> and texture details and struggle to scale to large <b>prompt</b> sets, so they generalize poorly. We introduce LATTE3D, addressing these limitations to achieve fast, high-quality generation on a significantly larger <b>prompt</b> set. Key to our method is 1) building a scalable architecture and 2) leveraging 3D data during optimization through 3D-aware diffusion priors, shape regularization, and model initialization to achieve robustness to diverse and complex training <b>prompts.</b> LATTE3D amortizes both neural field and textured surface generation to produce highly detailed textured meshes in a single forward pass. LATTE3D generates 3D objects in 400ms, and can be further enhanced with fast test-time optimization.

{{</citation>}}


### (44/63 | 110/202) Tri-Perspective View Decomposition for Geometry-Aware Depth Completion (Zhiqiang Yan et al., 2024)

{{<citation>}}

Zhiqiang Yan, Yuankai Lin, Kun Wang, Yupeng Zheng, Yufei Wang, Zhenyu Zhang, Jun Li, Jian Yang. (2024)  
**Tri-Perspective View Decomposition for Geometry-Aware Depth Completion**
<br/>
<button class="copy-to-clipboard" title="Tri-Perspective View Decomposition for Geometry-Aware Depth Completion" index=110>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 15  
Keywords: Convolution, Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15008v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15008v1.pdf" filename="2403.15008v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Depth completion is a vital task for autonomous driving, as it involves reconstructing the precise 3D <b>geometry</b> of a scene from sparse and noisy depth measurements. However, most existing methods either rely only on 2D depth representations or directly incorporate raw 3D point clouds for compensation, which are still insufficient to capture the fine-grained 3D <b>geometry</b> of the scene. To address this challenge, we introduce Tri-Perspective view Decomposition (TPVD), a novel framework that can explicitly model 3D <b>geometry.</b> In particular, (1) TPVD ingeniously decomposes the original point cloud into three 2D views, one of which corresponds to the sparse depth input. (2) We design TPV Fusion to update the 2D TPV features through recurrent 2D-3D-2D aggregation, where a Distance-Aware Spherical <b>Convolution</b> (DASC) is applied. (3) By adaptively choosing TPV affinitive neighbors, the newly proposed Geometric Spatial Propagation Network (GSPN) further improves the geometric consistency. As a result, our TPVD outperforms existing methods on KITTI, NYUv2, and SUN RGBD. Furthermore, we build a novel depth completion dataset named TOFDC, which is acquired by the time-of-flight (TOF) sensor and the color camera on smartphones. Project page: https://yanzq95.github.io/projectpage/TOFDC/index.html

{{</citation>}}


### (45/63 | 111/202) DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from Partially Annotated Data (Hanrong Ye et al., 2024)

{{<citation>}}

Hanrong Ye, Dan Xu. (2024)  
**DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from Partially Annotated Data**
<br/>
<button class="copy-to-clipboard" title="DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from Partially Annotated Data" index=111>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 13  
Keywords: Diffusion Model, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15389v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15389v1.pdf" filename="2403.15389v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recently, there has been an increased interest in the practical problem of learning multiple dense scene understanding tasks from partially annotated data, where each training sample is only labeled for a subset of the tasks. The missing of task labels in training leads to low-quality and noisy predictions, as can be observed from state-of-the-art methods. To tackle this issue, we reformulate the partially-labeled multi-task dense prediction as a pixel-level denoising problem, and propose a novel multi-task denoising <b>diffusion</b> <b>framework</b> coined as DiffusionMTL. It designs a joint <b>diffusion</b> <b>and</b> denoising paradigm to model a potential noisy distribution in the task prediction or feature maps and generate rectified outputs for different tasks. To exploit multi-task consistency in denoising, we further introduce a Multi-Task Conditioning strategy, which can implicitly utilize the complementary nature of the tasks to help learn the unlabeled tasks, leading to an improvement in the denoising performance of the different tasks. Extensive quantitative and qualitative experiments demonstrate that the proposed multi-task denoising <b>diffusion</b> <b>model</b> can significantly improve multi-task prediction maps, and outperform the state-of-the-art methods on three challenging multi-task <b>benchmarks,</b> under two different partial-labeling evaluation settings. The code is available at https://prismformore.github.io/diffusionmtl/.

{{</citation>}}


### (46/63 | 112/202) CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking (Nicolas Baumann et al., 2024)

{{<citation>}}

Nicolas Baumann, Michael Baumgartner, Edoardo Ghignone, Jonas Kühne, Tobias Fischer, Yung-Hsu Yang, Marc Pollefeys, Michele Magno. (2024)  
**CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking**
<br/>
<button class="copy-to-clipboard" title="CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking" index=112>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 13  
Keywords: Object Detection, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15313v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15313v1.pdf" filename="2403.15313v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Accurate detection and tracking of surrounding <b>objects</b> <b>is</b> essential to enable self-driving vehicles. While Light Detection and Ranging (LiDAR) sensors have set the <b>benchmark</b> for high performance, the appeal of camera-only solutions lies in their cost-effectiveness. Notably, despite the prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive systems, their potential in 3D detection and tracking has been largely disregarded due to data sparsity and measurement noise. As a recent development, the combination of RADARs and cameras is emerging as a promising solution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D <b>object</b> <b>detection,</b> and Multi-Object Tracking (MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporating the spatial and velocity information of the RADAR sensor. Experimental results demonstrate an absolute improvement in detection performance of 5.3% in mean Average Precision (mAP) and a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on the nuScenes dataset when leveraging both modalities. CR3DT bridges the gap between high-performance and cost-effective perception systems in autonomous driving, by capitalizing on the ubiquitous presence of RADAR in automotive applications.

{{</citation>}}


### (47/63 | 113/202) ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars (Zhenwei Wang et al., 2024)

{{<citation>}}

Zhenwei Wang, Tengfei Wang, Gerhard Hancke, Ziwei Liu, Rynson W. H. Lau. (2024)  
**ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars**
<br/>
<button class="copy-to-clipboard" title="ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars" index=113>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Knowledge Distillation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15383v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15383v1.pdf" filename="2403.15383v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Real-world applications often require a large gallery of 3D assets that share a consistent theme. While remarkable advances have been made in general 3D content creation from text or image, synthesizing customized 3D assets following the shared theme of input 3D exemplars remains an open and challenging problem. In this work, we present ThemeStation, a novel approach for theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D assets based on given few exemplars with two goals: 1) unity for generating 3D assets that thematically align with the given exemplars and 2) diversity for generating 3D assets with a high degree of variations. To this end, we design a two-stage framework that draws a concept image first, followed by a reference-informed 3D modeling stage. We propose a novel dual score <b>distillation</b> (DSD) loss to jointly leverage priors from both the input exemplars and the synthesized concept image. Extensive experiments and user studies confirm that ThemeStation surpasses prior works in producing diverse theme-aware 3D models with impressive quality. ThemeStation also enables various applications such as controllable 3D-to-3D generation.

{{</citation>}}


### (48/63 | 114/202) DragAPart: Learning a Part-Level Motion Prior for Articulated Objects (Ruining Li et al., 2024)

{{<citation>}}

Ruining Li, Chuanxia Zheng, Christian Rupprecht, Andrea Vedaldi. (2024)  
**DragAPart: Learning a Part-Level Motion Prior for Articulated Objects**
<br/>
<button class="copy-to-clipboard" title="DragAPart: Learning a Part-Level Motion Prior for Articulated Objects" index=114>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15382v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15382v1.pdf" filename="2403.15382v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We introduce DragAPart, a method that, given an image and a set of drags as input, can generate a new image of the same object in a new state, compatible with the action of the drags. Differently from prior works that focused on repositioning objects, DragAPart predicts part-level interactions, such as opening and closing a drawer. We study this problem as a proxy for learning a generalist motion model, not restricted to a specific kinematic structure or object category. To this end, we start from a pre-trained image generator and <b>fine-tune</b> it on a new synthetic dataset, Drag-a-Move, which we introduce. Combined with a new encoding for the drags and dataset randomization, the new model generalizes well to real images and different categories. Compared to prior motion-controlled generators, we demonstrate much better part-level motion understanding.

{{</citation>}}


### (49/63 | 115/202) LeGO: Leveraging a Surface Deformation Network for Animatable Stylized Face Generation with One Example (Soyeon Yoon et al., 2024)

{{<citation>}}

Soyeon Yoon, Kwan Yun, Kwanggyoon Seo, Sihun Cha, Jung Eun Yoo, Junyong Noh. (2024)  
**LeGO: Leveraging a Surface Deformation Network for Animatable Stylized Face Generation with One Example**
<br/>
<button class="copy-to-clipboard" title="LeGO: Leveraging a Surface Deformation Network for Animatable Stylized Face Generation with One Example" index=115>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: 68T45, I-4-9, cs-CV, cs-GR, cs.CV  
Keyword Score: 10  
Keywords: Zero-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15227v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15227v1.pdf" filename="2403.15227v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advances in 3D face stylization have made significant strides in few to <b>zero-shot</b> settings. However, the degree of stylization achieved by existing methods is often not sufficient for practical applications because they are mostly based on statistical 3D Morphable Models (3DMM) with limited variations. To this end, we propose a method that can produce a highly stylized 3D face model with desired topology. Our methods train a surface deformation network with 3DMM and translate its domain to the target style using a paired exemplar. The network achieves stylization of the 3D face mesh by mimicking the style of the target using a differentiable renderer and directional CLIP losses. Additionally, during the inference process, we utilize a Mesh Agnostic Encoder (MAGE) that takes deformation target, a mesh of diverse topologies as input to the stylization process and encodes its shape into our latent space. The resulting stylized face model can be animated by commonly used 3DMM blend shapes. A set of quantitative and qualitative evaluations demonstrate that our method can produce highly stylized face meshes according to a given style and output them in a desired topology. We also demonstrate example applications of our method including image-based stylized avatar generation, linear interpolation of geometric styles, and facial animation of stylized avatars.

{{</citation>}}


### (50/63 | 116/202) Your Image is My Video: Reshaping the Receptive Field via Image-To-Video Differentiable AutoAugmentation and Fusion (Sofia Casarin et al., 2024)

{{<citation>}}

Sofia Casarin, Cynthia I. Ugwu, Sergio Escalera, Oswald Lanz. (2024)  
**Your Image is My Video: Reshaping the Receptive Field via Image-To-Video Differentiable AutoAugmentation and Fusion**
<br/>
<button class="copy-to-clipboard" title="Your Image is My Video: Reshaping the Receptive Field via Image-To-Video Differentiable AutoAugmentation and Fusion" index=116>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 10  
Keywords: Data Augmentation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15194v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15194v1.pdf" filename="2403.15194v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The landscape of deep learning research is moving towards innovative strategies to harness the true potential of <b>data.</b> <b>Traditionally,</b> emphasis has been on scaling model architectures, resulting in large and complex neural networks, which can be difficult to train with limited computational resources. However, independently of the model size, <b>data</b> <b>quality</b> (i.e. amount and variability) is still a major factor that affects model generalization. In this work, we propose a novel technique to exploit available <b>data</b> <b>through</b> the use of automatic <b>data</b> <b>augmentation</b> for the tasks of image classification and semantic segmentation. We introduce the first Differentiable Augmentation Search method (DAS) to generate variations of images that can be processed as videos. Compared to previous approaches, DAS is extremely fast and flexible, allowing the search on very large search spaces in less than a GPU day. Our intuition is that the increased receptive field in the temporal dimension provided by DAS could lead to benefits also to the spatial receptive field. More specifically, we leverage DAS to guide the reshaping of the spatial receptive field by selecting task-dependant transformations. As a result, compared to standard augmentation alternatives, we improve in terms of accuracy on ImageNet, Cifar10, Cifar100, Tiny-ImageNet, Pascal-VOC-2012 and CityScapes datasets when plugging-in our DAS over different light-weight video backbones.

{{</citation>}}


### (51/63 | 117/202) SFOD: Spiking Fusion Object Detector (Yimeng Fan et al., 2024)

{{<citation>}}

Yimeng Fan, Wei Zhang, Changsong Liu, Mingyang Li, Wenrui Lu. (2024)  
**SFOD: Spiking Fusion Object Detector**
<br/>
<button class="copy-to-clipboard" title="SFOD: Spiking Fusion Object Detector" index=117>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Object Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15192v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15192v1.pdf" filename="2403.15192v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Event cameras, characterized by high temporal resolution, high dynamic range, low power consumption, and high pixel bandwidth, offer unique capabilities for <b>object</b> <b>detection</b> in specialized contexts. Despite these advantages, the inherent sparsity and asynchrony of event data pose challenges to existing <b>object</b> <b>detection</b> algorithms. Spiking Neural Networks (SNNs), inspired by the way the human brain codes and processes information, offer a potential solution to these difficulties. However, their performance in <b>object</b> <b>detection</b> using event cameras is limited in current implementations. In this paper, we propose the Spiking Fusion <b>Object</b> <b>Detector</b> (SFOD), a simple and efficient approach to SNN-based <b>object</b> <b>detection.</b> Specifically, we design a Spiking Fusion Module, achieving the first-time fusion of feature maps from different scales in SNNs applied to event cameras. Additionally, through integrating our analysis and experiments conducted during the pretraining of the backbone network on the NCAR dataset, we delve deeply into the impact of spiking decoding strategies and loss functions on model performance. Thereby, we establish state-of-the-art classification results based on SNNs, achieving 93.7\% accuracy on the NCAR dataset. Experimental results on the GEN1 detection dataset demonstrate that the SFOD achieves a state-of-the-art mAP of 32.1\%, outperforming existing SNN-based approaches. Our research not only underscores the potential of SNNs in <b>object</b> <b>detection</b> with event cameras but also propels the advancement of SNNs. Code is available at https://github.com/yimeng-fan/SFOD.

{{</citation>}}


### (52/63 | 118/202) Modular Deep Active Learning Framework for Image Annotation: A Technical Report for the Ophthalmo-AI Project (Md Abdul Kadir et al., 2024)

{{<citation>}}

Md Abdul Kadir, Hasan Md Tusfiqur Alam, Pascale Maul, Hans-Jürgen Profitlich, Moritz Wolf, Daniel Sonntag. (2024)  
**Modular Deep Active Learning Framework for Image Annotation: A Technical Report for the Ophthalmo-AI Project**
<br/>
<button class="copy-to-clipboard" title="Modular Deep Active Learning Framework for Image Annotation: A Technical Report for the Ophthalmo-AI Project" index=118>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Active Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15143v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15143v1.pdf" filename="2403.15143v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Image annotation is one of the most essential tasks for guaranteeing proper treatment for patients and tracking progress over the course of therapy in the field of medical imaging and disease diagnosis. However, manually annotating a lot of 2D and 3D imaging data can be extremely tedious. Deep Learning (DL) based segmentation algorithms have completely transformed this process and made it possible to automate image segmentation. By accurately segmenting medical images, these algorithms can greatly minimize the time and effort necessary for manual annotation. Additionally, by incorporating <b>Active</b> <b>Learning</b> (AL) methods, these segmentation algorithms can perform far more effectively with a smaller amount of ground truth data. We introduce MedDeepCyleAL, an end-to-end framework implementing the complete AL cycle. It provides researchers with the flexibility to choose the type of deep learning model they wish to employ and includes an annotation tool that supports the classification and segmentation of medical images. The user-friendly interface allows for easy alteration of the AL and DL model settings through a configuration file, requiring no prior programming experience. While MedDeepCyleAL can be applied to any kind of image data, we have specifically applied it to ophthalmology data in this project.

{{</citation>}}


### (53/63 | 119/202) Gradient-based Sampling for Class Imbalanced Semi-supervised Object Detection (Jiaming Li et al., 2024)

{{<citation>}}

Jiaming Li, Xiangru Lin, Wei Zhang, Xiao Tan, Yingying Li, Junyu Han, Errui Ding, Jingdong Wang, Guanbin Li. (2024)  
**Gradient-based Sampling for Class Imbalanced Semi-supervised Object Detection**
<br/>
<button class="copy-to-clipboard" title="Gradient-based Sampling for Class Imbalanced Semi-supervised Object Detection" index=119>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Object Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15127v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15127v1.pdf" filename="2403.15127v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Current semi-supervised <b>object</b> <b>detection</b> (SSOD) algorithms typically assume class balanced datasets (PASCAL VOC etc.) or slightly class imbalanced datasets (MS-COCO, etc). This assumption can be easily violated since real world datasets can be extremely class imbalanced in nature, thus making the performance of semi-supervised <b>object</b> <b>detectors</b> far from satisfactory. Besides, the research for this problem in SSOD is severely under-explored. To bridge this research gap, we comprehensively study the class imbalance problem for SSOD under more challenging scenarios, thus forming the first experimental setting for class imbalanced SSOD (CI-SSOD). Moreover, we propose a simple yet effective gradient-based sampling framework that tackles the class imbalance problem from the perspective of two types of confirmation biases. To tackle confirmation bias towards majority classes, the gradient-based reweighting and gradient-based thresholding modules leverage the gradients from each class to fully balance the influence of the majority and minority classes. To tackle the confirmation bias from incorrect pseudo labels of minority classes, the class-rebalancing sampling module resamples unlabeled data following the guidance of the gradient-based reweighting module. Experiments on three proposed sub-tasks, namely MS-COCO, MS-COCO to Object365 and LVIS, suggest that our method outperforms current class imbalanced <b>object</b> <b>detectors</b> by clear margins, serving as a baseline for future research in CI-SSOD. Code will be available at https://github.com/nightkeepers/CI-SSOD.

{{</citation>}}


### (54/63 | 120/202) IFSENet : Harnessing Sparse Iterations for Interactive Few-shot Segmentation Excellence (Shreyas Chandgothia et al., 2024)

{{<citation>}}

Shreyas Chandgothia, Ardhendu Sekhar, Amit Sethi. (2024)  
**IFSENet : Harnessing Sparse Iterations for Interactive Few-shot Segmentation Excellence**
<br/>
<button class="copy-to-clipboard" title="IFSENet : Harnessing Sparse Iterations for Interactive Few-shot Segmentation Excellence" index=120>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Few-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15089v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15089v1.pdf" filename="2403.15089v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Training a computer vision system to segment a novel class typically requires collecting and painstakingly annotating lots of images with objects from that class. <b>Few-shot</b> segmentation techniques reduce the required number of images to learn to segment a new class, but careful annotations of object boundaries are still required. On the other hand, interactive segmentation techniques only focus on incrementally improving the segmentation of one object at a time (typically, using clicks given by an expert) in a class-agnostic manner. We combine the two concepts to drastically reduce the effort required to train segmentation models for novel classes. Instead of trivially feeding interactive segmentation masks as ground truth to a <b>few-shot</b> segmentation model, we propose IFSENet, which can accept sparse supervision on a single or few support images in the form of clicks to generate masks on support (training, at least clicked upon once) as well as query (test, never clicked upon) images. To trade-off effort for accuracy flexibly, the number of images and clicks can be incrementally added to the support set to further improve the segmentation of support as well as query images. The proposed model approaches the accuracy of previous state-of-the-art <b>few-shot</b> segmentation models with considerably lower annotation effort (clicks instead of maps), when tested on Pascal and SBD datasets on query images. It also works well as an interactive segmentation method on support images.

{{</citation>}}


### (55/63 | 121/202) An Integrated Neighborhood and Scale Information Network for Open-Pit Mine Change Detection in High-Resolution Remote Sensing Images (Zilin Xie et al., 2024)

{{<citation>}}

Zilin Xie, Kangning Li, Jinbao Jiang, Jinzhong Yang, Xiaojun Qiao, Deshuai Yuan, Cheng Nie. (2024)  
**An Integrated Neighborhood and Scale Information Network for Open-Pit Mine Change Detection in High-Resolution Remote Sensing Images**
<br/>
<button class="copy-to-clipboard" title="An Integrated Neighborhood and Scale Information Network for Open-Pit Mine Change Detection in High-Resolution Remote Sensing Images" index=121>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15032v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15032v1.pdf" filename="2403.15032v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Open-pit mine change detection (CD) in high-resolution (HR) remote sensing images plays a crucial role in mineral development and environmental protection. Significant progress has been made in this field in recent years, largely due to the advancement of deep learning techniques. However, existing deep-learning-based CD methods encounter challenges in effectively integrating neighborhood and scale information, resulting in suboptimal performance. Therefore, by exploring the influence patterns of neighborhood and scale information, this paper proposes an Integrated Neighborhood and Scale Information Network (INSINet) for open-pit mine CD in HR remote sensing images. Specifically, INSINet introduces 8-neighborhood-image information to acquire a larger receptive field, improving the recognition of center image boundary regions. Drawing on techniques of skip connection, deep supervision, and attention mechanism, the multi-path deep <b>supervised</b> attention (MDSA) module is designed to enhance multi-scale information fusion and change feature extraction. Experimental analysis reveals that incorporating neighborhood and scale information enhances the F1 score of INSINet by 6.40%, with improvements of 3.08% and 3.32% respectively. INSINet outperforms existing methods with an Overall Accuracy of 97.69%, Intersection over Union of 71.26%, and F1 score of 83.22%. INSINet shows significance for open-pit mine CD in HR remote sensing images.

{{</citation>}}


### (56/63 | 122/202) VRSO: Visual-Centric Reconstruction for Static Object Annotation (Chenyao Yu et al., 2024)

{{<citation>}}

Chenyao Yu, Yingfeng Cai, Jiaxin Zhang, Hui Kong, Wei Sui, Cong Yang. (2024)  
**VRSO: Visual-Centric Reconstruction for Static Object Annotation**
<br/>
<button class="copy-to-clipboard" title="VRSO: Visual-Centric Reconstruction for Static Object Annotation" index=122>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Object Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15026v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15026v1.pdf" filename="2403.15026v1.pdf">Download PDF</button>

---


**ABSTRACT**  
As a part of the perception results of intelligent driving systems, static <b>object</b> <b>detection</b> (SOD) in 3D space provides crucial cues for driving environment understanding. With the rapid deployment of deep neural networks for SOD tasks, the demand for high-quality training samples soars. The traditional, also reliable, way is manual labeling over the dense LiDAR point clouds and reference images. Though most public driving datasets adopt this strategy to provide SOD ground truth (GT), it is still expensive (requires LiDAR scanners) and low-efficient (time-consuming and unscalable) in practice. This paper introduces VRSO, a visual-centric approach for static <b>object</b> <b>annotation.</b> VRSO is distinguished in low cost, high efficiency, and high quality: (1) It recovers static <b>objects</b> <b>in</b> 3D space with only camera images as input, and (2) manual labeling is barely involved since GT for SOD tasks is generated based on an automatic reconstruction and annotation pipeline. (3) Experiments on the Waymo Open Dataset show that the mean reprojection error from VRSO annotation is only 2.6 pixels, around four times lower than the Waymo labeling (10.6 pixels). Source code is available at: https://github.com/CaiYingFeng/VRSO.

{{</citation>}}


### (57/63 | 123/202) Cell Tracking according to Biological Needs -- Strong Mitosis-aware Random-finite Sets Tracker with Aleatoric Uncertainty (Timo Kaiser et al., 2024)

{{<citation>}}

Timo Kaiser, Maximilian Schier, Bodo Rosenhahn. (2024)  
**Cell Tracking according to Biological Needs -- Strong Mitosis-aware Random-finite Sets Tracker with Aleatoric Uncertainty**
<br/>
<button class="copy-to-clipboard" title="Cell Tracking according to Biological Needs -- Strong Mitosis-aware Random-finite Sets Tracker with Aleatoric Uncertainty" index=123>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Stemming  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15011v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15011v1.pdf" filename="2403.15011v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Cell tracking and segmentation assist biologists in extracting insights from large-scale microscopy time-lapse data. Driven by local accuracy metrics, current tracking approaches often suffer from a lack of long-term consistency. To address this issue, we introduce an uncertainty estimation technique for neural tracking-by-regression frameworks and incorporate it into our novel extended Poisson multi-Bernoulli mixture tracker. Our uncertainty estimation identifies uncertain associations within high-performing tracking-by-regression methods using problem-specific test-time augmentations. Leveraging this uncertainty, along with a novel mitosis-aware assignment problem formulation, our tracker resolves false associations and mitosis detections <b>stemming</b> from long-term conflicts. We evaluate our approach on nine competitive datasets and demonstrate that it outperforms the current state-of-the-art on biologically relevant metrics substantially, achieving improvements by a factor of approximately $5.75$. Furthermore, we uncover new insights into the behavior of tracking-by-regression uncertainty.

{{</citation>}}


### (58/63 | 124/202) Clean-image Backdoor Attacks (Dazhong Rong et al., 2024)

{{<citation>}}

Dazhong Rong, Shuheng Shen, Xinyi Fu, Peng Qian, Jianhai Chen, Qinming He, Xing Fu, Weiqiang Wang. (2024)  
**Clean-image Backdoor Attacks**
<br/>
<button class="copy-to-clipboard" title="Clean-image Backdoor Attacks" index=124>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CR, cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Fairness  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15010v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15010v1.pdf" filename="2403.15010v1.pdf">Download PDF</button>

---


**ABSTRACT**  
To gather a significant quantity of annotated training data for high-performance image classification models, numerous companies opt to enlist third-party providers to label their unlabeled data. This practice is widely regarded as secure, even in cases where some annotated errors occur, as the impact of these minor inaccuracies on the final performance of the models is negligible and existing backdoor attacks require attacker's ability to poison the training images. Nevertheless, in this paper, we propose clean-image backdoor attacks which uncover that backdoors can still be injected via a fraction of incorrect labels without modifying the training images. Specifically, in our attacks, the attacker first seeks a trigger feature to divide the training images into two parts: those with the feature and those without it. Subsequently, the attacker falsifies the labels of the former part to a backdoor class. The backdoor will be finally implanted into the target model after it is trained on the poisoned data. During the inference phase, the attacker can activate the backdoor in two ways: slightly modifying the input image to obtain the trigger feature, or taking an image that naturally has the trigger feature as input. We conduct extensive experiments to demonstrate the effectiveness and practicality of our attacks. According to the experimental results, we conclude that our attacks seriously jeopardize the <b>fairness</b> and robustness of image classification models, and it is necessary to be vigilant about the incorrect labels in outsourced labeling.

{{</citation>}}


### (59/63 | 125/202) Generative Active Learning for Image Synthesis Personalization (Xulu Zhang et al., 2024)

{{<citation>}}

Xulu Zhang, Wengyu Zhang, Xiao-Yong Wei, Jinlin Wu, Zhaoxiang Zhang, Zhen Lei, Qing Li. (2024)  
**Generative Active Learning for Image Synthesis Personalization**
<br/>
<button class="copy-to-clipboard" title="Generative Active Learning for Image Synthesis Personalization" index=125>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Active Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14987v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14987v1.pdf" filename="2403.14987v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents a pilot study that explores the application of <b>active</b> <b>learning,</b> traditionally studied in the context of discriminative models, to generative models. We specifically focus on image synthesis personalization tasks. The primary challenge in conducting <b>active</b> <b>learning</b> on generative models lies in the open-ended nature of querying, which differs from the closed form of querying in discriminative models that typically target a single concept. We introduce the concept of anchor directions to transform the querying process into a semi-open problem. We propose a direction-based uncertainty sampling strategy to enable generative <b>active</b> <b>learning</b> and tackle the exploitation-exploration dilemma. Extensive experiments are conducted to validate the effectiveness of our approach, demonstrating that an open-source model can achieve superior performance compared to closed-source models developed by large companies, such as Google's StyleDrop. The source code is available at https://github.com/zhangxulu1996/GAL4Personalization.

{{</citation>}}


### (60/63 | 126/202) A Multimodal Approach for Cross-Domain Image Retrieval (Lucas Iijima et al., 2024)

{{<citation>}}

Lucas Iijima, Tania Stathaki. (2024)  
**A Multimodal Approach for Cross-Domain Image Retrieval**
<br/>
<button class="copy-to-clipboard" title="A Multimodal Approach for Cross-Domain Image Retrieval" index=126>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 6  
Keywords: Multi-modal, Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15152v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15152v1.pdf" filename="2403.15152v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Image generators are gaining vast amount of popularity and have rapidly changed how digital content is created. With the latest AI technology, millions of high quality images are being generated by the public, which are constantly motivating the research community to push the limits of generative models to create more complex and realistic images. This paper focuses on Cross-Domain Image Retrieval (CDIR) which can be used as an additional tool to inspect collections of generated images by determining the level of similarity between images in a dataset. An ideal retrieval system would be able to generalize to unseen complex images from multiple domains (e.g., photos, drawings and paintings). To address this goal, we propose a novel caption-matching approach that leverages <b>multimodal</b> language-vision architectures pre-trained on large datasets. The method is tested on DomainNet and Office-Home datasets and consistently achieves state-of-the-art performance over the latest approaches in the literature for cross-domain image retrieval. In order to verify the effectiveness with AI-generated images, the method was also put to test with a database composed by samples collected from Midjourney, which is a widely used generative platform for content creation.

{{</citation>}}


### (61/63 | 127/202) Recent Trends in 3D Reconstruction of General Non-Rigid Scenes (Raza Yunus et al., 2024)

{{<citation>}}

Raza Yunus, Jan Eric Lenssen, Michael Niemeyer, Yiyi Liao, Christian Rupprecht, Christian Theobalt, Gerard Pons-Moll, Jia-Bin Huang, Vladislav Golyanik, Eddy Ilg. (2024)  
**Recent Trends in 3D Reconstruction of General Non-Rigid Scenes**
<br/>
<button class="copy-to-clipboard" title="Recent Trends in 3D Reconstruction of General Non-Rigid Scenes" index=127>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-GR, cs.CV  
Keyword Score: 5  
Keywords: Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15064v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15064v1.pdf" filename="2403.15064v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Reconstructing models of the real world, including 3D <b>geometry,</b> appearance, and motion of real scenes, is essential for computer graphics and computer vision. It enables the synthesizing of photorealistic novel views, useful for the movie industry and AR/VR applications. It also facilitates the content creation necessary in computer games and AR/VR by avoiding laborious manual design processes. Further, such models are fundamental for intelligent computing systems that need to interpret real-world scenes and actions to act and interact safely with the human world. Notably, the world surrounding us is dynamic, and reconstructing models of dynamic, non-rigidly moving scenes is a severely underconstrained and challenging problem. This state-of-the-art report (STAR) offers the reader a comprehensive summary of state-of-the-art techniques with monocular and multi-view inputs such as data from RGB and RGB-D sensors, among others, conveying an understanding of different approaches, their potential applications, and promising further research directions. The report covers 3D reconstruction of general non-rigid scenes and further addresses the techniques for scene decomposition, editing and controlling, and generalizable and generative modeling. More specifically, we first review the common and fundamental concepts necessary to understand and navigate the field and then discuss the state-of-the-art techniques by reviewing recent approaches that use traditional and machine-learning-based neural representations, including a discussion on the newly enabled applications. The STAR is concluded with a discussion of the remaining limitations and open challenges.

{{</citation>}}


### (62/63 | 128/202) Survey on Modeling of Articulated Objects (Jiayi Liu et al., 2024)

{{<citation>}}

Jiayi Liu, Manolis Savva, Ali Mahdavi-Amiri. (2024)  
**Survey on Modeling of Articulated Objects**
<br/>
<button class="copy-to-clipboard" title="Survey on Modeling of Articulated Objects" index=128>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 5  
Keywords: Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14937v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14937v1.pdf" filename="2403.14937v1.pdf">Download PDF</button>

---


**ABSTRACT**  
3D modeling of articulated objects is a research problem within computer vision, graphics, and robotics. Its objective is to understand the shape and motion of the articulated components, represent the <b>geometry</b> and mobility of object parts, and create realistic models that reflect articulated objects in the real world. This survey provides a comprehensive overview of the current state-of-the-art in 3D modeling of articulated objects, with a specific focus on the task of articulated part perception and articulated object creation (reconstruction and generation). We systematically review and discuss the relevant literature from two perspectives: <b>geometry</b> processing and articulation modeling. Through this survey, we highlight the substantial progress made in these areas, outline the ongoing challenges, and identify gaps for future research. Our survey aims to serve as a foundational reference for researchers and practitioners in computer vision and graphics, offering insights into the complexities of articulated object modeling.

{{</citation>}}


### (63/63 | 129/202) An Open-World, Diverse, Cross-Spatial-Temporal Benchmark for Dynamic Wild Person Re-Identification (Lei Zhang et al., 2024)

{{<citation>}}

Lei Zhang, Xiaowei Fu, Fuxiang Huang, Yi Yang, Xinbo Gao. (2024)  
**An Open-World, Diverse, Cross-Spatial-Temporal Benchmark for Dynamic Wild Person Re-Identification**
<br/>
<button class="copy-to-clipboard" title="An Open-World, Diverse, Cross-Spatial-Temporal Benchmark for Dynamic Wild Person Re-Identification" index=129>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15119v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15119v1.pdf" filename="2403.15119v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Person re-identification (ReID) has made great strides thanks to the data-driven deep learning techniques. However, the existing <b>benchmark</b> datasets lack diversity, and models trained on these data cannot generalize well to dynamic wild scenarios. To meet the goal of improving the explicit generalization of ReID models, we develop a new Open-World, Diverse, Cross-Spatial-Temporal dataset named OWD with several distinct features. 1) Diverse collection scenes: multiple independent open-world and highly dynamic collecting scenes, including streets, intersections, shopping malls, etc. 2) Diverse lighting variations: long time spans from daytime to nighttime with abundant illumination changes. 3) Diverse person status: multiple camera networks in all seasons with normal/adverse weather conditions and diverse pedestrian appearances (e.g., clothes, personal belongings, poses, etc.). 4) Protected privacy: invisible faces for privacy critical applications. To improve the implicit generalization of ReID, we further propose a Latent Domain Expansion (LDE) method to develop the potential of source data, which decouples discriminative identity-relevant and trustworthy domain-relevant features and implicitly enforces domain-randomized identity feature space expansion with richer domain diversity to facilitate domain invariant representations. Our comprehensive evaluations with most <b>benchmark</b> datasets in the community are crucial for progress, although this work is far from the grand goal toward open-world and dynamic wild applications.

{{</citation>}}


## cs.AI (5)



### (1/5 | 130/202) Sphere Neural-Networks for Rational Reasoning (Tiansi Dong et al., 2024)

{{<citation>}}

Tiansi Dong, Mateja Jamnik, Pietro Liò. (2024)  
**Sphere Neural-Networks for Rational Reasoning**
<br/>
<button class="copy-to-clipboard" title="Sphere Neural-Networks for Rational Reasoning" index=130>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 50  
Keywords: ChatGPT, Question Answering, Reasoning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15297v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15297v1.pdf" filename="2403.15297v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The success of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> e.g., <b>ChatGPT,</b> is witnessed by their planetary popularity, their capability of human-like <b>question-answering,</b> <b>and</b> also by their steadily improved <b>reasoning</b> performance. However, it remains unclear whether <b>LLMs</b> reason. It is an open problem how traditional neural networks can be qualitatively extended to go beyond the statistic paradigm and achieve high-level cognition. Here, we present a minimalist qualitative extension by generalising computational building blocks from vectors to spheres. We propose Sphere Neural Networks (SphNNs) for human-like <b>reasoning</b> through model construction and inspection, and develop SphNN for syllogistic <b>reasoning,</b> a microcosm of human rationality. Instead of training data, SphNN uses a neuro-symbolic transition map of neighbourhood spatial relations to guide transformations from the current sphere configuration towards the target. SphNN is the first neural model that can determine the validity of long-chained syllogistic <b>reasoning</b> in one epoch by constructing sphere configurations as Euler diagrams, with the worst computational complexity of O(N^2). SphNN can evolve into various types of <b>reasoning,</b> such as spatio-temporal <b>reasoning,</b> logical <b>reasoning</b> with negation and disjunction, event <b>reasoning,</b> neuro-symbolic <b>reasoning,</b> and humour understanding (the highest level of cognition). All these suggest a new kind of Herbert A. Simon's scissors with two neural blades. SphNNs will tremendously enhance interdisciplinary collaborations to develop the two neural blades and realise deterministic neural <b>reasoning</b> and human-bounded rationality and elevate <b>LLMs</b> to reliable psychological AI. This work suggests that the non-zero radii of spheres are the missing components that prevent traditional deep-learning systems from reaching the realm of rational <b>reasoning</b> and cause <b>LLMs</b> to be trapped in the swamp of hallucination.

{{</citation>}}


### (2/5 | 131/202) A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning (Changmeng Zheng et al., 2024)

{{<citation>}}

Changmeng Zheng, Dayong Liang, Wengyu Zhang, Xiao-Yong Wei, Tat-Seng Chua, Qing Li. (2024)  
**A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning**
<br/>
<button class="copy-to-clipboard" title="A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning" index=131>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs-CL, cs-MA, cs-MM, cs.AI  
Keyword Score: 39  
Keywords: Graph, Multi-modal, Multi-modal, Question Answering, Reasoning, Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14972v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14972v1.pdf" filename="2403.14972v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents a pilot study aimed at introducing multi-agent debate into <b>multimodal</b> <b>reasoning.</b> The study addresses two key challenges: the trivialization of opinions resulting from excessive <b>summarization</b> and the diversion of focus caused by distractor concepts introduced from images. These challenges stem from the inductive (bottom-up) nature of existing debating schemes. To address the issue, we propose a deductive (top-down) debating approach called Blueprint Debate on <b>Graphs</b> (BDoG). In BDoG, debates are confined to a blueprint <b>graph</b> to prevent opinion trivialization through world-level <b>summarization.</b> Moreover, by storing evidence in branches within the <b>graph,</b> BDoG mitigates distractions caused by frequent but irrelevant concepts. Extensive experiments validate BDoG, achieving state-of-the-art results in Science <b>QA</b> and MMBench with significant improvements over previous methods.

{{</citation>}}


### (3/5 | 132/202) CACA Agent: Capability Collaboration based AI Agent (Peng Xu et al., 2024)

{{<citation>}}

Peng Xu, Haoran Wang, Chuang Wang, Xu Liu. (2024)  
**CACA Agent: Capability Collaboration based AI Agent**
<br/>
<button class="copy-to-clipboard" title="CACA Agent: Capability Collaboration based AI Agent" index=132>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs-CL, cs-MA, cs.AI  
Keyword Score: 30  
Keywords: Reasoning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15137v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15137v1.pdf" filename="2403.15137v1.pdf">Download PDF</button>

---


**ABSTRACT**  
As AI Agents based on <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown potential in practical applications across various fields, how to quickly deploy an AI agent and how to conveniently expand the application scenario of AI agents has become a challenge. Previous studies mainly focused on implementing all the <b>reasoning</b> capabilities of AI agents within a single <b>LLM,</b> which often makes the model more complex and also reduces the extensibility of AI agent functionality. In this paper, we propose CACA Agent (Capability Collaboration based AI Agent), using an open architecture inspired by service computing. CACA Agent integrates a set of collaborative capabilities to implement AI Agents, not only reducing the dependence on a single <b>LLM,</b> but also enhancing the extensibility of both the planning abilities and the tools available to AI agents. Utilizing the proposed system, we present a demo to illustrate the operation and the application scenario extension of CACA Agent.

{{</citation>}}


### (4/5 | 133/202) Collaborative AI Teaming in Unknown Environments via Active Goal Deduction (Zuyuan Zhang et al., 2024)

{{<citation>}}

Zuyuan Zhang, Hanhan Zhou, Mahdi Imani, Taeyoung Lee, Tian Lan. (2024)  
**Collaborative AI Teaming in Unknown Environments via Active Goal Deduction**
<br/>
<button class="copy-to-clipboard" title="Collaborative AI Teaming in Unknown Environments via Active Goal Deduction" index=133>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs-MA, cs.AI  
Keyword Score: 10  
Keywords: Zero-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15341v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15341v1.pdf" filename="2403.15341v1.pdf">Download PDF</button>

---


**ABSTRACT**  
With the advancements of artificial intelligence (AI), we're seeing more scenarios that require AI to work closely with other agents, whose goals and strategies might not be known beforehand. However, existing approaches for training collaborative agents often require defined and known reward signals and cannot address the problem of teaming with unknown agents that often have latent objectives/rewards. In response to this challenge, we propose teaming with unknown agents framework, which leverages kernel density Bayesian inverse learning method for active goal deduction and utilizes pre-trained, goal-conditioned policies to enable <b>zero-shot</b> policy adaptation. We prove that unbiased reward estimates in our framework are sufficient for optimal teaming with unknown agents. We further evaluate the framework of redesigned multi-agent particle and StarCraft II micromanagement environments with diverse unknown agents of different behaviors/rewards. Empirical results demonstrate that our framework significantly advances the teaming performance of AI and unknown agents in a wide range of collaborative scenarios.

{{</citation>}}


### (5/5 | 134/202) Safe Learning of PDDL Domains with Conditional Effects -- Extended Version (Argaman Mordoch et al., 2024)

{{<citation>}}

Argaman Mordoch, Enrico Scala, Roni Stern, Brendan Juba. (2024)  
**Safe Learning of PDDL Domains with Conditional Effects -- Extended Version**
<br/>
<button class="copy-to-clipboard" title="Safe Learning of PDDL Domains with Conditional Effects -- Extended Version" index=134>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 10  
Keywords: Planning Domain Descrition Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15251v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15251v1.pdf" filename="2403.15251v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Powerful domain-independent planners have been developed to solve various types of planning problems. These planners often require a model of the acting agent's actions, given in some planning domain description language. Manually designing such an action model is a notoriously challenging task. An alternative is to automatically learn action models from observation. Such an action model is called safe if every plan created with it is consistent with the real, unknown action model. Algorithms for learning such safe action models exist, yet they cannot handle domains with conditional or universal effects, which are common constructs in many planning problems. We prove that learning non-trivial safe action models with conditional effects may require an exponential number of samples. Then, we identify reasonable assumptions under which such learning is tractable and propose SAM Learning of Conditional Effects (Conditional-SAM), the first algorithm capable of doing so. We analyze Conditional-SAM theoretically and evaluate it experimentally. Our results show that the action models learned by Conditional-SAM can be used to solve perfectly most of the test set problems in most of the experimented domains.

{{</citation>}}


## q-bio.OT (1)



### (1/1 | 135/202) Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review (Jinge Wang et al., 2024)

{{<citation>}}

Jinge Wang, Zien Cheng, Qiuming Yao, Li Liu, Dong Xu, Gangqing Hu. (2024)  
**Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review**
<br/>
<button class="copy-to-clipboard" title="Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review" index=135>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: q-bio.OT  
Categories: cs-AI, q-bio-OT, q-bio.OT  
Keyword Score: 50  
Keywords: ChatGPT, Chatbot, Text Mining, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15274v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15274v1.pdf" filename="2403.15274v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The year 2023 marked a significant surge in the exploration of applying <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> <b>chatbots,</b> notably <b>ChatGPT,</b> across various disciplines. We surveyed the applications of <b>ChatGPT</b> in various sectors of bioinformatics and biomedical informatics throughout the year, covering omics, genetics, biomedical <b>text</b> <b>mining,</b> drug discovery, biomedical image understanding, bioinformatics programming, and bioinformatics education. Our survey delineates the current strengths and limitations of this <b>chatbot</b> in bioinformatics and offers insights into potential avenues for future development.

{{</citation>}}


## cs.CY (5)



### (1/5 | 136/202) InstaSynth: Opportunities and Challenges in Generating Synthetic Instagram Data with ChatGPT for Sponsored Content Detection (Thales Bertaglia et al., 2024)

{{<citation>}}

Thales Bertaglia, Lily Heisig, Rishabh Kaushal, Adriana Iamnitchi. (2024)  
**InstaSynth: Opportunities and Challenges in Generating Synthetic Instagram Data with ChatGPT for Sponsored Content Detection**
<br/>
<button class="copy-to-clipboard" title="InstaSynth: Opportunities and Challenges in Generating Synthetic Instagram Data with ChatGPT for Sponsored Content Detection" index=136>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CY  
Categories: cs-CL, cs-CY, cs-SI, cs.CY  
Keyword Score: 50  
Keywords: ChatGPT, Content Detection, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15214v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15214v1.pdf" filename="2403.15214v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> raise concerns about lowering the cost of generating texts that could be used for unethical or illegal purposes, especially on social media. This paper investigates the promise of such models to help enforce legal requirements related to the disclosure of sponsored <b>content</b> <b>online.</b> We investigate the use of <b>LLMs</b> for generating synthetic Instagram captions with two objectives: The first objective (fidelity) is to produce realistic synthetic datasets. For this, we implement <b>content-level</b> <b>and</b> network-level metrics to assess whether synthetic captions are realistic. The second objective (utility) is to create synthetic data that is useful for sponsored <b>content</b> <b>detection.</b> For this, we evaluate the effectiveness of the generated synthetic data for training classifiers to identify undisclosed advertisements on Instagram. Our investigations show that the objectives of fidelity and utility may conflict and that <b>prompt</b> engineering is a useful but insufficient strategy. Additionally, we find that while individual synthetic posts may appear realistic, collectively they lack diversity, topic connectivity, and realistic user interaction patterns.

{{</citation>}}


### (2/5 | 137/202) Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception (Luyang Lin et al., 2024)

{{<citation>}}

Luyang Lin, Lingzhi Wang, Jinsong Guo, Kam-Fai Wong. (2024)  
**Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception**
<br/>
<button class="copy-to-clipboard" title="Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception" index=137>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CY  
Categories: cs-CY, cs.CY  
Keyword Score: 40  
Keywords: Fine-tuning, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14896v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14896v1.pdf" filename="2403.14896v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The pervasive spread of misinformation and disinformation in social media underscores the critical importance of detecting media bias. While robust <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have emerged as foundational tools for bias prediction, concerns about inherent biases within these models persist. In this work, we investigate the presence and nature of bias within <b>LLMs</b> and its consequential impact on media bias detection. Departing from conventional approaches that focus solely on bias detection in media content, we delve into biases within the <b>LLM</b> systems themselves. Through meticulous examination, we probe whether <b>LLMs</b> exhibit biases, particularly in political bias prediction and text continuation tasks. Additionally, we explore bias across diverse topics, aiming to uncover nuanced variations in bias expression within the <b>LLM</b> framework. Importantly, we propose debiasing strategies, including <b>prompt</b> engineering and model <b>fine-tuning.</b> Extensive analysis of bias tendencies across different <b>LLMs</b> sheds light on the broader landscape of bias propagation in language models. This study advances our understanding of <b>LLM</b> bias, offering critical insights into its implications for bias detection tasks and paving the way for more robust and equitable AI systems

{{</citation>}}


### (3/5 | 138/202) AI Teaches the Art of Elegant Coding: Timely, Fair, and Helpful Style Feedback in a Global Course (Juliette Woodrow et al., 2024)

{{<citation>}}

Juliette Woodrow, Ali Malik, Chris Piech. (2024)  
**AI Teaches the Art of Elegant Coding: Timely, Fair, and Helpful Style Feedback in a Global Course**
<br/>
<button class="copy-to-clipboard" title="AI Teaches the Art of Elegant Coding: Timely, Fair, and Helpful Style Feedback in a Global Course" index=138>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CY  
Categories: cs-CY, cs.CY  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14986v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14986v1.pdf" filename="2403.14986v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Teaching students how to write code that is elegant, reusable, and comprehensible is a fundamental part of CS1 education. However, providing this "style feedback" in a timely manner has proven difficult to scale. In this paper, we present our experience deploying a novel, real-time style feedback tool in Code in Place, a <b>large-scale</b> <b>online</b> <b>CS1</b> course. Our tool is based on the latest breakthroughs in <b>large-language</b> <b>models</b> <b>(LLMs)</b> and was carefully designed to be safe and helpful for students. We used our Real-Time Style Feedback tool (RTSF) in a class with over 8,000 diverse students from across the globe and ran a randomized control trial to understand its benefits. We show that students who received style feedback in real-time were five times more likely to view and engage with their feedback compared to students who received delayed feedback. Moreover, those who viewed feedback were more likely to make significant style-related edits to their code, with over 79% of these edits directly incorporating their feedback. We also discuss the practicality and dangers of <b>LLM-based</b> tools for feedback, investigating the quality of the feedback generated, <b>LLM</b> limitations, and techniques for consistency, standardization, and safeguarding against demographic bias, all of which are crucial for a tool utilized by students.

{{</citation>}}


### (4/5 | 139/202) Learners Teaching Novices: An Uplifting Alternative Assessment (Ali Malik et al., 2024)

{{<citation>}}

Ali Malik, Juliette Woodrow, Chris Piech. (2024)  
**Learners Teaching Novices: An Uplifting Alternative Assessment**
<br/>
<button class="copy-to-clipboard" title="Learners Teaching Novices: An Uplifting Alternative Assessment" index=139>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CY  
Categories: cs-CY, cs.CY  
Keyword Score: 10  
Keywords: Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14971v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14971v1.pdf" filename="2403.14971v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We propose and carry-out a novel method of formative assessment called Assessment via Teaching (AVT), in which learners demonstrate their understanding of CS1 topics by tutoring more novice students. AVT has powerful benefits over traditional forms of assessment: it is centered around service to others and is highly rewarding for the learners who teach. Moreover, teaching greatly improves the learners' own understanding of the material and has a huge positive impact on novices, who receive free 1:1 tutoring. Lastly, this form of assessment is naturally difficult to cheat -- a critical property for assessments in the era of <b>large-language</b> <b>models.</b> <b>We</b> use AVT in a randomised control trial with learners in a CS1 course at an R1 university. The learners provide tutoring sessions to more novice students taking a lagged online version of the same course. We show that learners who do an AVT session before the course exam performed 20 to 30 percentage points better than the class average on several questions. Moreover, compared to students who did a practice exam, the AVT learners enjoyed their experience more and were twice as likely to study for their teaching session. We believe AVT is a scalable and uplifting method for formative assessment that could one day replace traditional exams.

{{</citation>}}


### (5/5 | 140/202) KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing (Yahya Badran et al., 2024)

{{<citation>}}

Yahya Badran, Christine Preisach. (2024)  
**KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing**
<br/>
<button class="copy-to-clipboard" title="KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing" index=140>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CY  
Categories: cs-AI, cs-CY, cs-LG, cs.CY  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15304v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15304v1.pdf" filename="2403.15304v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Knowledge Tracing (KT) is concerned with predicting students' future performance on learning items in intelligent tutoring systems. Learning items are tagged with skill labels called knowledge concepts (KCs). Many KT models expand the sequence of item-student interactions into KC-student interactions by replacing learning items with their constituting KCs. This often results in a longer sequence length. This approach addresses the issue of sparse item-student interactions and minimises model parameters. However, two problems have been identified with such models. The first problem is the model's ability to learn correlations between KCs belonging to the same item, which can result in the leakage of ground truth labels and hinder performance. This problem can lead to a significant decrease in performance on datasets with a higher number of KCs per item. The second problem is that the available <b>benchmark</b> implementations ignore accounting for changes in sequence length when expanding KCs, leading to different models being tested with varying sequence lengths but still compared against the same <b>benchmark.</b> To address these problems, we introduce a general masking framework that mitigates the first problem and enhances the performance of such KT models while preserving the original model architecture without significant alterations. Additionally, we introduce KTbench, an open-source <b>benchmark</b> library designed to ensure the reproducibility of this work while mitigating the second problem.

{{</citation>}}


## eess.SP (1)



### (1/1 | 141/202) Adaptive Coded Federated Learning: Privacy Preservation and Straggler Mitigation (Chengxi Li et al., 2024)

{{<citation>}}

Chengxi Li, Ming Xiao, Mikael Skoglund. (2024)  
**Adaptive Coded Federated Learning: Privacy Preservation and Straggler Mitigation**
<br/>
<button class="copy-to-clipboard" title="Adaptive Coded Federated Learning: Privacy Preservation and Straggler Mitigation" index=141>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SP  
Categories: cs-CR, cs-LG, eess-SP, eess.SP  
Keyword Score: 50  
Keywords: Federated Learning, Mutual Information, Simulation, Simulator, Differential Privacy  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14905v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14905v1.pdf" filename="2403.14905v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this article, we address the problem of <b>federated</b> <b>learning</b> in the presence of stragglers. For this problem, a coded <b>federated</b> <b>learning</b> framework has been proposed, where the central server aggregates gradients received from the non-stragglers and gradient computed from a privacy-preservation global coded dataset to mitigate the negative impact of the stragglers. However, when aggregating these gradients, fixed weights are consistently applied across iterations, neglecting the generation process of the global coded dataset and the dynamic nature of the trained model over iterations. This oversight may result in diminished learning performance. To overcome this drawback, we propose a new method named adaptive coded <b>federated</b> <b>learning</b> (ACFL). In ACFL, before the training, each device uploads a coded local dataset with additive noise to the central server to generate a global coded dataset under privacy preservation requirements. During each iteration of the training, the central server aggregates the gradients received from the non-stragglers and the gradient computed from the global coded dataset, where an adaptive policy for varying the aggregation weights is designed. Under this policy, we optimize the performance in terms of privacy and learning, where the learning performance is analyzed through convergence analysis and the privacy performance is characterized via <b>mutual</b> <b>information</b> <b>differential</b> <b>privacy.</b> Finally, we perform <b>simulations</b> to demonstrate the superiority of ACFL compared with the non-adaptive methods.

{{</citation>}}


## cs.SI (5)



### (1/5 | 142/202) Hierarchical Information Enhancement Network for Cascade Prediction in Social Networks (Fanrui Zhang et al., 2024)

{{<citation>}}

Fanrui Zhang, Jiawei Liu, Qiang Zhang, Xiaoling Zhu, Zheng-Jun Zha. (2024)  
**Hierarchical Information Enhancement Network for Cascade Prediction in Social Networks**
<br/>
<button class="copy-to-clipboard" title="Hierarchical Information Enhancement Network for Cascade Prediction in Social Networks" index=142>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SI  
Categories: cs-AI, cs-SI, cs.SI  
Keyword Score: 46  
Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Multi-modal, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15257v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15257v1.pdf" filename="2403.15257v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Understanding information cascades in networks is a fundamental issue in numerous applications. Current researches often sample cascade information into several independent paths or subgraphs to learn a simple cascade representation. However, these approaches fail to exploit the hierarchical semantic associations between different modalities, limiting their predictive performance. In this work, we propose a novel Hierarchical Information Enhancement Network (HIENet) for cascade prediction. Our approach integrates fundamental cascade sequence, user social <b>graphs,</b> <b>and</b> <b>sub-cascade</b> <b>graph</b> <b>into</b> <b>a</b> unified framework. Specifically, HIENet utilizes DeepWalk to sample cascades information into a series of sequences. It then gathers path information between users to extract the social relationships of propagators. Additionally, we employ a time-stamped <b>graph</b> <b>convolutional</b> <b>network</b> to aggregate sub-cascade <b>graph</b> <b>information</b> <b>effectively.</b> Ultimately, we introduce a <b>Multi-modal</b> Cascade <b>Transformer</b> to powerfully fuse these clues, providing a comprehensive understanding of cascading process. Extensive experiments have demonstrated the effectiveness of the proposed method.

{{</citation>}}


### (2/5 | 143/202) LLM-Driven Agents for Influencer Selection in Digital Advertising Campaigns (Xiaoqing Zhang et al., 2024)

{{<citation>}}

Xiaoqing Zhang, Xiuying Chen, Yuhan Liu, Jianzhou Wang, Zhenxing Hu, Rui Yan. (2024)  
**LLM-Driven Agents for Influencer Selection in Digital Advertising Campaigns**
<br/>
<button class="copy-to-clipboard" title="LLM-Driven Agents for Influencer Selection in Digital Advertising Campaigns" index=143>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SI  
Categories: cs-SI, cs.SI  
Keyword Score: 40  
Keywords: Simulation, Simulator, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15105v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15105v1.pdf" filename="2403.15105v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In the digital world, influencers are pivotal as opinion leaders, shaping the views and choices of their influencees. Modern advertising often follows this trend, where marketers choose appropriate influencers for product endorsements, based on thorough market analysis. Previous studies on influencer selection have typically relied on numerical representations of individual opinions and interactions, a method that simplifies the intricacies of social dynamics. With the development of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> we now have the opportunity to capture the nuanced exchanges of information within social networks. Hence, in this work, we first introduce an Influencer Dynamics Simulator (IDS), helping promoters identify and select the right influencers to market their products, based on <b>LLM</b> <b>simulation.</b> Concretely, we first propose an influencer-influencee engagement-based pre-selection module to screen potential influencer candidates. Subsequently, a <b>simulation</b> is constructed for these candidates and their influencees. Each user is represented as an <b>LLM-based</b> agent, drawing from their interaction history to deduce their profile and interests. The influencee agents will predict their behavior in response to influencer advertising. Finally, we develop a ranking metric designed to pinpoint influencers who are most likely to drive product purchases based on feedback from their influencees. To evaluate our framework, we collect a real-world advertising network dataset, including social relations, post and comment content, and user behaviors. Our dataset covers six products in diverse categories with their promoter influencers. Experiments show that IDS accurately identifies influencers from hundreds of candidates and provides valuable insights through detailed comments and specific attitudes.

{{</citation>}}


### (3/5 | 144/202) Multi-perspective Memory Enhanced Network for Identifying Key Nodes in Social Networks (Qiang Zhang et al., 2024)

{{<citation>}}

Qiang Zhang, Jiawei Liu, Fanrui Zhang, Xiaoling Zhu, Zheng-Jun Zha. (2024)  
**Multi-perspective Memory Enhanced Network for Identifying Key Nodes in Social Networks**
<br/>
<button class="copy-to-clipboard" title="Multi-perspective Memory Enhanced Network for Identifying Key Nodes in Social Networks" index=144>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SI  
Categories: cs-AI, cs-SI, cs.SI  
Keyword Score: 13  
Keywords: Graph Attention Networks, Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15235v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15235v1.pdf" filename="2403.15235v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Identifying key nodes in social networks plays a crucial role in timely blocking false information. Existing key node identification methods usually consider node influence only from the propagation structure perspective and have insufficient generalization ability to unknown scenarios. In this paper, we propose a novel Multi-perspective Memory Enhanced Network (MMEN) for identifying key nodes in social networks, which mines key nodes from multiple perspectives and utilizes memory networks to store historical information. Specifically, MMEN first constructs two propagation networks from the perspectives of user attributes and propagation structure and updates node feature representations using <b>graph</b> <b>attention</b> <b>networks.</b> Meanwhile, the memory network is employed to store information of similar subgraphs, enhancing the model's generalization performance in unknown scenarios. Finally, MMEN applies adaptive weights to combine the node influence of the two propagation networks to select the ultimate key nodes. Extensive experiments demonstrate that our method significantly outperforms previous methods.

{{</citation>}}


### (4/5 | 145/202) Ellipsoidal embeddings of graphs (Michaël Fanuel et al., 2024)

{{<citation>}}

Michaël Fanuel, Antoine Aspeel, Michael T. Schaub, Jean-Charles Delvenne. (2024)  
**Ellipsoidal embeddings of graphs**
<br/>
<button class="copy-to-clipboard" title="Ellipsoidal embeddings of graphs" index=145>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SI  
Categories: cs-DM, cs-SI, cs.SI  
Keyword Score: 13  
Keywords: Graph, Graph Embedding  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15023v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15023v1.pdf" filename="2403.15023v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Due to their flexibility to represent almost any kind of relational data, <b>graph-based</b> <b>models</b> have enjoyed a tremendous success over the past decades. While <b>graphs</b> <b>are</b> inherently only combinatorial objects, however, many prominent analysis tools are based on the algebraic representation of <b>graphs</b> <b>via</b> matrices such as the <b>graph</b> <b>Laplacian,</b> or on associated <b>graph</b> <b>embeddings.</b> Such embeddings associate to each node a set of coordinates in a vector space, a representation which can then be employed for learning tasks such as the classification or alignment of the nodes of the <b>graph.</b> <b>As</b> the geometric picture provided by embedding methods enables the use of a multitude of methods developed for vector space data, embeddings have thus gained interest both from a theoretical as well as a practical perspective. Inspired by trace-optimization problems, often encountered in the analysis of <b>graph-based</b> <b>data,</b> here we present a method to derive ellipsoidal embeddings of the nodes of a <b>graph,</b> <b>in</b> which each node is assigned a set of coordinates on the surface of a hyperellipsoid. Our method may be seen as an alternative to popular spectral embedding techniques, to which it shares certain similarities we discuss. To illustrate the utility of the embedding we conduct a case study in which analyse synthetic and real world networks with modular structure, and compare the results obtained with known methods in the literature.

{{</citation>}}


### (5/5 | 146/202) Unraveling Contagion Origins: Optimal Estimation through Maximum-Likelihood and Starlike Tree Approximation in Markovian Spreading Models (Pei-Duo Yu et al., 2024)

{{<citation>}}

Pei-Duo Yu, Chee Wei Tan, Liang Zheng, Chao Zhao. (2024)  
**Unraveling Contagion Origins: Optimal Estimation through Maximum-Likelihood and Starlike Tree Approximation in Markovian Spreading Models**
<br/>
<button class="copy-to-clipboard" title="Unraveling Contagion Origins: Optimal Estimation through Maximum-Likelihood and Starlike Tree Approximation in Markovian Spreading Models" index=146>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SI  
Categories: cs-SI, cs.SI  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14890v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14890v1.pdf" filename="2403.14890v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Identifying the source of epidemic-like spread in networks is crucial for tasks like removing internet viruses or finding the rumor source in online social networks. The challenge lies in tracing the source from a snapshot observation of infected nodes. How do we accurately pinpoint the source? Utilizing snapshot data, we apply a probabilistic approach, focusing on the <b>graph</b> boundary and the observed time, to detect sources via an effective maximum likelihood algorithm. A novel starlike tree approximation extends applicability to general <b>graphs,</b> demonstrating versatility. We highlight the utility of the Gamma function for analyzing the asymptotic behavior of the likelihood ratio between nodes. Comprehensive evaluations confirm algorithmic effectiveness in diverse network scenarios, advancing rumor source detection in large-scale network analysis and information dissemination strategies.

{{</citation>}}


## q-fin.CP (2)



### (1/2 | 147/202) Construction of a Japanese Financial Benchmark for Large Language Models (Masanori Hirano, 2024)

{{<citation>}}

Masanori Hirano. (2024)  
**Construction of a Japanese Financial Benchmark for Large Language Models**
<br/>
<button class="copy-to-clipboard" title="Construction of a Japanese Financial Benchmark for Large Language Models" index=147>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: q-fin.CP  
Categories: cs-CL, q-fin-CP, q-fin.CP  
Keyword Score: 43  
Keywords: Benchmarking, GPT, GPT-4, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15062v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15062v1.pdf" filename="2403.15062v1.pdf">Download PDF</button>

---


**ABSTRACT**  
With the recent development of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> models that focus on certain domains and languages have been discussed for their necessity. There is also a growing need for <b>benchmarks</b> to evaluate the performance of current <b>LLMs</b> in each domain. Therefore, in this study, we constructed a <b>benchmark</b> comprising multiple tasks specific to the Japanese and financial domains and performed <b>benchmark</b> measurements on some models. Consequently, we confirmed that <b>GPT-4</b> is currently outstanding, and that the constructed <b>benchmarks</b> function effectively. According to our analysis, our <b>benchmark</b> can differentiate <b>benchmark</b> scores among models in all performance ranges by combining tasks with different difficulties.

{{</citation>}}


### (2/2 | 148/202) Robust Utility Optimization via a GAN Approach (Florian Krach et al., 2024)

{{<citation>}}

Florian Krach, Josef Teichmann, Hanna Wutte. (2024)  
**Robust Utility Optimization via a GAN Approach**
<br/>
<button class="copy-to-clipboard" title="Robust Utility Optimization via a GAN Approach" index=148>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: q-fin.CP  
Categories: 91-08, 68T07, 91G10, 91G60, cs-LG, q-fin-CP, q-fin-MF, q-fin-PM, q-fin.CP  
Keyword Score: 20  
Keywords: Generative Adversarial Network, Generative Adversarial Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15243v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15243v1.pdf" filename="2403.15243v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Robust utility optimization enables an investor to deal with market uncertainty in a structured way, with the goal of maximizing the worst-case outcome. In this work, we propose a <b>generative</b> <b>adversarial</b> <b>network</b> <b>(GAN)</b> approach to (approximately) solve robust utility optimization problems in general and realistic settings. In particular, we model both the investor and the market by neural networks (NN) and train them in a mini-max zero-sum game. This approach is applicable for any continuous utility function and in realistic market settings with trading costs, where only observable information of the market can be used. A large empirical study shows the versatile usability of our method. Whenever an optimal reference strategy is available, our method performs on par with it and in the (many) settings without known optimal strategy, our method outperforms all other reference strategies. Moreover, we can conclude from our study that the trained path-dependent strategies do not outperform Markovian ones. Lastly, we uncover that our <b>generative</b> <b>approach</b> <b>for</b> learning optimal, (non-) robust investments under trading costs generates universally applicable alternatives to well known asymptotic strategies of idealized settings.

{{</citation>}}


## cs.IT (7)



### (1/7 | 149/202) Mutual Information of a class of Poisson-type Channels using Markov Renewal Theory (Maximilian Gehri et al., 2024)

{{<citation>}}

Maximilian Gehri, Nicolai Engelmann, Heinz Koeppl. (2024)  
**Mutual Information of a class of Poisson-type Channels using Markov Renewal Theory**
<br/>
<button class="copy-to-clipboard" title="Mutual Information of a class of Poisson-type Channels using Markov Renewal Theory" index=149>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, math-IT  
Keyword Score: 40  
Keywords: Continuous Time, Continuous Time, Mutual Information, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15221v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15221v1.pdf" filename="2403.15221v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The <b>mutual</b> <b>information</b> (MI) of Poisson-type channels has been linked to a filtering problem since the 70s, but its evaluation for specific <b>continuous-time,</b> <b>discrete-state</b> systems remains a demanding task. As an advantage, Markov renewal processes (MrP) retain their renewal property under state space filtering. This offers a way to solve the filtering problem analytically for small systems. We consider a class of communication systems $X \to Y$ that can be derived from a MrP by a custom filtering procedure. For the subclasses, where (i) $Y$ is a renewal process or (ii) $(X,Y)$ belongs to a class of MrPs, we provide an evolution equation for finite transmission duration $T>0$ and limit theorems for $T \to \infty$ that facilitate <b>simulation-free</b> evaluation of the MI $\mathbb{I}(X_{[0,T]}; Y_{[0,T]})$ and its associated <b>mutual</b> <b>information</b> rate (MIR). In other cases, <b>simulation</b> cost is significantly reduced. We show that systems with an additional $X$-modulating level $C$, which statically chooses between different processes $X_{[0,T]}(c)$, can naturally be included in our framework thereby giving an expression for $\mathbb{I}(C; Y_{[0,T]})$. The theoretical framework is showcased in an application to bacterial gene expression, where filtering is analytically tractable.

{{</citation>}}


### (2/7 | 150/202) Information Rates of Successive Interference Cancellation for Optical Fiber (Alex Jäger et al., 2024)

{{<citation>}}

Alex Jäger, Gerhard Kramer. (2024)  
**Information Rates of Successive Interference Cancellation for Optical Fiber**
<br/>
<button class="copy-to-clipboard" title="Information Rates of Successive Interference Cancellation for Optical Fiber" index=150>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, eess-SP, math-IT  
Keyword Score: 30  
Keywords: Message-Passing, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15240v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15240v1.pdf" filename="2403.15240v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Successive interference cancellation (SIC) is used to approach the achievable information rates (AIRs) of joint detection and decoding for long-haul optical fiber links. The AIRs of memoryless ring constellations are compared to those of circularly symmetric complex Gaussian modulation for surrogate channel models with correlated phase noise. <b>Simulations</b> are performed for 1000 km of standard single-mode fiber with ideal Raman amplification. In this setup, 32 rings and 16 SIC-stages with Gaussian <b>message-passing</b> receivers achieve the AIR peaks of previous work. The computational complexity scales in proportion to the number of SIC-stages, where one stage has the complexity of separate detection and decoding.

{{</citation>}}


### (3/7 | 151/202) Uplink soft handover for LEO constellations: how strong the inter-satellite link should be (Houcem Ben Salem et al., 2024)

{{<citation>}}

Houcem Ben Salem, Alberto Tarable, Alessandro Nordio, Behrooz Makki. (2024)  
**Uplink soft handover for LEO constellations: how strong the inter-satellite link should be**
<br/>
<button class="copy-to-clipboard" title="Uplink soft handover for LEO constellations: how strong the inter-satellite link should be" index=151>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, eess-SP, math-IT  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15131v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15131v1.pdf" filename="2403.15131v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We consider a constellation of low-earth-orbit (LEO) satellites connected to a handheld device on the ground. Due to the very large orbital speed, an effective handover strategy becomes of paramount importance. In particular, we study the benefits of soft handover in the uplink from the physical-layer point of view. We give a realistic model for both the ground-to-satellite and the inter-satellite links, following the 3GPP channel model for the former. We suppose that, during handover from a serving satellite to a target satellite, one of the two satellites forwards the received signal from the ground user to the other, thus acting as a relay. We quantify through <b>simulations</b> the loss of hard handover, compared to soft handover. For the latter, we test both amplify-and-forward (AF) and decode-and-forward (DF) relaying techniques and verify that, at least in the simulated conditions, DF does not repay, in terms of block error rate (BLER), the increase of complexity with respect to AF. Also, we study the effect of the LEO constellation size on the network BLER. Finally, we show that, with soft handover, the impact of misalignment on the inter-satellite link is severe, especially at optical frequencies.

{{</citation>}}


### (4/7 | 152/202) Range-Angle Estimation for FDA-MIMO System With Frequency Offset (Mengjiang Sun et al., 2024)

{{<citation>}}

Mengjiang Sun, Peng Chen, Zhenxin Cao. (2024)  
**Range-Angle Estimation for FDA-MIMO System With Frequency Offset**
<br/>
<button class="copy-to-clipboard" title="Range-Angle Estimation for FDA-MIMO System With Frequency Offset" index=152>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, eess-SP, math-IT  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14978v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14978v1.pdf" filename="2403.14978v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Frequency diverse array multiple-input multiple-output (FDA-MIMO) radar differs from the traditional phased array (PA) radar, and can form range-angle-dependent beampattern and differentiate between closely spaced targets sharing the same angle but occupying distinct range cells. In the FDA-MIMO radar, target range estimation is achieved by employing a subtle frequency variation between adjacent array antennas, so the estimation performance is degraded severely in a practical scenario with frequency offset. In this paper, the range-angle estimation problem for FDA-MIMO radar is considered with frequency offsets in both transmitting and receiving arrays. First, we build a system model for the FDA-MIMO radar with transmitting and receiving frequency offsets. Then, the frequency offset is transferred into an equalized additional noise. The noise characteristics are analyzed in detail theoretically, together with the influence on the range-angle estimation. Moreover, since the effect of the transmitting frequency offset is similar to additional colored noise, denoising algorithms are introduced to mitigate the performance deterioration caused by the frequency offset. Finally, Cram\'{e}r-Rao lower bounds (CRLB) for the range-angle estimation are derived in the scenario with the frequency offsets. <b>Simulation</b> results show the analysis of frequency offset and the corresponding estimation performance using different algorithms.

{{</citation>}}


### (5/7 | 153/202) Secure Outage Analysis for RIS-Aided MISO Systems with Randomly Located Eavesdroppers (Wei Shi et al., 2024)

{{<citation>}}

Wei Shi, Jindan Xu, Wei Xu, Chau Yuen, A. Lee Swindlehurst, Xiaohu You, Chunming Zhao. (2024)  
**Secure Outage Analysis for RIS-Aided MISO Systems with Randomly Located Eavesdroppers**
<br/>
<button class="copy-to-clipboard" title="Secure Outage Analysis for RIS-Aided MISO Systems with Randomly Located Eavesdroppers" index=153>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, eess-SP, math-IT  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14911v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14911v1.pdf" filename="2403.14911v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we consider the physical layer security of an RIS-assisted multiple-antenna communication system with randomly located eavesdroppers. The exact distributions of the received signal-to-noise-ratios (SNRs) at the legitimate user and the eavesdroppers located according to a Poisson point process (PPP) are derived, and a closed-form expression for the secrecy outage probability (SOP) is obtained. It is revealed that the secrecy performance is mainly affected by the number of RIS reflecting elements, and the impact of the transmit antennas and transmit power at the base station is marginal. In addition, when the locations of the randomly located eavesdroppers are unknown, deploying the RIS closer to the legitimate user rather than to the base station is shown to be more efficient. We also perform an analytical study demonstrating that the secrecy diversity order depends on the path loss exponent of the RIS-to-ground links. Finally, numerical <b>simulations</b> are conducted to verify the accuracy of these theoretical observations.

{{</citation>}}


### (6/7 | 154/202) Robust Resource Allocation for STAR-RIS Assisted SWIPT Systems (Guangyu Zhu et al., 2024)

{{<citation>}}

Guangyu Zhu, Xidong Mu, Li Guo, Ao Huang, Shibiao Xu. (2024)  
**Robust Resource Allocation for STAR-RIS Assisted SWIPT Systems**
<br/>
<button class="copy-to-clipboard" title="Robust Resource Allocation for STAR-RIS Assisted SWIPT Systems" index=154>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, eess-SP, math-IT  
Keyword Score: 10  
Keywords: Fairness  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15145v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15145v1.pdf" filename="2403.15145v1.pdf">Download PDF</button>

---


**ABSTRACT**  
A simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) assisted simultaneous wireless information and power transfer (SWIPT) system is proposed. More particularly, an STAR-RIS is deployed to assist in the information/power transfer from a multi-antenna access point (AP) to multiple single-antenna information users (IUs) and energy users (EUs), where two practical STAR-RIS operating protocols, namely energy splitting (ES) and time switching (TS), are employed. Under the imperfect channel state information (CSI) condition, a multi-objective optimization problem (MOOP) framework, that simultaneously maximizes the minimum data rate and minimum harvested power, is employed to investigate the fundamental rate-energy trade-off between IUs and EUs. To obtain the optimal robust resource allocation strategy, the MOOP is first transformed into a single-objective optimization problem (SOOP) via the {\epsilon}-constraint method, which is then reformulated by approximating semi-infinite inequality constraints with the S-procedure. For ES, an alternating optimization (AO)-based algorithm is proposed to jointly design AP active beamforming and STAR-RIS passive beamforming, where a penalty method is leveraged in STAR-RIS beamforming design. Furthermore, the developed algorithm is extended to optimize the time allocation policy and beamforming vectors in a two-layer iterative manner for TS. Numerical results reveal that: 1) deploying STAR-RISs achieves a significant performance gain over conventional RISs, especially in terms of harvested power for EUs; 2) the ES protocol obtains a better user <b>fairness</b> performance when focusing only on IUs or EUs, while the TS protocol yields a better balance between IUs and EUs; 3) the imperfect CSI affects IUs more significantly than EUs, whereas TS can confer a more robust design to attenuate these effects.

{{</citation>}}


### (7/7 | 155/202) Coexisting Passive RIS and Active Relay Assisted NOMA Systems (Ao Huang et al., 2024)

{{<citation>}}

Ao Huang, Li Guo, Xidong Mu, Chao Dong, Yuanwei Liu. (2024)  
**Coexisting Passive RIS and Active Relay Assisted NOMA Systems**
<br/>
<button class="copy-to-clipboard" title="Coexisting Passive RIS and Active Relay Assisted NOMA Systems" index=155>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, eess-SP, math-IT  
Keyword Score: 10  
Keywords: Fairness  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15130v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15130v1.pdf" filename="2403.15130v1.pdf">Download PDF</button>

---


**ABSTRACT**  
A novel coexisting passive reconfigurable intelligent surface (RIS) and active decode-and-forward (DF) relay assisted non-orthogonal multiple access (NOMA) transmission framework is proposed. In particular, two communication protocols are conceived, namely Hybrid NOMA (H-NOMA) and Full NOMA (F-NOMA). Based on the proposed two protocols, both the sum rate maximization and max-min rate <b>fairness</b> problems are formulated for jointly optimizing the power allocation at the access point and relay as well as the passive beamforming design at the RIS. To tackle the non-convex problems, an alternating optimization (AO) based algorithm is first developed, where the transmit power and the RIS phase-shift are alternatingly optimized by leveraging the two-dimensional search and rank-relaxed difference-of-convex (DC) programming, respectively. Then, a two-layer penalty based joint optimization (JO) algorithm is developed to jointly optimize the resource allocation coefficients within each iteration. Finally, numerical results demonstrate that: i) the proposed coexisting RIS and relay assisted transmission framework is capable of achieving a significant user performance improvement than conventional schemes without RIS or relay; ii) compared with the AO algorithm, the JO algorithm requires less execution time at the cost of a slight performance loss; and iii) the H-NOMA and F-NOMA protocols are generally preferable for ensuring user rate <b>fairness</b> and enhancing user sum rate, respectively.

{{</citation>}}


## cs.AR (1)



### (1/1 | 156/202) Allspark: Workload Orchestration for Visual Transformers on Processing In-Memory Systems (Mengke Ge et al., 2024)

{{<citation>}}

Mengke Ge, Junpeng Wang, Binhan Chen, Yingjian Zhong, Haitao Du, Song Chen, Yi Kang. (2024)  
**Allspark: Workload Orchestration for Visual Transformers on Processing In-Memory Systems**
<br/>
<button class="copy-to-clipboard" title="Allspark: Workload Orchestration for Visual Transformers on Processing In-Memory Systems" index=156>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AR  
Categories: cs-AR, cs.AR  
Keyword Score: 40  
Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15069v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15069v1.pdf" filename="2403.15069v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The advent of <b>Transformers</b> has revolutionized computer vision, offering a powerful alternative to <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs),</b> especially with the local attention mechanism that excels at capturing local structures within the input and achieve state-of-the-art performance. Processing in-memory (PIM) architecture offers extensive parallelism, low data movement costs, and scalable memory bandwidth, making it a promising solution to accelerate <b>Transformer</b> with memory-intensive operations. However, the crucial challenge lies in efficiently deploying the entire model onto a resource-limited PIM system while parallelizing each <b>transformer</b> block with potentially many computational branches based on local attention mechanisms. We present Allspark, which focuses on workload orchestration for visual <b>Transformers</b> on PIM systems, aiming at minimizing inference latency. Firstly, to fully utilize the massive parallelism of PIM, Allspark empolys a finer-grained partitioning scheme for computational branches, and format a systematic layout and interleaved dataflow with maximized data locality and reduced data movement. Secondly, Allspark formulates the scheduling of the complete model on a resource-limited distributed PIM system as an integer linear programming (ILP) problem. Thirdly, as local-global data interactions exhibit complex yet regular dependencies, Allspark provides a greedy-based mapping method to allocate computational branches onto the PIM system and minimize NoC communication costs. Extensive experiments on 3D-stacked DRAM-based PIM systems show that Allspark brings 1.2x-24.0x inference speedup for various visual <b>Transformers</b> over baselines, and that Allspark-enriched PIM system yields average speedups of 2.3x and energy savings of 20x-55x over Nvidia V100 GPU.

{{</citation>}}


## physics.med-ph (1)



### (1/1 | 157/202) Digital twin model of colon electromechanics for manometry prediction of laser tissue soldering (René Thierry Djoumessi et al., 2024)

{{<citation>}}

René Thierry Djoumessi, Pietro Lenarda, Alessio Gizzi, Simone Giusti, Pietro Alduini, Marco Paggi. (2024)  
**Digital twin model of colon electromechanics for manometry prediction of laser tissue soldering**
<br/>
<button class="copy-to-clipboard" title="Digital twin model of colon electromechanics for manometry prediction of laser tissue soldering" index=157>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: physics.med-ph  
Categories: cs-CE, physics-med-ph, physics.med-ph  
Keyword Score: 35  
Keywords: Fine-tuning, Geometry, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15129v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15129v1.pdf" filename="2403.15129v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The present study introduces an advanced multi-physics and multi-scale modeling approach to investigate in silico colon motility. We introduce a generalized electromechanical framework, integrating cellular electrophysiology and smooth muscle contractility, thus advancing a first-of-its-kind computational model of laser tissue soldering after incision resection. The proposed theoretical framework comprises three main elements: a microstructural material model describing intestine wall <b>geometry</b> and composition of reinforcing fibers, with four fiber families, two active-conductive and two passive; an electrophysiological model describing the propagation of slow waves, based on a fully-coupled nonlinear phenomenological approach; and a thermodynamical consistent mechanical model describing the hyperelastic energetic contributions ruling tissue equilibrium under diverse loading conditions. The active strain approach was adopted to describe tissue electromechanics by exploiting the multiplicative decomposition of the deformation gradient for each active fiber family and solving the governing equations via a staggered finite element scheme. The computational framework was <b>fine-tuned</b> according to state-of-the-art experimental evidence, and extensive numerical analyses allowed us to compare manometric traces computed via numerical <b>simulations</b> with those obtained clinically in human patients. The model proved capable of reproducing both qualitatively and quantitatively high or low-amplitude propagation contractions. Colon motility after laser tissue soldering demonstrates that material properties and couplings of the deposited tissue are critical to reproducing a physiological muscular contraction, thus restoring a proper peristaltic activity.

{{</citation>}}


## eess.SY (4)



### (1/4 | 158/202) Cascading Blackout Severity Prediction with Statistically-Augmented Graph Neural Networks (Joe Gorka et al., 2024)

{{<citation>}}

Joe Gorka, Tim Hsu, Wenting Li, Yury Maximov, Line Roald. (2024)  
**Cascading Blackout Severity Prediction with Statistically-Augmented Graph Neural Networks**
<br/>
<button class="copy-to-clipboard" title="Cascading Blackout Severity Prediction with Statistically-Augmented Graph Neural Networks" index=158>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-LG, cs-SY, eess-SY, eess.SY  
Keyword Score: 33  
Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15363v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15363v1.pdf" filename="2403.15363v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Higher variability in grid conditions, resulting from growing renewable penetration and increased incidence of extreme weather events, has increased the difficulty of screening for scenarios that may lead to catastrophic cascading failures. Traditional power-flow-based tools for assessing cascading blackout risk are too slow to properly explore the space of possible failures and load/generation patterns. We add to the growing literature of faster <b>graph-neural-network</b> <b>(GNN)-based</b> <b>techniques,</b> developing two novel techniques for the estimation of blackout magnitude from initial grid conditions. First we propose several methods for employing an initial classification step to filter out safe "non blackout" scenarios prior to magnitude estimation. Second, using insights from the statistical properties of cascading blackouts, we propose a method for facilitating non-local message passing in our <b>GNN</b> models. We validate these two approaches on a large simulated dataset, and show the potential of both to increase blackout size estimation performance.

{{</citation>}}


### (2/4 | 159/202) Control Designs for Critical-Continegency Responsible Grid-Following Inverters and Seamless Transitions To and From Grid-Forming Modes (Jaesang Park et al., 2024)

{{<citation>}}

Jaesang Park, Alireza Askarian, Srinivasa Salapaka. (2024)  
**Control Designs for Critical-Continegency Responsible Grid-Following Inverters and Seamless Transitions To and From Grid-Forming Modes**
<br/>
<button class="copy-to-clipboard" title="Control Designs for Critical-Continegency Responsible Grid-Following Inverters and Seamless Transitions To and From Grid-Forming Modes" index=159>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15380v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15380v1.pdf" filename="2403.15380v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This article introduces two control frameworks: one for Grid-Following (GFL) inverters aiding Grid-Forming (GFM) inverters in voltage regulation during large contingency events and optimizing power transactions under normal conditions; and another for seamless transitions between grid-tied and grid-isolated setups, managing voltage transient characteristics. In microgrids, GFM inverters regulate voltage, while GFL inverters handle power transactions. The proposed GFL control detects abrupt load/generation changes, adjusting power transactions using local storage to support GFM inverters during contingencies. Additionally, a transition control ensures smooth GFL-GFM shifts, reducing power and voltage fluctuations. <b>Simulation</b> results validate improved voltage regulation during contingencies and enhanced power tracking during slow changes, alongside minimized transient overshoot.

{{</citation>}}


### (3/4 | 160/202) Optimal Data-Driven Prediction and Predictive Control using Signal Matrix Models (Roy S. Smith et al., 2024)

{{<citation>}}

Roy S. Smith, Mohamed Abdalmoaty, Mingzhou Yin. (2024)  
**Optimal Data-Driven Prediction and Predictive Control using Signal Matrix Models**
<br/>
<button class="copy-to-clipboard" title="Optimal Data-Driven Prediction and Predictive Control using Signal Matrix Models" index=160>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15329v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15329v1.pdf" filename="2403.15329v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Data-driven control uses a past signal trajectory to characterise the input-output behaviour of a system. Willems' lemma provides a data-based prediction model allowing a control designer to bypass the step of identifying a state-space or transfer function model. This paper provides a more parsimonious formulation of Willems' lemma that separates the model into initial condition matching and predictive control design parts. This avoids the need for regularisers in the predictive control problem that are found in other data-driven predictive control methods. It also gives a closed form expression for the optimal (minimum variance) unbiased predictor of the future output trajectory and applies it for predictive control. <b>Simulation</b> comparisons illustrate very good control performance.

{{</citation>}}


### (4/4 | 161/202) Event-Triggered State Estimation Through Confidence Level (Wei Liu, 2024)

{{<citation>}}

Wei Liu. (2024)  
**Event-Triggered State Estimation Through Confidence Level**
<br/>
<button class="copy-to-clipboard" title="Event-Triggered State Estimation Through Confidence Level" index=161>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 10  
Keywords: Discrete Time, Discrete Time  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15289v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15289v1.pdf" filename="2403.15289v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper considers the state estimation problem for <b>discrete-time</b> <b>linear</b> systems under event-triggered scheme. In order to improve performance, a novel event-triggered scheme based on confidence level is proposed using the chi-square distribution and mild regularity assumption. In terms of the novel event-triggered scheme, a minimum mean squared error (MMSE) state estimator is proposed using some results presented in this paper. Two algorithms for communication rate estimation of the proposed MMSE state estimator are developed where the first algorithm is based on information with one-step delay, and the second algorithm is based on information with two-step delay. The performance and effectiveness of the proposed MMSE state estimator and the two communication rate estimation algorithms are illustrated using a target tracking scenario.

{{</citation>}}


## cs.RO (15)



### (1/15 | 162/202) Safe and Stable Teleoperation of Quadrotor UAVs under Haptic Shared Autonomy (Dawei Zhang et al., 2024)

{{<citation>}}

Dawei Zhang, Roberto Tron. (2024)  
**Safe and Stable Teleoperation of Quadrotor UAVs under Haptic Shared Autonomy**
<br/>
<button class="copy-to-clipboard" title="Safe and Stable Teleoperation of Quadrotor UAVs under Haptic Shared Autonomy" index=162>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 30  
Keywords: Simulation, Simulator, human-in-the-loop  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15335v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15335v1.pdf" filename="2403.15335v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present a novel approach that aims to address both safety and stability of a haptic teleoperation system within a framework of Haptic Shared Autonomy (HSA). We use Control Barrier Functions (CBFs) to generate the control input that follows the user's input as closely as possible while guaranteeing safety. In the context of stability of the <b>human-in-the-loop</b> system, we limit the force feedback perceived by the user via a small $L_2$-gain, which is achieved by limiting the control and the force feedback via a differential constraint. Specifically, with the property of HSA, we propose two pathways to design the control and the force feedback: Sequential Control Force (SCF) and Joint Control Force (JCF). Both designs can achieve safety and stability but with different responses to the user's commands. We conducted experimental <b>simulations</b> to evaluate and investigate the properties of the designed methods. We also tested the proposed method on a physical quadrotor UAV and a haptic interface.

{{</citation>}}


### (2/15 | 163/202) Guided Decoding for Robot Motion Generation and Adaption (Nutan Chen et al., 2024)

{{<citation>}}

Nutan Chen, Elie Aljalbout, Botond Cseke, Patrick van der Smagt. (2024)  
**Guided Decoding for Robot Motion Generation and Adaption**
<br/>
<button class="copy-to-clipboard" title="Guided Decoding for Robot Motion Generation and Adaption" index=163>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-LG, cs-RO, cs.RO  
Keyword Score: 30  
Keywords: Autoencoder, Variational Autoencoder, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15239v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15239v1.pdf" filename="2403.15239v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We address motion generation for high-DoF robot arms in complex settings with obstacles, via points, etc. A significant advancement in this domain is achieved by integrating Learning from Demonstration (LfD) into the motion generation process. This integration facilitates rapid adaptation to new tasks and optimizes the utilization of accumulated expertise by allowing robots to learn and generalize from demonstrated trajectories. We train a <b>transformer</b> architecture on a large dataset of simulated trajectories. This architecture, based on a conditional <b>variational</b> <b>autoencoder</b> <b>transformer,</b> learns essential motion generation skills and adapts these to meet auxiliary tasks and constraints. Our auto-regressive approach enables real-time integration of feedback from the physical system, enhancing the adaptability and efficiency of motion generation. We show that our model can generate motion from initial and target points, but also that it can adapt trajectories in navigating complex tasks, including obstacle avoidance, via points, and meeting velocity and acceleration constraints, across platforms.

{{</citation>}}


### (3/15 | 164/202) Boundary-Aware Value Function Generation for Safe Stochastic Motion Planning (Junhong Xu et al., 2024)

{{<citation>}}

Junhong Xu, Kai Yin, Jason M. Gregory, Kris Hauser, Lantao Liu. (2024)  
**Boundary-Aware Value Function Generation for Safe Stochastic Motion Planning**
<br/>
<button class="copy-to-clipboard" title="Boundary-Aware Value Function Generation for Safe Stochastic Motion Planning" index=164>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 30  
Keywords: Human Intervention, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14956v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14956v1.pdf" filename="2403.14956v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Navigation safety is critical for many autonomous systems such as self-driving vehicles in an urban environment. It requires an explicit consideration of boundary constraints that describe the borders of any infeasible, non-navigable, or unsafe regions. We propose a principled boundary-aware safe stochastic planning framework with promising results. Our method generates a value function that can strictly distinguish the state values between free (safe) and non-navigable (boundary) spaces in the continuous state, naturally leading to a safe boundary-aware policy. At the core of our solution lies a seamless integration of finite elements and kernel-based functions, where the finite elements allow us to characterize safety-critical states' borders accurately, and the kernel-based function speeds up computation for the non-safety-critical states. The proposed method was evaluated through extensive <b>simulations</b> and demonstrated safe navigation behaviors in mobile navigation tasks. Additionally, we demonstrate that our approach can maneuver safely and efficiently in cluttered real-world environments using a ground vehicle with strong external disturbances, such as navigating on a slippery floor and against external <b>human</b> <b>intervention.</b>

{{</citation>}}


### (4/15 | 165/202) Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in Safety Monitoring Applications (Vít Krátký et al., 2024)

{{<citation>}}

Vít Krátký, Giuseppe Silano, Matouš Vrba, Christos Papaioannidis, Ioannis Mademlis, Robert Pěnička, Ioannis Pitas, Martin Saska. (2024)  
**Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in Safety Monitoring Applications**
<br/>
<button class="copy-to-clipboard" title="Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in Safety Monitoring Applications" index=165>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15333v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15333v1.pdf" filename="2403.15333v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents a formation control approach for contactless gesture-based Human-Swarm Interaction (HSI) between a team of multi-rotor Unmanned Aerial Vehicles (UAVs) and a human worker. The approach is intended for monitoring the safety of human workers, especially those working at heights. In the proposed dynamic formation scheme, one UAV acts as the leader of the formation and is equipped with sensors for human worker detection and gesture recognition. The follower UAVs maintain a predetermined formation relative to the worker's position, thereby providing additional perspectives of the monitored scene. Hand gestures allow the human worker to specify movements and action commands for the UAV team and initiate other mission-related commands without the need for an additional communication channel or specific markers. Together with a novel unified human detection and tracking algorithm, human pose estimation approach and gesture detection pipeline, the proposed approach forms a first instance of an HSI system incorporating all these modules onboard real-world UAVs. <b>Simulations</b> and field experiments with three UAVs and a human worker in a mock-up scenario showcase the effectiveness and responsiveness of the proposed approach.

{{</citation>}}


### (5/15 | 166/202) Infrastructure-Assisted Collaborative Perception in Automated Valet Parking: A Safety Perspective (Yukuan Jia et al., 2024)

{{<citation>}}

Yukuan Jia, Jiawen Zhang, Shimeng Lu, Baokang Fan, Ruiqing Mao, Sheng Zhou, Zhisheng Niu. (2024)  
**Infrastructure-Assisted Collaborative Perception in Automated Valet Parking: A Safety Perspective**
<br/>
<button class="copy-to-clipboard" title="Infrastructure-Assisted Collaborative Perception in Automated Valet Parking: A Safety Perspective" index=166>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-CV, cs-RO, cs-SY, cs.RO, eess-SY  
Keyword Score: 20  
Keywords: Autoencoder, Quantization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15156v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15156v1.pdf" filename="2403.15156v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Environmental perception in Automated Valet Parking (AVP) has been a challenging task due to severe occlusions in parking garages. Although Collaborative Perception (CP) can be applied to broaden the field of view of connected vehicles, the limited bandwidth of vehicular communications restricts its application. In this work, we propose a BEV feature-based CP network architecture for infrastructure-assisted AVP systems. The model takes the roadside camera and LiDAR as optional inputs and adaptively fuses them with onboard sensors in a unified BEV representation. <b>Autoencoder</b> and downsampling are applied for channel-wise and spatial-wise dimension reduction, while sparsification and <b>quantization</b> further compress the feature map with little loss in data precision. Combining these techniques, the size of a BEV feature map is effectively compressed to fit in the feasible data rate of the NR-V2X network. With the synthetic AVP dataset, we observe that CP can effectively increase perception performance, especially for pedestrians. Moreover, the advantage of infrastructure-assisted CP is demonstrated in two typical safety-critical scenarios in the AVP setting, increasing the maximum safe cruising speed by up to 3m/s in both scenarios.

{{</citation>}}


### (6/15 | 167/202) RHINO-VR Experience: Teaching Mobile Robotics Concepts in an Interactive Museum Exhibit (Erik Schlachhoff et al., 2024)

{{<citation>}}

Erik Schlachhoff, Nils Dengler, Leif Van Holland, Patrick Stotko, Jorge de Heuvel, Reinhard Klein, Maren Bennewitz. (2024)  
**RHINO-VR Experience: Teaching Mobile Robotics Concepts in an Interactive Museum Exhibit**
<br/>
<button class="copy-to-clipboard" title="RHINO-VR Experience: Teaching Mobile Robotics Concepts in an Interactive Museum Exhibit" index=167>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15151v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15151v1.pdf" filename="2403.15151v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In 1997, the very first tour guide robot RHINO was deployed in a museum in Germany. With the ability to navigate autonomously through the environment, the robot gave tours to over 2,000 visitors. Today, RHINO itself has become an exhibit and is no longer operational. In this paper, we present RHINO-VR, an interactive museum exhibit using virtual reality (VR) that allows museum visitors to experience the historical robot RHINO in operation in a virtual museum. RHINO-VR, unlike static exhibits, enables users to familiarize themselves with basic mobile robotics concepts without the fear of damaging the exhibit. In the virtual environment, the user is able to interact with RHINO in VR by pointing to a location to which the robot should navigate and observing the corresponding actions of the robot. To include other visitors who cannot use the VR, we provide an external observation view to make RHINO visible to them. We evaluated our system by measuring the frame rate of the VR <b>simulation,</b> comparing the generated virtual 3D models with the originals, and conducting a user study. The user-study showed that RHINO-VR improved the visitors' understanding of the robot's functionality and that they would recommend experiencing the VR exhibit to others.

{{</citation>}}


### (7/15 | 168/202) A Twin Delayed Deep Deterministic Policy Gradient Algorithm for Autonomous Ground Vehicle Navigation via Digital Twin Perception Awareness (Kabirat Olayemi et al., 2024)

{{<citation>}}

Kabirat Olayemi, Mien Van, Sean McLoone, Yuzhu Sun, Jack Close, Nguyen Minh Nhat, Stephen McIlvanna. (2024)  
**A Twin Delayed Deep Deterministic Policy Gradient Algorithm for Autonomous Ground Vehicle Navigation via Digital Twin Perception Awareness**
<br/>
<button class="copy-to-clipboard" title="A Twin Delayed Deep Deterministic Policy Gradient Algorithm for Autonomous Ground Vehicle Navigation via Digital Twin Perception Awareness" index=168>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs-SY, cs.RO, eess-SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15067v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15067v1.pdf" filename="2403.15067v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Autonomous ground vehicle (UGV) navigation has the potential to revolutionize the transportation system by increasing accessibility to disabled people, ensure safety and convenience of use. However, UGV requires extensive and efficient testing and evaluation to ensure its acceptance for public use. This testing are mostly done in a simulator which result to sim2real transfer gap. In this paper, we propose a digital twin perception awareness approach for the control of robot navigation without prior creation of the virtual environment (VT) environment state. To achieve this, we develop a twin delayed deep deterministic policy gradient (TD3) algorithm that ensures collision avoidance and goal-based path planning. We demonstrate the performance of our approach on different environment dynamics. We show that our approach is capable of efficiently avoiding collision with obstacles and navigating to its desired destination, while at the same time safely avoids obstacles using the information received from the LIDAR sensor mounted on the robot. Our approach bridges the gap between sim-to-real transfer and contributes to the adoption of UGVs in real world. We validate our approach in <b>simulation</b> and a real-world application in an office space.

{{</citation>}}


### (8/15 | 169/202) Linear Quadratic Guidance Law for Joint Motion Planning of a Pursuer-Turret Assembly (Bhargav Jha et al., 2024)

{{<citation>}}

Bhargav Jha, Shaunak Bopardikar, Alexander Von Moll, David Casbeer. (2024)  
**Linear Quadratic Guidance Law for Joint Motion Planning of a Pursuer-Turret Assembly**
<br/>
<button class="copy-to-clipboard" title="Linear Quadratic Guidance Law for Joint Motion Planning of a Pursuer-Turret Assembly" index=169>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs-SY, cs.RO, eess-SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14997v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14997v1.pdf" filename="2403.14997v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents joint motion planning of a vehicle with an attached rotating turret. The turret has a limited range as well as the field of view. The objective is capture a maneuvering target such that at the terminal time it is withing the field-of-view and range limits. Catering to it, we present a minimum effort guidance law that commensurate for the turn rate abilities of the vehicle and the turret. The guidance law is obtained using linearization about the collision triangle and admits an analytical solution. <b>Simulation</b> results are presented to exemplify the cooperation between the turret and the vehicle.

{{</citation>}}


### (9/15 | 170/202) OceanPlan: Hierarchical Planning and Replanning for Natural Language AUV Piloting in Large-scale Unexplored Ocean Environments (Ruochu Yang et al., 2024)

{{<citation>}}

Ruochu Yang, Fumin Zhang, Mengxue Hou. (2024)  
**OceanPlan: Hierarchical Planning and Replanning for Natural Language AUV Piloting in Large-scale Unexplored Ocean Environments**
<br/>
<button class="copy-to-clipboard" title="OceanPlan: Hierarchical Planning and Replanning for Natural Language AUV Piloting in Large-scale Unexplored Ocean Environments" index=170>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 10  
Keywords: Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15369v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15369v1.pdf" filename="2403.15369v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We develop a hierarchical <b>LLM-task-motion</b> planning and replanning framework to efficiently ground an abstracted human command into tangible Autonomous Underwater Vehicle (AUV) control through enhanced representations of the world. We also incorporate a holistic replanner to provide real-world feedback with all planners for robust AUV operation. While there has been extensive research in bridging the gap between <b>LLMs</b> and robotic missions, they are unable to guarantee success of AUV applications in the vast and unknown ocean environment. To tackle specific challenges in marine robotics, we design a hierarchical planner to compose executable motion plans, which achieves planning efficiency and solution quality by decomposing long-horizon missions into sub-tasks. At the same time, real-time data stream is obtained by a replanner to address environmental uncertainties during plan execution. Experiments validate that our proposed framework delivers successful AUV performance of long-duration missions through natural language piloting.

{{</citation>}}


### (10/15 | 171/202) HortiBot: An Adaptive Multi-Arm System for Robotic Horticulture of Sweet Peppers (Christian Lenz et al., 2024)

{{<citation>}}

Christian Lenz, Rohit Menon, Michael Schreiber, Melvin Paul Jacob, Sven Behnke, Maren Bennewitz. (2024)  
**HortiBot: An Adaptive Multi-Arm System for Robotic Horticulture of Sweet Peppers**
<br/>
<button class="copy-to-clipboard" title="HortiBot: An Adaptive Multi-Arm System for Robotic Horticulture of Sweet Peppers" index=171>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 10  
Keywords: Pruning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15306v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15306v1.pdf" filename="2403.15306v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Horticultural tasks such as <b>pruning</b> and selective harvesting are labor intensive and horticultural staff are hard to find. Automating these tasks is challenging due to the semi-structured greenhouse workspaces, changing environmental conditions such as lighting, dense plant growth with many occlusions, and the need for gentle manipulation of non-rigid plant organs. In this work, we present the three-armed system HortiBot, with two arms for manipulation and a third arm as an articulated head for active perception using stereo cameras. Its perception system detects not only peppers, but also peduncles and stems in real time, and performs online data association to build a world model of pepper plants. Collision-aware online trajectory generation allows all three arms to safely track their respective targets for observation, grasping, and cutting. We integrated perception and manipulation to perform selective harvesting of peppers and evaluated the system in lab experiments. Using active perception coupled with end-effector force torque sensing for compliant manipulation, HortiBot achieves high success rates.

{{</citation>}}


### (11/15 | 172/202) TriHelper: Zero-Shot Object Navigation with Dynamic Assistance (Lingfeng Zhang et al., 2024)

{{<citation>}}

Lingfeng Zhang, Qiang Zhang, Hao Wang, Erjia Xiao, Zixuan Jiang, Honglei Chen, Renjing Xu. (2024)  
**TriHelper: Zero-Shot Object Navigation with Dynamic Assistance**
<br/>
<button class="copy-to-clipboard" title="TriHelper: Zero-Shot Object Navigation with Dynamic Assistance" index=172>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 10  
Keywords: Zero-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15223v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15223v1.pdf" filename="2403.15223v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Navigating toward specific objects in unknown environments without additional training, known as <b>Zero-Shot</b> object navigation, poses a significant challenge in the field of robotics, which demands high levels of auxiliary information and strategic planning. Traditional works have focused on holistic solutions, overlooking the specific challenges agents encounter during navigation such as collision, low exploration efficiency, and misidentification of targets. To address these challenges, our work proposes TriHelper, a novel framework designed to assist agents dynamically through three primary navigation challenges: collision, exploration, and detection. Specifically, our framework consists of three innovative components: (i) Collision Helper, (ii) Exploration Helper, and (iii) Detection Helper. These components work collaboratively to solve these challenges throughout the navigation process. Experiments on the Habitat-Matterport 3D (HM3D) and Gibson datasets demonstrate that TriHelper significantly outperforms all existing baseline methods in <b>Zero-Shot</b> object navigation, showcasing superior success rates and exploration efficiency. Our ablation studies further underscore the effectiveness of each helper in addressing their respective challenges, notably enhancing the agent's navigation capabilities. By proposing TriHelper, we offer a fresh perspective on advancing the object navigation task, paving the way for future research in the domain of Embodied AI and visual-based navigation.

{{</citation>}}


### (12/15 | 173/202) CRPlace: Camera-Radar Fusion with BEV Representation for Place Recognition (Shaowei Fu et al., 2024)

{{<citation>}}

Shaowei Fu, Yifan Duan, Yao Li, Chengzhen Meng, Yingjie Wang, Jianmin Ji, Yanyong Zhang. (2024)  
**CRPlace: Camera-Radar Fusion with BEV Representation for Place Recognition**
<br/>
<button class="copy-to-clipboard" title="CRPlace: Camera-Radar Fusion with BEV Representation for Place Recognition" index=173>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 10  
Keywords: Object Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15183v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15183v1.pdf" filename="2403.15183v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The integration of complementary characteristics from camera and radar data has emerged as an effective approach in 3D <b>object</b> <b>detection.</b> However, such fusion-based methods remain unexplored for place recognition, an equally important task for autonomous systems. Given that place recognition relies on the similarity between a query scene and the corresponding candidate scene, the stationary background of a scene is expected to play a crucial role in the task. As such, current well-designed camera-radar fusion methods for 3D <b>object</b> <b>detection</b> can hardly take effect in place recognition because they mainly focus on dynamic foreground <b>objects.</b> <b>In</b> this paper, a background-attentive camera-radar fusion-based method, named CRPlace, is proposed to generate background-attentive global descriptors from multi-view images and radar point clouds for accurate place recognition. To extract stationary background features effectively, we design an adaptive module that generates the background-attentive mask by utilizing the camera BEV feature and radar dynamic points. With the guidance of a background mask, we devise a bidirectional cross-attention-based spatial fusion strategy to facilitate comprehensive spatial interaction between the background information of the camera BEV feature and the radar BEV feature. As the first camera-radar fusion-based place recognition network, CRPlace has been evaluated thoroughly on the nuScenes dataset. The results show that our algorithm outperforms a variety of baseline methods across a comprehensive set of metrics (recall@1 reaches 91.2%).

{{</citation>}}


### (13/15 | 174/202) ALPINE: a climbing robot for operations in mountain environments (Michele Focchi et al., 2024)

{{<citation>}}

Michele Focchi, Andrea Del Prete, Daniele Fontanelli, Marco Frego, Angelika Peer, Luigi Palopoli. (2024)  
**ALPINE: a climbing robot for operations in mountain environments**
<br/>
<button class="copy-to-clipboard" title="ALPINE: a climbing robot for operations in mountain environments" index=174>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 10  
Keywords: Human Intervention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15142v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15142v1.pdf" filename="2403.15142v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Mountain slopes are perfect examples of harsh environments in which <b>humans</b> <b>are</b> required to perform difficult and dangerous operations such as removing unstable boulders, dangerous vegetation or deploying safety nets. A good replacement for <b>human</b> <b>intervention</b> can be offered by climbing robots. The different solutions existing in the literature are not up to the task for the difficulty of the requirements (navigation, heavy payloads, flexibility in the execution of the tasks). In this paper, we propose a robotic platform that can fill this gap. Our solution is based on a robot that hangs on ropes, and uses a retractable leg to jump away from the mountain walls. Our package of mechanical solutions, along with the algorithms developed for motion planning and control, delivers swift navigation on irregular and steep slopes, the possibility to overcome or travel around significant natural barriers, and the ability to carry heavy payloads and execute complex tasks. In the paper, we give a full account of our main design and algorithmic choices and show the feasibility of the solution through a large number of physically simulated scenarios.

{{</citation>}}


### (14/15 | 175/202) Subequivariant Reinforcement Learning Framework for Coordinated Motion Control (Haoyu Wang et al., 2024)

{{<citation>}}

Haoyu Wang, Xiaoyu Tan, Xihe Qiu, Chao Qu. (2024)  
**Subequivariant Reinforcement Learning Framework for Coordinated Motion Control**
<br/>
<button class="copy-to-clipboard" title="Subequivariant Reinforcement Learning Framework for Coordinated Motion Control" index=175>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-AI, cs-RO, cs.RO  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15100v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15100v1.pdf" filename="2403.15100v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Effective coordination is crucial for motion control with <b>reinforcement</b> <b>learning,</b> especially as the complexity of agents and their motions increases. However, many existing methods struggle to account for the intricate dependencies between joints. We introduce CoordiGraph, a novel architecture that leverages subequivariant principles from physics to enhance coordination of motion control with <b>reinforcement</b> <b>learning.</b> This method embeds the principles of equivariance as inherent patterns in the learning process under gravity influence, which aids in modeling the nuanced relationships between joints vital for motion control. Through extensive experimentation with sophisticated agents in diverse environments, we highlight the merits of our approach. Compared to current leading methods, CoordiGraph notably enhances generalization and sample efficiency.

{{</citation>}}


### (15/15 | 176/202) Rethinking 6-Dof Grasp Detection: A Flexible Framework for High-Quality Grasping (Wei Tang et al., 2024)

{{<citation>}}

Wei Tang, Siang Chen, Pengwei Xie, Dingchang Hu, Wenming Yang, Guijin Wang. (2024)  
**Rethinking 6-Dof Grasp Detection: A Flexible Framework for High-Quality Grasping**
<br/>
<button class="copy-to-clipboard" title="Rethinking 6-Dof Grasp Detection: A Flexible Framework for High-Quality Grasping" index=176>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 10  
Keywords: Grounding  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15054v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15054v1.pdf" filename="2403.15054v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Robotic grasping is a primitive skill for complex tasks and is fundamental to intelligence. For general 6-Dof grasping, most previous methods directly extract scene-level semantic or geometric information, while few of them consider the suitability for various downstream applications, such as target-oriented grasping. Addressing this issue, we rethink 6-Dof grasp detection from a grasp-centric view and propose a versatile grasp framework capable of handling both scene-level and target-oriented grasping. Our framework, FlexLoG, is composed of a Flexible Guidance Module and a Local Grasp Model. Specifically, the Flexible Guidance Module is compatible with both global (e.g., grasp heatmap) and local (e.g., visual <b>grounding)</b> guidance, enabling the generation of high-quality grasps across various tasks. The Local Grasp Model focuses on object-agnostic regional points and predicts grasps locally and intently. Experiment results reveal that our framework achieves over 18% and 23% improvement on unseen splits of the GraspNet-1Billion Dataset. Furthermore, real-world robotic tests in three distinct settings yield a 95% success rate.

{{</citation>}}


## physics.geo-ph (1)



### (1/1 | 177/202) End-to-End Mineral Exploration with Artificial Intelligence and Ambient Noise Tomography (Jack Muir et al., 2024)

{{<citation>}}

Jack Muir, Gerrit Olivier, Anthony Reid. (2024)  
**End-to-End Mineral Exploration with Artificial Intelligence and Ambient Noise Tomography**
<br/>
<button class="copy-to-clipboard" title="End-to-End Mineral Exploration with Artificial Intelligence and Ambient Noise Tomography" index=177>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: physics.geo-ph  
Categories: cs-LG, physics-geo-ph, physics.geo-ph  
Keyword Score: 30  
Keywords: Fine-tuning, Fine-tuning, Foundation Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15095v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15095v1.pdf" filename="2403.15095v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents an innovative end-to-end workflow for mineral exploration, integrating ambient noise tomography (ANT) and artificial intelligence (AI) to enhance the discovery and delineation of mineral resources essential for the global transition to a low carbon economy. We focus on copper as a critical element, required in significant quantities for renewable energy solutions. We show the benefits of utilising ANT, characterised by its speed, scalability, depth penetration, resolution, and low environmental impact, alongside artificial intelligence (AI) techniques to refine a continent-scale prospectivity model at the deposit scale by <b>fine-tuning</b> our model on local high-resolution data. We show the promise of the method by first presenting a new data-driven AI prospectivity model for copper within Australia, which serves as our <b>foundation</b> <b>model</b> for further <b>fine-tuning.</b> We then focus on the Hillside IOCG deposit on the prospective Yorke Peninsula. We show that with relatively few local training samples (orebody intercepts), we can fine tune the <b>foundation</b> <b>model</b> to provide a good estimate of the Hillside orebody outline. Our methodology demonstrates how AI can augment geophysical data interpretation, providing a novel approach to mineral exploration with improved decision-making capabilities for targeting mineralization, thereby addressing the urgent need for increased mineral resource discovery.

{{</citation>}}


## cs.MM (2)



### (1/2 | 178/202) Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models (Qiong Wu et al., 2024)

{{<citation>}}

Qiong Wu, Weihao Ye, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji. (2024)  
**Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models**
<br/>
<button class="copy-to-clipboard" title="Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models" index=178>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.MM  
Categories: cs-CL, cs-MM, cs.MM  
Keyword Score: 26  
Keywords: Benchmarking, Multi-modal, Transfer Learning, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15226v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15226v1.pdf" filename="2403.15226v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we propose a novel parameter and computation efficient tuning method for <b>Multi-modal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs), termed Efficient Attention Skipping (EAS). Concretely, we first reveal that multi-head attentions (MHAs), the main computational overhead of MLLMs, are often redundant to downstream tasks. Based on this observation, EAS evaluates the attention redundancy and skips the less important MHAs to speed up inference. Besides, we also propose a novel propagation-of-information adapter (PIA) to serve the attention skipping of EAS and keep parameter efficiency, which can be further re-parameterized into feed-forward networks (FFNs) for zero-extra latency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN and a classic VL pre-trained model called METER, and conduct extensive experiments on a set of <b>benchmarks.</b> The experiments show that EAS not only retains high performance and parameter efficiency, but also greatly speeds up inference speed. For instance, LaVIN-EAS can obtain 89.98\% accuracy on ScineceQA while speeding up inference by 2.2 times to LaVIN

{{</citation>}}


### (2/2 | 179/202) Experimental Studies of Metaverse Streaming (Haopeng Wang et al., 2024)

{{<citation>}}

Haopeng Wang, Roberto Martinez-Velazquez, Haiwei Dong, Abdulmotaleb El Saddik. (2024)  
**Experimental Studies of Metaverse Streaming**
<br/>
<button class="copy-to-clipboard" title="Experimental Studies of Metaverse Streaming" index=179>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.MM  
Categories: cs-MM, cs-NI, cs.MM  
Keyword Score: 6  
Keywords: Multi-modal, Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15256v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15256v1.pdf" filename="2403.15256v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Metaverse aims to construct a large, unified, immersive, and shared digital realm by combining various technologies, namely XR (extended reality), blockchain, and digital twin, among others. This article explores the Metaverse from the perspective of multimedia communication by conducting and analyzing real-world experiments on four different Metaverse platforms: VR (virtual reality) Vircadia, VR Mozilla Hubs, VRChat, and MR (mixed reality) Virtual City. We first investigate the traffic patterns and network performance in the three VR platforms. After raising the challenges of the Metaverse streaming and investigating the potential methods to enhance Metaverse performance, we propose a remote rendering architecture and verify its advantages through a prototype involving the campus network and MR <b>multimodal</b> interaction by comparison with local rendering.

{{</citation>}}


## q-bio.NC (1)



### (1/1 | 180/202) Brain-grounding of semantic vectors improves neural decoding of visual stimuli (Shirin Vafaei et al., 2024)

{{<citation>}}

Shirin Vafaei, Ryohei Fukuma, Huixiang Yang, Haruhiko Kishima, Takufumi Yanagisawa. (2024)  
**Brain-grounding of semantic vectors improves neural decoding of visual stimuli**
<br/>
<button class="copy-to-clipboard" title="Brain-grounding of semantic vectors improves neural decoding of visual stimuli" index=180>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: q-bio.NC  
Categories: cs-AI, q-bio-NC, q-bio.NC  
Keyword Score: 25  
Keywords: Fine-tuning, Representation Learning, Zero-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15176v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15176v1.pdf" filename="2403.15176v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Developing algorithms for accurate and comprehensive neural decoding of mental contents is one of the long-cherished goals in the field of neuroscience and brain-machine interfaces. Previous studies have demonstrated the feasibility of neural decoding by training machine learning models to map brain activity patterns into a semantic vector <b>representation</b> <b>of</b> stimuli. These vectors, hereafter referred as pretrained feature vectors, are usually derived from semantic spaces based solely on image and/or text features and therefore they might have a totally different characteristics than how visual stimuli is represented in the human brain, resulting in limiting the capability of brain decoders to learn this mapping. To address this issue, we propose a <b>representation</b> <b>learning</b> framework, termed brain-grounding of semantic vectors, which <b>fine-tunes</b> pretrained feature vectors to better align with the neural <b>representation</b> <b>of</b> visual stimuli in the human brain. We trained this model this model with functional magnetic resonance imaging (fMRI) of 150 different visual stimuli categories, and then performed <b>zero-shot</b> brain decoding and identification analyses on 1) fMRI and 2) magnetoencephalography (MEG). Interestingly, we observed that by using the brain-grounded vectors, the brain decoding and identification accuracy on brain data from different neuroimaging modalities increases. These findings underscore the potential of incorporating a richer array of brain-derived features to enhance performance of brain decoding algorithms.

{{</citation>}}


## cs.NI (2)



### (1/2 | 181/202) A Modular, End-to-End Next-Generation Network Testbed: Towards a Fully Automated Network Management Platform (Ali Chouman et al., 2024)

{{<citation>}}

Ali Chouman, Dimitrios Michael Manias, Abdallah Shami. (2024)  
**A Modular, End-to-End Next-Generation Network Testbed: Towards a Fully Automated Network Management Platform**
<br/>
<button class="copy-to-clipboard" title="A Modular, End-to-End Next-Generation Network Testbed: Towards a Fully Automated Network Management Platform" index=181>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NI  
Categories: cs-NI, cs.NI  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15376v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15376v1.pdf" filename="2403.15376v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Experimentation in practical, end-to-end (E2E) next-generation networks deployments is becoming increasingly prevalent and significant in the realm of modern networking and wireless communications research. The prevalence of fifth-generation technology (5G) testbeds and the emergence of developing networks systems, for the purposes of research and testing, focus on the capabilities and features of analytics, intelligence, and automated management using novel testbed designs and architectures, ranging from simple <b>simulations</b> and setups to complex networking systems; however, with the ever-demanding application requirements for modern and future networks, 5G-and-beyond (denoted as 5G+) testbed experimentation can be useful in assessing the creation of large-scale network infrastructures that are capable of supporting E2E virtualized mobile network services. To this end, this paper presents a functional, modular E2E 5G+ system, complete with the integration of a Radio Access Network (RAN) and handling the connection of User Equipment (UE) in real-world scenarios. As well, this paper assesses and evaluates the effectiveness of emulating full network functionalities and capabilities, including a complete description of user-plane data, from UE registrations to communications sequences, and leads to the presentation of a future outlook in powering new experimentation for 6G and next-generation networks.

{{</citation>}}


### (2/2 | 182/202) Blockchain-based Pseudonym Management for Vehicle Twin Migrations in Vehicular Edge Metaverse (Jiawen Kang et al., 2024)

{{<citation>}}

Jiawen Kang, Xiaofeng Luo, Jiangtian Nie, Tianhao Wu, Haibo Zhou, Yonghua Wang, Dusit Niyato, Shiwen Mao, Shengli Xie. (2024)  
**Blockchain-based Pseudonym Management for Vehicle Twin Migrations in Vehicular Edge Metaverse**
<br/>
<button class="copy-to-clipboard" title="Blockchain-based Pseudonym Management for Vehicle Twin Migrations in Vehicular Edge Metaverse" index=182>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NI  
Categories: cs-CR, cs-HC, cs-LG, cs-NI, cs.NI  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15285v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15285v1.pdf" filename="2403.15285v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Driven by the great advances in metaverse and edge computing technologies, vehicular edge metaverses are expected to disrupt the current paradigm of intelligent transportation systems. As highly computerized avatars of Vehicular Metaverse Users (VMUs), the Vehicle Twins (VTs) deployed in edge servers can provide valuable metaverse services to improve driving safety and on-board satisfaction for their VMUs throughout journeys. To maintain uninterrupted metaverse experiences, VTs must be migrated among edge servers following the movements of vehicles. This can raise concerns about privacy breaches during the dynamic communications among vehicular edge metaverses. To address these concerns and safeguard location privacy, pseudonyms as temporary identifiers can be leveraged by both VMUs and VTs to realize anonymous communications in the physical space and virtual spaces. However, existing pseudonym management methods fall short in meeting the extensive pseudonym demands in vehicular edge metaverses, thus dramatically diminishing the performance of privacy preservation. To this end, we present a cross-metaverse empowered dual pseudonym management framework. We utilize cross-chain technology to enhance management efficiency and data security for pseudonyms. Furthermore, we propose a metric to assess the privacy level and employ a Multi-Agent Deep <b>Reinforcement</b> <b>Learning</b> (MADRL) approach to obtain an optimal pseudonym generating strategy. Numerical results demonstrate that our proposed schemes are high-efficiency and cost-effective, showcasing their promising applications in vehicular edge metaverses.

{{</citation>}}


## math.ST (1)



### (1/1 | 183/202) A Wasserstein perspective of Vanilla GANs (Lea Kunkel et al., 2024)

{{<citation>}}

Lea Kunkel, Mathias Trabs. (2024)  
**A Wasserstein perspective of Vanilla GANs**
<br/>
<button class="copy-to-clipboard" title="A Wasserstein perspective of Vanilla GANs" index=183>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.ST  
Categories: 62E17, 62G05, 68T07, cs-LG, math-ST, math.ST, stat-ML, stat-TH  
Keyword Score: 20  
Keywords: Generative Adversarial Network, Generative Adversarial Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15312v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15312v1.pdf" filename="2403.15312v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The empirical success of <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> caused an increasing interest in theoretical research. The statistical literature is mainly focused on Wasserstein <b>GANs</b> and generalizations thereof, which especially allow for good dimension reduction properties. Statistical results for Vanilla <b>GANs,</b> the original optimization problem, are still rather limited and require assumptions such as smooth activation functions and equal dimensions of the latent space and the ambient space. To bridge this gap, we draw a connection from Vanilla <b>GANs</b> to the Wasserstein distance. By doing so, existing results for Wasserstein <b>GANs</b> can be extended to Vanilla <b>GANs.</b> In particular, we obtain an oracle inequality for Vanilla <b>GANs</b> in Wasserstein distance. The assumptions of this oracle inequality are designed to be satisfied by network architectures commonly used in practice, such as feedforward ReLU networks. By providing a quantitative result for the approximation of a Lipschitz function by a feedforward ReLU network with bounded H\"older norm, we conclude a rate of convergence for Vanilla <b>GANs</b> as well as Wasserstein <b>GANs</b> as estimators of the unknown probability distribution.

{{</citation>}}


## math.NA (2)



### (1/2 | 184/202) Accelerating Aeroelastic UVLM Simulations by Inexact Newton Algorithms (Jenny Schubert et al., 2024)

{{<citation>}}

Jenny Schubert, Marc C. Steinbach, Christian Hente, David Märtins, Daniel Schuster. (2024)  
**Accelerating Aeroelastic UVLM Simulations by Inexact Newton Algorithms**
<br/>
<button class="copy-to-clipboard" title="Accelerating Aeroelastic UVLM Simulations by Inexact Newton Algorithms" index=184>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.NA  
Categories: 49M15, 90C53, 74F10, 76B47, cs-NA, math-NA, math.NA  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15286v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15286v1.pdf" filename="2403.15286v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We consider the aeroelastic <b>simulation</b> of flexible mechanical structures submerged in subsonic fluid flows at low Mach numbers. The nonlinear kinematics of flexible bodies are described in the total Lagrangian formulation and discretized by finite elements. The aerodynamic loads are computed using the unsteady vortex-lattice method wherein a free wake is tracked over time. Each implicit time step in the dynamic <b>simulation</b> then requires solving a nonlinear equation system in the structural variables with additional aerodynamic load terms. Our focus here is on the efficient numerical solution of this system by accelerating the Newton algorithm. The particular structure of the aeroelastic nonlinear system suggests the structural derivative as an approximation to the full derivative in the linear Newton system. We investigate and compare two promising algorithms based in this approximation, a quasi-Newton type algorithm and a novel inexact Newton algorithm. Numerical experiments are performed on a flexible plate and on a wind turbine. Our computational results show that the approximation can indeed accelerate the Newton algorithm substantially. Surprisingly, the theoretically preferable inexact Newton algorithm is much slower than the quasi-Newton algorithm, which motivates further research to speed up derivative evaluations.

{{</citation>}}


### (2/2 | 185/202) Two-scale Analysis for Multiscale Landau-Lifshitz-Gilbert Equation: Theory and Numerical Methods (Xiaofei Guan et al., 2024)

{{<citation>}}

Xiaofei Guan, Hang Qi, Zhiwei Sun. (2024)  
**Two-scale Analysis for Multiscale Landau-Lifshitz-Gilbert Equation: Theory and Numerical Methods**
<br/>
<button class="copy-to-clipboard" title="Two-scale Analysis for Multiscale Landau-Lifshitz-Gilbert Equation: Theory and Numerical Methods" index=185>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.NA  
Categories: cs-NA, math-MP, math-NA, math-ph, math.NA  
Keyword Score: 10  
Keywords: Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14957v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14957v1.pdf" filename="2403.14957v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper discusses the theory and numerical method of two-scale analysis for the multiscale Landau-Lifshitz-Gilbert equation in composite ferromagnetic materials. The novelty of this work can be <b>summarized</b> in three aspects: Firstly, the more realistic and complex model is considered, including the effects of the exchange field, anisotropy field, stray field, and external magnetic field. The explicit convergence orders in the $H^1$ norm between the classical solution and the two-scale solution are obtained. Secondly, we propose a robust numerical framework, which is employed in several comprehensive experiments to validate the convergence results for the Periodic and Neumann problems. Thirdly, we design an improved implicit numerical scheme to reduce the required number of iterations and relaxes the constraints on the time step size, which can significantly improve computational efficiency. Specifically, the projection and the expansion methods are given to overcome the inherent non-consistency in the initial data between the multiscale problem and homogenized problem.

{{</citation>}}


## math.OC (3)



### (1/3 | 186/202) Pursuit-Evasion on a Sphere and When It Can Be Considered Flat (Dejan Milutinovic et al., 2024)

{{<citation>}}

Dejan Milutinovic, Alexander Von Moll, Satyanarayana G. Manyam, David W. Casbeer, Isaac E. Weintraub, Meir Pachter. (2024)  
**Pursuit-Evasion on a Sphere and When It Can Be Considered Flat**
<br/>
<button class="copy-to-clipboard" title="Pursuit-Evasion on a Sphere and When It Can Be Considered Flat" index=186>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.OC  
Categories: 58E10, 86A30, 37D40, 91A23, 49L12, 49Q10, cs-SY, eess-SY, math-AG, math-DG, math-OC, math.OC  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15188v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15188v1.pdf" filename="2403.15188v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In classical works on a planar differential pursuit-evasion game with a faster pursuer, the intercept point resulting from the equilibrium strategies lies on the Apollonius circle. This property was exploited for the construction of the equilibrium strategies for two faster pursuers against one evader. Extensions for planar multiple-pursuer single-evader scenarios have been considered. We study a pursuit-evasion game on a sphere and the relation of the equilibrium intercept point to the Apollonius domain on the sphere. The domain is a generalization of the planar Apollonius circle set. We find a condition resulting in the intercept point belonging to the Apollonius domain, which is the characteristic of the planar game solution. Finally, we use this characteristic to discuss pursuit and evasion strategies in the context of two pursuers and a single slower evader on the sphere and illustrate it using numerical <b>simulations.</b>

{{</citation>}}


### (2/3 | 187/202) Optimal Contract Design for End-of-Life Care Payments (Muyan Jiang et al., 2024)

{{<citation>}}

Muyan Jiang, Ying Chen, Xin Chen, Javad Lavaei, Anil Aswani. (2024)  
**Optimal Contract Design for End-of-Life Care Payments**
<br/>
<button class="copy-to-clipboard" title="Optimal Contract Design for End-of-Life Care Payments" index=187>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.OC  
Categories: cs-NA, math-NA, math-OC, math.OC, stat-AP  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15099v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15099v1.pdf" filename="2403.15099v1.pdf">Download PDF</button>

---


**ABSTRACT**  
A large fraction of total healthcare expenditure occurs due to end-of-life (EOL) care, which means it is important to study the problem of more carefully incentivizing necessary versus unnecessary EOL care because this has the potential to reduce overall healthcare spending. This paper introduces a principal-agent model that integrates a mixed payment system of fee-for-service and pay-for-performance in order to analyze whether it is possible to better align healthcare provider incentives with patient outcomes and cost-efficiency in EOL care. The primary contributions are to derive optimal contracts for EOL care payments using a principal-agent framework under three separate models for the healthcare provider, where each model considers a different level of risk tolerance for the provider. We derive these optimal contracts by converting the underlying principal-agent models from a bilevel optimization problem into a single-level optimization problem that can be analytically solved. Our results are demonstrated using a <b>simulation</b> where an optimal contract is used to price intracranial pressure monitoring for traumatic brain injuries.

{{</citation>}}


### (3/3 | 188/202) Network Learning with Directional Sign Patterns (Anqi Dong et al., 2024)

{{<citation>}}

Anqi Dong, Can Chen, Tryphon T. Georgiou. (2024)  
**Network Learning with Directional Sign Patterns**
<br/>
<button class="copy-to-clipboard" title="Network Learning with Directional Sign Patterns" index=188>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.OC  
Categories: 62F15, 49Q22, 05Cxx, 92C42, cs-SY, eess-SY, math-OC, math.OC  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14915v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14915v1.pdf" filename="2403.14915v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Complex systems can be effectively modeled via <b>graphs</b> that encode networked interactions, where relations between entities or nodes are often quantified by signed edge weights, e.g., promotion/inhibition in gene regulatory networks, or encoding political of friendship differences in social networks. However, it is often the case that only an aggregate consequence of such edge weights that characterize relations may be directly observable, as in protein expression of in gene regulatory networks. Thus, learning edge weights poses a significant challenge that is further exacerbated for intricate and large-scale networks. In this article, we address a model problem to determine the strength of sign-indefinite relations that explain marginal distributions that constitute our data. To this end, we develop a paradigm akin to that of the Schr\"odinger bridge problem and an efficient Sinkhorn type algorithm (more properly, Schr\"odinger-Fortet-Sinkhorn algorithm) that allows fast convergence to parameters that minimize a relative entropy/likelihood criterion between the sought signed adjacency matrix and a prior. The formalism that we present represents a novel generalization of the earlier Schr\"odinger formalism in that marginal computations may incorporate weights that model directionality in underlying relations, and further, that it can be extended to high-order networks -- the Schr\"odinger-Fortet-Sinkhorn algorithm that we derive is applicable all the same and allows geometric convergence to a sought sign-indefinite adjacency matrix or tensor, for high-order networks. We demonstrate our framework with synthetic and real-world examples.

{{</citation>}}


## cs.PL (1)



### (1/1 | 189/202) A hybrid approach to semi-automated Rust verification (Sacha-Élie Ayoun et al., 2024)

{{<citation>}}

Sacha-Élie Ayoun, Xavier Denis, Petar Maksimović, Philippa Gardner. (2024)  
**A hybrid approach to semi-automated Rust verification**
<br/>
<button class="copy-to-clipboard" title="A hybrid approach to semi-automated Rust verification" index=189>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.PL  
Categories: cs-PL, cs.PL  
Keyword Score: 20  
Keywords: Fine-tuning, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15122v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15122v1.pdf" filename="2403.15122v1.pdf">Download PDF</button>

---


**ABSTRACT**  
While recent years have been witness to a large body of work on efficient and automated verification of safe Rust code, enabled by the rich guarantees of the Rust type system, much less progress has been made on <b>reasoning</b> about unsafe code due to its unique complexities. We propose a hybrid approach to end-to-end Rust verification in which powerful automated verification of safe Rust is combined with targeted semi-automated verification of unsafe~Rust. To this end, we present Gillian-Rust, a proof-of-concept semi-automated verification tool that is able to reason about type safety and functional correctness of unsafe~code. Built on top of the Gillian parametric compositional verification platform, Gillian-Rust automates a rich separation logic for real-world Rust, embedding the lifetime logic of RustBelt and the parametric propheciees of RustHornBelt. Using the unique extensibility of Gillian, our novel encoding of these features is <b>fine-tuned</b> to maximise automation and exposes a user-friendly API, allowing for low-effort verification of unsafe code. We link Gillian-Rust with Creusot, a state-of-the-art verifier for safe Rust, by providing a systematic encoding of unsafe code specifications that Creusot may use but not verify, demonstrating the feasibility of our hybrid~approach.

{{</citation>}}


## q-bio.QM (1)



### (1/1 | 190/202) Comprehensive Lipidomic Automation Workflow using Large Language Models (Connor Beveridge et al., 2024)

{{<citation>}}

Connor Beveridge, Sanjay Iyer, Caitlin E. Randolph, Matthew Muhoberac, Palak Manchanda, Amy C. Clingenpeel, Shane Tichy, Gaurav Chopra. (2024)  
**Comprehensive Lipidomic Automation Workflow using Large Language Models**
<br/>
<button class="copy-to-clipboard" title="Comprehensive Lipidomic Automation Workflow using Large Language Models" index=190>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: q-bio.QM  
Categories: cs-AI, q-bio-BM, q-bio-QM, q-bio-SC, q-bio.QM  
Keyword Score: 20  
Keywords: Chatbot, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15076v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15076v1.pdf" filename="2403.15076v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Lipidomics generates <b>large</b> <b>data</b> <b>that</b> makes manual annotation and interpretation challenging. Lipid chemical and structural diversity with structural isomers further complicates annotation. Although, several commercial and open-source software for targeted lipid identification exists, it lacks automated method generation workflows and integration with statistical and bioinformatics tools. We have developed the Comprehensive Lipidomic Automated Workflow (CLAW) platform with integrated workflow for parsing, detailed statistical analysis and lipid annotations based on custom multiple reaction monitoring (MRM) precursor and product ion pair transitions. CLAW contains several modules including identification of carbon-carbon double bond position(s) in unsaturated lipids when combined with ozone electrospray ionization (OzESI)-MRM methodology. To demonstrate the utility of the automated workflow in CLAW, <b>large-scale</b> <b>lipidomics</b> <b>data</b> was collected with traditional and OzESI-MRM profiling on biological and non-biological samples. Specifically, a total of 1497 transitions organized into 10 MRM-based mass spectrometry methods were used to profile lipid droplets isolated from different brain regions of 18-24 month-old Alzheimer's disease mice and age-matched wild-type controls. Additionally, triacyclglycerols (TGs) profiles with carbon-carbon double bond specificity were generated from canola oil samples using OzESI-MRM profiling. We also developed an integrated language user interface with <b>large</b> <b>language</b> <b>models</b> using artificially intelligent (AI) agents that permits users to interact with the CLAW platform using a <b>chatbot</b> terminal to perform statistical and bioinformatic analyses. We envision CLAW pipeline to be used in high-throughput lipid structural identification tasks aiding users to generate automated lipidomics workflows ranging from data acquisition to AI agent-based bioinformatic analysis.

{{</citation>}}


## cs.DC (2)



### (1/2 | 191/202) Modeling Distributed Computing Infrastructures for HEP Applications (Maximilian Horzela et al., 2024)

{{<citation>}}

Maximilian Horzela, Henri Casanova, Manuel Giffels, Artur Gottmann, Robin Hofsaess, Günter Quast, Simone Rossi Tisbeni, Achim Streit, Frédéric Suter. (2024)  
**Modeling Distributed Computing Infrastructures for HEP Applications**
<br/>
<button class="copy-to-clipboard" title="Modeling Distributed Computing Infrastructures for HEP Applications" index=191>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DC  
Categories: cs-DC, cs.DC, hep-ex  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14903v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14903v1.pdf" filename="2403.14903v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Predicting the performance of various infrastructure design options in complex federated infrastructures with computing sites distributed over a wide area network that support a plethora of users and workflows, such as the Worldwide LHC Computing Grid (WLCG), is not trivial. Due to the complexity and size of these infrastructures, it is not feasible to deploy experimental test-beds at large scales merely for the purpose of comparing and evaluating alternate designs. An alternative is to study the behaviours of these systems using <b>simulation.</b> This approach has been used successfully in the past to identify efficient and practical infrastructure designs for High Energy Physics (HEP). A prominent example is the Monarc <b>simulation</b> framework, which was used to study the initial structure of the WLCG. New <b>simulation</b> capabilities are needed to simulate large-scale heterogeneous computing systems with complex networks, data access and caching patterns. A modern tool to simulate HEP workloads that execute on distributed computing infrastructures based on the SimGrid and WRENCH <b>simulation</b> frameworks is outlined. Studies of its accuracy and scalability are presented using HEP as a case-study. Hypothetical adjustments to prevailing computing architectures in HEP are studied providing insights into the dynamics of a part of the WLCG and candidates for improvements.

{{</citation>}}


### (2/2 | 192/202) FSD-Inference: Fully Serverless Distributed Inference with Scalable Cloud Communication (Joe Oakley et al., 2024)

{{<citation>}}

Joe Oakley, Hakan Ferhatosmanoglu. (2024)  
**FSD-Inference: Fully Serverless Distributed Inference with Scalable Cloud Communication**
<br/>
<button class="copy-to-clipboard" title="FSD-Inference: Fully Serverless Distributed Inference with Scalable Cloud Communication" index=192>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DC  
Categories: cs-AI, cs-DC, cs-LG, cs.DC  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15195v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15195v1.pdf" filename="2403.15195v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Serverless computing offers attractive scalability, elasticity and cost-effectiveness. However, constraints on memory, CPU and function runtime have hindered its adoption for data-intensive applications and machine learning (ML) workloads. Traditional 'server-ful' platforms enable distributed computation via fast networks and well-established inter-process communication (IPC) mechanisms such as MPI and shared memory. In the absence of such solutions in the serverless domain, parallel computation with significant IPC requirements is challenging. We present FSD-Inference, the first fully serverless and highly scalable system for distributed ML inference. We explore potential communication channels, in conjunction with Function-as-a-Service (FaaS) compute, to design a state-of-the-art solution for distributed ML within the context of serverless data-intensive computing. We introduce novel fully serverless communication schemes for ML inference workloads, leveraging both cloud-based publish-subscribe/queueing and object storage offerings. We demonstrate how publish-subscribe/queueing services can be adapted for FaaS IPC with comparable performance to object storage, while offering significantly reduced cost at high parallelism levels. We conduct in-depth experiments on <b>benchmark</b> DNNs of various sizes. The results show that when compared to server-based alternatives, FSD-Inference is significantly more cost-effective and scalable, and can even achieve competitive performance against optimized HPC solutions. Experiments also confirm that our serverless solution can handle large distributed workloads and leverage high degrees of FaaS parallelism.

{{</citation>}}


## cs.ET (1)



### (1/1 | 193/202) Cross-layer Modeling and Design of Content Addressable Memories in Advanced Technology Nodes for Similarity Search (Siri Narla et al., 2024)

{{<citation>}}

Siri Narla, Piyush Kumar, Mohammad Adnaan, Azad Naeemi. (2024)  
**Cross-layer Modeling and Design of Content Addressable Memories in Advanced Technology Nodes for Similarity Search**
<br/>
<button class="copy-to-clipboard" title="Cross-layer Modeling and Design of Content Addressable Memories in Advanced Technology Nodes for Similarity Search" index=193>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.ET  
Categories: cs-AR, cs-ET, cs.ET  
Keyword Score: 16  
Keywords: Benchmarking, Benchmarking, Recommendation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15328v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15328v1.pdf" filename="2403.15328v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper we present a comprehensive design and <b>benchmarking</b> study of Content Addressable Memory (CAM) at the 7nm technology node in the context of similarity search applications. We design CAM cells based on SRAM, spin-orbit torque, and ferroelectric field effect transistor devices and from their layouts extract cell parasitics using state of the art EDA tools. These parasitics are used to develop SPICE netlists to model search operations. We use a CAM-based dataset search and a sequential <b>recommendation</b> system to highlight the application-level performance degradation due to interconnect parasitics. We propose and evaluate two solutions to mitigate interconnect effects.

{{</citation>}}


## quant-ph (1)



### (1/1 | 194/202) Image Classification with Rotation-Invariant Variational Quantum Circuits (Paul San Sebastian et al., 2024)

{{<citation>}}

Paul San Sebastian, Mikel Cañizo, Román Orús. (2024)  
**Image Classification with Rotation-Invariant Variational Quantum Circuits**
<br/>
<button class="copy-to-clipboard" title="Image Classification with Rotation-Invariant Variational Quantum Circuits" index=194>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: quant-ph  
Categories: cs-CV, cs-LG, quant-ph, quant-ph  
Keyword Score: 13  
Keywords: Benchmarking, Convolution  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15031v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15031v1.pdf" filename="2403.15031v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Variational quantum algorithms are gaining attention as an early application of Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of variational methods lies in the phenomenon of Barren Plateaus, present in the optimization of variational parameters. Adding geometric inductive bias to the quantum models has been proposed as a potential solution to mitigate this problem, leading to a new field called Geometric Quantum Machine Learning. In this work, an equivariant architecture for variational quantum classifiers is introduced to create a label-invariant model for image classification with $C_4$ rotational label symmetry. The equivariant circuit is <b>benchmarked</b> against two different architectures, and it is experimentally observed that the geometric approach boosts the model's performance. Finally, a classical equivariant <b>convolution</b> operation is proposed to extend the quantum model for the processing of larger images, employing the resources available in NISQ devices.

{{</citation>}}


## cs.CR (3)



### (1/3 | 195/202) Attacking with Something That Does Not Exist: Low-Rate Flood with 'Proof of Non-Existence' Can Exhaust DNS Resolver CPU (Olivia Gruza et al., 2024)

{{<citation>}}

Olivia Gruza, Elias Heftrig, Oliver Jacobsen, Haya Schulmann, Niklas Vogel, Michael Waidner. (2024)  
**Attacking with Something That Does Not Exist: Low-Rate Flood with 'Proof of Non-Existence' Can Exhaust DNS Resolver CPU**
<br/>
<button class="copy-to-clipboard" title="Attacking with Something That Does Not Exist: Low-Rate Flood with 'Proof of Non-Existence' Can Exhaust DNS Resolver CPU" index=195>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs.CR  
Keyword Score: 10  
Keywords: Recommendation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15233v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15233v1.pdf" filename="2403.15233v1.pdf">Download PDF</button>

---


**ABSTRACT**  
NSEC3 is a proof of non-existence in DNSSEC, which provides an authenticated assertion that a queried resource does not exist in the target domain. NSEC3 consists of alphabetically sorted hashed names before and after the queried hostname. To make dictionary attacks harder, the hash function can be applied in multiple iterations, which however also increases the load on the DNS resolver during the computation of the SHA-1 hashes in NSEC3 records. Concerns about the load created by the computation of NSEC3 records on the DNS resolvers were already considered in the NSEC3 specifications RFC5155 and RFC9276. In February 2024, the potential of NSEC3 to exhaust DNS resolvers' resources was assigned a CVE-2023-50868, confirming that extra iterations of NSEC3 created substantial load. However, there is no published evaluation of the attack and the impact of the attack on the resolvers was not clarified. In this work we perform the first evaluation of the NSEC3-encloser attack against DNS resolver implementations and find that the NSEC3-encloser attack can still create a 72x increase in CPU instruction count, despite the victim resolver following RFC5155 <b>recommendations</b> in limiting hash iteration counts. The impact of the attack varies across the different DNS resolvers, but we show that with a sufficient volume of DNS packets the attack can increase CPU load and cause packet loss. We find that at a rate of 150 malicious NSEC3 records per second, depending on the DNS implementation, the loss rate of benign DNS requests varies between 2.7% and 30%. We provide a detailed description and implementation the NSEC3-encloser attack along with evaluation against five popular DNS resolver implementations. We also develop the first analysis how each NSEC3 parameter impacts the load inflicted on the victim resolver during NSEC3-encloser attack.

{{</citation>}}


### (2/3 | 196/202) Differentially Private Ad Conversion Measurement (John Delaney et al., 2024)

{{<citation>}}

John Delaney, Badih Ghazi, Charlie Harrison, Christina Ilvento, Ravi Kumar, Pasin Manurangsi, Martin Pal, Karthik Prabhakar, Mariana Raykova. (2024)  
**Differentially Private Ad Conversion Measurement**
<br/>
<button class="copy-to-clipboard" title="Differentially Private Ad Conversion Measurement" index=196>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs-DS, cs.CR  
Keyword Score: 10  
Keywords: Differential Privacy  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15224v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15224v1.pdf" filename="2403.15224v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this work, we study ad conversion measurement, a central functionality in digital advertising, where an advertiser seeks to estimate advertiser website (or mobile app) conversions attributed to ad impressions that users have interacted with on various publisher websites (or mobile apps). Using <b>differential</b> <b>privacy</b> (DP), a notion that has gained in popularity due to its strong mathematical guarantees, we develop a formal framework for private ad conversion measurement. In particular, we define the notion of an operationally valid configuration of the attribution rule, DP adjacency relation, contribution bounding scope and enforcement point. We then provide, for the set of configurations that most commonly arises in practice, a complete characterization, which uncovers a delicate interplay between attribution and privacy.

{{</citation>}}


### (3/3 | 197/202) A Transfer Attack to Image Watermarks (Yuepeng Hu et al., 2024)

{{<citation>}}

Yuepeng Hu, Zhengyuan Jiang, Moyang Guo, Neil Gong. (2024)  
**A Transfer Attack to Image Watermarks**
<br/>
<button class="copy-to-clipboard" title="A Transfer Attack to Image Watermarks" index=197>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CL, cs-CR, cs-LG, cs.CR  
Keyword Score: 5  
Keywords: Black Box  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15365v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15365v1.pdf" filename="2403.15365v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Watermark has been widely deployed by industry to detect AI-generated images. The robustness of such watermark-based detector against evasion attacks in the white-box and <b>black-box</b> <b>settings</b> is well understood in the literature. However, the robustness in the no-box setting is much less understood. In particular, multiple studies claimed that image watermark is robust in such setting. In this work, we propose a new transfer evasion attack to image watermark in the no-box setting. Our transfer attack adds a perturbation to a watermarked image to evade multiple surrogate watermarking models trained by the attacker itself, and the perturbed watermarked image also evades the target watermarking model. Our major contribution is to show that, both theoretically and empirically, watermark-based AI-generated image detector is not robust to evasion attacks even if the attacker does not have access to the watermarking model nor the detection API.

{{</citation>}}


## cs.GT (2)



### (1/2 | 198/202) On the Weighted Top-Difference Distance: Axioms, Aggregation, and Approximation (Andrea Aveni et al., 2024)

{{<citation>}}

Andrea Aveni, Ludovico Crippa, Giulio Principi. (2024)  
**On the Weighted Top-Difference Distance: Axioms, Aggregation, and Approximation**
<br/>
<button class="copy-to-clipboard" title="On the Weighted Top-Difference Distance: Axioms, Aggregation, and Approximation" index=198>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.GT  
Categories: cs-DM, cs-GT, cs.GT, econ-TH, stat-ME  
Keyword Score: 10  
Keywords: Fairness  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15198v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15198v1.pdf" filename="2403.15198v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We study a family of distance functions on rankings that allow for asymmetric treatments of alternatives and consider the distinct relevance of the top and bottom positions for ordered lists. We provide a full axiomatic characterization of our distance. In doing so, we retrieve new characterizations of existing axioms and show how to effectively weaken them for our purposes. This analysis highlights the generality of our distance as it embeds many (semi)metrics previously proposed in the literature. Subsequently, we show that, notwithstanding its level of generality, our distance is still readily applicable. We apply it to preference aggregation, studying the features of the associated median voting rule. It is shown how the derived preference function satisfies many desirable features in the context of voting rules, ranging from <b>fairness</b> to majority and Pareto-related properties. We show how to compute consensus rankings exactly, and provide generalized Diaconis-Graham inequalities that can be leveraged to obtain approximation algorithms. Finally, we propose some truncation ideas for our distances inspired by Lu and Boutilier (2010). These can be leveraged to devise a Polynomial-Time-Approximation Scheme for the corresponding rank aggregation problem.

{{</citation>}}


### (2/2 | 199/202) Strategic Network Creation for Enabling Greedy Routing (Julian Berger et al., 2024)

{{<citation>}}

Julian Berger, Tobias Friedrich, Pascal Lenzner, Paraskevi Machaira, Janosch Ruff. (2024)  
**Strategic Network Creation for Enabling Greedy Routing**
<br/>
<button class="copy-to-clipboard" title="Strategic Network Creation for Enabling Greedy Routing" index=199>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.GT  
Categories: cs-GT, cs.GT  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15307v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15307v1.pdf" filename="2403.15307v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we present the first game-theoretic network creation model that incorporates greedy routing, i.e., the agents in our model are embedded in some metric space and strive for creating a network where all-pairs greedy routing is enabled. In contrast to <b>graph-theoretic</b> shortest paths, our agents route their traffic along greedy paths, which are sequences of nodes where the distance in the metric space to the respective target node gets strictly smaller by each hop. Besides enabling greedy routing, the agents also optimize their connection quality within the created network by constructing greedy paths with low stretch. This ensures that greedy routing is always possible in equilibrium networks, while realistically modeling the agents' incentives for local structural changes to the network. With this we augment the elegant network creation model by Moscibroda, Schmidt, and Wattenhofer (PODC'06) with the feature of greedy routing. For our model, we analyze the existence of (approximate)-equilibria and the computational hardness in different underlying metric spaces. E.g., we characterize the set of equilibria in 1-2-metrics and tree metrics, we show that in both metrics Nash equilibria always exist, and we prove that the well-known $\Theta$-graph construction yields constant-approximate Nash equilibria in Euclidean space. The latter justifies distributed network construction via $\Theta$-graphs from a new point-of-view, since it shows that this powerful technique not only guarantees networks having a low stretch but also networks that are almost stable.

{{</citation>}}


## cs.DS (1)



### (1/1 | 200/202) Fourier Transform-based Estimators for Data Sketches (Seth Pettie et al., 2024)

{{<citation>}}

Seth Pettie, Dingyu Wang. (2024)  
**Fourier Transform-based Estimators for Data Sketches**
<br/>
<button class="copy-to-clipboard" title="Fourier Transform-based Estimators for Data Sketches" index=200>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DS  
Categories: cs-DB, cs-DC, cs-DS, cs.DS  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15366v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15366v1.pdf" filename="2403.15366v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper we consider the problem of estimating the $f$-moment ($\sum_{v\in [n]} (f(\mathbf{x}(v))-f(0))$) of a dynamic vector $\mathbf{x}\in \mathbb{G}^n$ over some abelian group $(\mathbb{G},+)$, where the $\|f\|_\infty$ norm is bounded. We propose a simple sketch and new estimation framework based on the \emph{Fourier transform} of $f$. I.e., we decompose $f$ into a linear combination of homomorphisms $f_1,f_2,\ldots$ from $(\mathbb{G},+)$ to $(\mathbb{C},\times)$, estimate the $f_k$-moment for each $f_k$, and synthesize them to obtain an estimate of the $f$-moment. Our estimators are asymptotically unbiased and have variance asymptotic to $\|\mathbf{x}\|_0^2 (\|f\|_\infty^2 m^{-1} + \|\hat{f}\|_1^2 m^{-2})$, where the size of the sketch is $O(m\log n\log|\mathbb{G}|)$ bits. When $\mathbb{G}=\mathbb{Z}$ this problem can also be solved using off-the-shelf $\ell_0$-samplers with space $O(m\log^2 n)$ bits, which does not obviously generalize to finite groups. As a concrete <b>benchmark,</b> we extend Ganguly, Garofalakis, and Rastogi's singleton-detector-based sampler to work over $\mathbb{G}$ using $O(m\log n\log|\mathbb{G}|\log(m\log n))$ bits. We give some experimental evidence that the Fourier-based estimation framework is significantly more accurate than sampling-based approaches at the same memory footprint.

{{</citation>}}


## math.CO (1)



### (1/1 | 201/202) Flip-Breakability: A Combinatorial Dichotomy for Monadically Dependent Graph Classes (Jan Dreier et al., 2024)

{{<citation>}}

Jan Dreier, Nikolas Mählmann, Szymon Toruńczyk. (2024)  
**Flip-Breakability: A Combinatorial Dichotomy for Monadically Dependent Graph Classes**
<br/>
<button class="copy-to-clipboard" title="Flip-Breakability: A Combinatorial Dichotomy for Monadically Dependent Graph Classes" index=201>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.CO  
Categories: cs-DM, cs-LO, math-CO, math-LO, math.CO  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.15201v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.15201v1.pdf" filename="2403.15201v1.pdf">Download PDF</button>

---


**ABSTRACT**  
A conjecture in algorithmic model theory predicts that the model-checking problem for first-order logic is fixed-parameter tractable on a hereditary <b>graph</b> class if and only if the class is monadically dependent. Originating in model theory, this notion is defined in terms of logic, and encompasses nowhere dense classes, monadically stable classes, and classes of bounded twin-width. Working towards this conjecture, we provide the first two combinatorial characterizations of monadically dependent <b>graph</b> classes. This yields the following dichotomy. On the structure side, we characterize monadic dependence by a Ramsey-theoretic property called flip-breakability. This notion generalizes the notions of uniform quasi-wideness, flip-flatness, and bounded grid rank, which characterize nowhere denseness, monadic stability, and bounded twin-width, respectively, and played a key role in their respective model checking algorithms. Natural restrictions of flip-breakability additionally characterize bounded treewidth and cliquewidth and bounded treedepth and shrubdepth. On the non-structure side, we characterize monadic dependence by explicitly listing few families of forbidden induced subgraphs. This result is analogous to the characterization of nowhere denseness via forbidden subdivided cliques, and allows us to resolve one half of the motivating conjecture: First-order model checking is AW[$*$]-hard on every hereditary <b>graph</b> class that is monadically independent. The result moreover implies that hereditary <b>graph</b> classes which are small, have almost bounded twin-width, or have almost bounded flip-width, are monadically dependent. Lastly, we lift our result to also obtain a combinatorial dichotomy in the more general setting of monadically dependent classes of binary structures.

{{</citation>}}


## physics.soc-ph (1)



### (1/1 | 202/202) Reconstructing the evolution history of networked complex systems (Junya Wang et al., 2024)

{{<citation>}}

Junya Wang, Yi-Jiao Zhang, Cong Xu, Jiaze Li, Jiachen Sun, Jiarong Xie, Ling Feng, Tianshou Zhou, Yanqing Hu. (2024)  
**Reconstructing the evolution history of networked complex systems**
<br/>
<button class="copy-to-clipboard" title="Reconstructing the evolution history of networked complex systems" index=202>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: physics.soc-ph  
Categories: cs-SI, physics-soc-ph, physics.soc-ph  
Keyword Score: 3  
Keywords: Clustering  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2403.14983v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2403.14983v1.pdf" filename="2403.14983v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The evolution processes of complex systems carry key information in the systems' functional properties. Applying machine learning algorithms, we demonstrate that the historical formation process of various networked complex systems can be extracted, including protein-protein interaction, ecology, and social network systems. The recovered evolution process has demonstrations of immense scientific values, such as interpreting the evolution of protein-protein interaction network, facilitating structure prediction, and particularly revealing the key co-evolution features of network structures such as preferential attachment, community structure, local <b>clustering,</b> degree-degree correlation that could not be explained collectively by previous theories. Intriguingly, we discover that for large networks, if the performance of the machine learning model is slightly better than a random guess on the pairwise order of links, reliable restoration of the overall network formation process can be achieved. This suggests that evolution history restoration is generally highly feasible on empirical networks.

{{</citation>}}
