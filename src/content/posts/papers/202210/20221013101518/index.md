---
draft: false
title: "Time Series Data Augmentation for Deep Learning: A Survey"
date: 2022-10-13
author: "akitenkrad"
description: ""
tags: ["At:Round-1", "Published:20XX"]
menu:
  sidebar:
    name: "Time Series Data Augmentation for Deep Learning: A Survey"
    identifier: 20221013
    parent: 202210
    weight: 10
math: true
---

- [x] Round-1: Overview
- [ ] Round-2: Model Implementation Details
- [ ] Round-3: Experiments

## Citation

## Abstract

## Background & Wat's New

## Dataset

## Model Description

## Results

### Settings

## References


{{< ci-details summary="Adaptive Weighting Scheme for Automatic Time-Series Data Augmentation (Elizabeth Fons et al., 2021)">}}
Elizabeth Fons, Paula Dawson, Xiao-jun Zeng, J. Keane, Alexandros Iosifidis. (2021)  
**Adaptive Weighting Scheme for Automatic Time-Series Data Augmentation**  
ArXiv  
[Paper Link](https://www.semanticscholar.org/paper/c97baa22e29c53506086cfdab71bb511c5317912)  
Influential Citation Count (0), SS-ID (c97baa22e29c53506086cfdab71bb511c5317912)  
**ABSTRACT**  
Data augmentation methods have been shown to be a fundamental technique to improve generalization in tasks such as image, text and audio classification. Recently, automated augmentation methods have led to further improvements on image classification and object detection leading to state-of-the-art performances. Nevertheless, little work has been done on time-series data, an area that could greatly benefit from automated data augmentation given the usually limited size of the datasets. We present two sample-adaptive automatic weighting schemes for data augmentation: the first learns to weight the contribution of the augmented samples to the loss, and the second method selects a subset of transformations based on the ranking of the predicted training loss. We validate our proposed methods on a large, noisy financial dataset and on time-series datasets from the UCR archive. On the financial dataset, we show that the methods in combination with a trading strategy lead to improvements in annualized returns of over 50%, and on the time-series data we outperform state-of-the-art models on over half of the datasets, and achieve similar performance in accuracy on the others.
{{< /ci-details >}}
{{< ci-details summary="Improving the Accuracy of Global Forecasting Models using Time Series Data Augmentation (Kasun Bandara et al., 2020)">}}
Kasun Bandara, Hansika Hewamalage, Yuan-Hao Liu, Yanfei Kang, C. Bergmeir. (2020)  
**Improving the Accuracy of Global Forecasting Models using Time Series Data Augmentation**  
Pattern Recognit.  
[Paper Link](https://www.semanticscholar.org/paper/146f07a70a711dc5519b9d82adc5c6593227c112)  
Influential Citation Count (2), SS-ID (146f07a70a711dc5519b9d82adc5c6593227c112)  
{{< /ci-details >}}
{{< ci-details summary="An empirical survey of data augmentation for time series classification with neural networks (Brian Kenji Iwana et al., 2020)">}}
Brian Kenji Iwana, S. Uchida. (2020)  
**An empirical survey of data augmentation for time series classification with neural networks**  
PloS one  
[Paper Link](https://www.semanticscholar.org/paper/0bec1fb7f701ef127f22220bcc874120b008c441)  
Influential Citation Count (6), SS-ID (0bec1fb7f701ef127f22220bcc874120b008c441)  
**ABSTRACT**  
In recent times, deep artificial neural networks have achieved many successes in pattern recognition. Part of this success can be attributed to the reliance on big data to increase generalization. However, in the field of time series recognition, many datasets are often very small. One method of addressing this problem is through the use of data augmentation. In this paper, we survey data augmentation techniques for time series and their application to time series classification with neural networks. We propose a taxonomy and outline the four families in time series data augmentation, including transformation-based methods, pattern mixing, generative models, and decomposition methods. Furthermore, we empirically evaluate 12 time series data augmentation methods on 128 time series classification datasets with six different types of neural networks. Through the results, we are able to analyze the characteristics, advantages and disadvantages, and recommendations of each data augmentation method. This survey aims to help in the selection of time series data augmentation for neural network applications.
{{< /ci-details >}}
{{< ci-details summary="Normalizing Flows: An Introduction and Review of Current Methods (I. Kobyzev et al., 2020)">}}
I. Kobyzev, S. Prince, Marcus A. Brubaker. (2020)  
**Normalizing Flows: An Introduction and Review of Current Methods**  
IEEE Transactions on Pattern Analysis and Machine Intelligence  
[Paper Link](https://www.semanticscholar.org/paper/31b38a19d87711489786ad54a5a00d5f0b2ead43)  
Influential Citation Count (30), SS-ID (31b38a19d87711489786ad54a5a00d5f0b2ead43)  
**ABSTRACT**  
Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.
{{< /ci-details >}}
{{< ci-details summary="A Review of Deep Learning Models for Time Series Prediction (Zhongyang Han et al., 2019)">}}
Zhongyang Han, Jun Zhao, H. Leung, King Ma, Wei Wang. (2019)  
**A Review of Deep Learning Models for Time Series Prediction**  
IEEE Sensors Journal  
[Paper Link](https://www.semanticscholar.org/paper/1bb052f153ff08f743574fa87c910d039f328ccc)  
Influential Citation Count (0), SS-ID (1bb052f153ff08f743574fa87c910d039f328ccc)  
**ABSTRACT**  
In order to approximate the underlying process of temporal data, time series prediction has been a hot research topic for decades. Developing predictive models plays an important role in interpreting complex real-world elements. With the sharp increase in the quantity and dimensionality of data, new challenges, such as extracting deep features and recognizing deep latent patterns, have emerged, demanding novel approaches and effective solutions. Deep learning, composed of multiple processing layers to learn with multiple levels of abstraction, is, now, commonly deployed for overcoming the newly arisen difficulties. This paper reviews the state-of-the-art developments in deep learning for time series prediction. Based on modeling for the perspective of conditional or joint probability, we categorize them into discriminative, generative, and hybrids models. Experiments are implemented on both benchmarks and real-world data to elaborate the performance of the representative deep learning-based prediction methods. Finally, we conclude with comments on possible future perspectives and ongoing challenges with time series prediction.
{{< /ci-details >}}
{{< ci-details summary="MODALS: Modality-agnostic Automated Data Augmentation in the Latent Space (Tsz-Him Cheung et al., 2021)">}}
Tsz-Him Cheung, Dit-Yan Yeung. (2021)  
**MODALS: Modality-agnostic Automated Data Augmentation in the Latent Space**  
ICLR  
[Paper Link](https://www.semanticscholar.org/paper/d1b11e6c58e4a66ecb4f0dfd20a21b4099c4ede5)  
Influential Citation Count (2), SS-ID (d1b11e6c58e4a66ecb4f0dfd20a21b4099c4ede5)  
{{< /ci-details >}}
{{< ci-details summary="DATSING: Data Augmented Time Series Forecasting with Adversarial Domain Adaptation (Hailin Hu et al., 2020)">}}
Hailin Hu, Mingjian Tang, Chengcheng Bai. (2020)  
**DATSING: Data Augmented Time Series Forecasting with Adversarial Domain Adaptation**  
CIKM  
[Paper Link](https://www.semanticscholar.org/paper/9cc1181cfeb71fbdb98e925870698c8f75571466)  
Influential Citation Count (0), SS-ID (9cc1181cfeb71fbdb98e925870698c8f75571466)  
**ABSTRACT**  
Due to the high temporal uncertainty and low signal-to-noise ratio, transfer learning for univariate time series forecasting remains a challenging task. In addition, data scarcity, which is commonly encountered in business forecasting, further limits the application of conventional transfer learning protocols. In this work, we have developed, DATSING, a transfer learning-based framework that effectively leverages cross-domain time series latent representations to augment target domain forecasting. In particular, we aim to transfer domain-invariant feature representations from a pre-trained stacked deep residual network to the target domains, so as to assist the prediction of each target time series. To effectively avoid noisy feature representations, we propose a two-phased framework which first clusters similar mixed domains time series data and then performs a fine-tuning procedure with domain adversarial regularization to achieve better out-of-sample generalization. Extensive experiments with real-world datasets have demonstrated that our method significantly improves the forecasting performance of the pre-trained model. DATSING has the unique potential to empower forecasting practitioners to unleash the power of cross-domain time series data.
{{< /ci-details >}}
{{< ci-details summary="Fast RobustSTL: Efficient and Robust Seasonal-Trend Decomposition for Time Series with Complex Patterns (Qingsong Wen et al., 2020)">}}
Qingsong Wen, Zhe Zhang, Yan Li, Liang Sun. (2020)  
**Fast RobustSTL: Efficient and Robust Seasonal-Trend Decomposition for Time Series with Complex Patterns**  
KDD  
[Paper Link](https://www.semanticscholar.org/paper/3eea78101b2a314c9746013c5d21e212460fe99b)  
Influential Citation Count (1), SS-ID (3eea78101b2a314c9746013c5d21e212460fe99b)  
**ABSTRACT**  
Many real-world time series data exhibit complex patterns with trend, seasonality, outlier and noise. Robustly and accurately decomposing these components would greatly facilitate time series tasks including anomaly detection, forecasting and classification. RobustSTL is an effective seasonal-trend decomposition for time series data with complicated patterns. However, it cannot handle multiple seasonal components properly. Also it suffers from its high computational complexity, which limits its usage in practice. In this paper, we extend RobustSTL to handle multiple seasonality. To speed up the computation, we propose a special generalized ADMM algorithm to perform the decomposition efficiently. We rigorously prove that the proposed algorithm converges approximately as standard ADMM while reducing the complexity from O(N2) to O(N log N) for each iteration. We empirically study our proposed algorithm with other state-of-the-art seasonal-trend decomposition methods, including MSTL, STR, TBATS, on both synthetic and real-world datasets with single and multiple seasonality. The experimental results demonstrate the superior performance of our decomposition algorithm in terms of both effectiveness and efficiency.
{{< /ci-details >}}
{{< ci-details summary="Stock market forecasting with super-high dimensional time-series data using ConvLSTM, trend sampling, and specialized data augmentation (S. Lee et al., 2020)">}}
S. Lee, Ha Young Kim. (2020)  
**Stock market forecasting with super-high dimensional time-series data using ConvLSTM, trend sampling, and specialized data augmentation**  
Expert Syst. Appl.  
[Paper Link](https://www.semanticscholar.org/paper/28cf4b2a6be1160506126a461d2df410122c5e66)  
Influential Citation Count (1), SS-ID (28cf4b2a6be1160506126a461d2df410122c5e66)  
{{< /ci-details >}}
{{< ci-details summary="Data augmentation for time series: traditional vs generative models on capacitive proximity time series (Biying Fu et al., 2020)">}}
Biying Fu, Florian Kirchbuchner, Arjan Kuijper. (2020)  
**Data augmentation for time series: traditional vs generative models on capacitive proximity time series**  
PETRA  
[Paper Link](https://www.semanticscholar.org/paper/f214b3f7cdbabd0a2487797eeaffb86f0e934bfa)  
Influential Citation Count (0), SS-ID (f214b3f7cdbabd0a2487797eeaffb86f0e934bfa)  
**ABSTRACT**  
Large labeled quantities and diversities of training data are often needed for supervised, data-based modelling. Data distribution should cover a rich representation to support the generalizability of the trained end-to-end inference model. However, this is often hindered by limited labeled data and the expensive data collection process, especially for human activity recognition tasks. Extensive manual labeling is required. Data augmentation is thus a widely used regularization method for deep learning, especially applied on image data to increase the classification accuracy. But it is less researched for time series. In this paper, we investigate the data augmentation task on continuous capacitive time series with the example on exercise recognition. We show that the traditional data augmentation can enrich the source distribution and thus make the trained inference model more generalized. This further increases the recognition performance for unseen target data around 21.4 percentage points compared to inference model without data augmentation. The generative models such as variational autoencoder or conditional variational autoencoder can further reduce the variance on the target data.
{{< /ci-details >}}
{{< ci-details summary="Modeling Continuous Stochastic Processes with Dynamic Normalizing Flows (Ruizhi Deng et al., 2020)">}}
Ruizhi Deng, B. Chang, Marcus A. Brubaker, Greg Mori, Andreas M. Lehrmann. (2020)  
**Modeling Continuous Stochastic Processes with Dynamic Normalizing Flows**  
NeurIPS  
[Paper Link](https://www.semanticscholar.org/paper/b9df69fe39440ad9b131bc6dd850063b78604cba)  
Influential Citation Count (3), SS-ID (b9df69fe39440ad9b131bc6dd850063b78604cba)  
**ABSTRACT**  
Normalizing flows transform a simple base distribution into a complex target distribution and have proved to be powerful models for data generation and density estimation. In this work, we propose a novel type of normalizing flow driven by a differential deformation of the Wiener process. As a result, we obtain a rich time series model whose observable process inherits many of the appealing properties of its base process, such as efficient computation of likelihoods and marginals. Furthermore, our continuous treatment provides a natural framework for irregular time series with an independent arrival process, including straightforward interpolation. We illustrate the desirable properties of the proposed model on popular stochastic processes and demonstrate its superior flexibility to variational RNN and latent ODE baselines in a series of experiments on synthetic and real-world data.
{{< /ci-details >}}
{{< ci-details summary="RobustTAD: Robust Time Series Anomaly Detection via Decomposition and Convolutional Neural Networks (Jing Gao et al., 2020)">}}
Jing Gao, Xiaomin Song, Qingsong Wen, Pichao Wang, Liang Sun, Huan Xu. (2020)  
**RobustTAD: Robust Time Series Anomaly Detection via Decomposition and Convolutional Neural Networks**  
ArXiv  
[Paper Link](https://www.semanticscholar.org/paper/6c1eeab447252f5897209ca50abc863ad7e83f9e)  
Influential Citation Count (1), SS-ID (6c1eeab447252f5897209ca50abc863ad7e83f9e)  
**ABSTRACT**  
The monitoring and management of numerous and diverse time series data at Alibaba Group calls for an effective and scalable time series anomaly detection service. In this paper, we propose RobustTAD, a Robust Time series Anomaly Detection framework by integrating robust seasonal-trend decomposition and convolutional neural network for time series data. The seasonal-trend decomposition can effectively handle complicated patterns in time series, and meanwhile significantly simplifies the architecture of the neural network, which is an encoder-decoder architecture with skip connections. This architecture can effectively capture the multi-scale information from time series, which is very useful in anomaly detection. Due to the limited labeled data in time series anomaly detection, we systematically investigate data augmentation methods in both time and frequency domains. We also introduce label-based weight and value-based weight in the loss function by utilizing the unbalanced nature of the time series anomaly detection problem. Compared with the widely used forecasting-based anomaly detection algorithms, decomposition-based algorithms, traditional statistical algorithms, as well as recent neural network based algorithms, RobustTAD performs significantly better on public benchmark datasets. It is deployed as a public online service and widely adopted in different business scenarios at Alibaba Group.
{{< /ci-details >}}
{{< ci-details summary="Adversarial AutoAugment (Xinyu Zhang et al., 2019)">}}
Xinyu Zhang, Qiang Wang, Jian Zhang, Zhaobai Zhong. (2019)  
**Adversarial AutoAugment**  
ICLR  
[Paper Link](https://www.semanticscholar.org/paper/7d668ed2d5b383c7ff9641974b2bc0c84f43e251)  
Influential Citation Count (29), SS-ID (7d668ed2d5b383c7ff9641974b2bc0c84f43e251)  
**ABSTRACT**  
Data augmentation (DA) has been widely utilized to improve generalization in training deep neural networks. Recently, human-designed data augmentation has been gradually replaced by automatically learned augmentation policy. Through finding the best policy in well-designed search space of data augmentation, AutoAugment can significantly improve validation accuracy on image classification tasks. However, this approach is not computationally practical for large-scale problems. In this paper, we develop an adversarial method to arrive at a computationally-affordable solution called Adversarial AutoAugment, which can simultaneously optimize target related object and augmentation policy search loss. The augmentation policy network attempts to increase the training loss of a target network through generating adversarial augmentation policies, while the target network can learn more robust features from harder examples to improve the generalization. In contrast to prior work, we reuse the computation in target network training for policy evaluation, and dispense with the retraining of the target network. Compared to AutoAugment, this leads to about 12x reduction in computing cost and 11x shortening in time overhead on ImageNet. We show experimental results of our approach on CIFAR-10/CIFAR-100, ImageNet, and demonstrate significant performance improvements over state-of-the-art. On CIFAR-10, we achieve a top-1 test error of 1.36%, which is the currently best performing single model. On ImageNet, we achieve a leading performance of top-1 accuracy 79.40% on ResNet-50 and 80.00% on ResNet-50-D without extra data.
{{< /ci-details >}}
{{< ci-details summary="Randaugment: Practical automated data augmentation with a reduced search space (E. D. Cubuk et al., 2019)">}}
E. D. Cubuk, Barret Zoph, Jonathon Shlens, Quoc V. Le. (2019)  
**Randaugment: Practical automated data augmentation with a reduced search space**  
2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)  
[Paper Link](https://www.semanticscholar.org/paper/6bbd51697c25493ea15f4cac830e28eeac143898)  
Influential Citation Count (244), SS-ID (6bbd51697c25493ea15f4cac830e28eeac143898)  
**ABSTRACT**  
Recent work on automated augmentation strategies has led to state-of-the-art results in image classification and object detection. An obstacle to a large-scale adoption of these methods is that they require a separate and expensive search phase. A common way to overcome the expense of the search phase was to use a smaller proxy task. However, it was not clear if the optimized hyperparameters found on the proxy task are also optimal for the actual task. In this work, we rethink the process of designing automated augmentation strategies. We find that while previous work required a search for both magnitude and probability of each operation independently, it is sufficient to only search for a single distortion magnitude that jointly controls all operations. We hence propose a simplified search space that vastly reduces the computational expense of automated augmentation, and permits the removal of a separate proxy task.Despite the simplifications, our method achieves equal or better performance over previous automated augmentation strategies on on CIFAR-10/100, SVHN, ImageNet and COCO datasets. EfficientNet-B7, we achieve 85.0% accuracy, a 1.0% increase over baseline augmentation, a 0.6% improvement over AutoAugment on the ImageNet dataset. With EfficientNet-B8, we achieve 85.4% accuracy on ImageNet, which matches a previous result that used 3.5B extra images. On object detection, the same method as classification leads to 1.0-1.3% improvement over baseline augmentation. Code will be made available online.
{{< /ci-details >}}
{{< ci-details summary="GRATIS: GeneRAting TIme Series with diverse and controllable characteristics (Yanfei Kang et al., 2019)">}}
Yanfei Kang, R. Hyndman, Feng Li. (2019)  
**GRATIS: GeneRAting TIme Series with diverse and controllable characteristics**  
Stat. Anal. Data Min.  
[Paper Link](https://www.semanticscholar.org/paper/f0178f82e99e03558990ed3547cc841d7f1e0343)  
Influential Citation Count (3), SS-ID (f0178f82e99e03558990ed3547cc841d7f1e0343)  
**ABSTRACT**  
The explosion of time series data in recent years has brought a flourish of new time series analysis methods, for forecasting, clustering, classification and other tasks. The evaluation of these new methods requires either collecting or simulating a diverse set of time series benchmarking data to enable reliable comparisons against alternative approaches. We propose GeneRAting TIme Series with diverse and controllable characteristics, named GRATIS, with the use of mixture autoregressive (MAR) models. We simulate sets of time series using MAR models and investigate the diversity and coverage of the generated time series in a time series feature space. By tuning the parameters of the MAR models, GRATIS is also able to efficiently generate new time series with controllable features. In general, as a costless surrogate to the traditional data collection approach, GRATIS can be used as an evaluation tool for tasks such as time series forecasting and classification. We illustrate the usefulness of our time series generation process through a time series forecasting application.
{{< /ci-details >}}
{{< ci-details summary="DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks (Valentin Flunkert et al., 2017)">}}
Valentin Flunkert, David Salinas, Jan Gasthaus. (2017)  
**DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks**  
International Journal of Forecasting  
[Paper Link](https://www.semanticscholar.org/paper/4eebe0d12aefeedf3ca85256bc8aa3b4292d47d9)  
Influential Citation Count (114), SS-ID (4eebe0d12aefeedf3ca85256bc8aa3b4292d47d9)  
{{< /ci-details >}}
{{< ci-details summary="GluonTS: Probabilistic and Neural Time Series Modeling in Python (A. Alexandrov et al., 2020)">}}
A. Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C. Maddix, Syama Sundar Rangapuram, David Salinas, J. Schulz, Lorenzo Stella, Ali Caner Türkmen, Bernie Wang. (2020)  
**GluonTS: Probabilistic and Neural Time Series Modeling in Python**  
J. Mach. Learn. Res.  
[Paper Link](https://www.semanticscholar.org/paper/16c5cd802cdb64e484fa4c3f6df0b0afd57492d3)  
Influential Citation Count (7), SS-ID (16c5cd802cdb64e484fa4c3f6df0b0afd57492d3)  
**ABSTRACT**  
We introduce the Gluon Time Series Toolkit (GluonTS), a Python library for deep learning based time series modeling for ubiquitous tasks, such as forecasting and anomaly detection. GluonTS simpliﬁes the time series modeling pipeline by providing the necessary components and tools for quick model development, eﬃcient experimentation and evaluation. In addition, it contains reference implementations of state-of-the-art time series models that enable simple benchmarking of new algorithms. loss of various pre-built models in GluonTS on 10 open-source datasets: hourly electricity consumption of 370 customers (Dheeru and Karra Taniskidou, 2017), daily exchange rate between 8 currencies used in (Lai et al., 2017), 6 datasets from the M4 competition (Makridakis et al., 2018), hourly photo-voltaic production of 137 stations in used (Lai et al., 2017), occupancy between of (Dheeru and
{{< /ci-details >}}
{{< ci-details summary="Learning Data Manipulation for Augmentation and Weighting (Zhiting Hu et al., 2019)">}}
Zhiting Hu, Bowen Tan, R. Salakhutdinov, Tom Michael Mitchell, E. Xing. (2019)  
**Learning Data Manipulation for Augmentation and Weighting**  
NeurIPS  
[Paper Link](https://www.semanticscholar.org/paper/16908e1d9b47ebca816d8cc92c5aa101eb7d7605)  
Influential Citation Count (13), SS-ID (16908e1d9b47ebca816d8cc92c5aa101eb7d7605)  
**ABSTRACT**  
Manipulating data, such as weighting data examples or augmenting with new instances, has been increasingly used to improve model training. Previous work has studied various rule- or learning-based approaches designed for specific types of data manipulation. In this work, we propose a new method that supports learning different manipulation schemes with the same gradient-based algorithm. Our approach builds upon a recent connection of supervised learning and reinforcement learning (RL), and adapts an off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training. Different parameterization of the ``data reward'' function instantiates different manipulation schemes. We showcase data augmentation that learns a text transformation network, and data weighting that dynamically adapts the data sample importance. Experiments show the resulting algorithms significantly improve the image and text classification performance in low data regime and class-imbalance problems.
{{< /ci-details >}}
{{< ci-details summary="Times-series data augmentation and deep learning for construction equipment activity recognition (Khandakar M. Rashid et al., 2019)">}}
Khandakar M. Rashid, Joseph Louis. (2019)  
**Times-series data augmentation and deep learning for construction equipment activity recognition**  
Adv. Eng. Informatics  
[Paper Link](https://www.semanticscholar.org/paper/56246efd0efeaf96ef0bae43718b4bc42ffa0f20)  
Influential Citation Count (7), SS-ID (56246efd0efeaf96ef0bae43718b4bc42ffa0f20)  
{{< /ci-details >}}
{{< ci-details summary="Surrogate Rehabilitative Time Series Data for Image-based Deep Learning (K. Eileen et al., 2019)">}}
K. Eileen, Kuah-Li Kuah, K. Leo, S. Sanei, E. Chew, Ling Zhao. (2019)  
**Surrogate Rehabilitative Time Series Data for Image-based Deep Learning**  
2019 27th European Signal Processing Conference (EUSIPCO)  
[Paper Link](https://www.semanticscholar.org/paper/70ca28e8a244d7bda8e2db86072ed93e445cff4d)  
Influential Citation Count (0), SS-ID (70ca28e8a244d7bda8e2db86072ed93e445cff4d)  
**ABSTRACT**  
Big Data comprise the tools to analyse vast stores of data generated by the myriad of powerful, low-cost processors, sensors and networks around us. The spiralling demand for multi-sensored personal communication devices has played a major role in this, producing images and leaving digital trails of transactions and texts to be mined for patterns. Consequently, the cutting edge of data analysis tools has been targeted for images. However the copious amounts of data required to successfully train these tools are not available in several fields such as rehabilitation where there are constraints on data collection. And yet the need for timely clinical assessments grows.We consider how to address this situation by generating synthetic, surrogate data which preserves many properties of the original. Here we introduce a new application of surrogate time series in a novel classification scheme, compare methods of converting these into images and use a state of the art neural network framework for a successful improvement in classification results.This is a significant contribution to the art, demonstrating how scarce time series data can be successfully augmented to take advantage of cutting edge analytical tools.
{{< /ci-details >}}
{{< ci-details summary="BeatGAN: Anomalous Rhythm Detection using Adversarially Generated Time Series (Bin Zhou et al., 2019)">}}
Bin Zhou, Shenghua Liu, Bryan Hooi, Xueqi Cheng, Jing Ye. (2019)  
**BeatGAN: Anomalous Rhythm Detection using Adversarially Generated Time Series**  
IJCAI  
[Paper Link](https://www.semanticscholar.org/paper/715008d75d97a0a975bbd644a977f83942c5eb6a)  
Influential Citation Count (15), SS-ID (715008d75d97a0a975bbd644a977f83942c5eb6a)  
**ABSTRACT**  
Given a large-scale rhythmic time series containing mostly normal data segments (or `beats'), can we learn how to detect anomalous beats in an effective yet efficient way? For example, how can we detect anomalous beats from electrocardiogram (ECG) readings? Existing approaches either require excessively high amounts of labeled and balanced data for classification, or rely on less regularized reconstructions, resulting in lower accuracy in anomaly detection. Therefore, we propose BeatGAN, an unsupervised anomaly detection algorithm for time series data. BeatGAN outputs explainable results to pinpoint the anomalous time ticks of an input beat, by comparing them to adversarially generated beats. Its robustness is guaranteed by its regularization of reconstruction error using an adversarial generation approach, as well as data augmentation using time series warping. Experiments show that BeatGAN accurately and efficiently detects anomalous beats in ECG time series, and routes doctors' attention to anomalous time ticks, achieving accuracy of nearly 0.95 AUC, and very fast inference (2.6 ms per beat). In addition, we show that BeatGAN accurately detects unusual motions from multivariate motion-capture time series data, illustrating its generality.
{{< /ci-details >}}
{{< ci-details summary="A survey on Image Data Augmentation for Deep Learning (Connor Shorten et al., 2019)">}}
Connor Shorten, T. Khoshgoftaar. (2019)  
**A survey on Image Data Augmentation for Deep Learning**  
Journal of Big Data  
[Paper Link](https://www.semanticscholar.org/paper/3813b88a4ec3c63919df47e9694b577f4691f7e5)  
Influential Citation Count (98), SS-ID (3813b88a4ec3c63919df47e9694b577f4691f7e5)  
{{< /ci-details >}}
{{< ci-details summary="RobustTrend: A Huber Loss with a Combined First and Second Order Difference Regularization for Time Series Trend Filtering (Qingsong Wen et al., 2019)">}}
Qingsong Wen, Jing Gao, Xiaomin Song, Liang Sun, Jian Tan. (2019)  
**RobustTrend: A Huber Loss with a Combined First and Second Order Difference Regularization for Time Series Trend Filtering**  
IJCAI  
[Paper Link](https://www.semanticscholar.org/paper/c08b27d555d5fb8d8322311a9a6a66726abfe1cb)  
Influential Citation Count (0), SS-ID (c08b27d555d5fb8d8322311a9a6a66726abfe1cb)  
**ABSTRACT**  
Extracting the underlying trend signal is a crucial step to facilitate time series analysis like forecasting and anomaly detection. Besides noise signal, time series can contain not only outliers but also abrupt trend changes in real-world scenarios. To deal with these challenges, we propose a robust trend filtering algorithm based on robust statistics and sparse learning. Specifically, we adopt the Huber loss to suppress outliers, and utilize a combination of the first order and second order difference on the trend component as regularization to capture both slow and abrupt trend changes. Furthermore, an efficient method is designed to solve the proposed robust trend filtering based on majorization minimization (MM) and alternative direction method of multipliers (ADMM). We compared our proposed robust trend filter with other nine state-of-the-art trend filtering algorithms on both synthetic and real-world datasets. The experiments demonstrate that our algorithm outperforms existing methods.
{{< /ci-details >}}
{{< ci-details summary="AutoAugment: Learning Augmentation Strategies From Data (E. D. Cubuk et al., 2019)">}}
E. D. Cubuk, Barret Zoph, Dandelion Mané, Vijay Vasudevan, Quoc V. Le. (2019)  
**AutoAugment: Learning Augmentation Strategies From Data**  
2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)  
[Paper Link](https://www.semanticscholar.org/paper/21de3a36cb51adc205fad8a1d3d69118891dc3dd)  
Influential Citation Count (173), SS-ID (21de3a36cb51adc205fad8a1d3d69118891dc3dd)  
**ABSTRACT**  
Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many sub-policies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5% which is 0.4% better than the previous record of 83.1%. On CIFAR-10, we achieve an error rate of 1.5%, which is 0.6% better than the previous state-of-the-art. Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.
{{< /ci-details >}}
{{< ci-details summary="Time Series Anomaly Detection Using Convolutional Neural Networks and Transfer Learning (Tailai Wen et al., 2019)">}}
Tailai Wen, Roy Keyes. (2019)  
**Time Series Anomaly Detection Using Convolutional Neural Networks and Transfer Learning**  
ArXiv  
[Paper Link](https://www.semanticscholar.org/paper/0017aeb0c049a383d962399d26100ec2bf5cc7c7)  
Influential Citation Count (4), SS-ID (0017aeb0c049a383d962399d26100ec2bf5cc7c7)  
**ABSTRACT**  
Time series anomaly detection plays a critical role in automated monitoring systems. Most previous deep learning efforts related to time series anomaly detection were based on recurrent neural networks (RNN). In this paper, we propose a time series segmentation approach based on convolutional neural networks (CNN) for anomaly detection. Moreover, we propose a transfer learning framework that pretrains a model on a large-scale synthetic univariate time series data set and then fine-tunes its weights on small-scale, univariate or multivariate data sets with previously unseen classes of anomalies. For the multivariate case, we introduce a novel network architecture. The approach was tested on multiple synthetic and real data sets successfully.
{{< /ci-details >}}
{{< ci-details summary="Augmenting Physiological Time Series Data: A Case Study for Sleep Apnea Detection (K. Nikolaidis et al., 2019)">}}
K. Nikolaidis, Stein Kristiansen, V. Goebel, T. Plagemann, K. Liestøl, Mohan S. Kankanhalli. (2019)  
**Augmenting Physiological Time Series Data: A Case Study for Sleep Apnea Detection**  
ECML/PKDD  
[Paper Link](https://www.semanticscholar.org/paper/a2b33e86f583f1e9f85e4978782ba1e8113da04a)  
Influential Citation Count (0), SS-ID (a2b33e86f583f1e9f85e4978782ba1e8113da04a)  
{{< /ci-details >}}
{{< ci-details summary="Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules (Daniel Ho et al., 2019)">}}
Daniel Ho, Eric Liang, I. Stoica, P. Abbeel, Xi Chen. (2019)  
**Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules**  
ICML  
[Paper Link](https://www.semanticscholar.org/paper/60f7f9161eb88c71c893969fb0d6586f70be99fc)  
Influential Citation Count (45), SS-ID (60f7f9161eb88c71c893969fb0d6586f70be99fc)  
**ABSTRACT**  
A key challenge in leveraging data augmentation for neural network training is choosing an effective augmentation policy from a large search space of candidate operations. Properly chosen augmentation policies can lead to significant generalization improvements; however, state-of-the-art approaches such as AutoAugment are computationally infeasible to run for the ordinary user. In this paper, we introduce a new data augmentation algorithm, Population Based Augmentation (PBA), which generates nonstationary augmentation policy schedules instead of a fixed augmentation policy. We show that PBA can match the performance of AutoAugment on CIFAR-10, CIFAR-100, and SVHN, with three orders of magnitude less overall compute. On CIFAR-10 we achieve a mean test error of 1.46%, which is a slight improvement upon the current state-of-the-art. The code for PBA is open source and is available at this https URL.
{{< /ci-details >}}
{{< ci-details summary="SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition (Daniel S. Park et al., 2019)">}}
Daniel S. Park, William Chan, Yu Zhang, C. Chiu, Barret Zoph, E. D. Cubuk, Quoc V. Le. (2019)  
**SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition**  
INTERSPEECH  
[Paper Link](https://www.semanticscholar.org/paper/b0fae9fbb4e580d92395eabafe73e317ae6510e3)  
Influential Citation Count (312), SS-ID (b0fae9fbb4e580d92395eabafe73e317ae6510e3)  
**ABSTRACT**  
We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8% WER on test-other without the use of a language model, and 5.8% WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8%/14.1% with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.
{{< /ci-details >}}
{{< ci-details summary="Deep Learning for Anomaly Detection: A Survey (Raghavendra Chalapathy et al., 2019)">}}
Raghavendra Chalapathy, S. Chawla. (2019)  
**Deep Learning for Anomaly Detection: A Survey**  
ArXiv  
[Paper Link](https://www.semanticscholar.org/paper/84af0bbe339c1adbc9d075dee99b9d4f86a186c5)  
Influential Citation Count (5), SS-ID (84af0bbe339c1adbc9d075dee99b9d4f86a186c5)  
**ABSTRACT**  
Anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. The aim of this survey is two-fold, firstly we present a structured and comprehensive overview of research methods in deep learning-based anomaly detection. Furthermore, we review the adoption of these methods for anomaly across various application domains and assess their effectiveness. We have grouped state-of-the-art research techniques into different categories based on the underlying assumptions and approach adopted. Within each category we outline the basic anomaly detection technique, along with its variants and present key assumptions, to differentiate between normal and anomalous behavior. For each category, we present we also present the advantages and limitations and discuss the computational complexity of the techniques in real application domains. Finally, we outline open issues in research and challenges faced while adopting these techniques.
{{< /ci-details >}}
{{< ci-details summary="RobustSTL: A Robust Seasonal-Trend Decomposition Algorithm for Long Time Series (Qingsong Wen et al., 2018)">}}
Qingsong Wen, Jing Gao, Xiaomin Song, Liang Sun, Huan Xu, Shenghuo Zhu. (2018)  
**RobustSTL: A Robust Seasonal-Trend Decomposition Algorithm for Long Time Series**  
AAAI  
[Paper Link](https://www.semanticscholar.org/paper/ff454a661ade6321bb275b205159c84dc9729521)  
Influential Citation Count (3), SS-ID (ff454a661ade6321bb275b205159c84dc9729521)  
**ABSTRACT**  
Decomposing complex time series into trend, seasonality, and remainder components is an important task to facilitate time series anomaly detection and forecasting. Although numerous methods have been proposed, there are still many time series characteristics exhibiting in real-world data which are not addressed properly, including 1) ability to handle seasonality fluctuation and shift, and abrupt change in trend and reminder; 2) robustness on data with anomalies; 3) applicability on time series with long seasonality period. In the paper, we propose a novel and generic time series decomposition algorithm to address these challenges. Specifically, we extract the trend component robustly by solving a regression problem using the least absolute deviations loss with sparse regularization. Based on the extracted trend, we apply the the non-local seasonal filtering to extract the seasonality component. This process is repeated until accurate decomposition is obtained. Experiments on different synthetic and real-world time series datasets demonstrate that our method outperforms existing solutions.
{{< /ci-details >}}
{{< ci-details summary="Deep learning for time series classification: a review (Hassan Ismail Fawaz et al., 2018)">}}
Hassan Ismail Fawaz, G. Forestier, J. Weber, L. Idoumghar, Pierre-Alain Muller. (2018)  
**Deep learning for time series classification: a review**  
Data Mining and Knowledge Discovery  
[Paper Link](https://www.semanticscholar.org/paper/1c2efb418f79b5d29913e014a1dfd78865221c39)  
Influential Citation Count (99), SS-ID (1c2efb418f79b5d29913e014a1dfd78865221c39)  
{{< /ci-details >}}
{{< ci-details summary="Time-series Generative Adversarial Networks (Jinsung Yoon et al., 2019)">}}
Jinsung Yoon, Daniel Jarrett, M. Schaar. (2019)  
**Time-series Generative Adversarial Networks**  
NeurIPS  
[Paper Link](https://www.semanticscholar.org/paper/08d350a25720d865a52c00f3af6cb80e7af52d58)  
Influential Citation Count (39), SS-ID (08d350a25720d865a52c00f3af6cb80e7af52d58)  
**ABSTRACT**  
A good generative model for time-series data should preserve temporal dynamics, in the sense that new sequences respect the original relationships between variables across time. Existing methods that bring generative adversarial networks (GANs) into the sequential setting do not adequately attend to the temporal correlations unique to time-series data. At the same time, supervised models for sequence prediction - which allow finer control over network dynamics - are inherently deterministic. We propose a novel framework for generating realistic time-series data that combines the flexibility of the unsupervised paradigm with the control afforded by supervised training. Through a learned embedding space jointly optimized with both supervised and adversarial objectives, we encourage the network to adhere to the dynamics of the training data during sampling. Empirically, we evaluate the ability of our method to generate realistic samples using a variety of real and synthetic time-series datasets. Qualitatively and quantitatively, we find that the proposed framework consistently and significantly outperforms state-of-the-art benchmarks with respect to measures of similarity and predictive ability.
{{< /ci-details >}}
{{< ci-details summary="Feature Representation and Data Augmentation for Human Activity Classification Based on Wearable IMU Sensor Data Using a Deep LSTM Neural Network (Odongo Steven Eyobu et al., 2018)">}}
Odongo Steven Eyobu, D. Han. (2018)  
**Feature Representation and Data Augmentation for Human Activity Classification Based on Wearable IMU Sensor Data Using a Deep LSTM Neural Network**  
Sensors  
[Paper Link](https://www.semanticscholar.org/paper/2a3e74a5b9bec0b19306221e64fdc245a4ba3fe4)  
Influential Citation Count (5), SS-ID (2a3e74a5b9bec0b19306221e64fdc245a4ba3fe4)  
**ABSTRACT**  
Wearable inertial measurement unit (IMU) sensors are powerful enablers for acquisition of motion data. Specifically, in human activity recognition (HAR), IMU sensor data collected from human motion are categorically combined to formulate datasets that can be used for learning human activities. However, successful learning of human activities from motion data involves the design and use of proper feature representations of IMU sensor data and suitable classifiers. Furthermore, the scarcity of labelled data is an impeding factor in the process of understanding the performance capabilities of data-driven learning models. To tackle these challenges, two primary contributions are in this article: first; by using raw IMU sensor data, a spectrogram-based feature extraction approach is proposed. Second, an ensemble of data augmentations in feature space is proposed to take care of the data scarcity problem. Performance tests were conducted on a deep long term short term memory (LSTM) neural network architecture to explore the influence of feature representations and the augmentations on activity recognition accuracy. The proposed feature extraction approach combined with the data augmentation ensemble produces state-of-the-art accuracy results in HAR. A performance evaluation of each augmentation approach is performed to show the influence on classification accuracy. Finally, in addition to using our own dataset, the proposed data augmentation technique is evaluated against the University of California, Irvine (UCI) public online HAR dataset and yields state-of-the-art accuracy results at various learning rates.
{{< /ci-details >}}
{{< ci-details summary="DOPING: Generative Data Augmentation for Unsupervised Anomaly Detection with GAN (Swee Kiat Lim et al., 2018)">}}
Swee Kiat Lim, Yi Loo, Ngoc-Trung Tran, Ngai-Man Cheung, G. Roig, Y. Elovici. (2018)  
**DOPING: Generative Data Augmentation for Unsupervised Anomaly Detection with GAN**  
2018 IEEE International Conference on Data Mining (ICDM)  
[Paper Link](https://www.semanticscholar.org/paper/a47f8794d88c5c27123153c4eb9e08046e2b0c9d)  
Influential Citation Count (1), SS-ID (a47f8794d88c5c27123153c4eb9e08046e2b0c9d)  
**ABSTRACT**  
Recently, the introduction of the generative adversarial network (GAN) and its variants has enabled the generation of realistic synthetic samples, which has been used for enlarging training sets. Previous work primarily focused on data augmentation for semi-supervised and supervised tasks. In this paper, we instead focus on unsupervised anomaly detection and propose a novel generative data augmentation framework optimized for this task. In particular, we propose to oversample infrequent normal samples - normal samples that occur with small probability, e.g., rare normal events. We show that these samples are responsible for false positives in anomaly detection. However, oversampling of infrequent normal samples is challenging for real-world high-dimensional data with multimodal distributions. To address this challenge, we propose to use a GAN variant known as the adversarial autoencoder (AAE) to transform the high-dimensional multimodal data distributions into low-dimensional unimodal latent distributions with well-defined tail probability. Then, we systematically oversample at the 'edge' of the latent distributions to increase the density of infrequent normal samples. We show that our oversampling pipeline is a unified one: it is generally applicable to datasets with different complex data distributions. To the best of our knowledge, our method is the first data augmentation technique focused on improving performance in unsupervised anomaly detection. We validate our method by demonstrating consistent improvements across several real-world datasets.
{{< /ci-details >}}
{{< ci-details summary="Data augmentation using synthetic data for time series classification with deep residual networks (Hassan Ismail Fawaz et al., 2018)">}}
Hassan Ismail Fawaz, G. Forestier, J. Weber, L. Idoumghar, Pierre-Alain Muller. (2018)  
**Data augmentation using synthetic data for time series classification with deep residual networks**  
ArXiv  
[Paper Link](https://www.semanticscholar.org/paper/af885ec2163397018db9da8784d7489ca20d2d3a)  
Influential Citation Count (6), SS-ID (af885ec2163397018db9da8784d7489ca20d2d3a)  
**ABSTRACT**  
Data augmentation in deep neural networks is the process of generating artificial data in order to reduce the variance of the classifier with the goal to reduce the number of errors. This idea has been shown to improve deep neural network's generalization capabilities in many computer vision tasks such as image recognition and object localization. Apart from these applications, deep Convolutional Neural Networks (CNNs) have also recently gained popularity in the Time Series Classification (TSC) community. However, unlike in image recognition problems, data augmentation techniques have not yet been investigated thoroughly for the TSC task. This is surprising as the accuracy of deep learning models for TSC could potentially be improved, especially for small datasets that exhibit overfitting, when a data augmentation method is adopted. In this paper, we fill this gap by investigating the application of a recently proposed data augmentation technique based on the Dynamic Time Warping distance, for a deep learning model for TSC. To evaluate the potential of augmenting the training set, we performed extensive experiments using the UCR TSC benchmark. Our preliminary experiments reveal that data augmentation can drastically increase deep CNN's accuracy on some datasets and significantly improve the deep model's accuracy when the method is used in an ensemble approach.
{{< /ci-details >}}
{{< ci-details summary="Feature-based comparison and generation of time series (Lars Kegel et al., 2018)">}}
Lars Kegel, M. Hahmann, Wolfgang Lehner. (2018)  
**Feature-based comparison and generation of time series**  
SSDBM  
[Paper Link](https://www.semanticscholar.org/paper/6c6ae6c9d013e97caf452a285a47d510dc3cbb35)  
Influential Citation Count (0), SS-ID (6c6ae6c9d013e97caf452a285a47d510dc3cbb35)  
**ABSTRACT**  
For more than three decades, researchers have been developping generation methods for the weather, energy, and economic domain. These methods provide generated datasets for reasons like system evaluation and data availability. However, despite the variety of approaches, there is no comparative and cross-domain assessment of generation methods and their expressiveness. We present a similarity measure that analyzes generation methods regarding general time series features. By this means, users can compare generation methods and validate whether a generated dataset is considered similar to a given dataset. Moreover, we propose a feature-based generation method that evolves cross-domain time series datasets. This method outperforms other generation methods regarding the feature-based similarity.
{{< /ci-details >}}
{{< ci-details summary="Cost-Sensitive Convolution based Neural Networks for Imbalanced Time-Series Classification (Yue Geng et al., 2018)">}}
Yue Geng, Xinyu Luo. (2018)  
**Cost-Sensitive Convolution based Neural Networks for Imbalanced Time-Series Classification**  
ArXiv  
[Paper Link](https://www.semanticscholar.org/paper/21e82350472bf6a12af0f761b8dea91cb16bf42f)  
Influential Citation Count (3), SS-ID (21e82350472bf6a12af0f761b8dea91cb16bf42f)  
**ABSTRACT**  
Some deep convolutional neural networks were proposed for time-series classification and class imbalanced problems. However, those models performed degraded and even failed to recognize the minority class of an imbalanced temporal sequences dataset. Minority samples would bring troubles for temporal deep learning classifiers due to the equal treatments of majority and minority class. Until recently, there were few works applying deep learning on imbalanced time-series classification (ITSC) tasks. Here, this paper aimed at tackling ITSC problems with deep learning. An adaptive cost-sensitive learning strategy was proposed to modify temporal deep learning models. Through the proposed strategy, classifiers could automatically assign misclassification penalties to each class. In the experimental section, the proposed method was utilized to modify five neural networks. They were evaluated on a large volume, real-life and imbalanced time-series dataset with six metrics. Each single network was also tested alone and combined with several mainstream data samplers. Experimental results illustrated that the proposed cost-sensitive modified networks worked well on ITSC tasks. Compared to other methods, the cost-sensitive convolution neural network and residual network won out in the terms of all metrics. Consequently, the proposed cost-sensitive learning strategy can be used to modify deep learning classifiers from cost-insensitive to cost-sensitive. Those cost-sensitive convolutional networks can be effectively applied to address ITSC issues.
{{< /ci-details >}}
{{< ci-details summary="SMOTE for Learning from Imbalanced Data: Progress and Challenges, Marking the 15-year Anniversary (Alberto Fernández et al., 2018)">}}
Alberto Fernández, S. García, F. Herrera, N. Chawla. (2018)  
**SMOTE for Learning from Imbalanced Data: Progress and Challenges, Marking the 15-year Anniversary**  
J. Artif. Intell. Res.  
[Paper Link](https://www.semanticscholar.org/paper/974df2ce665155cbdba5873378f3602ed1c4ce82)  
Influential Citation Count (30), SS-ID (974df2ce665155cbdba5873378f3602ed1c4ce82)  
**ABSTRACT**  
The Synthetic Minority Oversampling Technique (SMOTE) preprocessing algorithm is considered "de facto" standard in the framework of learning from imbalanced data. This is due to its simplicity in the design of the procedure, as well as its robustness when applied to different type of problems. Since its publication in 2002, SMOTE has proven successful in a variety of applications from several different domains. SMOTE has also inspired several approaches to counter the issue of class imbalance, and has also significantly contributed to new supervised learning paradigms, including multilabel classification, incremental learning, semi-supervised learning, multi-instance learning, among others. It is standard benchmark for learning from imbalanced data. It is also featured in a number of different software packages -- from open source to commercial. In this paper, marking the fifteen year anniversary of SMOTE, we reect on the SMOTE journey, discuss the current state of affairs with SMOTE, its applications, and also identify the next set of challenges to extend SMOTE for Big Data problems.
{{< /ci-details >}}
{{< ci-details summary="A Multi-Horizon Quantile Recurrent Forecaster (Ruofeng Wen et al., 2017)">}}
Ruofeng Wen, K. Torkkola, Balakrishnan Narayanaswamy, Dhruv Madeka. (2017)  
**A Multi-Horizon Quantile Recurrent Forecaster**  
  
[Paper Link](https://www.semanticscholar.org/paper/92aacdebbf5c768abb8169047ae7d2702894259e)  
Influential Citation Count (44), SS-ID (92aacdebbf5c768abb8169047ae7d2702894259e)  
**ABSTRACT**  
We propose a framework for general probabilistic multi-step time series regression. Specifically, we exploit the expressiveness and temporal nature of Recurrent Neural Networks, the nonparametric nature of Quantile Regression and the efficiency of Direct Multi-Horizon Forecasting. A new training scheme for recurrent nets is designed to boost stability and performance. We show that the approach accommodates both temporal and static covariates, learning across multiple related series, shifting seasonality, future planned event spikes and cold-starts in real life large-scale forecasting. The performance of the framework is demonstrated in an application to predict the future demand of items sold on Amazon.com, and in a public probabilistic forecasting competition to predict electricity price and load.
{{< /ci-details >}}
{{< ci-details summary="Learning to Compose Domain-Specific Transformations for Data Augmentation (Alexander J. Ratner et al., 2017)">}}
Alexander J. Ratner, Henry R. Ehrenberg, Zeshan Hussain, J. Dunnmon, C. Ré. (2017)  
**Learning to Compose Domain-Specific Transformations for Data Augmentation**  
NIPS  
[Paper Link](https://www.semanticscholar.org/paper/1ae265be8ccf396115ef06405f3b8421851998a9)  
Influential Citation Count (15), SS-ID (1ae265be8ccf396115ef06405f3b8421851998a9)  
**ABSTRACT**  
Data augmentation is a ubiquitous technique for increasing the size of labeled training sets by leveraging task-specific data transformations that preserve class labels. While it is often easy for domain experts to specify individual transformations, constructing and tuning the more sophisticated compositions typically needed to achieve state-of-the-art results is a time-consuming manual task in practice. We propose a method for automating this process by learning a generative sequence model over user-specified transformation functions using a generative adversarial approach. Our method can make use of arbitrary, non-deterministic transformation functions, is robust to misspecified user input, and is trained on unlabeled data. The learned transformation model can then be used to perform data augmentation for any end discriminative model. In our experiments, we show the efficacy of our approach on both image and text datasets, achieving improvements of 4.0 accuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation approaches.
{{< /ci-details >}}
{{< ci-details summary="Attention is All you Need (Ashish Vaswani et al., 2017)">}}
Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. (2017)  
**Attention is All you Need**  
NIPS  
[Paper Link](https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776)  
Influential Citation Count (9647), SS-ID (204e3073870fae3d05bcbc2f6a8e263d9b72e776)  
**ABSTRACT**  
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.
{{< /ci-details >}}
{{< ci-details summary="Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs (Cristóbal Esteban et al., 2017)">}}
Cristóbal Esteban, Stephanie L. Hyland, G. Rätsch. (2017)  
**Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs**  
ArXiv  
[Paper Link](https://www.semanticscholar.org/paper/d01f5f3d1971e7906d58dcc8baf9efb406c47fd2)  
Influential Citation Count (55), SS-ID (d01f5f3d1971e7906d58dcc8baf9efb406c47fd2)  
**ABSTRACT**  
Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data. In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data. RGANs make use of recurrent neural networks (RNNs) in the generator and the discriminator. In the case of RCGANs, both of these RNNs are conditioned on auxiliary information. We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series. We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa. We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data. This is demonstrated on digit classification from ‘serialised’ MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit. We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data, and demonstrate results from differentially private training of the RCGAN.
{{< /ci-details >}}
{{< ci-details summary="Data augmentation of wearable sensor data for parkinson’s disease monitoring using convolutional neural networks (T. T. Um et al., 2017)">}}
T. T. Um, F. Pfister, Daniel Pichler, S. Endo, M. Lang, S. Hirche, U. Fietzek, D. Kulić. (2017)  
**Data augmentation of wearable sensor data for parkinson’s disease monitoring using convolutional neural networks**  
ICMI  
[Paper Link](https://www.semanticscholar.org/paper/b41de530f4e44926eb4d54f61403011a6d8b3e41)  
Influential Citation Count (23), SS-ID (b41de530f4e44926eb4d54f61403011a6d8b3e41)  
**ABSTRACT**  
While convolutional neural networks (CNNs) have been successfully applied to many challenging classification applications, they typically require large datasets for training. When the availability of labeled data is limited, data augmentation is a critical preprocessing step for CNNs. However, data augmentation for wearable sensor data has not been deeply investigated yet. In this paper, various data augmentation methods for wearable sensor data are proposed. The proposed methods and CNNs are applied to the classification of the motor state of Parkinson’s Disease patients, which is challenging due to small dataset size, noisy labels, and large intra-class variability. Appropriate augmentation improves the classification performance from 77.54% to 86.88%.
{{< /ci-details >}}
{{< ci-details summary="Doubly Stochastic Variational Inference for Deep Gaussian Processes (Hugh Salimbeni et al., 2017)">}}
Hugh Salimbeni, M. Deisenroth. (2017)  
**Doubly Stochastic Variational Inference for Deep Gaussian Processes**  
NIPS  
[Paper Link](https://www.semanticscholar.org/paper/0cfa8e90741899c8d844a1ec8802bcd8d4901783)  
Influential Citation Count (87), SS-ID (0cfa8e90741899c8d844a1ec8802bcd8d4901783)  
**ABSTRACT**  
Gaussian processes (GPs) are a good choice for function approximation as they are flexible, robust to over-fitting, and provide well-calibrated predictive uncertainty. Deep Gaussian processes (DGPs) are multi-layer generalisations of GPs, but inference in these models has proved challenging. Existing approaches to inference in DGP models assume approximate posteriors that force independence between the layers, and do not work well in practice. We present a doubly stochastic variational inference algorithm, which does not force independence between layers. With our method of inference we demonstrate that a DGP model can be used effectively on data ranging in size from hundreds to a billion points. We provide strong empirical evidence that our inference scheme for DGPs works well in practice in both classification and regression.
{{< /ci-details >}}
{{< ci-details summary="Dataset Augmentation in Feature Space (Terrance Devries et al., 2017)">}}
Terrance Devries, Graham W. Taylor. (2017)  
**Dataset Augmentation in Feature Space**  
ICLR  
[Paper Link](https://www.semanticscholar.org/paper/82070bef06578a24e63b2b739ec86d4d31bb576a)  
Influential Citation Count (22), SS-ID (82070bef06578a24e63b2b739ec86d4d31bb576a)  
**ABSTRACT**  
Dataset augmentation, the practice of applying a wide array of domain-specific transformations to synthetically expand a training set, is a standard tool in supervised learning. While effective in tasks such as visual recognition, the set of transformations must be carefully designed, implemented, and tested for every new domain, limiting its re-use and generality. In this paper, we adopt a simpler, domain-agnostic approach to dataset augmentation. We start with existing data points and apply simple transformations such as adding noise, interpolating, or extrapolating between them. Our main insight is to perform the transformation not in input space, but in a learned feature space. A re-kindling of interest in unsupervised representation learning makes this technique timely and more effective. It is a simple proposal, but to-date one that has not been tested empirically. Working in the space of context vectors generated by sequence-to-sequence models, we demonstrate a technique that is effective for both static and sequential data.
{{< /ci-details >}}
{{< ci-details summary="Deep Learning for Time-Series Analysis (J. Gamboa, 2017)">}}
J. Gamboa. (2017)  
**Deep Learning for Time-Series Analysis**  
ArXiv  
[Paper Link](https://www.semanticscholar.org/paper/5f3a79e184d4e4ebaee7fb5cd9a0304dd26a43b6)  
Influential Citation Count (19), SS-ID (5f3a79e184d4e4ebaee7fb5cd9a0304dd26a43b6)  
**ABSTRACT**  
In many real-world application, e.g., speech recognition or sleep stage classification, data are captured over the course of time, constituting a Time-Series. Time-Series often contain temporal dependencies that cause two otherwise identical points of time to belong to different classes or predict different behavior. This characteristic generally increases the difficulty of analysing them. Existing techniques often depended on hand-crafted features that were expensive to create and required expert knowledge of the field. With the advent of Deep Learning new models of unsupervised learning of features for Time-series analysis and forecast have been developed. Such new developments are the topic of this paper: a review of the main Deep Learning techniques is presented, and some applications on Time-Series analysis are summaried. The results make it clear that Deep Learning has a lot to contribute to the field.
{{< /ci-details >}}
{{< ci-details summary="Time series classification from scratch with deep neural networks: A strong baseline (Zhiguang Wang et al., 2016)">}}
Zhiguang Wang, Weizhong Yan, T. Oates. (2016)  
**Time series classification from scratch with deep neural networks: A strong baseline**  
2017 International Joint Conference on Neural Networks (IJCNN)  
[Paper Link](https://www.semanticscholar.org/paper/ca0917853ea55c0ec4cded50914eb825fd45d00f)  
Influential Citation Count (166), SS-ID (ca0917853ea55c0ec4cded50914eb825fd45d00f)  
**ABSTRACT**  
We propose a simple but strong baseline for time series classification from scratch with deep neural networks. Our proposed baseline models are pure end-to-end without any heavy preprocessing on the raw data or feature crafting. The proposed Fully Convolutional Network (FCN) achieves premium performance to other state-of-the-art approaches and our exploration of the very deep neural networks with the ResNet structure is also competitive. The global average pooling in our convolutional model enables the exploitation of the Class Activation Map (CAM) to find out the contributing region in the raw data for the specific labels. Our models provides a simple choice for the real world application and a good starting point for the future research. An overall analysis is provided to discuss the generalization capability of our models, learned features, network structures and the classification semantics.
{{< /ci-details >}}
{{< ci-details summary="Philosophical Transactions of the Royal Society A : Mathematical (S. Barnett et al., 2017)">}}
S. Barnett, M. Babiker, M. Padgett. (2017)  
**Philosophical Transactions of the Royal Society A : Mathematical**  
  
[Paper Link](https://www.semanticscholar.org/paper/57d607b699af46a60ffee860d12cc8119b795ccc)  
Influential Citation Count (118), SS-ID (57d607b699af46a60ffee860d12cc8119b795ccc)  
{{< /ci-details >}}
{{< ci-details summary="Data Augmentation for Time Series Classification using Convolutional Neural Networks (Arthur Le Guennec et al., 2016)">}}
Arthur Le Guennec, S. Malinowski, R. Tavenard. (2016)  
**Data Augmentation for Time Series Classification using Convolutional Neural Networks**  
  
[Paper Link](https://www.semanticscholar.org/paper/e467404d68c8c2f45bae0e4bdfda12fc7df65cce)  
Influential Citation Count (16), SS-ID (e467404d68c8c2f45bae0e4bdfda12fc7df65cce)  
**ABSTRACT**  
Time series classification has been around for decades in the data-mining and machine learning communities. In this paper, we investigate the use of convolutional neural networks (CNN) for time series classification. Such networks have been widely used in many domains like computer vision and speech recognition, but only a little for time series classification. We design a convolu-tional neural network that consists of two convolutional layers. One drawback with CNN is that they need a lot of training data to be efficient. We propose two ways to circumvent this problem: designing data-augmentation techniques and learning the network in a semi-supervised way using training time series from different datasets. These techniques are experimentally evaluated on a benchmark of time series datasets.
{{< /ci-details >}}
{{< ci-details summary="WaveNet: A Generative Model for Raw Audio (Aäron van den Oord et al., 2016)">}}
Aäron van den Oord, S. Dieleman, H. Zen, K. Simonyan, Oriol Vinyals, A. Graves, Nal Kalchbrenner, A. Senior, K. Kavukcuoglu. (2016)  
**WaveNet: A Generative Model for Raw Audio**  
SSW  
[Paper Link](https://www.semanticscholar.org/paper/df0402517a7338ae28bc54acaac400de6b456a46)  
Influential Citation Count (834), SS-ID (df0402517a7338ae28bc54acaac400de6b456a46)  
**ABSTRACT**  
This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.
{{< /ci-details >}}
{{< ci-details summary="Bagging exponential smoothing methods using STL decomposition and Box–Cox transformation (C. Bergmeir et al., 2016)">}}
C. Bergmeir, R. Hyndman, C. C. J. M. B. C. (2016)  
**Bagging exponential smoothing methods using STL decomposition and Box–Cox transformation**  
  
[Paper Link](https://www.semanticscholar.org/paper/6732ebf7bf3dd512d5e64a6b645ed46404f13c29)  
Influential Citation Count (13), SS-ID (6732ebf7bf3dd512d5e64a6b645ed46404f13c29)  
{{< /ci-details >}}
{{< ci-details summary="Multi-Scale Convolutional Neural Networks for Time Series Classification (Zhicheng Cui et al., 2016)">}}
Zhicheng Cui, Wenlin Chen, Yixin Chen. (2016)  
**Multi-Scale Convolutional Neural Networks for Time Series Classification**  
ArXiv  
[Paper Link](https://www.semanticscholar.org/paper/9e8cce4d2d0bc575c6a24e65398b43bf56ac150a)  
Influential Citation Count (34), SS-ID (9e8cce4d2d0bc575c6a24e65398b43bf56ac150a)  
**ABSTRACT**  
Time series classification (TSC), the problem of predicting class labels of time series, has been around for decades within the community of data mining and machine learning, and found many important applications such as biomedical engineering and clinical prediction. However, it still remains challenging and falls short of classification accuracy and efficiency. Traditional approaches typically involve extracting discriminative features from the original time series using dynamic time warping (DTW) or shapelet transformation, based on which an off-the-shelf classifier can be applied. These methods are ad-hoc and separate the feature extraction part with the classification part, which limits their accuracy performance. Plus, most existing methods fail to take into account the fact that time series often have features at different time scales. To address these problems, we propose a novel end-to-end neural network model, Multi-Scale Convolutional Neural Networks (MCNN), which incorporates feature extraction and classification in a single framework. Leveraging a novel multi-branch layer and learnable convolutional layers, MCNN automatically extracts features at different scales and frequencies, leading to superior feature representation. MCNN is also computationally efficient, as it naturally leverages GPU computing. We conduct comprehensive empirical evaluation with various existing methods on a large number of benchmark datasets, and show that MCNN advances the state-of-the-art by achieving superior accuracy performance than other leading methods.
{{< /ci-details >}}
{{< ci-details summary="Data Augmentation for Deep Neural Network Acoustic Modeling (Xiaodong Cui et al., 2015)">}}
Xiaodong Cui, Vaibhava Goel, Brian Kingsbury. (2015)  
**Data Augmentation for Deep Neural Network Acoustic Modeling**  
IEEE/ACM Transactions on Audio, Speech, and Language Processing  
[Paper Link](https://www.semanticscholar.org/paper/c083dc15b5e169e02e208b576d6991d93955b4eb)  
Influential Citation Count (11), SS-ID (c083dc15b5e169e02e208b576d6991d93955b4eb)  
**ABSTRACT**  
This paper investigates data augmentation for deep neural network acoustic modeling based on label-preserving transformations to deal with data sparsity. Two data augmentation approaches, vocal tract length perturbation (VTLP) and stochastic feature mapping (SFM), are investigated for both deep neural networks (DNNs) and convolutional neural networks (CNNs). The approaches are focused on increasing speaker and speech variations of the limited training data such that the acoustic models trained with the augmented data are more robust to such variations. In addition, a two-stage data augmentation scheme based on a stacked architecture is proposed to combine VTLP and SFM as complementary approaches. Experiments are conducted on Assamese and Haitian Creole, two development languages of the IARPA Babel program, and improved performance on automatic speech recognition (ASR) and keyword search (KWS) is reported.
{{< /ci-details >}}
{{< ci-details summary="Generic and Scalable Framework for Automated Time-series Anomaly Detection (N. Laptev et al., 2015)">}}
N. Laptev, S. Amizadeh, I. Flint. (2015)  
**Generic and Scalable Framework for Automated Time-series Anomaly Detection**  
KDD  
[Paper Link](https://www.semanticscholar.org/paper/435c170edd031b65cf9087cf3c8473f071477df5)  
Influential Citation Count (32), SS-ID (435c170edd031b65cf9087cf3c8473f071477df5)  
**ABSTRACT**  
This paper introduces a generic and scalable framework for automated anomaly detection on large scale time-series data. Early detection of anomalies plays a key role in maintaining consistency of person's data and protects corporations against malicious attackers. Current state of the art anomaly detection approaches suffer from scalability, use-case restrictions, difficulty of use and a large number of false positives. Our system at Yahoo, EGADS, uses a collection of anomaly detection and forecasting models with an anomaly filtering layer for accurate and scalable anomaly detection on time-series. We compare our approach against other anomaly detection systems on real and synthetic data with varying time-series characteristics. We found that our framework allows for 50-60% improvement in precision and recall for a variety of use-cases. Both the data and the framework are being open-sourced. The open-sourcing of the data, in particular, represents the first of its kind effort to establish the standard benchmark for anomaly detection.
{{< /ci-details >}}
{{< ci-details summary="A Parsimonious Mixture of Gaussian Trees Model for Oversampling in Imbalanced and Multimodal Time-Series Classification (Hong Cao et al., 2014)">}}
Hong Cao, V. Tan, John Z. F. Pang. (2014)  
**A Parsimonious Mixture of Gaussian Trees Model for Oversampling in Imbalanced and Multimodal Time-Series Classification**  
IEEE Transactions on Neural Networks and Learning Systems  
[Paper Link](https://www.semanticscholar.org/paper/7e85296d0bb4f8f5f4b9de48201ff4799e4f2112)  
Influential Citation Count (0), SS-ID (7e85296d0bb4f8f5f4b9de48201ff4799e4f2112)  
**ABSTRACT**  
We propose a novel framework of using a parsimonious statistical model, known as mixture of Gaussian trees, for modeling the possibly multimodal minority class to solve the problem of imbalanced time-series classification. By exploiting the fact that close-by time points are highly correlated due to smoothness of the time-series, our model significantly reduces the number of covariance parameters to be estimated from O(d2) to O(Ld), where L is the number of mixture components and d is the dimensionality. Thus, our model is particularly effective for modeling high-dimensional time-series with limited number of instances in the minority positive class. In addition, the computational complexity for learning the model is only of the order O(Ln+d2) where n+ is the number of positively labeled samples. We conduct extensive classification experiments based on several well-known time-series data sets (both singleand multimodal) by first randomly generating synthetic instances from our learned mixture model to correct the imbalance. We then compare our results with several state-of-the-art oversampling techniques and the results demonstrate that when our proposed model is used in oversampling, the same support vector machines classifier achieves much better classification accuracy across the range of data sets. In fact, the proposed method achieves the best average performance 30 times out of 36 multimodal data sets according to the F-value metric. Our results are also highly competitive compared with nonoversampling-based classifiers for dealing with imbalanced time-series data sets.
{{< /ci-details >}}
{{< ci-details summary="Volume 4 (Servicio Geológico Colombiano Sgc, 2013)">}}
Servicio Geológico Colombiano Sgc. (2013)  
**Volume 4**  
Journal of Diabetes Investigation  
[Paper Link](https://www.semanticscholar.org/paper/70850393a5efea8b407f2ba5cdf4eabe88827143)  
Influential Citation Count (26), SS-ID (70850393a5efea8b407f2ba5cdf4eabe88827143)  
**ABSTRACT**  
ABCC8 clinical and functional characterization of the Pro1198Leu ABCC8 gene mutation associated with permanent neonatal diabetes mellitus, Takagi 269–273 adipocyte fatty acid binding protein obesity as the common soil of non-alcoholic fatty liver disease and diabetes: role of adipokines (Review Article), Hui 413–425 adipokines glypican-4 is a new comer of adipokines working as insulin sensitizer, Tamori 250–251 adiponectin obesity as the common soil of non-alcoholic fatty liver disease and diabetes: role of adipokines (Review Article), Hui 413–425 serum adiponectin levels predict the risk of coronary heart disease in Japanese patients with type 2 diabetes, Obata 475–482 adverse drug reaction three ileus cases associated with the use of DPP-4 inhibitors in diabetic patients (Case Report), Kanasaki 676 albuminuria comparison of effects of cilnidipine and azelnidipine on blood pressure, heart rate and albuminuria in type 2 diabetics with hypertension: a pilot study, Abe 202–205 comparison of spironolactone and trichlormethiazide as add-on therapy to renin–angiotensin blockade for reduction of albuminuria in diabetic patients, Hase 316–319 aldosterone blockers comparison of spironolactone and trichlormethiazide as add-on therapy to renin–angiotensin blockade for reduction of albuminuria in diabetic patients, Hase 316–319 Alzheimer’s disease insulin resistance in the brain: a new therapeutic target for Alzheimer’s disease (Commentary), Umegaki 150–151 new therapeutic strategy for Alzheimer’s disease using antidiabetes agents (Commentary), Yokono 152–153 angiotensin-converting enzyme gene angiotensin-converting enzyme gene variants interact with the renin–angiotensin system pathway to confer risk and protection against type 2 diabetic retinopathy (Letter to the Editor), Singh Cheema 103–104 angiotensin-converting enzyme inhibitor combination therapy with an angiotensin-convertingenzyme inhibitor and an angiotensin II receptor antagonist ameliorates microinflammation and oxidative stress in patients with diabetic nephropathy, Nakamura 195–201 angiotensin receptor blocker combination therapy with an angiotensin-convertingenzyme inhibitor and an angiotensin II receptor antagonist ameliorates microinflammation and oxidative stress in patients with diabetic nephropathy, Nakamura 195–201 anthropometric indicators anthropometric indicators of obesity for identifying cardiometabolic risk factors in a rural Bangladeshi population, Bhowmik 361–368 anti-cancer ascorbic acid therapy pseudohypoglycemia or hyperglycemia caused by interference with self-monitoring blood glucose measurements in anti-cancer ascorbic acid therapy (Letter to the Editor), Kimura 682 antidiabetic agents new therapeutic strategy for Alzheimer’s disease using antidiabetes agents (Commentary), Yokono 152–153 antioxidant mechanism-based antioxidant therapies promise to prevent diabetic complications (Editorial), Nishikawa 105–107 arterial stiffness increased arterial stiffness is closely associated with hyperglycemia and improved by glycemic control in diabetic patients, Ibata 82–87 Asian changing characteristics of the type 2 diabetes epidemic in China and other Asian countries, Yang 223–224 efficacy and safety of exenatide once-weekly vs exenatide twice-daily in Asian patients with type 2 diabetes mellitus, Ji 53–61 insulin degludec compared with insulin glargine in insulin-na€ıve patients with type 2 diabetes: a 26-week, randomized, controlled, Pan-Asian, treat-to-target trial, Onishi 605 atherosclerosis extension of the mitochondria dysfunction hypothesis of metabolic syndrome to atherosclerosis with emphasis on the endocrine-disrupting chemicals and biophysical laws (Review Article), Lee 19–33 atorvastatin comparison of effects of pitavastatin and atorvastatin on glucose metabolism in type 2 diabetic patients with hypercholesterolemia, Mita 297–303 effects of patient-tailored atorvastatin therapy on ameliorating the levels of atherogenic lipids and inflammation beyond lowering low-density lipoprotein cholesterol in patients with type 2 diabetes, Son 466–474 azelnidipine comparison of effects of cilnidipine and azelnidipine on blood pressure, heart rate and albuminuria in type 2 diabetics with hypertension: a pilot study, Abe 202–205
{{< /ci-details >}}
{{< ci-details summary="Gaussian processes for time-series modelling (S. Roberts et al., 2013)">}}
S. Roberts, M. Osborne, M. Ebden, S. Reece, N. Gibson, S. Aigrain. (2013)  
**Gaussian processes for time-series modelling**  
Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences  
[Paper Link](https://www.semanticscholar.org/paper/64e615720384525dfa317605e154ee259ba1013d)  
Influential Citation Count (14), SS-ID (64e615720384525dfa317605e154ee259ba1013d)  
**ABSTRACT**  
In this paper, we offer a gentle introduction to Gaussian processes for time-series data analysis. The conceptual framework of Bayesian modelling for time-series data is discussed and the foundations of Bayesian non-parametric modelling presented for Gaussian processes. We discuss how domain knowledge influences design of the Gaussian process models and provide case examples to highlight the approaches.
{{< /ci-details >}}
{{< ci-details summary="Deep Gaussian Processes (A. Damianou et al., 2012)">}}
A. Damianou, Neil D. Lawrence. (2012)  
**Deep Gaussian Processes**  
AISTATS  
[Paper Link](https://www.semanticscholar.org/paper/b5f5ed292ce93404a0985754b2b531eca4e6cf50)  
Influential Citation Count (140), SS-ID (b5f5ed292ce93404a0985754b2b531eca4e6cf50)  
**ABSTRACT**  
In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.
{{< /ci-details >}}
{{< ci-details summary="ImageNet classification with deep convolutional neural networks (A. Krizhevsky et al., 2012)">}}
A. Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton. (2012)  
**ImageNet classification with deep convolutional neural networks**  
Commun. ACM  
[Paper Link](https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff)  
Influential Citation Count (11562), SS-ID (abd1c342495432171beb7ca8fd9551ef13cbd0ff)  
**ABSTRACT**  
We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.
{{< /ci-details >}}
{{< ci-details summary="Et al (P. Cochat et al., 2008)">}}
P. Cochat, L. Vaucoret, J. Sarles. (2008)  
**Et al**  
Archives de pediatrie : organe officiel de la Societe francaise de pediatrie  
[Paper Link](https://www.semanticscholar.org/paper/bc6dff14a130c57a91d5a21339c23471faf1d46f)  
Influential Citation Count (6708), SS-ID (bc6dff14a130c57a91d5a21339c23471faf1d46f)  
**ABSTRACT**  
disasters. Plenum, 2001. 11. Haley R, Thomas L, Hom J. Is there a Gulf War Syndrome? Searching for syndromes by factor analysis of symptoms. JAMA 1997;277:215–22. 12. Fukuda K, Nisenbaum R, Stewart G, et al. Chronic multi-symptom illness affecting Air Force veterans of the Gulf War. JAMA 1998;280:981–8. 13. Ismail K, Everitt B, Blatchley N, et al. Is there a Gulf War Syndrome? Lancet 1999;353:179–82. 14. Shapiro S, Lasarev M, McCauley L. Factor analysis of Gulf War illness: what does it add to our understanding of possible health effects of deployment. Am J Epidemiol 2002;156:578–85. 15. Doebbeling B, Clarke W, Watson D, et al. Is there a Persian Gulf War Syndrome? Evidence from a large population-based survey of veterans and nondeployed controls. Am J Med 2000;108:695–704. 16. Knoke J, Smith T, Gray G, et al. Factor analysis of self reported symptoms: Does it identify a Gulf War Syndrome? Am J Epidemiol 2000;152:379–88. 17. Kang H, Mahan C, Lee K, et al. Evidence for a deployment-related Gulf War syndrome by factor analysis. Arch Environ Health 2002;57:61–8.
{{< /ci-details >}}
{{< ci-details summary="Gaussian Processes for Machine Learning (C. Rasmussen et al., 2003)">}}
C. Rasmussen, Christopher K. I. Williams. (2003)  
**Gaussian Processes for Machine Learning**  
Adaptive computation and machine learning  
[Paper Link](https://www.semanticscholar.org/paper/82266f6103bade9005ec555ed06ba20b5210ff22)  
Influential Citation Count (3209), SS-ID (82266f6103bade9005ec555ed06ba20b5210ff22)  
{{< /ci-details >}}
{{< ci-details summary="Advanced Engineering Informatics (J. Kunz et al., 2007)">}}
J. Kunz, Ian F. C. Smith, T. Tomiyama. (2007)  
**Advanced Engineering Informatics**  
Adv. Eng. Informatics  
[Paper Link](https://www.semanticscholar.org/paper/5d9b31fe2623f72cfef0f5797a897288af4edbac)  
Influential Citation Count (0), SS-ID (5d9b31fe2623f72cfef0f5797a897288af4edbac)  
{{< /ci-details >}}
{{< ci-details summary="Constrained surrogate time series with preservation of the mean and variance structure. (C. Keylock, 2006)">}}
C. Keylock. (2006)  
**Constrained surrogate time series with preservation of the mean and variance structure.**  
Physical review. E, Statistical, nonlinear, and soft matter physics  
[Paper Link](https://www.semanticscholar.org/paper/e8f5a5f77b324415e72b66d45388a50b5def908d)  
Influential Citation Count (5), SS-ID (e8f5a5f77b324415e72b66d45388a50b5def908d)  
**ABSTRACT**  
A method is presented for generating surrogates that are constrained realizations of a time series but which preserve the local mean and variance of the original signal. The method is based on the popular iterated amplitude adjusted Fourier transform method but makes use of a wavelet transform to constrain behavior in the time domain. Using this method it is possible to test for local changes in the nonlinear properties of the signal. We present an example for a change in Hurst exponent in a time series produced by fractional Brownian motion.
{{< /ci-details >}}
{{< ci-details summary="Surrogate time series (T. Schreiber et al., 1999)">}}
T. Schreiber, Andreas Schmitz. (1999)  
**Surrogate time series**  
  
[Paper Link](https://www.semanticscholar.org/paper/32791891ffb166cbad7e33590cd508d15a7db29b)  
Influential Citation Count (126), SS-ID (32791891ffb166cbad7e33590cd508d15a7db29b)  
{{< /ci-details >}}
{{< ci-details summary="STL: A seasonal-trend decomposition procedure based on loess (with discussion) (Rb Cleveland et al., 1990)">}}
Rb Cleveland, W. Cleveland, J. E. McRae, Irma J. Terpenning. (1990)  
**STL: A seasonal-trend decomposition procedure based on loess (with discussion)**  
  
[Paper Link](https://www.semanticscholar.org/paper/585bf445ec84c1d9621b2726bdcce9f544b515c8)  
Influential Citation Count (170), SS-ID (585bf445ec84c1d9621b2726bdcce9f544b515c8)  
{{< /ci-details >}}
{{< ci-details summary="Sensors. (H. S. Wolff, 1970)">}}
H. S. Wolff. (1970)  
**Sensors.**  
  
[Paper Link](https://www.semanticscholar.org/paper/b8d9b10cf54629364523ec065e6307ab87f7d4f0)  
Influential Citation Count (44), SS-ID (b8d9b10cf54629364523ec065e6307ab87f7d4f0)  
**ABSTRACT**  
Sensors are used for process monitoring and for process control. These are essential elements of safe and profitable plant operation that can be achieved only if the proper sensors are selected and installed in the correct locations. While sensors differ greatly in their physical principles, their selection can be guided by the analysis of a small set of issues, which are presented in this section. Each issue is introduced here with process examples, and details on the issues are provided in the remainder of this site for the most common sensors in the process industries.
{{< /ci-details >}}
