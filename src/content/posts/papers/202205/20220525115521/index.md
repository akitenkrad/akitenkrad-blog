---
draft: false
title: "Improving Language Understanding by Generative Pre-Training"
date: 2022-05-25
author: "akitenkrad"
description: ""
tags: ["At:Round-1", "Published:2006"]
menu:
  sidebar:
    name: "Improving Language Understanding by Generative Pre-Training"
    identifier: 20220525
    parent: 202205
    weight: 10
math: true
---

- [x] Round-1: Overview
- [ ] Round-2: Model Implementation Details
- [ ] Round-3: Experiments

## Citation

## Abstract

## Background & Wat's New

## Dataset

## Model Description

### Training Settings

## Results

## References


{{< ci-details summary="Semi-Supervised Text Classification Using EM (O. Chapelle et al., 2006)">}}

O. Chapelle, Bernhard SchÃ¶lkopf, A. Zien. (2006)  
**Semi-Supervised Text Classification Using EM**  
  
[Paper Link](https://www.semanticscholar.org/paper/03bafef700d35112a9926dd1b2be91a4aa6984a4)  
Influential Citation Count (2), SS-ID (03bafef700d35112a9926dd1b2be91a4aa6984a4)  

**ABSTRACT**  
This chapter contains sections titled: Introduction, A Generative Model for Text, Experimental Results with Basic EM, Using a More Expressive Generative Model, Overcoming the Challenges of Local Maxima, Conclusions and Summary

{{< /ci-details >}}

{{< ci-details summary="SQuAD: 100,000+ Questions for Machine Comprehension of Text (Pranav Rajpurkar et al., 2016)">}}

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang. (2016)  
**SQuAD: 100,000+ Questions for Machine Comprehension of Text**  
EMNLP  
[Paper Link](https://www.semanticscholar.org/paper/05dd7254b632376973f3a1b4d39485da17814df5)  
Influential Citation Count (1099), SS-ID (05dd7254b632376973f3a1b4d39485da17814df5)  

**ABSTRACT**  
We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.  The dataset is freely available at this https URL

{{< /ci-details >}}

{{< ci-details summary="Semi-supervised sequence tagging with bidirectional language models (Matthew E. Peters et al., 2017)">}}

Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power. (2017)  
**Semi-supervised sequence tagging with bidirectional language models**  
ACL  
[Paper Link](https://www.semanticscholar.org/paper/0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38)  
Influential Citation Count (49), SS-ID (0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38)  

**ABSTRACT**  
Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre- trained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.

{{< /ci-details >}}

{{< ci-details summary="Discriminative Improvements to Distributional Sentence Similarity (Yangfeng Ji et al., 2013)">}}

Yangfeng Ji, Jacob Eisenstein. (2013)  
**Discriminative Improvements to Distributional Sentence Similarity**  
EMNLP  
[Paper Link](https://www.semanticscholar.org/paper/0e5fa90e28fab414c8ef3ac6ca937c6195c2860e)  
Influential Citation Count (17), SS-ID (0e5fa90e28fab414c8ef3ac6ca937c6195c2860e)  

**ABSTRACT**  
Matrix and tensor factorization have been applied to a number of semantic relatedness tasks, including paraphrase identification. The key idea is that similarity in the latent space implies semantic relatedness. We describe three ways in which labeled data can improve the accuracy of these approaches on paraphrase classification. First, we design a new discriminative term-weighting metric called TF-KLD, which outperforms TF-IDF. Next, we show that using the latent representation from matrix factorization as features in a classification algorithm substantially improves accuracy. Finally, we combine latent features with fine-grained n-gram overlap features, yielding performance that is 3% more accurate than the prior state-of-the-art.

{{< /ci-details >}}

{{< ci-details summary="Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books (Yukun Zhu et al., 2015)">}}

Yukun Zhu, Ryan Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, S. Fidler. (2015)  
**Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books**  
2015 IEEE International Conference on Computer Vision (ICCV)  
[Paper Link](https://www.semanticscholar.org/paper/0e6824e137847be0599bb0032e37042ed2ef5045)  
Influential Citation Count (172), SS-ID (0e6824e137847be0599bb0032e37042ed2ef5045)  

**ABSTRACT**  
Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.

{{< /ci-details >}}

{{< ci-details summary="Learning Entity Representation for Entity Disambiguation (Z. He et al., 2013)">}}

Z. He, Shujie Liu, Mu Li, Ming Zhou, Longkai Zhang, Houfeng Wang. (2013)  
**Learning Entity Representation for Entity Disambiguation**  
ACL  
[Paper Link](https://www.semanticscholar.org/paper/10ce58375314f90f83ca3bb88840cbc67bf8050f)  
Influential Citation Count (16), SS-ID (10ce58375314f90f83ca3bb88840cbc67bf8050f)  

**ABSTRACT**  


{{< /ci-details >}}

{{< ci-details summary="Neural Machine Translation of Rare Words with Subword Units (Rico Sennrich et al., 2015)">}}

Rico Sennrich, B. Haddow, Alexandra Birch. (2015)  
**Neural Machine Translation of Rare Words with Subword Units**  
ACL  
[Paper Link](https://www.semanticscholar.org/paper/1af68821518f03568f913ab03fc02080247a27ff)  
Influential Citation Count (840), SS-ID (1af68821518f03568f913ab03fc02080247a27ff)  

**ABSTRACT**  
Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.

{{< /ci-details >}}

{{< ci-details summary="Universal Language Model Fine-tuning for Text Classification (Jeremy Howard et al., 2018)">}}

Jeremy Howard, Sebastian Ruder. (2018)  
**Universal Language Model Fine-tuning for Text Classification**  
ACL  
[Paper Link](https://www.semanticscholar.org/paper/1e077413b25c4d34945cc2707e17e46ed4fe784a)  
Influential Citation Count (224), SS-ID (1e077413b25c4d34945cc2707e17e46ed4fe784a)  

**ABSTRACT**  
Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.

{{< /ci-details >}}

{{< ci-details summary="Convolutional Neural Networks for Sentence Classification (Yoon Kim, 2014)">}}

Yoon Kim. (2014)  
**Convolutional Neural Networks for Sentence Classification**  
EMNLP  
[Paper Link](https://www.semanticscholar.org/paper/1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba)  
Influential Citation Count (1603), SS-ID (1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba)  

**ABSTRACT**  
We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.

{{< /ci-details >}}

{{< ci-details summary="Attention is All you Need (Ashish Vaswani et al., 2017)">}}

Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. (2017)  
**Attention is All you Need**  
NIPS  
[Paper Link](https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776)  
Influential Citation Count (7881), SS-ID (204e3073870fae3d05bcbc2f6a8e263d9b72e776)  

**ABSTRACT**  
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.

{{< /ci-details >}}

{{< ci-details summary="Reasoning about Entailment with Neural Attention (Tim Rocktäschel et al., 2015)">}}

Tim Rocktäschel, Edward Grefenstette, K. Hermann, Tomás Kociský, P. Blunsom. (2015)  
**Reasoning about Entailment with Neural Attention**  
ICLR  
[Paper Link](https://www.semanticscholar.org/paper/2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45)  
Influential Citation Count (86), SS-ID (2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45)  

**ABSTRACT**  
While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.

{{< /ci-details >}}

{{< ci-details summary="Story Comprehension for Predicting What Happens Next (Snigdha Chaturvedi et al., 2017)">}}

Snigdha Chaturvedi, Haoruo Peng, D. Roth. (2017)  
**Story Comprehension for Predicting What Happens Next**  
EMNLP  
[Paper Link](https://www.semanticscholar.org/paper/2cdc28b4f34410ff70099ae845daaaa25813f0e9)  
Influential Citation Count (9), SS-ID (2cdc28b4f34410ff70099ae845daaaa25813f0e9)  

**ABSTRACT**  
Automatic story comprehension is a fundamental challenge in Natural Language Understanding, and can enable computers to learn about social norms, human behavior and commonsense. In this paper, we present a story comprehension model that explores three distinct semantic aspects: (i) the sequence of events described in the story, (ii) its emotional trajectory, and (iii) its plot consistency. We judge the model’s understanding of real-world stories by inquiring if, like humans, it can develop an expectation of what will happen next in a given story. Specifically, we use it to predict the correct ending of a given short story from possible alternatives. The model uses a hidden variable to weigh the semantic aspects in the context of the story. Our experiments demonstrate the potential of our approach to characterize these semantic aspects, and the strength of the hidden variable based approach. The model outperforms the state-of-the-art approaches and achieves best results on a publicly available dataset.

{{< /ci-details >}}

{{< ci-details summary="Semi-Supervised Learning for Natural Language (P. Liang, 2005)">}}

P. Liang. (2005)  
**Semi-Supervised Learning for Natural Language**  
  
[Paper Link](https://www.semanticscholar.org/paper/31b4c03d721dc10b87c178277c1d369f91db8f0e)  
Influential Citation Count (33), SS-ID (31b4c03d721dc10b87c178277c1d369f91db8f0e)  

**ABSTRACT**  
Statistical supervised learning techniques have been successful for many natural language processing tasks, but they require labeled datasets, which can be expensive to obtain. On the other hand, unlabeled data (raw text) is often available "for free" in large quantities. Unlabeled data has shown promise in improving the performance of a number of tasks, e.g. word sense disambiguation, information extraction, and natural language parsing. In this thesis, we focus on two segmentation tasks, named-entity recognition and Chinese word segmentation. The goal of named-entity recognition is to detect and classify names of people, organizations, and locations in a sentence. The goal of Chinese word segmentation is to find the word boundaries in a sentence that has been written as a string of characters without spaces. Our approach is as follows: In a preprocessing step, we use raw text to cluster words and calculate mutual information statistics. The output of this step is then used as features in a supervised model, specifically a global linear model trained using the Perceptron algorithm. We also compare Markov and semi-Markov models on the two segmentation tasks. Our results show that features derived from unlabeled data substantially improves performance, both in terms of reducing the amount of labeled data needed to achieve a certain performance level and in terms of reducing the error using a fixed amount of labeled data. We find that sometimes semi-Markov models can also improve performance over Markov models. Thesis Supervisor: Michael Collins Title: Assistant Professor, CSAIL

{{< /ci-details >}}

{{< ci-details summary="A Stochastic Approximation Method (H. Robbins, 1951)">}}

H. Robbins. (1951)  
**A Stochastic Approximation Method**  
  
[Paper Link](https://www.semanticscholar.org/paper/34ddd8865569c2c32dec9bf7ffc817ff42faaa01)  
Influential Citation Count (739), SS-ID (34ddd8865569c2c32dec9bf7ffc817ff42faaa01)  

**ABSTRACT**  
Let M(x) denote the expected value at level x of the response to a certain experiment. M(x) is assumed to be a monotone function of x but is unknown tot he experiment, and it is desire to find the solution x=0 of the equation M(x) = a, where x is a given constant. we give a method for making successive experiments at levels x1, x2,... in such a way that x, will tend to 0 in probability.

{{< /ci-details >}}

{{< ci-details summary="A Simple but Tough-to-Beat Baseline for Sentence Embeddings (Sanjeev Arora et al., 2017)">}}

Sanjeev Arora, Yingyu Liang, Tengyu Ma. (2017)  
**A Simple but Tough-to-Beat Baseline for Sentence Embeddings**  
ICLR  
[Paper Link](https://www.semanticscholar.org/paper/3f1802d3f4f5f6d66875dac09112f978f12e1e1e)  
Influential Citation Count (156), SS-ID (3f1802d3f4f5f6d66875dac09112f978f12e1e1e)  

**ABSTRACT**  


{{< /ci-details >}}

{{< ci-details summary="Deep Contextualized Word Representations (Matthew E. Peters et al., 2018)">}}

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer. (2018)  
**Deep Contextualized Word Representations**  
NAACL  
[Paper Link](https://www.semanticscholar.org/paper/3febb2bed8865945e7fddc99efd791887bb7e14f)  
Influential Citation Count (1362), SS-ID (3febb2bed8865945e7fddc99efd791887bb7e14f)  

**ABSTRACT**  
We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.

{{< /ci-details >}}

{{< ci-details summary="Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units (Dan Hendrycks et al., 2016)">}}

Dan Hendrycks, Kevin Gimpel. (2016)  
**Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units**  
ArXiv  
[Paper Link](https://www.semanticscholar.org/paper/4361e64f2d12d63476fdc88faf72a0f70d9a2ffb)  
Influential Citation Count (45), SS-ID (4361e64f2d12d63476fdc88faf72a0f70d9a2ffb)  

**ABSTRACT**  
We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map, combining the intuitions of dropout and zoneout while respecting neuron values. This connection suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.

{{< /ci-details >}}

{{< ci-details summary="Greedy Layer-Wise Training of Deep Networks (B. Schölkopf et al., 2007)">}}

B. Schölkopf, J. Platt, T. Hofmann. (2007)  
**Greedy Layer-Wise Training of Deep Networks**  
  
[Paper Link](https://www.semanticscholar.org/paper/43c8a545f7166659e9e21c88fe234e0323855216)  
Influential Citation Count (134), SS-ID (43c8a545f7166659e9e21c88fe234e0323855216)  

**ABSTRACT**  
Complexity theory of circuits strongly suggests that deep architectures can be much more ef cient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.

{{< /ci-details >}}

{{< ci-details summary="Fixing Weight Decay Regularization in Adam (I. Loshchilov et al., 2017)">}}

I. Loshchilov, F. Hutter. (2017)  
**Fixing Weight Decay Regularization in Adam**  
ArXiv  
[Paper Link](https://www.semanticscholar.org/paper/45dfef0cc1ed96558c1c650432ce39d6a1050b6a)  
Influential Citation Count (97), SS-ID (45dfef0cc1ed96558c1c650432ce39d6a1050b6a)  

**ABSTRACT**  
We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. We propose a simple way to resolve this issue by decoupling weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. Our source code will become available after the review process.

{{< /ci-details >}}

{{< ci-details summary="Automatically Constructing a Corpus of Sentential Paraphrases (W. Dolan et al., 2005)">}}

W. Dolan, Chris Brockett. (2005)  
**Automatically Constructing a Corpus of Sentential Paraphrases**  
IJCNLP  
[Paper Link](https://www.semanticscholar.org/paper/475354f10798f110d34792b6d88f31d6d5cb099e)  
Influential Citation Count (143), SS-ID (475354f10798f110d34792b6d88f31d6d5cb099e)  

**ABSTRACT**  
An obstacle to research in automatic paraphrase identification and generation is the lack of large-scale, publiclyavailable labeled corpora of sentential paraphrases. This paper describes the creation of the recently-released Microsoft Research Paraphrase Corpus, which contains 5801 sentence pairs, each hand-labeled with a binary judgment as to whether the pair constitutes a paraphrase. The corpus was created using heuristic extraction techniques in conjunction with an SVM-based classifier to select likely sentence-level paraphrases from a large corpus of topicclustered news data. These pairs were then submitted to human judges, who confirmed that 67% were in fact semantically equivalent. In addition to describing the corpus itself, we explore a number of issues that arose in defining guidelines for the human raters.

{{< /ci-details >}}

{{< ci-details summary="Semi-supervised Sequence Learning (Andrew M. Dai et al., 2015)">}}

Andrew M. Dai, Quoc V. Le. (2015)  
**Semi-supervised Sequence Learning**  
NIPS  
[Paper Link](https://www.semanticscholar.org/paper/4aa9f5150b46320f534de4747a2dd0cd7f3fe292)  
Influential Citation Count (54), SS-ID (4aa9f5150b46320f534de4747a2dd0cd7f3fe292)  

**ABSTRACT**  
We present two approaches to use unlabeled data to improve Sequence Learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a language model in NLP. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" algorithm for a later supervised sequence learning algorithm. In other words, the parameters obtained from the pretraining step can then be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after pretrained with the two approaches become more stable to train and generalize better. With pretraining, we were able to achieve strong performance in many classification tasks, such as text classification with IMDB, DBpedia or image recognition in CIFAR-10.

{{< /ci-details >}}

{{< ci-details summary="Towards Human-level Machine Reading Comprehension: Reasoning and Inference with Multiple Strategies (Yichong Xu et al., 2017)">}}

Yichong Xu, Jingjing Liu, Jianfeng Gao, Yelong Shen, Xiaodong Liu. (2017)  
**Towards Human-level Machine Reading Comprehension: Reasoning and Inference with Multiple Strategies**  
ArXiv  
[Paper Link](https://www.semanticscholar.org/paper/4e013e6c800666c5bc611ca820ae437a7139cbb6)  
Influential Citation Count (2), SS-ID (4e013e6c800666c5bc611ca820ae437a7139cbb6)  

**ABSTRACT**  
This paper presents a new MRC model that is capable of three key comprehension skills: 1) handling rich variations in question types; 2) understanding potential answer choices; and 3) drawing inference through multiple sentences. The model is based on the proposed MUlti-Strategy Inference for Comprehension (MUSIC) architecture, which is able to dynamically apply different attention strategies to different types of questions on the fly. By incorporating a multi-step inference engine analogous to ReasoNet (Shen et al., 2017), MUSIC can also effectively perform multi-sentence inference in generating answers. Evaluation on the RACE dataset shows that the proposed method significantly outperforms previous state-of-the-art models by 7.5% in relative accuracy.

{{< /ci-details >}}

{{< ci-details summary="A unified architecture for natural language processing: deep neural networks with multitask learning (Ronan Collobert et al., 2008)">}}

Ronan Collobert, J. Weston. (2008)  
**A unified architecture for natural language processing: deep neural networks with multitask learning**  
ICML '08  
[Paper Link](https://www.semanticscholar.org/paper/57458bc1cffe5caa45a885af986d70f723f406b4)  
Influential Citation Count (259), SS-ID (57458bc1cffe5caa45a885af986d70f723f406b4)  

**ABSTRACT**  
We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.

{{< /ci-details >}}

{{< ci-details summary="UNSUPERVISED MACHINE TRANSLATION USING MONOLINGUAL CORPORA ONLY (Piotr, 2017)">}}

Piotr. (2017)  
**UNSUPERVISED MACHINE TRANSLATION USING MONOLINGUAL CORPORA ONLY**  
  
[Paper Link](https://www.semanticscholar.org/paper/57ea3bc5ad8d67909b086a92a801a5e5f2d17035)  
Influential Citation Count (9), SS-ID (57ea3bc5ad8d67909b086a92a801a5e5f2d17035)  

**ABSTRACT**  
Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.

{{< /ci-details >}}

{{< ci-details summary="A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference (Adina Williams et al., 2017)">}}

Adina Williams, Nikita Nangia, Samuel R. Bowman. (2017)  
**A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference**  
NAACL  
[Paper Link](https://www.semanticscholar.org/paper/5ded2b8c64491b4a67f6d39ce473d4b9347a672e)  
Influential Citation Count (463), SS-ID (5ded2b8c64491b4a67f6d39ce473d4b9347a672e)  

**ABSTRACT**  
This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.

{{< /ci-details >}}

{{< ci-details summary="Stochastic Answer Networks for Natural Language Inference (Xiaodong Liu et al., 2018)">}}

Xiaodong Liu, Kevin Duh, Jianfeng Gao. (2018)  
**Stochastic Answer Networks for Natural Language Inference**  
ArXiv  
[Paper Link](https://www.semanticscholar.org/paper/6084b58d8b4b0caf3a2a7f3a1bee1cc527927e39)  
Influential Citation Count (3), SS-ID (6084b58d8b4b0caf3a2a7f3a1bee1cc527927e39)  

**ABSTRACT**  
We propose a stochastic answer network (SAN) to explore multi-step inference strategies in Natural Language Inference. Rather than directly predicting the results given the inputs, the model maintains a state and iteratively refines its predictions. Our experiments show that SAN achieves the state-of-the-art results on three benchmarks: Stanford Natural Language Inference (SNLI) dataset, MultiGenre Natural Language Inference (MultiNLI) dataset and Quora Question Pairs dataset.

{{< /ci-details >}}

{{< ci-details summary="Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction (Richard Zhang et al., 2016)">}}

Richard Zhang, Phillip Isola, Alexei A. Efros. (2016)  
**Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction**  
2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  
[Paper Link](https://www.semanticscholar.org/paper/62f3d3015cee122bd147d7d878c85f70cc15680d)  
Influential Citation Count (37), SS-ID (62f3d3015cee122bd147d7d878c85f70cc15680d)  

**ABSTRACT**  
We propose split-brain autoencoders, a straightforward modification of the traditional autoencoder architecture, for unsupervised representation learning. The method adds a split to the network, resulting in two disjoint sub-networks. Each sub-network is trained to perform a difficult task &#x2013; predicting one subset of the data channels from another. Together, the sub-networks extract features from the entire input signal. By forcing the network to solve cross-channel prediction tasks, we induce a representation within the network which transfers well to other, unseen tasks. This method achieves state-of-the-art performance on several large-scale transfer learning benchmarks.

{{< /ci-details >}}

{{< ci-details summary="RACE: Large-scale ReAding Comprehension Dataset From Examinations (Guokun Lai et al., 2017)">}}

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, E. Hovy. (2017)  
**RACE: Large-scale ReAding Comprehension Dataset From Examinations**  
EMNLP  
[Paper Link](https://www.semanticscholar.org/paper/636a79420d838eabe4af7fb25d6437de45ab64e8)  
Influential Citation Count (183), SS-ID (636a79420d838eabe4af7fb25d6437de45ab64e8)  

**ABSTRACT**  
We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students’ ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/~glai1/data/race/ and the code is available at https://github.com/qizhex/RACE_AR_baselines.

{{< /ci-details >}}

{{< ci-details summary="Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank (R. Socher et al., 2013)">}}

R. Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. Ng, Christopher Potts. (2013)  
**Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank**  
EMNLP  
[Paper Link](https://www.semanticscholar.org/paper/687bac2d3320083eb4530bf18bb8f8f721477600)  
Influential Citation Count (885), SS-ID (687bac2d3320083eb4530bf18bb8f8f721477600)  

**ABSTRACT**  
Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.

{{< /ci-details >}}

{{< ci-details summary="Skip-Thought Vectors (Ryan Kiros et al., 2015)">}}

Ryan Kiros, Yukun Zhu, R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, S. Fidler. (2015)  
**Skip-Thought Vectors**  
NIPS  
[Paper Link](https://www.semanticscholar.org/paper/6e795c6e9916174ae12349f5dc3f516570c17ce8)  
Influential Citation Count (259), SS-ID (6e795c6e9916174ae12349f5dc3f516570c17ce8)  

**ABSTRACT**  
We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.

{{< /ci-details >}}

{{< ci-details summary="A Simple and Effective Approach to the Story Cloze Test (Siddarth Srinivasan et al., 2018)">}}

Siddarth Srinivasan, Richa Arora, Mark O. Riedl. (2018)  
**A Simple and Effective Approach to the Story Cloze Test**  
NAACL  
[Paper Link](https://www.semanticscholar.org/paper/72c2cc507bc7203bcb4eaf6a3df6e9e8f8514e31)  
Influential Citation Count (2), SS-ID (72c2cc507bc7203bcb4eaf6a3df6e9e8f8514e31)  

**ABSTRACT**  
In the Story Cloze Test, a system is presented with a 4-sentence prompt to a story, and must determine which one of two potential endings is the ‘right’ ending to the story. Previous work has shown that ignoring the training set and training a model on the validation set can achieve high accuracy on this task due to stylistic differences between the story endings in the training set and validation and test sets. Following this approach, we present a simpler fully-neural approach to the Story Cloze Test using skip-thought embeddings of the stories in a feed-forward network that achieves close to state-of-the-art performance on this task without any feature engineering. We also find that considering just the last sentence of the prompt instead of the whole prompt yields higher accuracy with our approach.

{{< /ci-details >}}

{{< ci-details summary="Generating Wikipedia by Summarizing Long Sequences (Peter J. Liu et al., 2018)">}}

Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, Noam M. Shazeer. (2018)  
**Generating Wikipedia by Summarizing Long Sequences**  
ICLR  
[Paper Link](https://www.semanticscholar.org/paper/7570afa31c68e24fce1342b7d67c591787219bc1)  
Influential Citation Count (77), SS-ID (7570afa31c68e24fce1342b7d67c591787219bc1)  

**ABSTRACT**  
We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.

{{< /ci-details >}}

{{< ci-details summary="When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation? (Ye Qi et al., 2018)">}}

Ye Qi, Devendra Singh Sachan, Matthieu Felix, S. Padmanabhan, Graham Neubig. (2018)  
**When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?**  
NAACL  
[Paper Link](https://www.semanticscholar.org/paper/7b29f45df975ed1e4c3864b6ab4483f11086aa76)  
Influential Citation Count (27), SS-ID (7b29f45df975ed1e4c3864b6ab4483f11086aa76)  

**ABSTRACT**  
The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases – providing gains of up to 20 BLEU points in the most favorable setting.

{{< /ci-details >}}

{{< ci-details summary="Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data (Jun Suzuki et al., 2008)">}}

Jun Suzuki, Hideki Isozaki. (2008)  
**Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data**  
ACL  
[Paper Link](https://www.semanticscholar.org/paper/7ece4e8d31f872d928369ac2cf58a616a7182112)  
Influential Citation Count (11), SS-ID (7ece4e8d31f872d928369ac2cf58a616a7182112)  

**ABSTRACT**  
This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition. We first propose a simple yet powerful semi-supervised discriminative model appropriate for handling large scale unlabeled data. Then, we describe experiments performed on widely used test collections, namely, PTB III data, CoNLL’00 and ’03 shared task data for the above three NLP tasks, respectively. We incorporate up to 1G-words (one billion tokens) of unlabeled data, which is the largest amount of unlabeled data ever used for these tasks, to investigate the performance improvement. In addition, our results are superior to the best reported results for all of the above test collections.

{{< /ci-details >}}

{{< ci-details summary="Extracting and composing robust features with denoising autoencoders (Pascal Vincent et al., 2008)">}}

Pascal Vincent, H. Larochelle, Yoshua Bengio, Pierre-Antoine Manzagol. (2008)  
**Extracting and composing robust features with denoising autoencoders**  
ICML '08  
[Paper Link](https://www.semanticscholar.org/paper/843959ffdccf31c6694d135fad07425924f785b1)  
Influential Citation Count (454), SS-ID (843959ffdccf31c6694d135fad07425924f785b1)  

**ABSTRACT**  
Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.

{{< /ci-details >}}

{{< ci-details summary="Unsupervised Pretraining for Sequence to Sequence Learning (Prajit Ramachandran et al., 2016)">}}

Prajit Ramachandran, Peter J. Liu, Quoc V. Le. (2016)  
**Unsupervised Pretraining for Sequence to Sequence Learning**  
EMNLP  
[Paper Link](https://www.semanticscholar.org/paper/85f94d8098322f8130512b4c6c4627548ce4a6cc)  
Influential Citation Count (13), SS-ID (85f94d8098322f8130512b4c6c4627548ce4a6cc)  

**ABSTRACT**  
This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main result is that pretraining improves the generalization of seq2seq models. We achieve state-of-the-art results on the WMT English→German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves a significant improvement of 1.3 BLEU from th previous best models on both WMT’14 and WMT’15 English→German. We also conduct human evaluations on abstractive summarization and find that our method outperforms a purely supervised learning baseline in a statistically significant manner.

{{< /ci-details >}}

{{< ci-details summary="Distributed Representations of Words and Phrases and their Compositionality (Tomas Mikolov et al., 2013)">}}

Tomas Mikolov, Ilya Sutskever, Kai Chen, G. Corrado, J. Dean. (2013)  
**Distributed Representations of Words and Phrases and their Compositionality**  
NIPS  
[Paper Link](https://www.semanticscholar.org/paper/87f40e6f3022adbc1f1905e3e506abad05a9964f)  
Influential Citation Count (3626), SS-ID (87f40e6f3022adbc1f1905e3e506abad05a9964f)  

**ABSTRACT**  
The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.    An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.

{{< /ci-details >}}

{{< ci-details summary="A Fast Learning Algorithm for Deep Belief Nets (Geoffrey E. Hinton et al., 2006)">}}

Geoffrey E. Hinton, Simon Osindero, Y. Teh. (2006)  
**A Fast Learning Algorithm for Deep Belief Nets**  
Neural Computation  
[Paper Link](https://www.semanticscholar.org/paper/8978cf7574ceb35f4c3096be768c7547b28a35d0)  
Influential Citation Count (1144), SS-ID (8978cf7574ceb35f4c3096be768c7547b28a35d0)  

**ABSTRACT**  
We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.

{{< /ci-details >}}

{{< ci-details summary="Semi-Supervised Text Classification Using EM (K. Nigam et al., 2006)">}}

K. Nigam, A. McCallum, Tom Michael Mitchell. (2006)  
**Semi-Supervised Text Classification Using EM**  
Semi-Supervised Learning  
[Paper Link](https://www.semanticscholar.org/paper/8eefd28eb47e72794bb0355d8abcbebaac9d8ab1)  
Influential Citation Count (11), SS-ID (8eefd28eb47e72794bb0355d8abcbebaac9d8ab1)  

**ABSTRACT**  
For several decades, statisticians have advocated using a combination of labeled and unlabeled data to train classifiers by estimating parameters of a generative model through iterative Expectation-Maximization (EM) techniques. This chapter explores the effectiveness of this approach when applied to the domain of text classification. Text documents are represented here with a bag-of-words model, which leads to a generative classification model based on a mixture of multinomials. This model is an extremely simplistic representation of the complexities of written text. This chapter explains and illustrates three key points about semi-supervised learning for text classification with generative models. First, despite the simplistic representation, some text domains have a high positive correlation between generative model probability and classification accuracy. In these domains, a straightforward application of EM with the naive Bayes text model works well. Second, some text domains do not have this correlation. Here we can adopt a more expressive and appropriate generative model that does have a positive correlation. In these domains, semi-supervised learning again improves classification accuracy. Finally, EM suffers from the problem of local maxima, especially in high dimension domains such as text classification. We demonstrate that deterministic annealing, a variant of EM, can help overcome the problem of local maxima and increase classification accuracy further when the generative model is appropriate.

{{< /ci-details >}}

{{< ci-details summary="Quora Question Pairs (Zihang Chen et al., 2017)">}}

Zihang Chen, Hongbo Zhang, Xiaoji Zhang, Leqi Zhao. (2017)  
**Quora Question Pairs**  
  
[Paper Link](https://www.semanticscholar.org/paper/8ff46c88964a36985f2b45933a3d47b81bd87bd0)  
Influential Citation Count (9), SS-ID (8ff46c88964a36985f2b45933a3d47b81bd87bd0)  

**ABSTRACT**  
Quora Question Pairs is an active Kaggle Competition, which challenges participants to tackle the natural language processing (NLP) problem of identifying duplicate questions [1]. The issue of duplicate questions stems from the enormous number of visitors on the Quora website (a platform for asking questions and connecting with people that contribute answers), making it hard to avoid having similar worded questions from different users. Effectively detecting duplicate questions not only saves time for seekers to find the best answer to their questions, but also reduces the effort of writers in terms of answering multiple versions of the same question[1].

{{< /ci-details >}}

{{< ci-details summary="Constituency Parsing with a Self-Attentive Encoder (Nikita Kitaev et al., 2018)">}}

Nikita Kitaev, D. Klein. (2018)  
**Constituency Parsing with a Self-Attentive Encoder**  
ACL  
[Paper Link](https://www.semanticscholar.org/paper/928f9dccb806a3278d20d82cc53781c5f44e2bb1)  
Influential Citation Count (40), SS-ID (928f9dccb806a3278d20d82cc53781c5f44e2bb1)  

**ABSTRACT**  
We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.

{{< /ci-details >}}

{{< ci-details summary="Efficient Learning of Sparse Representations with an Energy-Based Model (Marc'Aurelio Ranzato et al., 2006)">}}

Marc'Aurelio Ranzato, Christopher S. Poultney, S. Chopra, Yann LeCun. (2006)  
**Efficient Learning of Sparse Representations with an Energy-Based Model**  
NIPS  
[Paper Link](https://www.semanticscholar.org/paper/932c2a02d462abd75af018125413b1ceaa1ee3f4)  
Influential Citation Count (55), SS-ID (932c2a02d462abd75af018125413b1ceaa1ee3f4)  

**ABSTRACT**  
We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces "stroke detectors" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.

{{< /ci-details >}}

{{< ci-details summary="GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding (Alex Wang et al., 2018)">}}

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman. (2018)  
**GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding**  
BlackboxNLP@EMNLP  
[Paper Link](https://www.semanticscholar.org/paper/93b8da28d006415866bf48f9a6e06b5242129195)  
Influential Citation Count (672), SS-ID (93b8da28d006415866bf48f9a6e06b5242129195)  

**ABSTRACT**  
Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.

{{< /ci-details >}}

{{< ci-details summary="LSDSem 2017 Shared Task: The Story Cloze Test (N. Mostafazadeh et al., 2017)">}}

N. Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, James F. Allen. (2017)  
**LSDSem 2017 Shared Task: The Story Cloze Test**  
LSDSem@EACL  
[Paper Link](https://www.semanticscholar.org/paper/97394554eb5a74c3160c6bd743fcd3e4bd6cbe28)  
Influential Citation Count (14), SS-ID (97394554eb5a74c3160c6bd743fcd3e4bd6cbe28)  

**ABSTRACT**  
The LSDSem’17 shared task is the Story Cloze Test, a new evaluation for story understanding and script learning. This test provides a system with a four-sentence story and two possible endings, and the system must choose the correct ending to the story. Successful narrative understanding (getting closer to human performance of 100%) requires systems to link various levels of semantics to commonsense knowledge. A total of eight systems participated in the shared task, with a variety of approaches including.

{{< /ci-details >}}

{{< ci-details summary="Why Does Unsupervised Pre-training Help Deep Learning? (P. Glöckner, 2013)">}}

P. Glöckner. (2013)  
**Why Does Unsupervised Pre-training Help Deep Learning?**  
  
[Paper Link](https://www.semanticscholar.org/paper/97474c55c834584b71f006557aed70e09eb6eb47)  
Influential Citation Count (53), SS-ID (97474c55c834584b71f006557aed70e09eb6eb47)  

**ABSTRACT**  


{{< /ci-details >}}

{{< ci-details summary="Layer Normalization (Jimmy Ba et al., 2016)">}}

Jimmy Ba, J. Kiros, Geoffrey E. Hinton. (2016)  
**Layer Normalization**  
ArXiv  
[Paper Link](https://www.semanticscholar.org/paper/97fb4e3d45bb098e27e0071448b6152217bd35a5)  
Influential Citation Count (168), SS-ID (97fb4e3d45bb098e27e0071448b6152217bd35a5)  

**ABSTRACT**  
Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feedforward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.

{{< /ci-details >}}

{{< ci-details summary="Semi-Supervised Learning Literature Survey (Xiaojin Zhu, 2005)">}}

Xiaojin Zhu. (2005)  
**Semi-Supervised Learning Literature Survey**  
  
[Paper Link](https://www.semanticscholar.org/paper/a007f46b3303bdb50e705b441c367e595666538c)  
Influential Citation Count (244), SS-ID (a007f46b3303bdb50e705b441c367e595666538c)  

**ABSTRACT**  


{{< /ci-details >}}

{{< ci-details summary="GPU Kernels for Block-Sparse Weights (Scott Gray et al., 2017)">}}

Scott Gray, Alec Radford, Diederik P. Kingma. (2017)  
**GPU Kernels for Block-Sparse Weights**  
  
[Paper Link](https://www.semanticscholar.org/paper/a07609c2ed39d049d3e59b61408fb600c6ab0950)  
Influential Citation Count (12), SS-ID (a07609c2ed39d049d3e59b61408fb600c6ab0950)  

**ABSTRACT**  
We’re releasing highly optimized GPU kernels for an underexplored class of neural network architectures: networks with block-sparse weights. The kernels allow for efficient evaluation and differentiation of linear layers, including convolutional layers, with flexibly configurable block-sparsity patterns in the weight matrix. We find that depending on the sparsity, these kernels can run orders of magnitude faster than the best available alternatives such as cuBLAS. Using the kernels we improve upon the state-of-the-art in text sentiment analysis and generative modeling of text and images. By releasing our kernels in the open we aim to spur further advancement in model and algorithm design.

{{< /ci-details >}}

{{< ci-details summary="A Fast and Accurate Dependency Parser using Neural Networks (Danqi Chen et al., 2014)">}}

Danqi Chen, Christopher D. Manning. (2014)  
**A Fast and Accurate Dependency Parser using Neural Networks**  
EMNLP  
[Paper Link](https://www.semanticscholar.org/paper/a14045a751f5d8ed387c8630a86a3a2861b90643)  
Influential Citation Count (143), SS-ID (a14045a751f5d8ed387c8630a86a3a2861b90643)  

**ABSTRACT**  
Almost all current dependency parsers classify based on millions of sparse indicator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed significantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based dependency parser. Because this classifier learns and uses just a small number of dense features, it can work very fast, while achieving an about 2% improvement in unlabeled and labeled attachment scores on both English and Chinese datasets. Concretely, our parser is able to parse more than 1000 sentences per second at 92.2% unlabeled attachment score on the English Penn Treebank.

{{< /ci-details >}}

{{< ci-details summary="SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation (Daniel Matthew Cer et al., 2017)">}}

Daniel Matthew Cer, Mona T. Diab, Eneko Agirre, I. Lopez-Gazpio, Lucia Specia. (2017)  
**SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation**  
SemEval@ACL  
[Paper Link](https://www.semanticscholar.org/paper/a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096)  
Influential Citation Count (165), SS-ID (a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096)  

**ABSTRACT**  
Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).

{{< /ci-details >}}

{{< ci-details summary="Adam: A Method for Stochastic Optimization (Diederik P. Kingma et al., 2014)">}}

Diederik P. Kingma, Jimmy Ba. (2014)  
**Adam: A Method for Stochastic Optimization**  
ICLR  
[Paper Link](https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8)  
Influential Citation Count (14937), SS-ID (a6cb366736791bcccc5c8639de5a8f9636bf87e8)  

**ABSTRACT**  
We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.

{{< /ci-details >}}

{{< ci-details summary="Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning (Yacine Jernite et al., 2017)">}}

Yacine Jernite, Samuel R. Bowman, D. Sontag. (2017)  
**Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning**  
ArXiv  
[Paper Link](https://www.semanticscholar.org/paper/a97dc52807d80454e78d255f9fbd7b0fab56bd03)  
Influential Citation Count (9), SS-ID (a97dc52807d80454e78d255f9fbd7b0fab56bd03)  

**ABSTRACT**  
This work presents a novel objective function for the unsupervised training of neural network sentence encoders. It exploits signals from paragraph-level discourse coherence to train these models to understand text. Our objective is purely discriminative, allowing us to train models many times faster than was possible under prior methods, and it yields models which perform well in extrinsic evaluations.

{{< /ci-details >}}

{{< ci-details summary="Semi-supervised Multitask Learning for Sequence Labeling (Marek Rei, 2017)">}}

Marek Rei. (2017)  
**Semi-supervised Multitask Learning for Sequence Labeling**  
ACL  
[Paper Link](https://www.semanticscholar.org/paper/ac17cfa150d802750b46220084d850cfdb64d1c1)  
Influential Citation Count (17), SS-ID (ac17cfa150d802750b46220084d850cfdb64d1c1)  

**ABSTRACT**  
We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data.

{{< /ci-details >}}

{{< ci-details summary="Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning (Sandeep Subramanian et al., 2018)">}}

Sandeep Subramanian, Adam Trischler, Yoshua Bengio, C. Pal. (2018)  
**Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning**  
ICLR  
[Paper Link](https://www.semanticscholar.org/paper/afc2850945a871e72c245818f9bc141bd659b453)  
Influential Citation Count (38), SS-ID (afc2850945a871e72c245818f9bc141bd659b453)  

**ABSTRACT**  
A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. We train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.

{{< /ci-details >}}

{{< ci-details summary="Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge (Altaf Rahman et al., 2012)">}}

Altaf Rahman, Vincent Ng. (2012)  
**Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge**  
EMNLP  
[Paper Link](https://www.semanticscholar.org/paper/b0c5f72790cca220541ea4809d1e43b4bdad1124)  
Influential Citation Count (37), SS-ID (b0c5f72790cca220541ea4809d1e43b4bdad1124)  

**ABSTRACT**  
We examine the task of resolving complex cases of definite pronouns, specifically those for which traditional linguistic constraints on coreference (e.g., Binding Constraints, gender and number agreement) as well as commonly-used resolution heuristics (e.g., string-matching facilities, syntactic salience) are not useful. Being able to solve this task has broader implications in artificial intelligence: a restricted version of it, sometimes referred to as the Winograd Schema Challenge, has been suggested as a conceptually and practically appealing alternative to the Turing Test. We employ a knowledge-rich approach to this task, which yields a pronoun resolver that outperforms state-of-the-art resolvers by nearly 18 points in accuracy on our dataset.

{{< /ci-details >}}

{{< ci-details summary="Natural Language Processing (Almost) from Scratch (Ronan Collobert et al., 2011)">}}

Ronan Collobert, J. Weston, L. Bottou, Michael Karlen, K. Kavukcuoglu, P. Kuksa. (2011)  
**Natural Language Processing (Almost) from Scratch**  
J. Mach. Learn. Res.  
[Paper Link](https://www.semanticscholar.org/paper/bc1022b031dc6c7019696492e8116598097a8c12)  
Influential Citation Count (680), SS-ID (bc1022b031dc6c7019696492e8116598097a8c12)  

**ABSTRACT**  
We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.

{{< /ci-details >}}

{{< ci-details summary="An efficient framework for learning sentence representations (L. Logeswaran et al., 2018)">}}

L. Logeswaran, Honglak Lee. (2018)  
**An efficient framework for learning sentence representations**  
ICLR  
[Paper Link](https://www.semanticscholar.org/paper/bc1d609520290e0460c49b685675eb5a57fa5935)  
Influential Citation Count (29), SS-ID (bc1d609520290e0460c49b685675eb5a57fa5935)  

**ABSTRACT**  
In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. Given a sentence and its context, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.

{{< /ci-details >}}

{{< ci-details summary="Learned in Translation: Contextualized Word Vectors (Bryan McCann et al., 2017)">}}

Bryan McCann, James Bradbury, Caiming Xiong, R. Socher. (2017)  
**Learned in Translation: Contextualized Word Vectors**  
NIPS  
[Paper Link](https://www.semanticscholar.org/paper/bc8fa64625d9189f5801837e7b133e7fe3c581f7)  
Influential Citation Count (65), SS-ID (bc8fa64625d9189f5801837e7b133e7fe3c581f7)  

**ABSTRACT**  
Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.

{{< /ci-details >}}

{{< ci-details summary="ECNU at SemEval-2017 Task 1: Leverage Kernel-based Traditional NLP features and Neural Networks to Build a Universal Model for Multilingual and Cross-lingual Semantic Textual Similarity (Junfeng Tian et al., 2017)">}}

Junfeng Tian, Zhi-Min Zhou, Man Lan, Yuanbin Wu. (2017)  
**ECNU at SemEval-2017 Task 1: Leverage Kernel-based Traditional NLP features and Neural Networks to Build a Universal Model for Multilingual and Cross-lingual Semantic Textual Similarity**  
SemEval@ACL  
[Paper Link](https://www.semanticscholar.org/paper/caa8a41d58e386c56f56d46bbe79df9cb1087338)  
Influential Citation Count (9), SS-ID (caa8a41d58e386c56f56d46bbe79df9cb1087338)  

**ABSTRACT**  
To address semantic similarity on multilingual and cross-lingual sentences, we firstly translate other foreign languages into English, and then feed our monolingual English system with various interactive features. Our system is further supported by combining with deep learning semantic similarity and our best run achieves the mean Pearson correlation 73.16% in primary track.

{{< /ci-details >}}

{{< ci-details summary="SciTaiL: A Textual Entailment Dataset from Science Question Answering (Tushar Khot et al., 2018)">}}

Tushar Khot, Ashish Sabharwal, Peter Clark. (2018)  
**SciTaiL: A Textual Entailment Dataset from Science Question Answering**  
AAAI  
[Paper Link](https://www.semanticscholar.org/paper/cf8c493079702ec420ab4fc9c0fabb56b2a16c84)  
Influential Citation Count (75), SS-ID (cf8c493079702ec420ab4fc9c0fabb56b2a16c84)  

**ABSTRACT**  
We present a new dataset and model for textual entailment, derived from treating multiple-choice question-answering as an entailment problem. SCITAIL is the first entailment set that is created solely from natural sentences that already exist independently “in the wild” rather than sentences authored specifically for the entailment task. Different from existing entailment datasets, we create hypotheses from science questions and the corresponding answer candidates, and premises from relevant web sentences retrieved from a large corpus. These sentences are often linguistically challenging. This, combined with the high lexical similarity of premise and hypothesis for both entailed and non-entailed pairs, makes this new entailment task particularly difficult. The resulting challenge is evidenced by state-of-the-art textual entailment systems achieving mediocre performance on SCITAIL, especially in comparison to a simple majority class baseline. As a step forward, we demonstrate that one can improve accuracy on SCITAIL by 5% using a new neural model that exploits linguistic structure.

{{< /ci-details >}}

{{< ci-details summary="Teaching Machines to Read and Comprehend (K. Hermann et al., 2015)">}}

K. Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, P. Blunsom. (2015)  
**Teaching Machines to Read and Comprehend**  
NIPS  
[Paper Link](https://www.semanticscholar.org/paper/d1505c6123c102e53eb19dff312cb25cea840b72)  
Influential Citation Count (399), SS-ID (d1505c6123c102e53eb19dff312cb25cea840b72)  

**ABSTRACT**  
Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.

{{< /ci-details >}}

{{< ci-details summary="Semi-Supervised Conditional Random Fields for Improved Sequence Segmentation and Labeling (Feng Jiao et al., 2006)">}}

Feng Jiao, Shaojun Wang, Chi-Hoon Lee, R. Greiner, Dale Schuurmans. (2006)  
**Semi-Supervised Conditional Random Fields for Improved Sequence Segmentation and Labeling**  
ACL  
[Paper Link](https://www.semanticscholar.org/paper/d5f5110d65eda0d2df7329582a232a86bf9a3a65)  
Influential Citation Count (21), SS-ID (d5f5110d65eda0d2df7329582a232a86bf9a3a65)  

**ABSTRACT**  
We present a new semi-supervised training procedure for conditional random fields (CRFs) that can be used to train sequence segmentors and labelers from a combination of labeled and unlabeled training data. Our approach is based on extending the minimum entropy regularization framework to the structured prediction case, yielding a training objective that combines unlabeled conditional entropy with labeled conditional likelihood. Although the training objective is no longer concave, it can still be used to improve an initial model (e.g. obtained from supervised training) by iterative ascent. We apply our new training algorithm to the problem of identifying gene and protein mentions in biological texts, and show that incorporating unlabeled data improves the performance of the supervised CRF in this case.

{{< /ci-details >}}

{{< ci-details summary="Multi-range Reasoning for Machine Comprehension (Yi Tay et al., 2018)">}}

Yi Tay, Anh Tuan Luu, S. C. Hui. (2018)  
**Multi-range Reasoning for Machine Comprehension**  
ArXiv  
[Paper Link](https://www.semanticscholar.org/paper/d66ad3628c11c45bde5d4b65b9c1109a95d364d4)  
Influential Citation Count (4), SS-ID (d66ad3628c11c45bde5d4b65b9c1109a95d364d4)  

**ABSTRACT**  
We propose MRU (Multi-Range Reasoning Units), a new fast compositional encoder for machine comprehension (MC). Our proposed MRU encoders are characterized by multi-ranged gating, executing a series of parameterized contract-and-expand layers for learning gating vectors that benefit from long and short-term dependencies. The aims of our approach are as follows: (1) learning representations that are concurrently aware of long and short-term context, (2) modeling relationships between intra-document blocks and (3) fast and efficient sequence encoding. We show that our proposed encoder demonstrates promising results both as a standalone encoder and as well as a complementary building block. We conduct extensive experiments on three challenging MC datasets, namely RACE, SearchQA and NarrativeQA, achieving highly competitive performance on all. On the RACE benchmark, our model outperforms DFN (Dynamic Fusion Networks) by 1.5%-6% without using any recurrent or convolution layers. Similarly, we achieve competitive performance relative to AMANDA on the SearchQA benchmark and BiDAF on the NarrativeQA benchmark without using any LSTM/GRU layers. Finally, incorporating MRU encoders with standard BiLSTM architectures further improves performance, achieving state-of-the-art results.

{{< /ci-details >}}

{{< ci-details summary="The Sixth PASCAL Recognizing Textual Entailment Challenge (L. Bentivogli et al., 2009)">}}

L. Bentivogli, Peter Clark, Ido Dagan, Danilo Giampiccolo. (2009)  
**The Sixth PASCAL Recognizing Textual Entailment Challenge**  
TAC  
[Paper Link](https://www.semanticscholar.org/paper/db8885a0037fe47d973ade79d696586453710233)  
Influential Citation Count (83), SS-ID (db8885a0037fe47d973ade79d696586453710233)  

**ABSTRACT**  
This paper presents the Sixth Recognizing Textual Entailment (RTE-6) challenge. This year a major innovation was introduced, as the traditional Main Task was replaced by a new task, similar to the RTE-5 Search Pilot, in which Textual Entailment is performed on a real corpus in the Update Summarization scenario. A subtask was also proposed, aimed at detecting novel information. To continue the effort of testing RTE in NLP applications, a KBP Validation Pilot Task was set up, in which RTE systems had to validate the output of systems participating in the KBP Slot Filling Task. Eighteen teams participated in the Main Task (48 submitted runs) and 9 in the Novelty Detection Subtask (22 submitted runs). As for the Pilot, 10 runs were submitted by 3 participants. Finally, the exploratory effort started in RTE-5 to perform resource evaluation through ablation tests was not only reiterated in RTE-6, but also extended to tools.

{{< /ci-details >}}

{{< ci-details summary="Unsupervised Machine Translation Using Monolingual Corpora Only (Guillaume Lample et al., 2017)">}}

Guillaume Lample, Ludovic Denoyer, Marc'Aurelio Ranzato. (2017)  
**Unsupervised Machine Translation Using Monolingual Corpora Only**  
ICLR  
[Paper Link](https://www.semanticscholar.org/paper/e3d772986d176057aca2f5e3eb783da53b559134)  
Influential Citation Count (141), SS-ID (e3d772986d176057aca2f5e3eb783da53b559134)  

**ABSTRACT**  
Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.

{{< /ci-details >}}

{{< ci-details summary="Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition (Dong Yu et al., 2010)">}}

Dong Yu, L. Deng, George E. Dahl. (2010)  
**Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition**  
  
[Paper Link](https://www.semanticscholar.org/paper/ecd4bc32bb2717c96f76dd100fcd1255a07bd656)  
Influential Citation Count (3), SS-ID (ecd4bc32bb2717c96f76dd100fcd1255a07bd656)  

**ABSTRACT**  
Recently, deep learning techniques have been successfully applied to automatic speech recognition tasks -first to phonetic recognition with context-independent deep belief network (DBN) hidden Markov models (HMMs) and later to large vocabulary continuous speech recognition using context-dependent (CD) DBN-HMMs. In this paper, we report our most recent experiments designed to understand the roles of the two main phases of the DBN learning -pre-training and fine tuning -in the recognition performance of a CD-DBN-HMM based large-vocabulary speech recognizer. As expected, we show that pre-training can initialize weights to a point in the space where fine-tuning can be effective and thus is crucial in training deep structured models. However, a moderate increase of the amount of unlabeled pre-training data has an insignificant effect on the final recognition results as long as the original training size is sufficiently large to initialize the DBN weights. On the other hand, with additional labeled training data, the fine-tuning phase of DBN training can significantly improve the recognition accuracy.

{{< /ci-details >}}

{{< ci-details summary="Supervised Learning of Universal Sentence Representations from Natural Language Inference Data (A. Conneau et al., 2017)">}}

A. Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, Antoine Bordes. (2017)  
**Supervised Learning of Universal Sentence Representations from Natural Language Inference Data**  
EMNLP  
[Paper Link](https://www.semanticscholar.org/paper/ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c)  
Influential Citation Count (319), SS-ID (ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c)  

**ABSTRACT**  
Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.

{{< /ci-details >}}

{{< ci-details summary="A large annotated corpus for learning natural language inference (Samuel R. Bowman et al., 2015)">}}

Samuel R. Bowman, Gabor Angeli, Christopher Potts, Christopher D. Manning. (2015)  
**A large annotated corpus for learning natural language inference**  
EMNLP  
[Paper Link](https://www.semanticscholar.org/paper/f04df4e20a18358ea2f689b4c129781628ef7fc1)  
Influential Citation Count (665), SS-ID (f04df4e20a18358ea2f689b4c129781628ef7fc1)  

**ABSTRACT**  
Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.

{{< /ci-details >}}

{{< ci-details summary="A Compare-Propagate Architecture with Alignment Factorization for Natural Language Inference (Yi Tay et al., 2017)">}}

Yi Tay, Anh Tuan Luu, S. C. Hui. (2017)  
**A Compare-Propagate Architecture with Alignment Factorization for Natural Language Inference**  
ArXiv  
[Paper Link](https://www.semanticscholar.org/paper/f14dc3dba4a58fd380cce41e44552fd5f7812a4c)  
Influential Citation Count (5), SS-ID (f14dc3dba4a58fd380cce41e44552fd5f7812a4c)  

**ABSTRACT**  
This paper presents a new deep learning architecture for Natural Language Inference (NLI). Firstly, we introduce a new compare-propagate architecture where alignments pairs are compared and then propagated to upper layers for enhanced representation learning. Secondly, we adopt novel factorization layers for efficient compression of alignment vectors into scalar valued features, which are then be used to augment the base word representations. The design of our approach is aimed to be conceptually simple, compact and yet powerful. We conduct experiments on three popular benchmarks, SNLI, MultiNLI and SciTail, achieving state-of-the-art performance on all. A lightweight parameterization of our model enjoys a $\approx 300\%$ reduction in parameter size compared to the ESIM and DIIN, while maintaining competitive performance. Visual analysis shows that our propagated features are highly interpretable, opening new avenues to explainability in neural NLI models.

{{< /ci-details >}}

{{< ci-details summary="GloVe: Global Vectors for Word Representation (Jeffrey Pennington et al., 2014)">}}

Jeffrey Pennington, R. Socher, Christopher D. Manning. (2014)  
**GloVe: Global Vectors for Word Representation**  
EMNLP  
[Paper Link](https://www.semanticscholar.org/paper/f37e1b62a767a307c046404ca96bc140b3e68cb5)  
Influential Citation Count (3515), SS-ID (f37e1b62a767a307c046404ca96bc140b3e68cb5)  

**ABSTRACT**  
Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.

{{< /ci-details >}}

{{< ci-details summary="Distributed Representations of Sentences and Documents (Quoc V. Le et al., 2014)">}}

Quoc V. Le, Tomas Mikolov. (2014)  
**Distributed Representations of Sentences and Documents**  
ICML  
[Paper Link](https://www.semanticscholar.org/paper/f527bcfb09f32e6a4a8afc0b37504941c1ba2cee)  
Influential Citation Count (964), SS-ID (f527bcfb09f32e6a4a8afc0b37504941c1ba2cee)  

**ABSTRACT**  
Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.

{{< /ci-details >}}

